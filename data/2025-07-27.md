<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 31]
- [cs.CL](#cs.CL) [Total: 49]
- [math.OC](#math.OC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.SP](#eess.SP) [Total: 9]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 11]
- [cs.AR](#cs.AR) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 21]
- [cs.RO](#cs.RO) [Total: 16]
- [quant-ph](#quant-ph) [Total: 37]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.SY](#eess.SY) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 本研究提出了一种资源高效的YOLOv5目标检测系统，适用于边缘FPGA，实现了高准确率和低功耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于YOLO的目标检测与分类系统在资源效率方面难以满足边缘FPGA平台部署的挑战。

Method: 提出了一种基于YOLOv5的、为FPGA部署优化的、资源高效的实时目标检测与分类系统，并使用COCO和GTSRD数据集进行了训练。

Result: 实验结果表明，所提出的系统实现了99%的分类准确率、3.5W的功耗和9帧/秒的处理速度。

Conclusion: 本研究提出的基于YOLOv5的资源高效实时目标检测与分类系统在Xilinx Kria KV260 FPGA板上实现了99%的分类准确率，同时功耗为3.5W，处理速度为9帧/秒，证明了其在边缘计算应用中的有效性。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


### [2] [A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation](https://arxiv.org/abs/2507.18323)
*Minje Park,Jeonghwa Lim,Taehyung Yu,Sunghoon Joo*

Main category: cs.CV

TL;DR: 本研究首次对心电图描绘的半监督语义分割进行了基准测试，评估了不同的算法和架构，并提出了针对性优化策略。结果显示Transformer优于卷积网络，该基准测试将推动相关领域的研究。


<details>
  <summary>Details</summary>
Motivation: 心电图描绘（即有意义的心电图波形特征分割）对于临床诊断至关重要。尽管深度学习取得了进展，但公共可用带注释数据集的稀缺性限制了其发展。半监督学习通过利用大量的未标记心电图数据，为解决这一问题提供了一个有前途的解决方案。

Method: 本研究首次对心电图描绘的半监督语义分割（SemiSeg）进行了系统的基准测试。研究人员整理并统一了多个公共数据集，包括先前未充分利用的数据源，以支持进行稳健且多样化的评估。研究人员采用了五种代表性的SemiSeg算法，并将它们应用于两种不同的架构：卷积网络和Transformer。研究人员在两种不同的设置下进行了评估：域内和域外。此外，研究人员还提出了针对心电图的特定训练配置和数据增强策略，并引入了一个标准化的评估框架。

Result: 研究结果表明，在半监督心电图描绘方面，Transformer的性能优于卷积网络。

Conclusion: 该基准测试将为推进半监督心电图描绘方法奠定基础，并促进该领域的进一步研究。

Abstract: Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.

</details>


### [3] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: FishDet-M是最大的鱼类检测基准，包含13个数据集，并对28种模型进行了评估。提出了一种基于CLIP的零样本模型选择框架，以适应不同水下场景。


<details>
  <summary>Details</summary>
Motivation: 解决水下鱼类检测中存在的碎片化数据集、异构成像条件和不一致的评估协议等问题，以促进生态监测、水产养殖自动化和机器人感知等领域的实际应用。

Method: 提出FishDet-M基准，整合13个数据集，采用COCO格式标注；系统性基准测试28种目标检测模型，评估mAP、尺度分析和推理效率；提出基于CLIP的零样本模型选择框架。

Result: FishDet-M是最大的鱼类检测基准；模型在FishDet-M上的表现各异，准确性和效率存在权衡；CLIP模型选择框架实现了高性能的零样本选择，适用于实时应用。

Conclusion: FishDet-M是一个统一的鱼类检测基准，它整合了13个跨越不同水生环境的数据集，并采用COCO格式的标注。该基准对28种当代目标检测模型进行了系统性基准测试，评估了它们在mAP、尺度分析和推理效率方面的表现。此外，研究提出了一种基于CLIP的模型选择框架，能够通过视觉-语言对齐实现零样本的动态模型选择，为自适应部署提供了高效的解决方案。FishDet-M旨在为水下计算机视觉和智能海洋系统领域的研究提供一个标准化、可复现的评估平台，并公开了所有数据集、预训练模型和评估工具。

Abstract: Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [4] [DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition](https://arxiv.org/abs/2507.18444)
*Haiyang Jiang,Songhao Piao,Chao Gao,Lei Yu,Liguo Chen*

Main category: cs.CV

TL;DR: 提出了一种名为 DSFormer 的 Transformer 模型和块聚类策略，用于视觉原地识别，提高了鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决视觉原地识别（VPR）在变化的视觉环境和视角下性能不可靠的挑战。

Method: 提出了一种名为 DSFormer 的新型 Transformer-based cross-learning 模块，并结合了创新的块聚类策略。DSFormer 通过双尺度特征的双向信息交互来增强特征表示，利用自注意力机制捕捉长距离依赖关系，并通过共享交叉注意力机制进行跨尺度学习。块聚类策略则从多个不同视角重新划分了 SF-XL 训练数据集，以优化数据组织，从而提高对视角变化的鲁棒性。

Result: 该框架能够生成适应环境变化的鲁棒全局嵌入，并将训练数据量减少了约 30%。实验表明，该方法在大多数基准数据集上均达到了最先进的性能，并显著提高了计算效率。

Conclusion: 该方法在 SF-XL 数据集上实现了最先进的性能，并且优于 DELG、Patch-NetVLAD、TransVPR 和 R2Former 等高级重排方法，同时显著提高了计算效率。

Abstract: Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.

</details>


### [5] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为HDF的新框架，通过DAM和DSM模块有效解决了动态面部表情识别中因数据异构性导致的性能下降问题，并在多个数据集上取得了优越的识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有的动态面部表情识别（DFER）方法在处理多源数据和个体表情差异引起的样本异构性时，性能会下降。

Method: 提出了一种名为异构感知分布框架（HDF）的新框架，并设计了时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM）两个即插即用模块，以增强时频建模并缓解由难例引起的优化不平衡。DAM通过双分支注意力设计捕获时序一致性和频率鲁棒性，DSM动态平衡分类和对比损失。

Result: 实验证明，HDF相比现有方法在DFEW和FERV39k数据集上取得了更好的性能，提高了WAR和UAR指标，并展现了在多样和不平衡场景下的泛化能力。

Conclusion: HDF框架在DFEW和FERV39k数据集上显著提高了识别准确性和鲁棒性，实现了优越的加权平均召回率（WAR）和未加权平均召回率（UAR），并在多样化和不平衡场景下保持了强大的泛化能力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [6] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本篇论文介绍了一种名为SV3.3B的轻量级视频理解模型，用于高效的体育视频分析。该模型结合了新颖的技术，能在设备端运行，并优于GPT-4o等大型模型，能够生成详细的体育动作描述。


<details>
  <summary>Details</summary>
Motivation: 本篇论文解决了自动体育视频分析的挑战，传统上受限于计算密集型模型，需要服务器端处理，并且缺乏对运动动作的细粒度理解。当前的方法难以捕捉有意义的体育分析所必需的细微生物力学转变，常常遗漏发生在几秒钟内的准备、执行和完成等关键阶段。

Method: SV3.3B模型结合了新颖的时间运动差分采样和自监督学习，用于高效的设备端部署。该方法采用基于DWT-VGG16-LDA的关键帧提取机制，可以从体育序列中智能地识别出16个最具代表性的帧。然后使用通过掩码去噪目标预训练的V-DWT-JEPA2编码器，并对针对体育动作描述生成进行微调的LLM解码器。

Result: SV3.3B在NSVA篮球数据集的子集上进行了评估，在传统的文本生成指标和特定于体育的评估标准上均取得了卓越的性能，在保持显著更低计算需求的同时，在性能上超越了包括GPT-4o变体在内的大型闭源模型。我们的模型在生成技术上详细且分析上丰富的体育描述方面表现出卓越的能力，在地面真实验证指标上比GPT-4o提高了29.2%，并有信息密度、动作复杂性和测量精度指标上的显著改进。

Conclusion: SV3.3B在NSVA篮球数据集的子集上进行了评估，在传统的文本生成指标和特定于体育的评估标准上均取得了卓越的性能，在保持显著更低计算需求的同时，在性能上超越了包括GPT-4o变体在内的大型闭源模型。我们的模型在生成技术上详细且分析上丰富的体育描述方面表现出卓越的能力，在地面真实验证指标上比GPT-4o提高了29.2%，并在信息密度、动作复杂性和测量精度指标上取得了实质性改进，这些对于全面的运动分析至关重要。

Abstract: This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [7] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

TL;DR: Detail++ 通过渐进式细节注入 (PDI) 策略和质心对齐损失来改进文本到图像模型，以处理复杂提示，尤其是在涉及多个对象和属性时。


<details>
  <summary>Details</summary>
Motivation: 该模型在处理涉及多个具有不同属性的主题的复杂提示时面临重大挑战。

Method: Detail++ 是一个无需训练的框架，它引入了一种新颖的渐进式细节注入 (PDI) 策略，将复杂提示分解为一系列简化的子提示，分阶段指导生成过程。该方法利用自注意力机制的内在布局控制能力来确保全局构图，然后通过利用交叉注意力机制进行精确细化，并在测试时引入质心对齐损失来减少绑定噪声和提高属性一致性。

Result: Detail++ 在 T2I-CompBench 和新构建的风格合成基准的广泛实验中，在涉及多个对象和复杂风格条件的情况下，显著优于现有方法。

Conclusion: Detail++ 在涉及多个对象和复杂风格条件的情况下，显著优于现有方法，特别是在这些场景中。

Abstract: Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [8] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 深度学习在皮肤癌筛查方面潜力巨大，但存在偏差。本研究使用 GenAI LightningDiT 模型评估黑色瘤分类器的公平性，发现使用合成数据是一种有前途的方法，但当模型训练数据与合成数据来源不同时，验证会很困难。该研究为使用合成数据提高医学成像 GenAI 系统的公平性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 深度学习及其在边缘计算上的应用在皮肤癌（如黑色瘤）的常规筛查方面具有革命性的潜力。然而，除了预期的好处之外，还存在因意想不到的固有偏差而带来的潜在危险。因此，评估和改进此类系统的公平性至关重要。公平性评估的一个关键挑战是确保评估数据集充分代表不同的个人身份信息（PII）（性别、年龄和种族）以及其他少数群体。

Method: 利用最先进的生成式人工智能(GenAI) LightningDiT模型来评估公开的黑色瘤分类器的公平性。

Result: 结果表明，使用高度逼真的合成数据进行公平性评估是一个有前途的方向。然而，研究结果表明，当用于评估的黑色瘤检测模型的训练数据与支持合成图像的数据集不同时，公平性验证会变得困难。

Conclusion: 使用高度逼真的合成数据进行公平性评估是一个有前途的方向，但当用于评估的黑色瘤检测模型的训练数据与支持合成图像的数据集不同时，公平性验证会变得困难。然而，本研究提出的方法为使用合成数据来衡量和增强医学成像生成式人工智能系统中的公平性提供了一条有价值的新途径。

Abstract: Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [9] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [10] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: ViGText 是一种新的深度伪造检测方法，它结合了图像和视觉大语言模型 (VLLM) 的文本解释，在一个基于图的框架中进行分析。该方法通过多层次特征提取来提高检测的准确性和鲁棒性，尤其在处理定制深度伪造方面表现优越，显著提高了 F1 分数和召回率，并能抵抗定向攻击。


<details>
  <summary>Details</summary>
Motivation: 传统的深度伪造检测方法在处理复杂的、定制的深度伪造方面存在泛化性和鲁棒性方面的不足。本研究提出 ViGText，一种将图像与视觉大语言模型 (VLLM) 文本解释集成在基于图的框架中的新方法，以改进深度伪造检测。

Method: ViGText 将图像划分为块，构建图像和文本图，并使用图神经网络 (GNN) 将它们集成起来进行分析，以识别深度伪造。该方法利用跨空间和频域的多层次特征提取，以提高鲁棒性和准确性。

Result: ViGText 在泛化评估中将平均 F1 分数从 72.45% 提高到 98.32%，在检测用户定制的深度伪造方面表现出色。在鲁棒性方面，与其他深度伪造检测方法相比，ViGText 的召回率提高了 11.1%，并且在面对利用其基于图的架构的定向攻击时，分类性能下降不到 4%。

Conclusion: ViGText 通过详细的视觉和文本分析为检测深度伪造树立了新标准，有助于确保媒体的真实性和信息的完整性。

Abstract: The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.

</details>


### [11] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: TAV数据集通过包含多个场景转换的视频剪辑，提高了AI生成长视频时场景转换的连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频模型在生成具有连贯场景转换的长视频方面存在挑战，因为它们无法从提示中推断何时需要转换。现有模型主要在由单个场景视频剪辑组成的数据集上进行训练，这限制了它们学习和响应需要多个场景的提示的能力。因此，开发场景转换意识对于多场景生成至关重要，它允许模型通过准确检测转换来识别和分割视频。

Method: 提出TAV（Transition-Aware Video）数据集，该数据集由具有多个场景转换的预处理视频剪辑组成。

Result: 实验表明，在TAV数据集上进行训练可以提高模型对基于提示的场景转换的理解能力，缩小了所需场景与生成场景之间的差距，并保持了图像质量。

Conclusion: 通过在TAV数据集上进行训练，可以提高模型对基于提示的场景转换的理解能力，缩小了所需场景与生成场景之间的差距，并保持了图像质量。

Abstract: Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.

</details>


### [12] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为TextSAM-EUS的新型轻量级文本驱动SAM模型，用于内窥镜超声（EUS）图像中的胰腺肿瘤分割。该模型结合了BiomedCLIP文本编码器和LoRA技术，仅调整少量参数即可实现自动分割，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决EUS图像中胰腺肿瘤分割的挑战，如噪声、低对比度和直观性差，以及全监督深度学习模型对大量专家注释数据的依赖性，我们提出了TextSAM-EUS。

Method: 提出了一种新颖的、轻量级的、文本驱动的SAM模型（TextSAM-EUS），并结合了BiomedCLIP文本编码器和基于LoRA的SAM架构调整，实现了EUS中胰腺肿瘤的自动分割，仅调整了0.86%的参数。

Result: TextSAM-EUS在公共内窥镜超声胰腺数据库上，使用自动提示时达到了82.69%的Dice和85.28%的NSD，使用手动提示时达到了83.10%的Dice和85.70%的NSD，优于现有的SOTA监督深度学习模型和基础模型。

Conclusion: TextSAM-EUS是第一个将提示学习集成到基于SAM的医学图像分割中的方法，为EUS的自动分割提供了一种实用、高效且鲁棒的解决方案。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.

</details>


### [13] [Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning](https://arxiv.org/abs/2507.18100)
*Ruizhe Chen,Zhiting Fan,Tianze Luo,Heqing Zou,Zhaopeng Feng,Guiyang Xie,Hansheng Zhang,Zhuochen Wang,Zuozhu Liu,Huaijian Zhang*

Main category: cs.CV

TL;DR: 通过SFT+RL两阶段训练框架，提升视频时序定位模型的准确性和鲁棒性，尤其擅长处理挑战性场景。


<details>
  <summary>Details</summary>
Motivation: 现有视频时序定位（VTG）方法在时序感知和泛化能力方面存在局限性，尤其是在大型视觉语言模型（LVLM）和指令微调的研究进展下，仍需改进。

Method: 提出了一种两阶段训练框架，首先使用高质量的监督微调（SFT）数据进行初始化，然后采用难度可控的强化学习（RL）进行进一步优化，以提升模型的时序定位和推理能力。

Result: 在多个VTG基准测试中，本研究提出的方法表现持续优于现有模型，尤其是在更具挑战性和开放域的场景中。研究还强调了高质量冷启动数据和难度可控RL的重要性。

Conclusion: 本研究提出的两阶段训练框架通过结合监督微调（SFT）和强化学习（RL），有效提升了视频时序定位（VTG）模型的准确性和鲁棒性，特别是在处理具有挑战性的开放域场景时表现优于现有模型。通过对训练策略和数据集构建的深入分析，证明了高质量冷启动数据和难度可控RL的重要性。

Abstract: Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.

</details>


### [14] [Distributional Uncertainty for Out-of-Distribution Detection](https://arxiv.org/abs/2507.18106)
*JinYoung Kim,DaeUng Jo,Kimin Yun,Jeonghyo Song,Youngjoon Yoo*

Main category: cs.CV

TL;DR: 提出了一种新的框架，使用自由能和贝塔分布来改进深度学习中的不确定性估计，以更好地检测OOD样本，并实现了更高效和语义相关的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习不确定性估计方法（如MC Dropout）往往只关注模型或数据不确定性，未能与检测OOD样本的语义目标保持一致。因此，需要一种能够同时考虑分布不确定性并有效检测OOD样本的方法。

Method: 提出了一种名为自由能后验网络（Free-Energy Posterior Network）的新框架，该框架结合了自由能和贝塔分布来估计不确定性，并集成了一个损失函数，可以直接从学习到的参数中估计不确定性，无需随机采样。

Result: 在Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can等数据集上进行了验证，证明了该方法的有效性。

Conclusion: 该方法通过结合自由能和贝塔分布，实现了更精细的估计，并与RPL框架集成，能够学习区分OOD区域，提供了一种语义上更具意义且计算高效的解决方案。

Abstract: Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.

</details>


### [15] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: SynC框架通过重新分配标题到最语义匹配的图像来精炼用于零样本图像描述的合成图像-标题数据集，解决了现有方法不适用于合成数据的问题，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集剪枝技术不适用于T2I模型生成的合成数据，因为这些数据虽然标题格式良好，但图像可能存在语义错位（如缺少对象、属性错误）。

Method: SynC框架通过一对多映射策略，首先为每个标题检索多个相关候选图像，然后应用受周期一致性启发的对齐评分器，通过验证图像检索原始标题的能力来选择最佳图像。

Result: SynC在MS-COCO、Flickr30k和NoCaps等标准数据集上，在多种ZIC模型上持续显著地提高了性能，并在某些情况下达到了最先进的结果。

Conclusion: SynC提供了一种有效的精炼合成数据以增强零样本图像描述（ZIC）的方法。

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


### [16] [Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios](https://arxiv.org/abs/2507.18177)
*Dhruv Jain,Romain Modzelewski,Romain Hérault,Clement Chatelain,Eva Torfeh,Sebastien Thureau*

Main category: cs.CV

TL;DR: Diff-UMamba是一种结合了UNet和mamba机制的新型医学图像分割架构，通过其噪声消除模块（NRM）在数据稀疏的情况下提高了分割准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀疏的情况下，深度学习模型常常会过度拟合噪声和不相关的模式，这限制了它们对未见样本的泛化能力。为了解决医学图像分割中的这些挑战，我们引入了Diff-UMamba。

Method: Diff-UMamba是一种新颖的架构，它将UNet框架与mamba机制相结合，用于模拟长距离依赖关系。其核心是一个噪声消除模块（NRM），它采用信号差分策略来抑制编码器内的噪声或不相关激活，从而鼓励模型过滤掉虚假特征并增强与任务相关的表示，从而提高其对临床有意义区域的关注度。

Result: Diff-UMamba在MSD（肺和胰腺）和AIIB23等多个公共数据集上进行了评估，在多种分割任务中，其性能比基线方法一致提高了1-3%。此外，在NSCLC数据集上，该方法实现了5%的改进。

Conclusion: Diff-UMamba在低数据设置下实现了改进的分割准确性和鲁棒性，在MSD、AIIB23和NSCLC等多个数据集上展示了优于基线方法的性能，并在BraTS-21数据集上通过改变可用训练样本的比例进行了评估。

Abstract: In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.

</details>


### [17] [DepthDark: Robust Monocular Depth Estimation for Low-Light Environments](https://arxiv.org/abs/2507.18243)
*Longjian Zeng,Zunjie Zhu,Rongfeng Lu,Ming Lu,Bolun Zheng,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 提出 DepthDark 模型，通过模拟低光成像和采用低光 PEFT 策略，解决了单目深度估计在低光环境下效果不佳的问题，并在相关数据集上取得了 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 当前单目深度估计基础模型在低光环境下效果不佳，现有模型主要针对典型的日间条件，缺乏专门为低光场景设计的鲁棒基础模型。

Method: 提出了一种新颖的 flare 模拟模块和噪声模拟模块来模拟夜间成像过程，并开发了一种利用光照引导和多尺度特征融合的低光 PEFT 策略。

Result: 通过模拟低光成像过程，生成高质量的低光配对深度数据集，并采用光照引导和多尺度特征融合的低光 PEFT 策略，提升了模型在低光环境下的能力。

Conclusion: DepthDark 在具有挑战性的 nuScenes-Night 和 RobotCar-Night 数据集上实现了最先进的深度估计性能，并使用有限的训练数据和计算资源验证了其有效性。

Abstract: In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.

</details>


### [18] [Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection](https://arxiv.org/abs/2507.18260)
*Junyao Li,Yahao Lu,Xingyuan Guo,Xiaoyu Xian,Tiantian Wang,Yukai Shi*

Main category: cs.CV

TL;DR: 红外小目标检测（ISTD）的性能受数据质量影响很大。本研究提出的高斯不变表示学习方法，通过高斯分组压缩器和两阶段扩散模型，提高了模型在数据稀疏情况下的鲁棒性和检测精度。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测（ISTD）方法依赖大规模、昂贵的手动标注数据，这使得它们在实际应用中表现脆弱。现有方法在数据稀疏场景下的表现边界尚不明确。

Method: 本研究提出高斯不变表示学习，包括高斯分组压缩器（利用高斯采样和压缩进行非均匀量化）和两阶段扩散模型（通过对齐量化信号与真实世界分布进行真实世界重建）。

Result: 通过利用多样化的训练样本，增强了ISTD模型在各种挑战下的鲁棒性。通过对齐量化信号与真实世界分布，显著提高了合成样本的质量和保真度。

Conclusion: 提出的方法在各种数据稀疏场景下都优于最先进的检测方法，有效提高了红外小目标检测模型的鲁棒性。

Abstract: Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.

</details>


### [19] [Improving Bird Classification with Primary Color Additives](https://arxiv.org/abs/2507.18334)
*Ezhini Rasendiran R,Chandresh Kumar Maurya*

Main category: cs.CV

TL;DR: Bird song classification improved by coloring spectrograms with frequency information.


<details>
  <summary>Details</summary>
Motivation: Classifying bird species using song recordings is challenging due to environmental noise, overlapping vocalizations, and missing labels, with existing models struggling with low-SNR or multi-species recordings.

Method: Embedding frequency information into spectrograms using primary color additives to enhance species distinction.

Result: The proposed approach achieved statistically significant gains over models without colorization and surpassed the BirdCLEF 2024 winner, improving F1 by 7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%.

Conclusion: The proposed approach effectively incorporates frequency information via colorization, achieving statistically significant gains over existing models and surpassing the BirdCLEF 2024 winner.

Abstract: We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.

</details>


### [20] [Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols](https://arxiv.org/abs/2507.18457)
*Luo Cheng,Hanwei Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.CV

TL;DR: 为物理激光雷达对抗性攻击提供了一个标准化、设备无关的框架，并在模拟和现实世界中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 物理对抗性物体攻击在激光雷达基础的3D物体检测中至关重要，但现有研究缺乏物理可实现性和可复现性。

Method: 提出一个设备无关的标准化框架，该框架抽象了物理对抗性物体攻击的关键要素，支持多种方法，并提供开源代码和基准测试协议。

Result: 该框架成功地将模拟攻击转移到了物理激光雷达系统，并为影响攻击成功率的因素提供了见解，从而促进了对现实世界激光雷达感知的对抗鲁棒性的理解。

Conclusion: 该研究提出了一个设备无关的标准化框架，用于物理对抗性激光雷达物体攻击，并提供了模拟和真实世界场景的开源代码和基准测试协议。

Abstract: Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.

</details>


### [21] [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](https://arxiv.org/abs/2507.18484)
*Xiao Yang,Lingxuan Wu,Lizhong Wang,Chengyang Ying,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Rein-EAD 是一种主动防御框架，通过与环境互动来提高 3D 对抗性场景下的感知鲁棒性，解决了现有被动防御方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的防御机制（如对抗性训练和净化）主要是被动策略，并且通常依赖于对对抗性策略的预定义假设，这限制了它们在动态 3D 环境中的适应性。

Method: 提出了一种名为 Rein-EAD 的主动防御框架，该框架利用自适应探索和与环境的交互来提高 3D 对抗性背景下的感知鲁棒性。它采用多步目标来平衡即时预测准确性和预测熵最小化，并通过面向不确定性的奖励塑造机制来优化防御策略，从而减少计算开销，无需可微分环境即可实现。

Result: 实验证明 Rein-EAD 有效，可将攻击成功率大幅降低，同时保持标准准确性，并能很好地泛化到未见过的和自适应的攻击。

Conclusion: Rein-EAD 框架在各种任务中有效，可将攻击成功率大幅降低，同时保持标准准确性，并能很好地泛化到未见过的和自适应的攻击。

Abstract: Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.

</details>


### [22] [Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover](https://arxiv.org/abs/2507.18099)
*Naman Srivastava,Joel D Joy,Yash Dixit,Swarup E,Rakshit Ramesh*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.

</details>


### [23] [Explaining How Visual, Textual and Multimodal Encoders Share Concepts](https://arxiv.org/abs/2507.18512)
*Clément Cornet,Romaric Besançon,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 提出了一种新的指标，可以跨模态比较SAE特征，并发现视觉语言模型（VLMs）的视觉特征与文本编码器共享，这可能得益于文本预训练。


<details>
  <summary>Details</summary>
Motivation: 以往对SAE特征的比较仅限于同一模态的模型，缺乏跨模态比较的工具。本研究旨在弥补这一不足。

Method: 提出了一种新的指标，用于量化比较不同模态（视觉、文本、多模态）编码器的SAE特征。同时，提出了量化特征在不同模型类别之间比较共享度的方法。

Result: 通过对21种不同类型和规模的编码器进行研究，发现不同模型之间存在特征共享。特别是，VLMs中特定于视觉的特征与文本编码器共享，证明了文本预训练的重要性。

Conclusion: SAE可以用于提取可解释的特征，并且可以跨模型模态进行比较。研究结果表明，视觉语言模型（VLMs）中特定于视觉的特征与文本编码器共享，这可能归因于文本预训练的影响。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts

</details>


### [24] [VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding](https://arxiv.org/abs/2507.18552)
*Baoyao Yang,Wanyun Li,Dixin Chen,Junxiang Chen,Wenbin Yao,Haifeng Lin*

Main category: cs.CV

TL;DR: VideoMind 是一个包含 103K 视频、音频和分层级文本描述（含意图）的新型全模态数据集，旨在提升视频理解能力，特别是细粒度跨模态对齐和意图识别。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在创建一个用于深度视频内容认知和增强多模态特征表示的视频中心全模态数据集 VideoMind，以解决现有数据集在深度视频理解方面的不足，特别是提供需要跨视频上下文整合的意图表达。

Method: VideoMind 数据集包含 103K 视频样本，配有音频和三层级文本描述（事实、抽象、意图）。意图表达采用思维链（COT）方法生成，并包含主题、地点、时间、事件、动作和意图的标注。使用 3,000 个手动验证样本建立黄金标准基准，并设计混合认知检索实验和多层次检索指标来评估深度视频理解能力。

Result: VideoMind 数据集包含 103K 视频样本，并提供了详尽的标注和黄金标准基准。评估结果表明，该数据集可有效评估和提升模型（如 InternVideo, VAST, UMT-L）在细粒度跨模态对齐和深度视频理解（如情感和意图识别）方面的能力。

Conclusion: VideoMind 是一个视频驱动的全模态数据集，旨在促进深度视频内容认知和多模态特征表示。它包含 103K 视频样本，每个样本都配有音频和分层级的文本描述（事实、抽象和意图）。该数据集的独特之处在于提供了需要跨视频整合上下文的意图表达，这些表达是通过思维链（COT）方法生成的。此外，还提供了包含主题、地点、时间、事件、动作和意图的标注，以及 3,000 个手动验证的样本作为黄金标准基准。通过多层次检索指标评估的混合认知检索实验表明，VideoMind 是细粒度跨模态对齐和情感、意图识别等深度视频理解任务的有力基准。

Abstract: This paper introduces VideoMind, a video-centric omni-modal dataset designed
for deep video content cognition and enhanced multi-modal feature
representation. The dataset comprises 103K video samples (3K reserved for
testing), each paired with audio and systematically detailed textual
descriptions. Specifically, every video and its audio is described across three
hierarchical layers (factual, abstract, and intent), progressing from surface
to depth. It contains over 22 million words, averaging ~225 words per sample.
VideoMind's key distinction from existing datasets is its provision of intent
expressions, which require contextual integration across the entire video and
are not directly observable. These deep-cognitive expressions are generated
using a Chain-of-Thought (COT) approach, prompting the mLLM through
step-by-step reasoning. Each description includes annotations for subject,
place, time, event, action, and intent, supporting downstream recognition
tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually
validated samples for evaluating deep-cognitive video understanding. We design
hybrid-cognitive retrieval experiments, scored by multi-level retrieval
metrics, to appropriately assess deep video comprehension. Evaluation results
for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a
powerful benchmark for fine-grained cross-modal alignment and advances fields
requiring in-depth video understanding, such as emotion and intent recognition.
The data is publicly available on GitHub, HuggingFace, and OpenDataLab,
https://github.com/cdx-cindy/VideoMind.

</details>


### [25] [GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences](https://arxiv.org/abs/2507.18330)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Franck Ballerini,Stephania-Denisa Bocu*

Main category: cs.CV

TL;DR: 该研究提出了一个包含时间追踪和来源航班信息的 contrail 数据集（GVCCS）和一个用于 contrail 分析的深度学习框架，以改进 contrail 监测和气候影响评估。


<details>
  <summary>Details</summary>
Motivation: 现有 contrail 形成和气候影响的物理模型依赖于输入数据的质量和复杂过程的假设，而现有的观测数据集在时间追踪和来源航班归属方面存在不足，无法充分验证和校准这些模型。

Method: 提出了一种统一的深度学习框架，利用全景分割模型同时进行语义分割（识别 contrail 像素）、实例分割（分离单个 contrail）和时间追踪，并构建了一个包含 122 个视频序列（24,228 帧）的 GVCCS 数据集，其中 contrail 被单独标记并随时间追踪，部分 contrail 包含来源航班标识符。

Result: GVCCS 数据集和统一的深度学习框架为 contrail 分析提供了高质量、时间连续的注释和评估基准，有助于提高 contrail 监测的准确性，改进物理模型，并为更精确的气候影响评估奠定基础。

Conclusion: 该研究通过引入包含时间追踪和来源航班信息的新型地面可见相机 contrail (GVCCS) 数据集，并提出一个统一的深度学习框架，为 contrail 的分析提供了高质量、时间解析的注释和模型评估基准，旨在支持改进的 contrail 监测，促进物理模型的校准，从而实现更准确的气候影响理解和评估。

Abstract: Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.

</details>


### [26] [Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows](https://arxiv.org/abs/2507.18405)
*Simin Huo,Ning Li*

Main category: cs.CV

TL;DR: Iwin Transformer 是一种新的视觉 Transformer，它使用交错窗口注意力和深度可分离卷积，无需位置嵌入即可从低分辨率微调到高分辨率，并且比 Swin Transformer 更能有效地进行全局信息交换。


<details>
  <summary>Details</summary>
Motivation: 克服 Swin Transformer 需要连续两个块来近似全局注意力。

Method: 提出了一种新颖的、无需位置嵌入的、可从低分辨率直接微调到高分辨率的、分层视觉 Transformer，通过创新的交错窗口注意力和深度可分离卷积的协作实现。该方法利用注意力连接远程标记，并应用卷积连接相邻标记，从而在单个模块内实现全局信息交换，克服了 Swin Transformer 需要连续两个块来近似全局注意力的限制。

Result: Iwin Transformer 在 ImageNet-1K 上达到了 87.4 的 Top-1 准确率，并在语义分割和视频动作识别等任务中表现出强大的竞争力。其核心组件在类别条件图像生成中也得到了验证。

Conclusion: Iwin Transformer 可以在图像分类（ImageNet-1K 上 87.4 Top-1 准确率）、语义分割和视频动作识别等任务中展现出强大的竞争力。其核心组件也可作为独立模块，无缝替换 Transformer 中的自注意力模块，应用于类别条件图像生成。

Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.

</details>


### [27] [DRWKV: Focusing on Object Edges for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18594)
*Xuecheng Bai,Yuxiang Wang,Boyu Hu,Qinyuan Jie,Chuanzhi Xu,Hongru Xiao,Kechen Li,Vera Chung*

Main category: cs.CV

TL;DR: DRWKV是一种新的低光图像增强模型，通过GER理论、演化WKV注意力和Bi-SAB模块，有效提升了边缘保真度、空间连续性和视觉自然度，并在多项指标上取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 低光图像增强（LLIE）在保持物体边缘连续性和极端光照退化下精细结构细节方面仍然是一个挑战。DRWKV模型旨在解决这一问题。

Method: DRWKV模型集成了全局边缘Retinex（GER）理论，通过演化WKV注意力（一种螺旋扫描机制）捕捉空间边缘连续性并对不规则结构进行建模，并设计了双边谱对齐（Bi-SAB）和定制的MS2-Loss来联合对齐亮度和色度特征。

Result: DRWKV在五个LLIE基准测试中取得了领先的性能，并在低光多目标跟踪任务中提高了下游性能。

Conclusion: DRWKV在PSNR、SSIM和NIQE方面实现了领先性能，同时保持了较低的计算复杂度，并在低光多目标跟踪任务中展现了良好的泛化能力。

Abstract: Low-light image enhancement remains a challenging task, particularly in
preserving object edge continuity and fine structural details under extreme
illumination degradation. In this paper, we propose a novel model, DRWKV
(Detailed Receptance Weighted Key Value), which integrates our proposed Global
Edge Retinex (GER) theory, enabling effective decoupling of illumination and
edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV
Attention, a spiral-scanning mechanism that captures spatial edge continuity
and models irregular structures more effectively. Thirdly, we design the
Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align
luminance and chrominance features, improving visual naturalness and mitigating
artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV
achieves leading performance in PSNR, SSIM, and NIQE while maintaining low
computational complexity. Furthermore, DRWKV enhances downstream performance in
low-light multi-object tracking tasks, validating its generalization
capabilities.

</details>


### [28] [NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning](https://arxiv.org/abs/2507.18429)
*Mahdi Ghafourian,Federico M. Sukno*

Main category: cs.CV

TL;DR: 一种新的深度学习方法，利用非线性流形学习和张量分解，在数据有限的情况下实现头姿估计，并提供实时性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决头姿估计（HPE）在计算机视觉应用中的关键作用，以及在有限的训练数据下进行头姿估计的挑战。

Method: 通过使用 Tucker 分解将每个欧拉角（偏航、俯仰、滚动）拆分到单独的子空间，并以余弦曲线模拟潜在流形的每个维度。

Result: 提出了一种新颖的深度学习方法（NLML-HPE），通过非线性流形学习结合张量分解和前馈神经网络，实现了头姿估计，解决了数据注释不准确和训练数据有限的问题，并实现了实时性能。

Conclusion: 该方法通过非线性流形学习（NLML-HPE）实现了头姿估计，该方法结合了张量分解和前馈神经网络，并将头姿估计视为回归问题，将输入标志点映射到姿势角度的连续表示。

Abstract: Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.

</details>


### [29] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: Scenethesis是一种新的3D软件合成方法，使用ScenethesisLang语言，能够精确满足用户需求和复杂约束，并在视觉评估方面超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有3D软件生成方法在处理复杂空间和语义约束方面存在困难，并且无法修改或控制特定元素。

Method: Scenethesis是一种新颖的、对需求敏感的3D软件合成方法，它在用户需求和生成的3D软件之间保持正式的可追溯性。Scenethesis基于ScenethesisLang，这是一种特定领域的语言，作为一种细粒度的、约束感知的中间表示（IR），用于连接自然语言需求和可执行的3D软件。

Result: Scenethesis准确捕获了超过80%的用户需求，满足了超过90%的硬约束，并且能够同时处理超过100个约束。与现有最先进的方法相比，Scenethesis在BLIP-2视觉评估分数方面提高了42.8%。

Conclusion: Scenethesis在准确捕获用户需求和满足约束条件方面表现出色，并在视觉评估方面优于现有最先进的方法。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [30] [Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models](https://arxiv.org/abs/2507.18534)
*Xingyu Qiu,Mengying Yang,Xinghua Ma,Dong Liang,Yuzhen Li,Fanding Li,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: EDA是一种基于任意噪声的扩散模型，解决了EDM在图像恢复中的局限性，在多种任务中取得了优异的成果。


<details>
  <summary>Details</summary>
Motivation: EDM的固定噪声模式限制了其在图像恢复领域的应用，因为强制注入高斯噪声会破坏退化图像，增加恢复的复杂性。

Method: EDA（Elucidates the Design space of Arbitrary-noise-based diffusion models）通过扩展噪声模式的自由度来解决EDM（EDM elucidates the unified design space of diffusion models）在图像恢复任务中仅限于纯高斯噪声的问题，同时不增加计算开销。

Result: EDA在MRI偏置场校正、CT金属伪影减少和自然图像阴影去除任务中表现出色，尤其是在MRI偏置场校正和自然图像阴影去除方面达到了最先进的性能，并且仅使用5个采样步。

Conclusion: EDA通过在扩散模型中引入任意噪声模式，扩展了噪声模式的自由度，同时保持了EDM的原始模块灵活性，并且在三种典型任务上进行了验证，在MRI偏置场校正和自然图像阴影去除方面取得了最先进的性能。

Abstract: EDM elucidates the unified design space of diffusion models, yet its fixed
noise patterns restricted to pure Gaussian noise, limit advancements in image
restoration. Our study indicates that forcibly injecting Gaussian noise
corrupts the degraded images, overextends the image transformation distance,
and increases restoration complexity. To address this problem, our proposed EDA
Elucidates the Design space of Arbitrary-noise-based diffusion models.
Theoretically, EDA expands the freedom of noise pattern while preserving the
original module flexibility of EDM, with rigorous proof that increased noise
complexity incurs no additional computational overhead during restoration. EDA
is validated on three typical tasks: MRI bias field correction (global smooth
noise), CT metal artifact reduction (global sharp noise), and natural image
shadow removal (local boundary-aware noise). With only 5 sampling steps, EDA
outperforms most task-specific methods and achieves state-of-the-art
performance in bias field correction and shadow removal.

</details>


### [31] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: SIDA 是一种新的零样本领域自适应方法，它使用合成图像而不是文本描述来更好地捕捉风格变化，并通过领域混合和斑块风格迁移模块实现高效适应，在各种场景下都取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本领域自适应方法依赖于 CLIP 的嵌入空间和文本描述来模拟目标域的风格特征，但这些方法难以捕捉复杂的真实世界变化，并且由于其对齐过程而显著增加了自适应时间。因此，本研究探索利用图像数据来获取更细粒度的风格线索，以克服这些限制。

Method: SIDA 提出了一种新颖且高效的零样本领域自适应方法，该方法利用合成图像。首先，创建源域图像，然后应用图像翻译来反映目标域的风格，从而生成合成图像。利用这些合成图像的风格特征作为目标域的代理。基于这些特征，引入了领域混合（Domain Mix）和斑块风格迁移（Patch Style Transfer）模块，以有效地模拟真实世界的变化。其中，领域混合通过融合多种风格来扩展域内表示，斑块风格迁移则为单个斑块分配不同的风格。

Result: SIDA 方法在各种零样本领域自适应场景中取得了最先进的性能，并且在减少整体自适应时间方面表现出高效率。

Conclusion: SIDA 方法在各种零样本领域自适应场景中，尤其是在具有挑战性的领域中，展现了最先进的性能，并且通过显著减少整体自适应时间实现了高效率。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 通过一种新的RL框架Shop-R1，利用自我监督式推理和分层奖励机制，LLM在模拟在线购物人类行为方面的推理能力得到了显著提升，性能提升超65%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在模拟人类行为方面的性能受限于用于生成解释的模型的推理能力。本研究旨在通过一种新的RL框架来增强LLM在在线购物环境中的推理能力，以更准确地模拟真实人类行为。

Method: Shop-R1框架采用两阶段方法：1. 自我监督式推理生成，利用内部模型信号（如logit分布）指导推理过程。2. 分层奖励的动作预测，结合难度感知缩放，以防止奖励破解并实现细粒度的奖励分配，同时评估高级动作类型和细粒度的子动作细节（属性和值）。

Result: 与基线方法相比，Shop-R1在实验中实现了超过65%的相对改进。

Conclusion: Shop-R1通过其新颖的RL框架，在在线购物环境中的人类行为模拟方面，显著提高了LLM的推理能力，相比基线方法取得了超过65%的改进。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [33] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: DG-PRM通过引入奖励树和帕累托支配估计，解决了现有过程奖励模型的泛化性和细粒度监督问题，并在各种任务中取得了显著的性能提升和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRM）主要依赖启发式方法，在跨域泛化方面存在困难。尽管LLM-as-judge已被提出用于提供泛化奖励，但当前研究主要关注反馈结果，忽略了文本中蕴含的有意义的指导。此外，静态和粗粒度的评估标准难以适应复杂的过程监督。

Method: 提出了一种名为动态和可泛化过程奖励建模（DG-PRM）的新方法，该方法包含一个奖励树，用于捕获和存储细粒度的、多维度的奖励标准。DG-PRM动态地选择奖励信号来进行逐步奖励评分。为了处理多方面奖励信号，该方法开创性地采用了帕累托支配估计来识别具有区分性的正负样本对。

Result: DG-PRM在现有基准测试中取得了惊人的性能，显著提高了模型在密集奖励任务中的表现。进一步的分析表明，DG-PRM能够很好地适应分布外场景，展现出卓越的泛化能力。

Conclusion: DG-PRM在现有基准测试中表现出色，显著提高了模型在需要密集奖励的任务中的性能，并且在开箱即用的场景中表现出良好的适应性和出色的泛化能力。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [34] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder 是一个交互式系统，用于检测和减轻自然语言数据库接口（NLIDB）中的认知偏差，通过创新的框架和 LLM 驱动的提示来提高数据分析的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言数据库接口（NLIDB）的普及，需要帮助没有统计分析背景的用户提出无偏见的分析性问题，因为现有研究较少关注分析性问题中的认知偏差。

Method: VeriMinder 系统引入了三项关键创新：1. 针对特定分析背景的上下文语义映射框架，用于识别偏差。2. 一个操作化“难变性”原则的分析框架，指导用户进行系统性数据分析。3. 一个优化的、由 LLM 驱动的系统，通过多候选者、批评者反馈和自我反思的结构化过程生成高质量、特定任务的提示。

Result: VeriMinder 在用户测试中表现出色，用户体验评估和与替代方法的比较评估均证实了其有效性。

Conclusion: VeriMinder 系统通过了用户测试，82.5% 的参与者认为它对分析质量产生了积极影响。与替代方法相比，VeriMinder 在分析的具体性、全面性和准确性方面表现更优，提高了至少 20%。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [35] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 提出了一种新颖高效的端到端自动口语评估系统，使用单个Whisper-small编码器处理多部分口语测试，无需转录，可大规模应用于语言学习系统。该系统在Speak & Improve挑战赛中取得了优异成绩，并且具有良好的数据效率。


<details>
  <summary>Details</summary>
Motivation: 为2025年Speak & Improve挑战赛开发全方位自动口语评估（ASA）系统，旨在提高效率和实用性。

Method: 提出了一种高效的端到端方法，使用单个Whisper-small编码器处理所有四个口语回答，并通过轻量级聚合器结合信息进行最终分数预测。

Result: RMSE为0.384，优于文本基线（0.44）；使用最多1.68亿参数（约Whisper-small的70%）；提出的数据采样策略使模型在仅使用44.8%的说话者数据的情况下达到0.383 RMSE。

Conclusion: 该方法在2025年Speak & Improve挑战赛中表现出色，RMSE为0.384，优于文本基线（0.44）。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [36] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 本研究评估了六种 AI 探测器和 DeepSeek 本身（通过少量示例提示和思维链）识别 DeepSeek 生成文本的能力，并测试了释义和人类化等对抗性攻击的效果。结果显示，QuillBot 和 Copyleaks 对原始和释义文本的识别效果较好，但对人类化攻击敏感。DeepSeek 作为探测器表现出高准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的出现引发了对写作完整性的担忧，并推动了人工智能（AI）检测技术的发展。然而，像标准释义和人类化这样的对抗性攻击会阻碍检测器识别机器生成文本的能力。以往的研究主要集中在 ChatGPT 等知名 LLM 上，但对 DeepSeek 这一新近发布的 LLM 的研究存在空白。因此，本研究旨在探究现有的 AI 检测工具在面对 DeepSeek 生成的文本及其对抗性攻击时的识别能力，并评估 DeepSeek 本身作为探测器的潜力。

Method: 本研究使用 49 对人类创作的问答作为原始数据，并使用 DeepSeek-v3 生成匹配的回答，从而创建了 49 个 AI 生成的样本。随后，通过释义和人类化等对抗性技术，又增加了 196 个样本，以测试探测器的稳健性并评估准确性影响。研究还评估了六种通用 AI 探测工具（AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2 和 GPTZero）识别 DeepSeek 生成文本的能力，并探索了将 DeepSeek 本身作为探测器（通过少量示例提示和思维链推理）的可能性。

Result: QuillBot 和 Copyleaks 在识别原始和释义的 DeepSeek 文本方面表现出近乎完美，而 AI Text Classifier 和 GPT-2 等探测器则表现出不稳定的结果。人类化攻击显著降低了探测器的准确性，使 Copyleaks、QuillBot 和 GPTZero 的准确率分别降至 71%、58% 和 52%。DeepSeek 作为探测器，通过少量示例和思维链提示，在识别 AI 和人类文本方面表现出高准确率，其中五次少量示例提示的最佳结果仅错误分类了一个样本（AI 召回率 96%，人类召回率 100%）。

Conclusion: 虽然 QuillBot 和 Copyleaks 在识别原始和释义的 DeepSeek 文本方面表现出近乎完美，但其他探测器（尤其是 AI Text Classifier 和 GPT-2）的结果却不一致。人类化是最有效的攻击，导致 Copyleaks 的准确率降至 71%，QuillBot 降至 58%，GPTZero 降至 52%。少量示例和思维链提示表现出高准确率，其中最好的五次示例仅错误分类了 49 个样本中的 1 个（AI 召回率 96%，人类召回率 100%）。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [37] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 较大的语言模型在处理上下文证据时，能更一致地运用贝叶斯定理更新其信念。


<details>
  <summary>Details</summary>
Motivation: 研究更大、更强的语言模型在接收到上下文证据时，是否能更一致地根据贝叶斯定理更新其关于命题的“信念”。

Method: 提出了一种贝叶斯相干系数（BCC）度量，并生成了一个用于测量BCC的数据集，用于测量多个预训练语言模型。

Result: 结果表明，更大的、能力更强的预训练语言模型在分配信念时与贝叶斯定理更加一致。

Conclusion: 更大的、能力更强的预训练语言模型在分配信念时与贝叶斯定理更加一致。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [38] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 这项工作对特格里尼亚的自然语言处理 (NLP) 研究进行了全面的调查，分析了 2011 年至 2025 年间的 40 多项研究。该分析揭示了从基于规则的系统到现代神经架构的发展轨迹，并指出了包括形态感知建模、跨语言转移和社区驱动的资源开发在内的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管特格里尼亚有数百万人使用，但在自然语言处理研究中的代表性严重不足。

Method: 对 2011 年至 2025 年间超过 40 项关于特格里尼亚自然语言处理的研究进行了系统回顾，涵盖了形态处理、机器翻译、语音识别和问答等十个不同的下游任务。

Result: 分析显示，从基础的、基于规则的系统到现代的神经架构，其发展轨迹清晰，并且进步一直由资源创建的里程碑所推动。研究确定了源于特格里尼亚的形态复杂性和资源稀缺性的关键挑战，同时强调了有希望的研究方向，包括面向形态的建模、跨语言转移和以社区为中心 的资源开发。

Conclusion: 这项工作为研究人员提供了全面的参考，并为推进特格里尼亚自然语言处理制定了路线图。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [39] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 新一代TeleChat模型（TeleChat2、TeleChat2.5、T1）通过改进训练策略和引入新功能，在性能上超越了前代模型，并在数学、代码生成和推理方面取得了显著进展，部分模型甚至优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 为了提供比前代TeleChat更强大的语言模型，满足在复杂推理、数学和代码生成等任务上的更高性能需求，并促进AI研究和开发。

Method: 研究通过改进预训练和后训练策略来提升模型性能。TeleChat2在10万亿高质量、多样化的tokens上进行预训练，并结合监督微调（SFT）和直接偏好优化（DPO）。TeleChat2.5和T1则增加了持续预训练阶段，使用了特定领域的数据集，并结合了强化学习（RL），以增强代码生成和数学推理能力。T1模型支持长链思维（CoT），而TeleChat2.5则优化了推理速度。

Result: TeleChat2、TeleChat2.5和T1模型系列在性能上取得了显著进步。T1模型在数学和代码生成任务上表现出色，尤其在长链思维方面。TeleChat2.5在推理速度上具有优势。T1-115B模型性能优于OpenAI的o1-mini和GPT-4o。研究团队已公开发布了包含35B和115B参数的TeleChat2、TeleChat2.5和T1模型。

Conclusion: 该研究发布了TeleChat2、TeleChat2.5和T1三个模型系列，它们在性能上相比前代TeleChat有了显著提升。T1系列特别擅长复杂推理、长链思维以及数学和代码生成，而TeleChat2.5则侧重于快速推理。所有模型均采用115B参数的Transformer架构，并且T1-115B在多个基准测试中超越了OpenAI的o1-mini和GPT-4o等专有模型。研究团队公开发布了这些模型，以促进社区的进一步研究和开发。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [40] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: NeuralDB是一种新的LLM编辑框架，通过神经键值数据库和门控检索模块，实现了高效、准确且能保留模型通用能力的大规模事实编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型（LLM）编辑方法在扩展到大量编辑时，可能会损害模型的通用能力，甚至导致遗忘已编辑的事实。本研究旨在解决这一问题，提出一种能够高效编辑知识并保留模型通用能力的方法。

Method: 将现有的线性定位与编辑（L&E）方法建模为对键值（KV）数据库的查询，并在此基础上提出了NeuralDB框架。NeuralDB使用带非线性门控检索模块的神经KV数据库来表示编辑过的事实，该模块仅在推理涉及编辑过的事实时运行，从而有效保留LLM的通用能力。

Result: 在ZsRE和CounterFacts数据集上，使用GPT2-XL、GPT-J（6B）和Llama-3（8B）模型进行了大规模事实编辑实验（10,000个事实）。实验结果表明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面优于现有方法，并能很好地保留模型在六项代表性文本理解和生成任务上的整体性能。此外，NeuralDB在扩展到100,000个事实（比先前工作多50倍）时仍能保持有效性。

Conclusion: NeuralDB通过明确将编辑过的事实表示为带非线性门控检索模块的神经键值（KV）数据库，实现了高效的模型编辑。该框架在编辑效率、泛化性、特异性、流畅性和一致性方面表现优异，并能有效保留LLM的整体性能，即使在处理100,000个事实时也能保持有效性。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [41] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [42] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: LLM可以生成合成数据，用于改善短语断点预测，并为语音领域提供了一种新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 通过利用LLM生成合成短语断点标注，以应对传统方法在数据标注和语音任务中的挑战。

Method: 利用LLM生成合成短语断点标注，并与传统标注进行比较，评估其跨多种语言的有效性。

Result: LLM生成的合成数据在短语断点预测任务中表现有效，为语音领域提供了一种可行的解决方案。

Conclusion: LLM生成的合成数据可有效缓解短语断点预测中的数据挑战，并凸显了LLM作为语音领域可行解决方案的潜力。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [43] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 大型语言模型生成的合成数据在多样性和隐私方面存在不足，提出了一种基于提示的方法来改进合成评论的多样性并保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型（LLMs）生成的合成数据的使用日益增加，在数据驱动的应用中带来了机遇和挑战。虽然合成数据为促进模型训练提供了比真实世界数据更具成本效益、可扩展的替代方案，但其多样性和隐私风险仍有待充分探索。

Method: 提出了一套全面的指标，用于量化评估由多个最先进的大型语言模型生成的合成数据的多样性（语言表达、情感和用户视角）和隐私（重新识别风险和风格外点）。

Result: 评估结果显示，大型语言模型在生成多样化和保护隐私的合成数据方面能力有限。

Conclusion: 实验结果揭示了大型语言模型在生成多样化和保护隐私的合成数据方面存在显著局限性。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [44] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: TELEVAL是一个针对中文口语语言模型的新型动态评估基准，它模拟真实对话场景，评估模型在理解隐式信息和进行自然交互方面的能力，结果显示现有模型仍有待改进。


<details>
  <summary>Details</summary>
Motivation: 现有口语语言模型（SLMs）的评估基准未能充分反映用户在真实世界对话中的自然交互方式，倾向于评估复杂任务处理能力，而非作为对话代理的实际效果。

Method: TELEVAL评估框架通过定义显式语义、副语言与隐式语义、系统能力三个维度，并采用真实世界的对话格式，分别评估文本和音频输出，特别关注模型提取隐式线索并做出恰当响应的能力。

Result: 实验表明，尽管SLMs已有显著进展，但在自然对话任务方面仍有较大提升空间。

Conclusion: TELEVAL旨在为中文口语语言模型提供一个用户中心、贴合实际交互场景的动态评估框架，以期促进更优秀的面向对话的口语语言模型的发展。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [45] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 该研究提出了一种结合BOFT和LoRA-GA的混合参数高效微调（PEFT）方法，并引入了uRNN的酉约束，在减少训练时间和内存消耗方面取得了显著成果，同时保持了与全量微调相当的模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决微调大型语言模型（LLMs）因其规模和内存需求而带来的计算瓶颈问题。

Method: 提出了一种新颖的混合策略，该策略动态集成BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛性。通过计算受梯度范数指导的逐层自适应更新，并首次探索了将酉RNN（uRNN）原理应用于基于Transformer的LLM，通过结构化酉约束增强梯度稳定性。

Result: 在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试中，使用7B到405B参数的模型进行了实证评估。结果表明，该混合方法在收敛效率和泛化能力上始终优于单独的PEFT基线方法，能够接近全量微调的准确性，同时将训练时间减少高达2.1倍，内存使用量减少50%。

Conclusion: 该混合方法通过动态集成BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛性，并结合逐层自适应更新，在GLUE、GSM8K、MT-Bench和HumanEval等基准测试中表现出色，在保证模型性能的同时，显著减少了训练时间和内存消耗，为资源受限的LLM实际部署提供了高效可扩展的解决方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [46] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: New 2024 English GloVe models trained on updated data show improved performance on recent NER tasks and comparable results on others, with better documentation.


<details>
  <summary>Details</summary>
Motivation: The original 2014 GloVe models were not updated to reflect language evolution and lacked careful documentation regarding data versions and preprocessing. This work aims to provide updated models with clear documentation.

Method: Two sets of word embeddings were trained using Wikipedia, Gigaword, and a subset of Dolma. Evaluation involved vocabulary comparison, direct testing, and NER tasks.

Result: The 2024 GloVe vectors include new culturally and linguistically relevant words and perform comparably on analogy and similarity tasks, with improved performance on recent, temporally dependent NER datasets.

Conclusion: The 2024 English GloVe models incorporate new words, perform comparably on structural tasks, and show improved performance on recent, temporally dependent NER datasets, especially non-Western newswire data.

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [47] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: GOAT-SLM 是一个创新的语音语言模型，它能够识别和利用语音中的副语言和说话者特征（如情绪、方言和年龄），而不仅仅是语言内容。它通过新颖的双模态架构和分阶段训练策略实现这一点，并在各种语音任务中表现出色，尤其是在处理情绪和方言方面。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常将语音视为语言内容的载体，忽略了人类语音中蕴含的丰富的副语言和说话者特征线索（例如方言、年龄、情绪和非语音发声）。

Method: GOAT-SLM 采用双模态头部架构，将语言建模与声学实现分离，从而在支持富有表现力和自适应的语音生成的同时，实现强大的语言理解。为了提高模型效率和通用性，我们提出了一种模块化、分阶段的训练策略，该策略利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。

Result: 在 TELEVAL（一个多维度评估基准）上的实验结果表明，GOAT-SLM 在语义和非语义任务上都取得了均衡的性能，并且在处理情绪、方言变异和年龄敏感交互方面优于现有的开源模型。

Conclusion: GOAT-SLM 标志着将语言建模扩展到语言语义之外的重要性，并推动了更自然、更具适应性和更具社会意识的口语系统的发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [48] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本研究通过MCG和MCE评估了MLLM在多模态数学推理中的代码能力，发现现有模型在视觉操作方面与人类存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要集中在纯文本推理输出，而MLLM通过代码执行准确的视觉操作的能力在很大程度上未被探索。

Method: 本研究提出了一种评估框架，包含多模态代码生成（MCG）和多模态代码编辑（MCE）两个关键评估方面。MCG评估模型从头开始准确理解和构建可视化的能力，MCE评估模型进行细粒度操作（包括删除、修改和注释）的能力。该框架采用了一个涵盖五种最常见的数学图形（包括几何图、函数图和三种统计图）的数据集，以全面有效地衡量现有MLLMs的能力。实验评估了九种主流MLLMs。

Result: 实验评估了九种主流MLLMs，结果显示现有模型在执行细粒度视觉操作方面仍显著落后于人类表现。

Conclusion: 现有模型在执行细粒度视觉操作方面仍显著落后于人类表现。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [49] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: LLM在HIV管理方面有潜力，但需要针对性开发和评估。Gemini 2.5 Pro表现最佳，但复杂问题和认知偏见是挑战。


<details>
  <summary>Details</summary>
Motivation: 由于HIV管理的复杂性，LLM在支持临床决策方面具有巨大潜力，但目前在AI在HIV护理中的应用和LLM基准测试方面仍有待探索。本研究旨在评估LLM在HIV管理中的能力，并强调其优势和局限性。

Method: 本研究引入了HIVMedQA基准，包含由传染病医生制定的临床相关问题，并评估了七个通用LLM和三个专业LLM在HIV管理方面的能力。评估框架结合了词汇相似度和LLM-as-a-judge方法，并考虑了问题理解、推理、知识回忆、偏见、潜在危害和事实准确性等关键维度。

Result: Gemini 2.5 Pro在大多数维度上表现优于其他模型，但问题复杂性增加时性能下降。经过医学微调的模型并不总是优于通用模型，并且模型大小与性能之间没有可靠的关联。LLM在推理和理解方面比事实回忆更具挑战性，并且观察到近期效应和现状偏见等认知偏见。

Conclusion: LLM在HIV管理方面的整合需要有针对性的开发和评估，以确保其安全有效地应用于临床。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [50] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 粘性标记会破坏 Transformer 文本嵌入模型的可靠性，本研究提出了 STD 检测方法，发现了 868 个粘性标记，并揭示了它们对下游任务的负面影响，强调了改进分词策略和模型设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管 Transformer 文本嵌入模型被广泛使用，但令人惊讶的‘粘性标记’会破坏嵌入的可靠性，并降低下游任务的性能，因此有必要系统地研究这些异常标记。

Method: 提出了一种名为粘性标记检测器（Sticky Token Detector, STD）的高效检测方法，该方法基于句子和标记过滤。通过将 STD 应用于 14 个模型家族的 40 个检查点，系统地识别了 868 个粘性标记。

Result: 发现了 868 个粘性标记，它们通常来源于词汇表中的特殊或未使用条目，以及多语言语料库中的碎片化子词。粘性标记的存在与模型大小或词汇表大小没有严格相关性。在下游任务（如聚类和检索）中，粘性标记会导致高达 50% 的性能下降。通过注意力层分析表明，粘性标记在模型的内部表示中占有不成比例的主导地位。

Conclusion: 粘性标记（Sticky tokens）的存在会严重影响 Transformer 文本嵌入模型的可靠性，尤其是在聚类和检索等下游任务中，可能导致高达 50% 的性能下降。该研究揭示了粘性标记的来源（如词汇表中的特殊或未使用的条目、多语言语料库中的碎片化子词）及其对模型内部表示的不成比例的影响，强调了在未来的文本嵌入应用中需要更好的分词策略和模型设计来缓解粘性标记的影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [51] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: LLM在多项选择任务中可能存在位置偏见，导致分数虚高。SCOPE是一个评估框架，通过消除这种偏见来提高评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）可以通过利用选项位置或标签的固有偏见来在多项选择任务中获得虚高的分数，而不是展示真正的理解。本研究旨在解决这一问题。

Method: SCOPE通过重复调用不包含语义内容的空提示来估计模型独特的按位置偏见分布，然后根据逆偏见分布重新分配答案槽，从而使随机选择正确答案的概率（幸运率）均等化。此外，它阻止了语义相似的干扰项靠近答案，从而阻止了基于表面邻近线索的近乎正确猜测。

Result: 在多个基准实验中，SCOPE在稳定的性能改进方面始终优于现有的去偏方法，并显示出在正确选项上更清晰的置信度分布。

Conclusion: SCOPE提供了一个新的标准，用于提高LLM评估的公平性和可靠性。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [52] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: AI struggles with Root Cause Analysis in telecom networks because it's complex and lacks good test data.


<details>
  <summary>Details</summary>
Motivation: The motivation is the criticality of Root Cause Analysis (RCA) in telecommunication networks and the challenges AI faces in this domain due to complex, graph-based reasoning and lack of realistic benchmarks.

Method: The abstract does not specify a method.

Result: The abstract does not provide results.

Conclusion: The abstract does not provide a conclusion.

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [53] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 本文介绍了ISO9001和ISO30401标准，并探讨了如何将知识管理系统（KMS）与现有业务流程相结合，特别是通过SECI模型和PDCA循环来实现。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决“ISO30401实施者”在向客户解释ISO30401中描述的知识开发、转化和传递活动如何与现有业务流程相结合时面临的挑战。

Method: 本文通过回顾ISO9001流程建模原则，并结合实际经验，探讨了如何将ISO30401标准的知识管理系统（KMS）与整合管理体系的其他流程相结合。文章提出，可以通过在PDCA循环的各个阶段部署SECI模型的机制来实现KMS的实施。

Result: 本文探讨了符合ISO30401标准的知识管理系统（KMS）如何与整合管理体系中的其他流程相结合，特别是如何通过部署SECI模型的机制在PDCA循环中实施KMS。

Conclusion: 本文回顾了ISO9001背景下的流程建模原则，并基于实践经验，探讨了符合ISO30401标准的知识管理系统（KMS）如何与整合管理体系中的其他流程相结合，特别是通过在PDCA循环的各个阶段部署SECI模型的机制来实现KMS的实施。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [54] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [55] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: Instruction-tuning makes LLMs more likely to believe user-provided misinformation, highlighting a need for better safeguards.


<details>
  <summary>Details</summary>
Motivation: To understand how instruction-tuning affects LLM's receptiveness to misinformation, an area with limited prior research, despite its potential to increase reliance on user input and lead to acceptance of false information.

Method: Investigated the impact of instruction-tuning on LLM susceptibility to misinformation through comparative analysis with base models and exploration of factors like user role, misinformation length, and system prompt warnings.

Result: Instruction-tuned LLMs are significantly more likely to accept misinformation provided by users compared to base models. Factors like user role, misinformation length, and warning presence also influence this susceptibility.

Conclusion: Instruction-tuning, while improving LLM usability, increases susceptibility to misinformation presented by users, shifting reliance from the model to the user. This necessitates mitigation strategies for reliable LLM deployment.

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [56] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: Prune&Comp是一种无需训练的即插即用层剪枝方法，通过幅度补偿解决剪枝导致的性能下降问题，并在LLaMA-3-8B模型上取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 识别并解决移除模型层引起的隐藏状态幅度差异问题，该差异会导致显著的性能下降。

Method: Prune&Comp方案通过估计层移除引起的幅度差异，并离线地重新缩放剩余权重来消除这种差异，实现了零运行时开销。

Result: 在LLaMA-3-8B模型上，当剪枝5层并结合迭代剪枝策略时，Prune&Comp将困惑度降低了近一半，并保留了原始模型93.19%的问答性能，相比基线模型提升了4.01%。

Conclusion: Prune&Comp通过采用新颖的即插即用层剪枝方案，利用幅度补偿以无训练的方式来解决剪枝引起的隐藏状态幅度差异问题，并展示了其在迭代剪枝策略中的优势，在LLaMA-3-8B模型上实现了显著的性能提升。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [57] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了一种新的定位和聚焦方法，通过精确定位包含术语的语音片段并利用相关翻译知识，有效提高了语音翻译中术语翻译的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的语音翻译（ST）研究主要集中于利用各种翻译知识到ST模型，但这些方法在处理无关噪音干扰和充分利用翻译知识方面存在不足。为了解决这些问题，需要一种新的方法来提高术语翻译的准确性。

Method: 提出了一种新颖的定位和聚焦（Locate-and-Focus）方法，用于术语翻译。首先，该方法有效地定位了包含术语的语音片段以构建翻译知识，最大限度地减少了对语音翻译模型的不相关信息干扰。随后，将翻译知识与来自音频和文本模态的语音及其假设相关联，使语音翻译模型能够更好地聚焦于翻译知识。

Result: 实验结果表明，该方法在多个数据集上有效定位了语音中的术语，并提高了术语翻译的成功率，同时保持了稳健的通用翻译性能。

Conclusion: 该方法有效定位了语音中的术语，提高了术语翻译的成功率，同时保持了稳健的通用翻译性能。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [58] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本研究比較了六種OCR引擎在Sinhala和Tamil這兩種低資源語言上的零樣本性能。結果顯示，Surya在Sinhala上表現最佳，Document AI在Tamil上表現最佳。此外，研究還提出了一個新的合成Tamil OCR基準測試數據集。


<details>
  <summary>Details</summary>
Motivation: 雖然印刷文字的光學字元識別（OCR）在拉丁字母及其衍生腳本方面已經取得很大進展，但對於使用獨特腳本的低資源語言（LRL）來說，這仍然是一個未解決的問題。本研究旨在解決低資源語言的OCR難題，特別關注Sinhala和Tamil這兩種語言。

Method: 本研究採用了六種不同的OCR引擎（Cloud Vision API, Surya, Document AI, Tesseract, Subasa OCR, EasyOCR），並在Sinhala和Tamil這兩種低資源語言上對它們的零樣本 OCR 性能進行了比較分析。研究使用了五種不同的評估指標，包括字元錯誤率（CER）和單詞錯誤率（WER），來評估這些引擎的準確性。此外，研究還創建了一個新的合成Tamil OCR基準測試數據集。

Result: 在Sinhala的評估中，Surya在所有指標上表現最好，其單詞錯誤率（WER）為2.61%。在Tamil的評估中，Document AI在所有指標上表現突出，字元錯誤率（CER）低至0.78%。

Conclusion: Surya在Sinhala的OCR表現最佳，而Document AI在Tamil的OCR表現最佳。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [59] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: StyleAdaptedLM通过LoRA适配器将风格迁移到大型语言模型中，实现了风格定制和指令遵循能力的平衡。


<details>
  <summary>Details</summary>
Motivation: 为了在企业沟通中适应特定风格（如品牌声音或作者语气）对大型语言模型（LLMs）进行定制，但现有的方法在缺乏指令-响应格式的语料库上进行训练时，难以在不影响指令遵循能力的情况下实现。

Method: StyleAdaptedLM框架利用低秩适配（LoRA）技术，首先在具有多样化非结构化风格语料库的基础模型上训练LoRA适配器，然后将这些适配器与单独的指令遵循模型合并。

Result: StyleAdaptedLM在风格一致性方面表现出改进，同时保持了指令遵循能力，并且在人工评估中证实了其能够采纳品牌特定的惯例。

Conclusion: StyleAdaptedLM提供了一种在不损害指令遵循能力的情况下，通过LoRA高效地将风格特征迁移到指令遵循模型中的框架，实现了鲁棒的风格定制，并且在多个数据集和模型上进行了验证，人工评估也证实了其在品牌特定惯例上的有效性。

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [60] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 一种针对大型推理模型（LRM）的新型“过度思考后门”攻击被提出，它通过数据中毒和可控的冗长推理链来增加模型的计算负担，同时保持结果的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了探索一种针对大型推理模型（LRM）的新型攻击向量，该向量利用了LRM的核心特性——链式思考（CoT）推理能力，并旨在精确控制其推理过程的冗长性，以实现资源消耗。

Method: 通过数据中毒方法实现，该方法结合了可调触发器（通过重复次数信号控制强度）和相应的冗长推理链（CoT）。通过指示教师LLM在正确的推理过程中注入可控制的冗余细化步骤来生成这些响应。

Result: 实验证明，该方法能够可靠地触发LRM推理过程长度的可控、多倍增加，同时保持最终答案的正确性，验证了其作为纯粹的资源消耗攻击向量的有效性。

Conclusion: 该研究提出了一种新颖的可调后门攻击方法，称为“过度思考后门”，可以精确控制大型推理模型（LRM）的推理冗长性，而不会损害最终答案的正确性。通过数据中毒方法，该攻击利用可调触发器和程序生成的冗长推理链来消耗资源。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [61] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 机器翻译模型在处理性别模糊的词汇时，即使在明确的实例上表现良好，也不一定能保持不确定性，并且消除偏见的效果也因情况而异。


<details>
  <summary>Details</summary>
Motivation: 机器翻译模型在处理源语言中性别不明确但目标语言需要性别区分的词汇时，需要从上下文或外部知识中推断性别。模型可能存在性别偏见，依赖刻板印象。本研究旨在探讨模型在性别模糊情况下的表现，并评估其不确定性。

Method: 使用最近提出的语义不确定性度量来评估机器翻译模型在处理性别模糊实例时的不确定性。

Result: 具有高翻译和性别准确性的模型在性别模糊实例上不一定表现出预期的不确定性水平。消除偏见对性别模糊和性别明确的翻译实例也有独立的影响。

Conclusion: 机器翻译模型在处理源语言中性别不明确但目标语言需要性别区分的词汇时，需要从上下文或外部知识中推断性别。研究表明，机器翻译模型存在性别偏见，即使与上下文信息冲突，也倾向于依赖刻板印象。我们认为，除了在性别明确时准确翻译外，模型在性别模糊时还应保持不确定性。我们使用最近提出的语义不确定性度量，发现那些在性别明确的实例上具有高翻译和性别准确性的模型，在性别模糊的实例上不一定表现出预期的不确定性水平。同样，消除偏见对性别模糊和性别明确的翻译实例也有独立的影响。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [62] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: TDR是一个创新的框架，通过任务解耦和LLM细粒度反馈来改进ICL示例检索，显著提高了30项NLP任务的性能，并且易于集成到现有LLM中。


<details>
  <summary>Details</summary>
Motivation: ICL的有效性在很大程度上依赖于示例的质量。尽管先前的研究通过增强示例检索能力取得了显著的性能，但仍存在两个挑战：区分跨任务数据分布的难度，以及在检索器输出和LLM反馈之间建立细粒度连接的难度。

Method: TDR框架通过解耦不同任务的ICL示例，使检索模块能够从多任务数据集中检索特定于目标任务的示例。此外，TDR还对LLM的细粒度反馈进行建模，以监督和指导检索模块的训练，从而检索高质量的示例。

Result: 在30项NLP任务的广泛实验中，TDR被证明能够跨所有数据集持续改进结果，并实现最先进的性能。

Conclusion: TDR框架通过解耦不同任务的ICL示例并利用LLM的细粒度反馈来优化示例检索，在30项NLP任务的广泛实验中表现出色，一致性地提高了结果，并达到了最先进的性能。该方法是即插即用的，可以轻松地与各种LLM结合，以提高ICL的示例检索能力。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [63] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本研究提出了一种结合人类专家和LLM的宣传检测方法，通过LLM辅助标注提高了效率和一致性，并使用知识蒸馏训练了更小的模型，旨在实现可扩展和鲁棒的宣传检测。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的宣传检测任务复杂且缺乏高质量的标注数据，这使得检测工作具有挑战性。因此，本研究旨在提高宣传检测的标注一致性和可扩展性。

Method: 本研究提出了一种结合人类专家和大型语言模型（LLM）的宣传检测框架。具体方法包括：1. 提出一个分层分类法，将14种细粒度的宣传技术归入三个更广泛的类别。2. 在HQP数据集上进行人类标注研究，发现细粒度标签的一致性较低。3. 实现一个LLM辅助的预标注流程，该流程能提取宣传性文本片段、生成简洁的解释，并分配局部标签和全局标签。4. 进行二次人类验证研究，证明该方法在一致性和时间效率方面有显著提升。5. 通过知识蒸馏，在LLM生成的高质量数据上微调较小的语言模型（SLM），使其能够执行结构化标注。

Result: LLM辅助的预标注流程在二次人类验证研究中显著提高了标注的一致性和时间效率。通过知识蒸馏在LLM生成的数据上微调SLM，实现了可扩展的宣传检测。

Conclusion: 本研究通过结合人类专业知识和大型语言模型（LLM）的辅助，提出了一种改进社交媒体上传播检测的方法。研究通过分层分类法将14种细粒度的宣传技术归纳为三个更广泛的类别，并通过人类标注研究揭示了细粒度标签之间较低的一致性。LLM辅助预标注流程提取宣传性文本片段、生成解释并分配局部和全局标签，并在二次人类验证研究中显示出一致性和时间效率的显著提高。最终，通过知识蒸馏，在高质量的LLM生成数据上微调了较小的语言模型（SLM）以执行结构化标注。本研究为开发可扩展、鲁棒的宣传检测系统做出了贡献，并支持了透明、可问责的媒体生态系统的理念，符合可持续发展目标16。代码已公开于GitHub存储库。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [64] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: CLEAR是一个开源的LLM错误分析包，提供逐实例反馈、错误分类和交互式仪表板，以深入了解模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式通常只提供单一分数或排名，无法解释模型表现优劣的原因。为了解决这个问题，需要一种能够进行详细错误分析的方法。

Method: CLEAR包首先生成逐实例的文本反馈，然后创建系统级错误问题的集合，并量化每个已识别问题的普遍性。其交互式仪表板提供聚合可视化、交互式筛选和向下钻取到示例特定行为模式的单个实例。

Result: CLEAR包展示了其在RAG和数学基准测试上的分析能力，并通过用户案例研究展示了其实用性。

Conclusion: CLEAR包通过生成逐实例的文本反馈、系统级错误问题的集合以及量化每个已识别问题的普遍性，并提供交互式仪表板，实现了LLM的错误分析。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [65] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 维基百科不同语言版本存在事实不一致，影响AI系统可靠性。研究开发方法分析表格数据不一致性，为事实核查和AI系统设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 维基百科不同语言版本之间存在事实不一致性，这会影响其作为人工智能系统主要训练来源的中立性和可靠性。

Method: 本研究开发了一种收集、对齐和分析维基百科多语言文章中的表格数据的方法，并定义了不一致的类别。研究使用了各种定量和定性指标来评估多语言对齐。

Result: 研究揭示了维基百科结构化内容中，特别是表格数据方面，跨语言不一致性的存在。

Conclusion: Wikipedia的多语言版本在事实核查、多语言知识交互以及可靠的人工智能系统设计方面具有重要意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [66] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: FinDPO是一个基于DPO的金融领域大语言模型框架，在情感分析和投资组合策略方面表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）的大语言模型（LLM）可能导致对训练数据的记忆，并且难以泛化到未见过的样本，这在金融领域是一个关键限制，模型必须适应以前未发生的事件和细微的、特定于领域的金融语言。

Method: FinDPO框架是第一个基于训练后人类偏好对齐（通过直接偏好优化 DPO）的特定金融领域的大语言模型框架。

Result: FinDPO在标准情感分类基准上取得了最先进的性能，平均优于现有的监督微调模型11%。此外，FinDPO是第一个能够保持大量正回报（年化67%）和强风险调整后收益（夏普比率为2.0）的情感驱动方法，即使在实际交易成本为5个基点（bps）的情况下也是如此。

Conclusion: FinDPO框架成功地将经过微调的因果语言模型集成到实际的投资组合策略中，通过新颖的“logit-to-score”转换，将离散的情感预测转化为连续、可排名的情感分数（概率）。

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [67] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: AraTable是一个新的阿拉伯语表格数据基准，旨在评估LLM的能力。研究发现LLM在简单任务上表现尚可，但在复杂推理任务上存在不足，并提出了一个自动评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，针对阿拉伯语表格数据的基准评估资源有限，而LLM在理解结构化数据（尤其是表格格式）方面的能力仍有待提高，特别是考虑到阿拉伯语独特的语言特征。

Method: 提出了一种名为AraTable的新型基准，用于评估LLM在阿拉伯语表格数据上的推理和理解能力。该基准包含多种评估任务（如直接问答、事实核查和复杂推理），并采用混合方法，结合LLM生成和人工专家筛选/验证来确保数据质量。此外，还提出了一种使用自我反思机制的全自动评估框架。

Result: 初始分析表明，LLM在直接问答等简单任务上表现尚可，但在需要更深层次推理和事实核查的任务上则面临挑战。全自动评估框架的表现与人工判断接近。

Conclusion: LLMs在处理阿拉伯语表格数据时，尤其是在需要更深层次推理和事实核查的任务上，仍然面临显著的认知挑战。该研究提供了一个宝贵的、公开可用的资源和评估框架，以加速阿拉伯语结构化数据处理和分析基础模型的开发。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [68] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 本文利用 XLM-RoBERTa-large 模型成功地为孟加拉语恢复了标点，在新闻数据集上达到了 97.1% 的准确率，并公开了数据集和代码以供未来研究。


<details>
  <summary>Details</summary>
Motivation: 标点恢复对于提高文本可读性至关重要，并且对于自动语音识别（ASR）的后处理任务，特别是像孟加拉语这样的低资源语言，也起着关键作用。

Method: 本文探索了基于 Transformer 的模型（特别是 XLM-RoBERTa-large）在孟加拉语未标点文本中自动恢复标点的应用，重点预测句号、逗号、问号和感叹号。为了解决带注释资源稀缺的问题，我们构建了一个大型、多样化的训练语料库并应用了数据增强技术。

Result: 研究结果表明，所提出的模型在新闻测试集上达到了 97.1% 的准确率，在参考集上达到了 91.2%，在自动语音识别集上达到了 90.2%。模型在参考和自动语音识别记录中表现出强大的泛化能力，证明了其在现实世界嘈杂场景中的有效性。

Conclusion: 该研究为孟加拉语的标点恢复建立了一个强大的基准，并为低资源自然语言处理的未来研究提供了公开的数据集和代码。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [69] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 这项研究对生成合成医疗文本进行了系统性综述，重点关注其目的、技术和评估方法。研究发现，Transformer架构（尤其是GPT）是主流技术，效用是主要的评估指标。合成数据在增强文本和克服稀疏性方面很有用，但隐私问题仍然存在，需要进一步的人工评估。


<details>
  <summary>Details</summary>
Motivation: 生成临床合成文本是解决临床NLP中常见的稀疏性和隐私等问题的有效解决方案。

Method: 通过对（i）生成目的、（ii）技术和（iii）评估方法三个研究问题进行量化分析，对生成合成医学自由文本进行系统性综述。搜索了PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv数据库中与生成合成医学非结构化自由文本相关的出版物。

Result: 在1398篇收集的文章中，我们确定了94篇相关的文章。从2018年开始，人们对生成合成医疗文本给予了极大的关注，其主要目的是文本增强、辅助写作、语料库构建、隐私保护、注释和实用性。Transformer架构是生成文本的主要技术，尤其是GPT。评估方面包括相似性、隐私、结构和效用，其中效用是评估生成的合成医疗文本最常用的方法。

Conclusion: 尽管生成的合成医疗文本在不同下游NLP任务中表现出作为真实医疗文档的中等可能性，但它在增强、补充真实文档以提高准确性和克服稀疏/欠采样问题方面已被证明是一项巨大的资产。然而，隐私仍然是生成合成医疗文本背后的主要问题，需要更多的人工评估来检查是否存在任何敏感信息。尽管如此，生成合成医疗文本的进展将大大加快工作流程和管道开发的采用，摒弃了耗时的法律数据传输。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [70] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: GraDe通过将稀疏依赖图集成到LLM的注意力机制中，改进了表格数据的生成，在复杂数据集上表现更好。


<details>
  <summary>Details</summary>
Motivation: LLM在表格数据生成方面显示出巨大潜力，但表格数据固有的稀疏特征级依赖性，许多特征交互在结构上并不重要，这与LLM的自注意力机制（会分散注意力到所有特征对）产生了根本性的不匹配，特别是在具有复杂依赖性或语义模糊特征的数据集中，会稀释对关键关系的注意力。

Method: GraDe（Graph-Guided Dependency Learning）是一种新颖的方法，它将稀疏依赖图显式地集成到LLM的注意力机制中。GraDe采用了一个由外部提取的功能依赖引导的轻量级动态图学习模块，优先处理关键特征交互，同时抑制不相关的交互。

Result: GraDe在复杂数据集上的表现优于现有的基于LLM的方法（最高可达12%），并在合成数据质量方面取得了与最先进方法相媲美的结果。

Conclusion: GraDe是一种新颖的、侵入性小且有效的方法，通过将稀疏依赖图集成到LLM的注意力机制中，实现了对LLM进行结构感知表格数据建模的实用解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [71] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 与提示工程相比，针对特定任务的微调在道德推理应用中仍然优越。


<details>
  <summary>Details</summary>
Motivation: 道德基础检测对于分析社会话语和开发符合道德的 AI 系统至关重要，但大型语言模型在专业道德推理方面的表现尚不清楚。

Method: 使用 ROC、PR 和 DET 曲线分析，在 Twitter 和 Reddit 数据集上对最先进的 LLM 和微调的 Transformer 进行了全面的比较。

Result: 研究结果显示，尽管付出了提示工程的努力，LLM 在道德基础检测方面仍然存在显著的性能差距，表现出较高的假阴性率和系统性的漏检，这表明它们在检测道德内容方面不如微调的 Transformer。

Conclusion: 与提示工程相比，针对特定任务的微调在道德推理应用中仍然优越。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [72] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: SRU-NER通过多任务学习和动态损失调整来处理嵌套实体和多数据集，提高了生物医学命名实体识别的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别因生物医学术语的复杂性和注释 across 数据集的不一致性而带来重大挑战。

Method: SRU-NER（基于槽的循环单元命名实体识别）是一种新颖的方法，通过有效的多任务学习策略处理嵌套命名实体并整合多个数据集。SRU-NER通过动态调整损失计算来弥补注释差距，避免惩罚在给定数据集中不存在的实体类型的预测。

Result: SRU-NER通过广泛的实验，包括跨语料库评估和对模型预测的人工评估，在生物医学和通用领域命名实体识别任务中取得了有竞争力的性能，同时提高了跨领域泛化能力。

Conclusion: SRU-NER在生物医学和通用领域命名实体识别任务中取得了有竞争力的性能，并提高了跨领域泛化能力。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [73] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2 是一个高效、通用的信息抽取框架，支持多种任务，性能与 LLM 相当，且易于部署。


<details>
  <summary>Details</summary>
Motivation: 现有信息抽取（IE）解决方案需要针对不同任务的专门模型或依赖计算成本高昂的大型语言模型，因此需要一个更高效、更通用的框架。

Method: GLiNER2 是一个统一的框架，通过直观的基于模式的接口引入多任务组合，并在预训练的 transformer 编码器架构上构建，以支持各种提取和分类任务。

Result: GLiNER2 在提取和分类任务上取得了有竞争力的性能，并且比基于 LLM 的替代方案在部署可访问性方面有了实质性的改进，同时保持了 CPU 效率和紧凑的尺寸。

Conclusion: GLiNER2 在命名实体识别、文本分类和分层结构化数据提取等多种任务中展现出与大型语言模型相当的性能，同时在部署的可访问性方面有显著提升，并且保持了 CPU 效率和紧凑的尺寸。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [74] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: GIIFT是一种新颖的无图像多模态机器翻译框架，通过多模态场景图和图注意力网络，在跨模态知识学习和无图像推理方面取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用视觉信息增强机器翻译方面存在挑战，它们在强制对齐视觉和语言信息时存在模态间隙问题，并且其推理能力仅限于训练时的多模态域。

Method: 提出了一种名为GIIFT（Graph-guided Inductive Image-Free MMT）的框架，该框架采用两阶段方法，利用跨模态图注意力网络适配器在统一的融合空间中学习多模态知识，并通过图引导归纳能力将其泛化到更广泛的无图像翻译领域。该框架的关键在于构建了新颖的多模态场景图，以保留和整合特定模态的信息，并解决了现有方法在利用模态间隙时强制进行刚性视觉-语言对齐以及局限于训练的多模态域内的推理的挑战。

Result: 在Multi30K数据集的英译法和英译德任务上，GIIFT在无图像推理的情况下超越了现有方法，达到了最先进的水平。在WMT基准测试中，GIIFT也显著优于图像不可用翻译基线，证明了其在归纳式图像不可用推理方面的优势。

Conclusion: GIIFT框架在多模态机器翻译中取得了显著的成果，尤其是在图像不可用的推理场景下，它能够超越现有的方法并达到最先进的水平，同时在WMT基准测试中也显著优于图像不可用翻译基线。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [75] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 通过结合6-mer和BPE-600的混合标记策略，提高了DNA语言模型在DNA序列建模方面的性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统k-mer标记在捕捉DNA序列局部结构方面的有效性，但存在标记分布不均和全局序列背景理解有限的挑战，提出混合标记策略。

Method: 提出了一种结合6-mer标记和600次BPE迭代的混合标记策略，以解决传统k-mer标记的不足之处。

Result: 在下一个k-mer预测任务上，基于混合标记词汇的DLM在3-mers（10.78%）、4-mers（10.1%）和5-mers（4.12%）的预测精度上优于NT、DNABERT2和GROVER等先进模型。

Conclusion: 该混合标记策略通过结合6-mer标记和BPE-600，在DNA语言模型（DLM）中取得了显著的性能提升，同时保留了局部序列结构和全局上下文信息，为基因组语言建模和下游DNA序列分析奠定了基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [76] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: WINO 算法通过并行草稿和验证机制，解决了 DLLM 的质量-速度权衡问题，在提高生成速度的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的 DLLM 存在严重的质量-速度权衡问题，因为更快的并行解码会导致性能显著下降。这归因于 DLLM 标准解码的不可逆性，它容易在错误的解码方向上出现极化，并伴随着早期错误的上下文累积。

Method: WINO 采用并行草稿和验证机制，利用模型的双向上下文来验证和重新掩盖可疑的 token 进行优化。

Result: WINO 显著改善了质量-速度权衡。例如，在 GSM8K 数学基准测试中，它将推理速度提高了 6 倍，准确率提高了 2.58%；在 Flickr30K 字幕生成任务中，它实现了 10 倍的加速，同时性能更高。

Conclusion: WINO 是一种训练无关的解码算法，通过并行草稿和验证机制，利用模型的双向上下文来验证和重新掩盖可疑的 token 进行优化，从而解决了 DLLM 的质量-速度权衡问题。在 LLaDA 和 MMaDA 等开源 DLLM 上进行了验证，WINO 显著改善了质量-速度权衡。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [77] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 本研究提出SRAG-MAV框架，通过任务重构、动态检索和多轮投票，在中文仇恨言论识别任务中取得优于GPT-4o和微调Qwen2.5-7B的成果。


<details>
  <summary>Details</summary>
Motivation: 为了解决细粒度中文仇恨言论识别（FGCHSR）问题，并提高模型在相关任务上的性能和稳定性。

Method: 该研究提出了一种新颖的SRAG-MAV框架，该框架将任务重构为三元组提取，利用从训练集中动态检索创建上下文提示，并通过多轮推理和投票来提高输出的稳定性和性能。模型基于Qwen2.5-7B。

Result: SRAG-MAV框架在STATE ToxiCN数据集上取得了26.66的硬分数，48.35的软分数和37.505的平均分数，显著优于基线模型。

Conclusion: 该研究提出的SRAG-MAV框架在CCL25-Eval任务10的细粒度中文仇恨言论识别（FGCHSR）方面取得了显著成果，其在STATE ToxiCN数据集上的平均得分（37.505）超越了GPT-4o（15.63）和微调的Qwen2.5-7B（35.365）。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [78] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出AQuilt框架，使用无标签数据为特定领域构建指令调优数据，相比现有方法计算成本更低，性能更好，泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在专业领域表现不佳，而基于无标签数据的合成方法计算成本高、性能有限且泛化能力不足。为了解决这些挑战，提出AQuilt框架。

Method: AQuilt框架通过结合逻辑和检验（Inspection）来鼓励推理过程和自我检验，以增强模型性能。可定制的任务指令支持为任何任务生成高质量数据。

Result: 构建了一个包含703k个示例的数据集来训练一个强大的数据合成模型，实验结果表明AQuilt在性能上与DeepSeek-V3相当，同时生产成本仅为其17%，并且生成的数据与下游任务具有更高的相关性。

Conclusion: AQuilt框架能够从无标签数据构建特定领域指令调优数据，并且可以处理任何任务，实验证明AQuilt与DeepSeek-V3相当，但生产成本仅为其17%，并且生成的数据与下游任务具有更高的相关性。

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [79] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: TRPrompt框架通过整合文本反馈到提示模型训练中，提升了LLM的推理能力，无需模型参数更新，并在数学推理任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 为了在不更新目标模型参数的情况下，提升大型语言模型（LLMs）的推理能力，并整合现有的基于文本反馈和数值奖励的提示优化方法。

Method: 提出了一种名为TRPrompt的框架，该框架通过直接整合文本反馈到提示模型的训练中，统一了基于文本反馈和基于数值奖励的提示优化方法。该框架无需预先收集数据集，并利用LLM对“好”提示的理解能力，通过文本奖励信号迭代优化提示模型。

Result: TRPrompt框架能够训练出针对特定问题的、高质量的查询特定提示，在GSMHard和MATH两个具有挑战性的数学数据集上取得了最先进的性能。

Conclusion: TRPrompt框架通过将文本反馈直接纳入提示模型训练，统一了基于文本反馈和数值奖励的方法，实现了无需参数更新即可提升LLM推理能力，并在GSMHard和MATH数据集上取得了最先进的查询特定提示。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [80] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: RLCF是一种新的强化学习方法，使用指令特定的清单反馈来训练语言模型，提高了其遵循指令的能力，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统使用固定标准（如“有用性”和“有害性”）的强化学习方法的局限性，提出使用灵活的、特定于指令的标准来拓宽强化学习在引发指令遵循方面的影响。

Method: 提出了一种名为“RLCF”（Reinforcement Learning from Checklist Feedback）的方法，该方法从指令中提取清单，并利用AI和验证程序来评估响应满足清单项的程度，然后将这些分数组合起来计算RL奖励。

Result: RLCF在五个广泛研究的基准测试中，相较于其他对齐方法，在所有基准测试中均提升了性能，包括在FollowBench上硬满意率提高了4个点，在InFoBench上提高了6个点，在Arena-Hard上胜率提高了3个点。

Conclusion: RLCF通过使用灵活的、特定于指令的标准来改进语言模型指令遵循能力，并在五个基准测试中均取得了性能提升，证明了其作为改进语言模型支持多方面需求查询的关键工具的有效性。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [81] [Designing efficient interventions for pre-disease states using control theory](https://arxiv.org/abs/2507.18269)
*Makito Oku*

Main category: math.OC

TL;DR: 为疾病前期治疗提供新的数理框架，通过稀疏控制识别干预点。


<details>
  <summary>Details</summary>
Motivation: 为了在老龄化社会中延长健康预期寿命，有必要在疾病前期预防各种疾病。尽管动力学网络生物标志物理论已被用于疾病前期的检测，但疾病前期治疗的数理框架尚未完善。

Method: 提出了一种名为马尔可夫链稀疏控制（MCSC）的控制理论方法，该方法将马尔可夫链上概率分布的时间演化描述为离散时间线性系统，并通过设计稀疏控制器来识别干预的候选状态。

Result: 通过数值模拟和实际数据分析验证了MCSC的有效性。

Conclusion: 文章提出了基于马尔可夫链稀疏控制（MCSC）的数理框架，通过设计稀疏控制器来识别干预的候选状态，以实现疾病的预防性治疗。

Abstract: To extend healthy life expectancy in an aging society, it is crucial to
prevent various diseases at pre-disease states. Although dynamical network
biomarker theory has been developed for pre-disease detection, mathematical
frameworks for pre-disease treatment have not been well established. Here I
propose a control theory-based approach for pre-disease treatment, named Markov
chain sparse control (MCSC), where time evolution of a probability distribution
on a Markov chain is described as a discrete-time linear system. By designing a
sparse controller, a few candidate states for intervention are identified. The
validity of MCSC is demonstrated using numerical simulations and real-data
analysis.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [82] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 一种新的全零样本文本到视频个性化方法，使用2x2视频网格和Grid-LoRA适配器，无需微调即可实现高质量、时间连贯且身份保持的动态概念编辑和生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在个性化动态概念（如主体特定的外观和运动）时，通常需要每个实例进行微调，这限制了其可扩展性。本研究旨在解决这一问题，提出一种无需测试时优化的全零样本框架。

Method: 本研究提出了一种全零样本框架，用于文本到视频模型中的动态概念个性化。该方法利用结构化的2x2视频网格来空间组织输入和输出对，从而训练轻量级的Grid-LoRA适配器，用于在网格内进行编辑和组合。在推理时，一个专门的Grid Fill模块可以补全部分观测到的布局，生成时间连贯且保持身份一致的输出。整个系统在训练完成后，只需一次前向传播即可运行，无需任何测试时优化，即可泛化到先前未见过的动态概念。

Result: 实验结果表明，该框架能够生成高质量且一致的输出，并且在训练过的概念和编辑场景之外，能够广泛泛化到各种主体。

Conclusion: 该研究提出的全零样本框架在文本到视频模型中实现了动态概念的个性化，无需每次推理都进行微调，从而提高了可扩展性。通过使用结构化的2x2视频网格和轻量级的Grid-LoRA适配器，该方法能够在单个前向传播中生成时间连贯且保持身份一致的输出，并能泛化到未见过的动态概念。

Abstract: Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.

</details>


### [83] [DanceGraph: A Complementary Architecture for Synchronous Dancing Online](https://arxiv.org/abs/2507.18052)
*David Sinclair,Ademyemi Ademola,Babis Koniaris,Kenny Mitchell*

Main category: cs.GR

TL;DR: DanceGraph是一个用于同步在线跳舞的架构，它通过实时带宽高效架构最小化延迟，并通过在线舞蹈修正实现风格化。


<details>
  <summary>Details</summary>
Motivation: 克服网络身体姿势共享的延迟，实现同步在线跳舞，并为节奏性舞蹈提供参数化风格化。

Method: DanceGraph架构，实时带宽高效架构，在线舞蹈修正。

Result: 实现了同步在线跳舞，最小化了延迟，减少了运动预测的时间范围，并提供了一种用于节奏性舞蹈的参数化风格化。

Conclusion: DanceGraph通过实时带宽高效架构克服了网络身体姿势共享的延迟，实现了同步在线跳舞。它还提供了一种交互式方法，用于通过在线舞蹈修正来参数化风格化舞蹈动作以实现节奏性舞蹈。

Abstract: DanceGraph is an architecture for synchronized online dancing overcoming the
latency of networked body pose sharing. We break down this challenge by
developing a real-time bandwidth-efficient architecture to minimize lag and
reduce the timeframe of required motion prediction for synchronization with the
music's rhythm. In addition, we show an interactive method for the
parameterized stylization of dance motions for rhythmic dance using online
dance correctives.

</details>


### [84] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: GeoAvatar通过自适应几何高斯泼溅技术，改进了3D头像生成，特别是在嘴部动画和身份保持方面。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在将高斯应用于不同面部区域几何偏差时存在的挑战，以实现身份保持（重建）和新姿势/表情（动画）之间的平衡。

Method: GeoAvatar框架，采用自适应几何高斯泼溅技术，包括自适应预分配阶段（APS）用于高斯分割和正则化，以及基于嘴部解剖学和动力学的嘴部结构和部件化变形策略，并引入高斯与3DMM人脸之间精确装配的正则化损失。

Result: GeoAvatar在重建和新动画场景中取得了优越的性能，优于现有最先进的方法。

Conclusion: GeoAvatar在重建和新动画场景中优于最先进的方法。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

</details>


### [85] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: PS-GS通过结合高斯泼溅法和逆渲染技术，利用多视角和多光照图像，有效解决了多视角光度立体法的逆渲染问题，提高了重建的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决高效的多视角光度立体法（MVPS）逆渲染的挑战，该方法引入了PS-GS。

Method: 1. 使用高斯泼溅法（Gaussian Splatting）重建标准2D模型作为初始几何。 2. 基于初始模型，通过包含光照计算多层感知器的完整渲染方程进行延迟逆渲染。 3. 在整个优化过程中，使用未校准的光度立体法估计的法线来规范化渲染的法线图。 4. 提出2D高斯射线追踪法（2D Gaussian ray-tracing）用于单方向光照，以优化入射光照。

Result: PS-GS能够高效且联合地估计具有不同方向光（多光）照射下物体的几何、材质和光照。重建后的物体可用于新视角合成、重新照亮以及材质和形状编辑。

Conclusion: PS-GS方法在合成和真实数据集上的实验证明，其在重建准确性和计算效率方面优于现有方法。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.

</details>


### [86] [Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation](https://arxiv.org/abs/2507.18352)
*Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage*

Main category: cs.GR

TL;DR: 本研究通过知识蒸馏和伪标签技术，成功训练出能在设备上实时运行的小型语音驱动面部动画模型，解决了数据稀疏和模型巨大的问题，为实现数字人铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 为了解决高质量语音驱动3D面部动画数据集缺乏的问题，并克服现有方法生成的模型体积大、仅适用于离线推理的限制，本研究旨在探索能在设备上进行实时推理的面部动画模型。

Method: 采用混合知识蒸馏和伪标签技术，利用大型预训练教师模型来训练小型学生模型，这些模型仅包含卷积层和全连接层，无需注意力上下文或循环更新。

Result: 成功将模型内存占用减少到3.4MB，所需未来音频上下文减少到81ms，同时保持了高质量的面部动画效果。

Conclusion: 本文的研究为实现高质量、实时、跨语言的语音驱动面部动画铺平了道路，特别是在游戏开发领域。

Abstract: The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [87] [Mapping Technological Futures: Anticipatory Discourse Through Text Mining](https://arxiv.org/abs/2504.02853)
*Maciej Skorski,Alina Landowska,Krzysztof Rajda*

Main category: cs.SI

TL;DR: 研究分析了400位关键意见领袖在X平台上关于新兴技术的150万条帖子，发现他们通过传播乐观的愿景和影响当前的社会辩论，在塑造围绕技术的公众认知方面发挥着关键作用，尽管某些话题（如气候变化）会引发焦虑。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是理解人工智能（AI）等新兴技术的波动性和不可预测性所产生的重大不确定性，以及这种不确定性如何在社交媒体上被讨论。具体来说，它旨在检查围绕技术未来的预期性论述。

Method: 该研究分析了2021年至2023年间在X平台（前身为Twitter）上发布的150万条帖子，这些帖子来自400位关键意见领袖（KOL）。研究采用了包括BERTopic建模、情感、情绪和态度分析在内的先进文本挖掘技术，以识别和理解围绕人工智能（AI）等新兴技术的预期性论述。

Result: 研究识别了100个反映预期技术驱动的未来的不同主题。发现KOL在构建“现在的未来”（例如AI和物联网的乐观愿景）和影响“未来的现在”（这些预测塑造了当代的社会和地缘政治辩论）方面发挥着双重作用。虽然“机器学习、数据科学和深度学习”等主题中的希望等积极情绪占主导地位，但“气候变化”和“战争、乌克兰和特朗普”等主题会引发焦虑。

Conclusion: 该研究强调了关键意见领袖（KOL）在塑造围绕新兴技术的社会叙事中的关键作用，他们通过提出乐观的愿景并影响当前的社会和地缘政治辩论，充当了想象中的未来与当前现实之间的桥梁。

Abstract: The volatility and unpredictability of emerging technologies, such as
artificial intelligence (AI), generate significant uncertainty, which is widely
discussed on social media. This study examines anticipatory discourse
surrounding technological futures by analysing 1.5 million posts from 400 key
opinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using
advanced text mining techniques, including BERTopic modelling, sentiment,
emotion, and attitude analyses, the research identifies 100 distinct topics
reflecting anticipated tech-driven futures. Our findings emphasize the dual
role of KOLs in framing \textit{present futures} -- optimistic visions of
transformative technologies like AI and IoT -- and influencing \textit{future
presents}, where these projections shape contemporary societal and geopolitical
debates. Positive emotions such as Hope dominate, outweighing Anxiety,
particularly in topics like ``Machine Learning, Data Science, and Deep
Learning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and
Trump People'' elicit \textit{Anxiety}. By framing technologies as solutions to
societal challenges, KOLs act as mediators of societal narratives, bridging
imagined futures and current realities. These insights underscore their pivotal
role in directing public attention with emerging technologies during periods of
heightened uncertainty, advancing our understanding of anticipatory discourse
in technology-mediated contexts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [88] [Time and Frequency Synchronization for Multiuser OTFS in Uplink](https://arxiv.org/abs/2507.17966)
*Mohsen Bayat,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 本论文提出了用于高移动性MU-OTFS系统的时间和频率同步技术，包括改进的导频结构和用于TO/CFO估计的算法，以提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决高移动性场景下上行多用户OTFS（MU-OTFS）系统中精确估计和校正时间偏移（TO）和载波频率偏移（CFO）的需求。TO估计对于在延迟-时间平面上定位用户导引至关重要，而CFO估计则能提高信道估计的准确性。

Method: 首先，论文提出了一种针对现有MU-OTFS系统中多用户导频结构的时间偏移（TO）估计技术，用具有循环前缀的导引（PCP）替换了脉冲导引（IMP），并利用不同的Zadoff-Chu（ZC）序列实现接收端的导引分离。接着，提出了一种频谱有效且实用的导引模式（MU-PCP），其中每个用户在延迟-多普勒平面上的共享导引区域内传输PCP。在接收端，利用滤波器组分离不同用户的信号并估计其TO。此外，论文还提出了一种通过寻找相关函数中的第一个主要峰值来提高TO估计精度的数学阈值范围。在定位接收到的用户导引信号后，提出的CFO估计技术将多维最大似然（ML）搜索问题简化为多个一维搜索问题，并应用第一类切比雪夫多项式基展开模型（CPF-BEM）来有效处理信道的时间变化，以获得所有用户的CFO估计。

Result: 通过提出的TO估计技术和CFO估计技术，实现了高移动性场景下MU-OTFS系统的时间和频率同步，提高了信道估计的准确性。

Conclusion: 该论文提出了一种用于高移动性场景下上行多用户OTFS（MU-OTFS）系统的时频同步技术，包括时间偏移（TO）和载波频率偏移（CFO）的估计与校正。

Abstract: In this paper, we propose time and frequency synchronization techniques for
uplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work
focuses on accurately estimating and correcting timing offsets (TOs) and
carrier frequency offsets (CFOs). Specifically, TO estimation is essential for
locating users' pilots on the delay-time plane, while CFO estimation enhances
channel estimation accuracy. First, we propose a TO estimation technique for an
existing multiuser pilot structure in MU-OTFS. We replace the impulse pilot
(IMP) in this pilot structure with a more practical pilot with a cyclic prefix
(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs
different Zadoff-Chu (ZC) sequences, which enables pilot separation via
correlation at the receiver side. Consequently, we introduce a
correlation-based TO estimation technique for uplink MU-OTFS using this pilot
structure. Next, a spectrally efficient and practical pilot pattern is
proposed, where each user transmits a PCP within a shared pilot region on the
delay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO
estimation technique utilizes a bank of filters to separate different users'
signals and accurately estimate their TOs. Then, we derive a mathematical
threshold range to enhance TO estimation accuracy by finding the first major
peak in the correlation function rather than relying solely on the highest
peak. After locating the received users' pilot signals using one of the
proposed TO estimation techniques, our proposed CFO estimation technique
reduces the multi-dimensional maximum likelihood (ML) search problem into
multiple one-dimensional search problems. In this technique, we apply the
Chebyshev polynomials of the first kind basis expansion model (CPF-BEM) to
effectively handle the time-variations of the channel in obtaining the CFO
estimates for all the users.

</details>


### [89] [Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model](https://arxiv.org/abs/2507.17982)
*Pablo Ramírez-Espinosa,Cleofás Segura-Gómez,Ángel Palomares-Caballero,F. Javier López-Martínez,David Morales-Jiménez*

Main category: eess.SP

TL;DR: This paper presents an analytical model for fluid antenna systems (FASs) using dynamic metasurface antennas (DMAs). It uses circuit theory to model the system, addressing the limitations of previous methods that relied heavily on simulations. The model is validated and provides formulas for performance analysis, showing that DMAs can perform as well as ideal flexible antennas.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of analytical modeling for electronically reconfigurable antennas, which are practical implementations of FASs, due to the limitations of physically moving radiating elements and the frequent need for full-wave simulations or measurements.

Method: Leveraging circuit theory to rewrite the conventional signal model of FASs using admittance matrices that account for the electromagnetic effects of metasurfaces.

Result: A complete analytical model for metasurface-based FASs, specifically DMAs, validated with full-wave simulations, showing good agreement. The model allows for standard performance analysis and provides closed-form expressions for key metrics like the signal covariance matrix.

Conclusion: DMA-based FASs can achieve performance similar to idealized position-flexible antennas, validated by full-wave simulations and providing closed-form expressions for key metrics.

Abstract: Fluid antenna systems (FASs) have become a popular topic in the wireless
community as an effective yet simple means of exploiting spatial diversity. Due
to the limitations of physically moving radiating elements, electronically
reconfigurable antennas are emerging as practical implementations of FASs,
since changing the radiation pattern is functionally equivalent to physically
moving the device. However, electronically reconfigurable antennas pose a
challenge in terms of analytical modeling, often requiring full-wave
simulations or measurements for their characterization; this severely limits
the extraction of theoretical insights useful for system design. Motivated by
these difficulties and the growing interest in FASs, we propose in this paper a
complete analytical model for metasurface-based embodiments of FASs.
Specifically, we advocate for the implementation of the FAS concept through
dynamic metasurface antennas (DMAs), hitherto proposed as array replacements in
multiple-input multiple-output (MIMO) systems. We leverage circuit theory to
rewrite the conventional signal model of FASs in terms of admittance matrices
accounting for the electromagnetic effects inherent to metasurfaces. The model
is validated with full-wave simulations, showing good agreement. We further
illustrate how to apply the model for standard performance analysis, and
provide closed-form expressions for key metrics, including the resulting signal
covariance matrix. Results confirm that practical DMA-based FASs can achieve
similar performance to that of idealized implementations of position-flexible
antennas.

</details>


### [90] [Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming](https://arxiv.org/abs/2507.18035)
*Hyeonho Noh,Hyeonsu Lyu,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 该论文提出了一种利用多主动STAR-RIS网络的ISAC网络，并通过联合优化BS波束成器和STAR-RIS系数来最大化通信速率，同时满足传感和安全约束。


<details>
  <summary>Details</summary>
Motivation: 为了最大化总和速率，同时满足传感SINR要求、保密性约束以及BS和STAR-RIS的硬件和总功率约束，对BS发送波束成器和STAR-RIS的反射/透射系数进行了联合优化。

Method: 首先，通过使用KKT条件以及SCA（生成半定规划，然后通过半定松弛求解）来更新BS波束成器和STAR-RIS反射/透射向量，将原始的混合整数非凸问题分解为子问题，从而提出了一种有效的AO框架来解决该问题。

Result: 仿真结果表明，该算法在满足传感和安全约束的同时，显著提高了通信速率。

Conclusion: 与无源RIS和单STAR-RIS基线相比，所提出的算法在满足规定的传感和安全约束的同时，带来了可观的总速率增益。

Abstract: This paper explores an integrated sensing and communication (ISAC) network
empowered by multiple active simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes
downlink communication to multiple users while concurrently interrogating a
sensing target. We jointly optimize the BS transmit beamformer and the
reflection/transmission coefficients of every active STAR-RIS in order to
maximize the aggregate communication sum-rate, subject to (i) a stringent
sensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an
upper bound on the leakage of confidential information, and (iii) individual
hardware and total power constraints at both the BS and the STAR-RISs. The
resulting highly non-convex program is tackled with an efficient alternating
optimization (AO) framework. First, the original formulation is reformulated
into an equivalent yet more tractable representation and partitioned into
subproblems. The BS beamformer is updated in closed form via the
Karush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and
transmission vectors are refined through successive convex approximation (SCA),
yielding a semidefinite program that is then solved via semidefinite
relaxation. Comprehensive simulations demonstrate that the proposed algorithm
delivers substantial sum-rate gains over passive-RIS and single STAR-RIS
baselines, all the while rigorously meeting the prescribed sensing and security
constraints.

</details>


### [91] [Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation](https://arxiv.org/abs/2507.18096)
*Jihong Huang,Rong Yang,Wei Gao,Xingqun Zhan,Zheng Yao*

Main category: eess.SP

TL;DR: 本研究为DPE的多路径误差提供了理论表征和量化方法，并通过SCMB模型和仿真验证了其有效性，为优化DPE性能提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有DPE理论缺乏对多路径误差的理论表征，本研究旨在解决这一问题，通过几何分析量化多路径误差，并提出SCMB模型来改善DPE的鲁棒性。

Method: 本研究首先基于几何观测，对DPE误差（由多路径和热噪声引起）进行了区分，将其分别视为估计偏差和方差。然后，在DPE噪声方差的理论框架基础上，通过几何分析量化了多路径误差对CAF和PVT解的影响。最后，引入了卫星圆多路径偏差（SCMB）模型，该模型整合了多通道的CAF和PVT误差，并讨论了在不同多路径条件下PVT偏差的最大或最小值边界。

Result: 研究结果表明，最大PVT偏差受最大多路径误差影响，且随卫星高度角增加而增加，这为DPE卫星选择提供了几何参考。

Conclusion: 本研究通过几何分析对DPE的测距多路径误差进行了量化，并引入了卫星圆多路径偏差（SCMB）模型，通过蒙特卡洛模拟和城市峡谷测试验证了该模型的正确性。研究结果表明，最大PVT偏差取决于最大多路径误差，并且随着卫星高度角的增加而增加，这可为从几何角度选择DPE卫星提供参考，强调了选择高低角度组合以优化卫星几何构型的重要性。

Abstract: Direct Position Estimation (DPE) is a method that directly estimate position,
velocity, and time (PVT) information from cross ambiguity function (CAF) of the
GNSS signals, significantly enhancing receiver robustness in urban
environments. However, there is still a lack of theoretical characterization on
multipath errors in the context of DPE theory. Geometric observations highlight
the unique characteristics of DPE errors stemming from multipath and thermal
noise as estimation bias and variance respectively. Expanding upon the
theoretical framework of DPE noise variance through geometric analysis, this
paper focuses on a geometric representation of multipath errors by quantifying
the deviations in CAF and PVT solutions caused by off-centering bias relative
to the azimuth and elevation angles. A satellite circular multipath bias (SCMB)
model is introduced, amalgamating CAF and PVT errors from multiple satellite
channels. The boundaries for maximum or minimum PVT bias are established
through discussions encompassing various multipath conditions. The correctness
of the multipath geometrical portrait is confirmed through both Monte Carlo
simulations and urban canyon tests. The findings indicate that the maximum PVT
bias depends on the largest multipath errors observed across various satellite
channels. Additionally, the PVT bias increases with satellite elevation angles,
influenced by the CAF multipath bias projection. This serves as a reference for
selecting DPE satellites from a geometric standpoint, underscoring the
importance of choosing a balanced combination of high and low elevation angles
to achieve an optimal satellite geometry configuration.

</details>


### [92] [Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems](https://arxiv.org/abs/2507.18149)
*Dongdong Zou,Wei Wang,Jiawen Yao,Zhongxing Tian,Zeyu Feng,Huan Huang,Fan Li,Gordon Ning Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 本研究提出了一种新颖的概率整形方案，用于解决具有内存效应的峰值功率受限直接检测系统中的挑战，并在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决概率整形技术在具有内存效应的峰值功率受限强度调制直接检测系统中的应用难题，并减轻内存引起的损伤。

Method: 提出了一种新颖的间接概率整形方案，并结合了发送端的动态选择性映射（DSLM）和接收端的改进M-BCJR算法的Turbo均衡器。

Result: 在56GBaud PAM8系统中，与传统的概率整形方案相比，提出的方案在2km单模光纤传输中实现了1dB的接收灵敏度提升，并兼容传统的概率整形架构，实现了简单的细粒度速率自适应能力。

Conclusion: 该研究为具有内存效应的峰值功率受限强度调制直接检测系统中的概率整形技术开辟了新的应用前景。

Abstract: Probabilistic shaping (PS) has attracted significant attention in
intensity-modulation and direct-detection (IM-DD) systems. However, due to the
unique system model and inherent constraints, the effective application of the
PS technique is still an open question in IM-DD systems, particularly in
systems with memory effects. In this paper, a novel indirect PS scheme tailored
for peak power constrained (PPC) IM-DD systems is proposed. The key idea lies
in strategically controlling the signal envelope to mitigate memory-induced
impairments, such as nonlinearity, overshoot, peak-to-average power ratio
enhancement, etc. The proposed scheme incorporates a dynamic selective mapping
(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol
mapping in which the current symbol is not only determined by the current bits
pattern but also by previously generated symbols within a specified memory
length. At the receiver side, a turbo equalizer with a modified M-BCJR
algorithm is proposed to achieve the recovery of ambiguous bits induced by
DSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the
proposed scheme exhibits 1dB receiver sensitivity improvement over 2km
single-mode fiber transmission. In addition, the proposed scheme has also been
demonstrated to be compatible with the typical probabilistic amplitude shaping
architecture, enabling a simple and fine-granularity rate adaptation
capability. To the best of our knowledge, this work opens a new sight for the
application of the PS technique in PPC IM-DD systems with memory effects.

</details>


### [93] [GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing](https://arxiv.org/abs/2507.18166)
*Jonas Elmiger,Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: SCHIEBER是一种用于多天线GNSS接收器的新方法，可以抵抗干扰和欺骗攻击，无需任何先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有的全球导航卫星系统（GNSS）接收器易受干扰和欺骗攻击，因为其接收功率低，容易受到恶意发射器的干扰和模仿。因此，需要一种能够抵御这些攻击的新方法。

Method: SCHIEBER方法在信号采集期间使用自适应空间滤波技术来抑制干扰，并在信号采集后通过比较信号的到达方向（DoA）和伪距估计来识别和拒绝欺骗信号。该方法利用了信号到达方向和伪距估计之间与未知接收器位置无关的一致性来检测欺骗信号。

Result: 通过对GPS L1 C/A系统在欺骗和干扰攻击下的广泛模拟，证明了SCHIEBER方法的有效性。

Conclusion: SCHIEBER是一种新颖的多天线GNSS接收器方法，可以在不知道接收器位置或攻击类型的情况下，通过自适应空间滤波技术在信号采集过程中抑制干扰，并通过比较信号到达方向（DoA）和伪距估计来识别和拒绝欺骗信号，从而有效抵御干扰和欺骗攻击。

Abstract: Modern positioning relies on radio signals from global navigation satellite
systems (GNSS). Their low receive power renders these radio signals susceptible
to jamming attacks, in which malicious transmitters emit strong interference to
disrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,
in which malicious transmitters mimic legitimate satellites by transmitting
spurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna
GNSS receivers that mitigates jammers as well as spoofers without requiring any
prior knowledge of the receiver position or attack type: Jammers are mitigated
during signal acquisition using a recently developed adaptive spatial filtering
technique. Spoofers are identified and rejected after signal acquisition using
a novel approach that tests the consistency of acquired signals by comparing
their respective direction of arrival (DoA) and pseudorange estimates in a test
that is invariant with respect to the unknown receiver position. We demonstrate
the efficacy of our method using extensive simulations of a GPS L1 C/A system
under spoofing and jamming attacks.

</details>


### [94] [ICWLM: A Multi-Task Wireless Large Model via In-Context Learning](https://arxiv.org/abs/2507.18167)
*Yuxuan Wen,Xiaoming Chen,Maojun Zhang,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: ICWLM是一个新颖的无线通信情境大模型，它从头开始训练，并使用情境学习（ICL）和动态权重平均（DWA）来解决物理层中的多个任务，例如预编码和信道预测。它在未见的配置上表现出良好的泛化能力，并有可能简化未来无线网络的部署。


<details>
  <summary>Details</summary>
Motivation: 深度学习（DL）方法虽然在提高物理层性能方面取得了显著进展，但通常是特定于任务的，并且在数据稀缺和泛化方面存在挑战。为了应对这些挑战，需要一种新的方法。

Method: 提出了一种新颖的无线通信情境大模型（ICWLM），这是一种原生于无线领域的模型，用于物理层的同步多任务学习。与将无线数据适配到预训练的大型语言模型（LLM）的传统方法不同，ICWLM直接从大规模混合无线数据集开始训练。它联合解决了多个经典的物理层问题，包括多用户预编码（总速率最大化和最大最小信干噪比）和信道预测。ICWLM的一个关键创新是利用了情境学习（ICL），使模型能够以最少Demonstration对适应变化的系统配置和信道条件，无需进行广泛的重新训练。此外，我们采用了动态权重平均（DWA）算法来动态平衡多任务训练期间的个体任务损失，确保跨不同目标的有效和稳定的学习。

Result: 大量的仿真结果表明，ICWLM在与特定任务的方法相比时，达到了有竞争力的性能，同时对未见的系统配置表现出卓越的泛化能力。

Conclusion: ICWLM提供了一个有前途的范式，用于开发统一的、自适应的未来无线网络AI模型，有潜力降低部署复杂性并增强智能资源管理。

Abstract: The rapid evolution of wireless communication technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (LLMs), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.

</details>


### [95] [Quantized Signal Recovery with Interference via Parametrized Look-Up Tables](https://arxiv.org/abs/2507.18370)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 利用参数化查找表（LUT）和三种解析估计算法，高效校正低分辨率ADC，并有效抑制干扰和失真，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了实现高效的全数字低分辨率模数转换器（ADC）后校正，研究者们采用了查找表（LUT）的方法。为了优化LUT的性能，需要结合输入信号、噪声水平和干扰信号的参数化模型。

Method: 本研究提出了一种基于参数化查找表（LUT）的数字后校正方法，用于高效校正低分辨率模数转换器（ADC）。研究中评估了三种解析估计算法与参数化LUT的结合应用，特别针对低分辨率、非线性或宽带量化器。此外，还提出了一些近似方法以提高输入信号为相移键控（PSK）和线性调频（LFM）干扰信号时的估计问题的可处理性。

Result: 仿真结果表明，所提出的估计算法能够实时、高精度地恢复期望输入信号的瞬时值，包括对由高功率带外干扰引起的前端饱和而混叠到期望信号带宽内的谐波失真进行抵消。与传统的线性滤波技术相比，该估计算法实现了显著的性能增益，并且对输入参数变化、非线性量化器和时变干扰源具有鲁棒性。具体而言，在3位量化和固定12抽头的模型阶数下，均方误差（MSE）改善了超过10 dB，无杂散动态范围（SFDR）改善了超过20 dBc。

Conclusion: 该研究提出的估计算法在低分辨率、非线性或宽带量化器应用中表现出优越性能，能够实时准确地恢复输入信号的瞬时值，并有效抑制由前端饱和引起的谐波失真。与传统线性滤波技术相比，该算法在均方误差和无杂散动态范围方面均有显著提升，且对输入参数、非线性量化器和时变干扰源具有良好的鲁棒性。

Abstract: Efficient all-digital post-correction of low-resolution analog-to-digital
converters can be achieved by using Look-Up Tables (LUTs). The performance of a
LUT can be optimized by incorporating a parametric model for the expected input
signal, noise level, and interference signals. We evaluate three analytical
estimators for integration with parametrized LUTs, especially with applications
to low-resolution, non-linear, or wideband quantizers. We also propose several
approximations to improve tractability of the estimation problem for
Phase-Shift Keyed input signals and Linear Frequency Modulated interference
signals. Simulated results validate the ability of our estimator to recover the
instantaneous value of the desired input signal in real-time with a high degree
of accuracy. This includes cancellation of harmonic distortion that aliases
into the desired signal bandwidth from front-end saturation due to high-power
out-of-band interference. Our estimators are shown to achieve a significant
gain over conventional linear-filtering techniques while also being robust to
changes in input parameters, non-linear quantizers, and time-variant
interference sources. For a tone input quantized to 3 bits and estimated with a
fixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error
and $>$20 dBc improvement in Spurious-Free Dynamic Range.

</details>


### [96] [A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff](https://arxiv.org/abs/2507.18587)
*Jérôme Emery,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau*

Main category: eess.SP

TL;DR: 通过基于 Transformer 的基础模型和数据增强技术，解决了 mMIMO 预编码的数据稀疏和训练复杂性问题，实现了高性能和低复杂度，并支持动态用户速率配置。


<details>
  <summary>Details</summary>
Motivation: 解决在 mMIMO 系统中部署基于深度学习的预编码解决方案所面临的数据收集困难（需要高质量、本地化数据集）和训练复杂性问题。

Method: 提出了一种基于 Transformer 的基础模型用于 mMIMO 预编码，旨在最小化发射机能耗并动态适应用户速率需求。同时，引入了一种数据增强方法，通过计算预训练特征提取器输出之间的余弦相似度来寻找与目标分布相似的训练样本。

Result: 在能耗相同的情况下，提出的基础模型在零样本部署时性能显著优于迫零（Zero Forcing）方法，并且以 8 倍的复杂度降低逼近了加权最小均方误差（Weighted Minimum Mean Squared Error）的性能。数据增强方法解决了数据稀疏场景下的模型适应性问题。

Conclusion: 该研究通过提出一种基于 Transformer 的基础模型解决了大规模 MIMO（mMIMO）预编码中的数据可用性和训练复杂性挑战，实现了在能耗相同的情况下，零样本部署性能显著优于迫零方法，并以更低的复杂度接近加权最小均方误差性能。此外，通过引入数据增强方法，解决了数据稀疏场景下的模型适应性问题。

Abstract: Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [97] [Quantum Geometric Injection and Shift Optical Forces Drive Coherent Phonons](https://arxiv.org/abs/2507.17814)
*J. Luke Pimlott,Habib Rostami*

Main category: cond-mat.mes-hall

TL;DR: 该研究发现了两种新的晶格振动驱动机制（注入力和移位力），它们可以被调控以实现对量子材料的超快相干操控。


<details>
  <summary>Details</summary>
Motivation: 为了探究驱动晶格振动并触发瞬态涌现特性的声子机制。

Method: 该研究使用双层Haldane模型，通过解析和数值方法量化了作用于层间剪切声子的注入力和移位力。

Result: 研究揭示了通过改变驱动频率和磁通量，可以实现对整流力的大小和方向的强大调控，发现了通过量子几何机制超快相干操控量子材料的独特方法。

Conclusion: 该研究揭示了注入力和移位力这两种新的晶格振动驱动机制，它们是光电流效应的声子对应物，并与量子几何张量、声子移位矢量和带间电子-声子耦合不对称性有关。

Abstract: We identify {\em injection} and {\em shift} rectified Raman forces, which are
phononic counterparts of the photogalvanic effect, that drive lattice
vibrations and trigger transient emergent properties. These forces are governed
by the {\em quantum geometric tensor}, a {\em phononic shift vector}, and
interband asymmetries in the electron-phonon coupling. The injection force acts
displacively, while -- unlike conventional impulsive mechanisms -- the shift
force emerges impulsively in the resonant interband absorbing regime when
time-reversal symmetry is broken. Using the bilayer Haldane model, we quantify
the injection and shift forces acting on interlayer shear phonons through both
analytical and numerical methods. Strikingly, we reveal strong tunability, both
in magnitude and direction, of the rectified forces by varying the driving
frequency and magnetic flux, uncovering a distinct quantum geometric mechanism
for ultrafast and coherent manipulation of quantum materials.

</details>


### [98] [Higher Chern bands in helical homotrilayer transition metal dichalcogenides](https://arxiv.org/abs/2507.17819)
*Jungho Daniel Choi,Nicolás Morales-Durán,Yves H. Kwan,Andrew J. Millis,Nicolas Regnault,Daniele Guerci*

Main category: cond-mat.mes-hall

TL;DR: 文章提出利用螺旋扭曲的同三层过渡金属二硫化物实现高陈数拓扑相，并通过理论模型和计算验证了其可行性及可调性。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探索和实现具有更高和可调陈数的关联拓扑相。

Method: 文章通过推导低能连续模型来捕捉莫尔（moiré）尺度域内的物理现象，并推导了高对称堆垛域的有效紧束缚描述，该描述适用于广泛的扭曲角度。

Result: 文章确定了扭曲角度和位移场能够分离最高空的空穴带（hole band）并使其具有拓扑性质，陈数为 C=-2。研究还表明，改变位移场可以实现从 C=-2 到 C=-1 的转变，以及从拓扑平庸带到 C=-1 带的转变。最后，文章证明了 C=-2 带在 Hartree-Fock 计算中，即使在存在相互作用的情况下，也能在填充分数 ν=-1 时保持稳定。

Conclusion: 文章提出了以螺旋扭曲的同三层过渡金属二硫属化物的平台，用于实现具有更高且可调的陈数（Chern number）的关联拓扑相。

Abstract: We propose helically twisted homotrilayer transition metal dichalcogenides as
a platform for realizing correlated topological phases of matter with higher
and tunable Chern numbers. We show that a clear separation of scales emerges
for small twist angles, allowing us to derive a low-energy continuum model that
captures the physics within moir\'e-scale domains. We identify regimes of twist
angle and displacement field for which the highest-lying hole band is isolated
from other bands and is topological with $K$-valley Chern number $C=-2$. We
demonstrate that varying the displacement field can induce a transition from
$C=-2$ to $C=-1$, as well as from a topologically trivial band to a $C=-1$
band. We derive an effective tight-binding description for a high-symmetry
stacking domain which is valid for a wide range of twist angles, and we show
that the $C=-2$ band can remain stable at filling fraction $\nu=-1$ in the
presence of interactions in Hartree-Fock calculations.

</details>


### [99] [Extending exciton and trion lifetimes in MoSe$_{2}$ with a nanoscale plasmonic cavity](https://arxiv.org/abs/2507.17879)
*Grace H. Chen,Anchita Addhya,Ian N. Hammock,Philip Kim,Alexander A. High*

Main category: cond-mat.mes-hall

TL;DR: 通过将 MoSe$_{2}$ 置于银腔中，成功延长了激子寿命，改善了其在光电器件中的性能。


<details>
  <summary>Details</summary>
Motivation: 激子在过渡金属硫族化合物 (TMDs) 中的寿命极短（皮秒量级），这阻碍了激子的热化、限制了集体相干性的出现，并降低了激子在光电器件中的传输。

Method: 利用全光学方法，将 MoSe$_{2}$ 放置于深亚波长法布里-珀罗银腔中，以抑制诸如亮激子和三子等面内光学偶极子的辐射复合。

Result: 实验观察到激子和三子的光致发光 (PL) 线宽持续减小（约 1 nm），同时寿命相应增加（约 10 ps）。通过蚀刻掉顶层银膜，PL 线宽和寿命均恢复到初始值，证实了实验观察结果源于激子-腔相互作用。

Conclusion: 这项工作为调控二维材料中的激发态寿命提供了一条途径，可用于研究光学暗激子和开发新型光电器件。

Abstract: Excitons in transition metal dichalcogenides (TMDs) have extremely short,
picosecond-scale lifetimes which hinders exciton thermalization, limits the
emergence of collective coherence, and reduces exciton transport in
optoelectronic devices. In this work, we explore an all-optical pathway to
extend exciton lifetimes by placing MoSe$_2$ in a deep-subwavelength
Fabry-Perot silver cavity. The cavity structure is designed to suppress
radiative recombination from in-plane optical dipoles, such as bright excitons
and trions. We observe a consistent decrease in photoluminescence (PL)
linewidths of excitons and trions (~1 nm), along with a corresponding lifetime
increase (~10 ps). We confirm the experimental observations arise purely from
exciton-cavity interactions-etching back the top silver layer returns the PL
linewidth and lifetimes return to their original values. Our study offers a
pathway to engineer excited state lifetimes in 2D materials which can be
utilized for studies of optically dark excitons and have potential applications
for novel optoelectronic devices.

</details>


### [100] [Multipole order in two-dimensional altermagnets](https://arxiv.org/abs/2507.18020)
*Tenta Tani,Ulrich Zülicke*

Main category: cond-mat.mes-hall

TL;DR: 二维反磁性材料中的磁多极序研究：通用模型有磁八极序，单层FeSe模型则表现出磁十六极序。


<details>
  <summary>Details</summary>
Motivation: 理论研究二维反磁性材料中的磁多极序。

Method: 通过构建低能有效哈密顿量并计算其各自的多极矩指标来表征潜在的磁序。

Result: 通用的最小模型表现出预期的非零磁八极序，而单层FeSe模型中的磁八极序全局消失，取而代之的是磁十六极序，并通过亚晶格-同旋自由度的相互作用在能带结构中产生反磁性分裂。

Conclusion: 本研究表明，二维反磁性材料的分类和全面理解超越了体相描述。

Abstract: We theoretically investigate the magnetic-multipole orders in two-dimensional
(2D) altermagnets, focusing on two representative models: a generic minimal
three-site model, and a four-site model representative of monolayer FeSe. We
construct low-energy effective Hamiltonians for both systems and calculate
their respective multipole indicators to characterize the underlying magnetic
order. Our analysis reveals an intriguing contrast between the two systems. We
find that the generic minimal model exhibits the expected non-zero
magnetic-octupole order. In the monolayer-FeSe model, however, the
magnetic-octupole order vanishes globally, and a magnetic-hexadecapole order is
present instead. The emergence of altermagnetic splitting in the band structure
then arises via the interplay with a sublattice-isospin degree of freedom. Our
work demonstrates how the classification and comprehensive understanding of 2D
altermagnetic materials transcends bulk descriptions.

</details>


### [101] [Topological Layer-Spin Filter in Screw Dislocation](https://arxiv.org/abs/2507.18089)
*Jiaojiao Zhou,Hong Hu,Jiangying Yu,Lin Xu,Shu-guang Cheng,Hua Jiang*

Main category: cond-mat.mes-hall

TL;DR: 研究了具有螺位错的多层Kane-Mele模型的输运性质，发现螺位错可以作为层-自旋滤波器，实现对电子自旋和层自由度的协同调控。


<details>
  <summary>Details</summary>
Motivation: 探索三维系统中利用层结构操控自旋的物理机制，以及层位错对量子自旋霍尔效应的影响。

Method: 通过数值模拟研究了具有螺位错的多层Kane-Mele模型的电子输运性质。

Result: 螺位错处的无耗散量子自旋霍尔边缘态能够作为层-自旋滤波器，实现了自旋向上和自旋向下载流子在不同层中的流动，且传输系数和自旋极化对安德森无序具有鲁棒性。磁性无序会抑制传输系数但保持自旋极化不变。

Conclusion: 该研究为调控电子的输运提供了创新的层和自旋分辨率方法学。

Abstract: While the quantum spin Hall effect leverages two-dimensional topological
states to manipulate spin without dissipation, layertonics extends this
paradigm to three dimension by enabling control over the layer degree of
freedom. Topological materials incorporating screw dislocations exhibit the
capability for simultaneous manipulation of both electronic spin and layer
degrees of freedom. In this work, the electronic transport properties of a
multilayer Kane-Mele model with screw dislocations is studied theoretically.
Numerical simulations of a screw dislocation reveal that dissipationless
quantum spin Hall edge states propagate not only at the outer boundaries of the
structure but also along the screw dislocation itself, working as layer-spin
filter. In detail, 1) the spin-up and spin-down carriers starting from the same
source layer flow to different drain layers along the topological channels,
respectively. 2) The spin of carriers flowing into a given drain layer is
determined by the input source layer. Moreover, we found that the transmission
coefficient and spin polarization remain robust against Anderson disorder.
Under magnetic disorder, spin flip and backscattering occur, suppressing the
transmission coefficient while maintaining nearly unchanged spin polarization.
Finally, the layer- and spin-resolved transport properties in a device with two
screw dislocations are investigated as well. We have developed an innovative
methodology to modulate electron transport with simultaneous layer and spin
resolution.

</details>


### [102] [Dislocation-Driven Nucleation Type Switching Across Repeated Ultrafast Magnetostructural Phase Transition](https://arxiv.org/abs/2507.18364)
*Jan Hajduček,Antoine Andrieux,Jon Ander Arregi,Martin Tichý,Paolo Cattaneo,Beatrice Ferrari,Fabrizio Carbone,Vojtěch Uhlíř,Thomas LaGrange*

Main category: cond-mat.mes-hall

TL;DR: 在FeRh薄膜中，研究表明累积激光辐照会改变反铁磁-铁磁相变的成核机制，导致转变温度降低，并形成由位错网络固定的磁涡。


<details>
  <summary>Details</summary>
Motivation: 在磁性研究中，在超快时间尺度上控制磁性顺序是主要研究方向之一，但在微观结构和动态成核之间的局部联系方面，仍缺乏实验演示。

Method: 利用原位透射电子显微镜的高结构和磁分辨率，观察到累积激光辐照显著改变了FeRh薄膜第一类反铁磁-铁磁相变的成核途径，导致相变从均匀成核转变为非均匀成核。

Result: 相变从均匀成核转变为非均匀成核，导致转变温度降低20 K，并出现优先成核图案——亚微米磁涡，这些涡被固定在薄膜中由下方的位错网络固定。

Conclusion: 研究确立了缺陷形成、成核能量学和成核铁磁相的微观形貌之间的直接联系，这对快门超快实验和功能材料中的缺陷介导相变具有广泛的意义。

Abstract: Controlling magnetic order on ultrafast timescales, driven by spintronic and
recording applications, is one of the main directions of current research in
magnetism. Despite major advances in understanding the temporal evolution of
magnetic order upon its emergence or quenching, experimental demonstration of
the local link between microstructure and dynamic nucleation is missing. Here,
taking advantage of the high structural and magnetic resolution of in situ
transmission electron microscopy, we observe that cumulative laser irradiation
significantly alters the nucleation pathway of the first-order
antiferromagnetic to ferromagnetic phase transition of FeRh thin films, causing
the transition to switch from homogeneous to heterogeneous nucleation. This
leads to a decrease of 20 K in transition temperature and the emergence of
sub-micron magnetic vortices as preferential nucleation motifs. These vortices
are pinned in the film by underlying dislocation networks. We observe that the
dislocation networks are formed and rearranged upon repeated crossing of the
phase transition using femtosecond and picosecond laser pulses. Our results
establish a direct link between defect formation, nucleation energetics, and
the microscopic morphology of the nucleated ferromagnetic phase, with broad
implications for ultrafast stroboscopic experiments and defect-mediated phase
transitions in functional materials.

</details>


### [103] [Quasicrystalline Altermagnetism](https://arxiv.org/abs/2507.18408)
*Rui Chen,Bin Zhou,Dong-Hui Xu*

Main category: cond-mat.mes-hall

TL;DR: 准晶可以支持外延磁序，特别是在八角形和十二边形准晶中，这为研究非常规磁性开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 探索在准晶中实现非常规外延磁序的可能性，因为准晶具有周期晶体所不允许的旋转对称性。

Method: 通过对称性分析和自洽平均场理论，预测了准晶中的外延磁序。

Result: 预测了在八角形和十二边形准晶中存在稳定的g波和i波外延磁性，并指出了其独特的各向异性赝分裂和节点结构，可作为实验指纹。Tldr: 准晶可以支持外延磁序，特别是在八角形和十二边形准晶中，这为研究非常规磁性开辟了新途径。The study demonstrates that quasicrystals can host altermagnetic orders, predicting stable g-wave and i-wave altermagnetism in octagonal and dodecagonal quasicrystals, respectively. This expands the understanding of altermagnetism and highlights the potential of quasicrystals in realizing unconventional magnetism.

Conclusion: 该研究证明了准晶可以支持外延磁序，并预测了在八角形和十二边形准晶中稳定存在的g波和i波外延磁性。这扩展了对外延磁性的理解，并强调了准晶在实现非常规磁性方面的潜力。

Abstract: Altermagnets are a recently discovered class of magnetic materials that
combine a collinear, zero-magnetization spin structure, characteristic of
antiferromagnets, with spin-split electronic bands, a hallmark of ferromagnets.
This unique behavior arises from the breaking of combined time-reversal and
spatial symmetries (such as inversion or lattice translation), which are
preserved in conventional antiferromagnets. To date, research has focused on
altermagnetic phases in periodic crystals, where the order is linked to
specific crystallographic rotation symmetries. In this work, we demonstrate
that quasicrystals, which possess rotational symmetries forbidden in periodic
lattices, can host exotic altermagnetic orders. Using symmetry analysis and
self-consistent mean-field theory, we predict stable $g$-wave and $i$-wave
altermagnetism in octagonal and dodecagonal quasicrystals, respectively. These
novel phases are characterized by global $C_8T$ and $C_{12}T$ symmetries and
manifest as unique anisotropic spin-splittings in their spectral functions and
spin conductance, featuring characteristic eight- and twelve-fold nodal
structures that serve as unambiguous experimental fingerprints. Our findings
establish quasicrystals as a versatile platform for realizing unconventional
altermagnetic orders beyond the constraints of crystallographic symmetry.

</details>


### [104] [Local Hall Conductivity in Disordered Topological Insulators](https://arxiv.org/abs/2507.18441)
*Zachariah Addison,Nandini Trivedi*

Main category: cond-mat.mes-hall

TL;DR: 我们导出了缺乏平移对称性的系统的局域霍尔电导率表达式，发现无序会增加陈绝缘体和拓扑安德森绝缘体的存在范围，并期望我们的结果能促进相关实验的发展。


<details>
  <summary>Details</summary>
Motivation: 研究磁绝缘体中缺乏平移对称性的系统的局域霍尔电导率及其局域涨落，以期促进对拓扑安德森绝缘体相空间的理解和实验可视化。

Method: 推导了缺乏平移对称性的系统的局域霍尔电导率表达式，并研究了磁绝缘体中围绕无序区域的霍尔信号的局域涨落。

Result: 在参数空间中，包含非磁性势能无序后，陈绝缘体状态的区域会增加。将单个无序区域分解为多个较小的区域可以扩大拓扑安德森绝缘体存在的相空间。

Conclusion: 对于缺乏平移对称性的系统，我们导出了局域霍尔电导率的表达式，并用它来研究磁绝缘体中围绕无序区域的霍尔信号的局域涨落。我们发现，在参数空间中，系统作为陈绝缘体状态的区域，在包含非磁性势能无序后会增加。此外，通过将单个无序区域分解为多个具有相同总无序量的较小区域，可以扩大拓扑安德森绝缘体存在的相空间。

Abstract: We derive the expression for the local Hall conductivity for systems that
lack translation symmetry and use it to study the local fluctuations of the
Hall signal around disordered patches in magnetic insulators. We find that the
regime in parameter space over which the system is a Chern insulating state
increases upon inclusion of non-magnetic potential disorder. In addition, the
phase space over which the topological Anderson insulator exists can be
enhanced by breaking up a single disordered patch into multiple smaller patches
with the same total amount of disorder. We expect our results will motivate the
next generation of local scanning and local impedance spectroscopy experiments
to visualize Hall currents around patches in the bulk of a disordered
topological insulator.

</details>


### [105] [Symmetry driven spin anisotropic magnetotransport in quantum spin Hall insulator WTe2 1T](https://arxiv.org/abs/2507.18543)
*Shrushti Tapar,Bent Weber,Saroj P Dash,3 Shantanu Mukherjee,Bhaskaran Muralidharan*

Main category: cond-mat.mes-hall

TL;DR: 单层1T-WTe2的磁输运分析表明，非密性对称性保护了x轴边缘态的自旋简并性，导致了磁响应的各向异性，这在自旋电子器件中具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 突出非密性对称性在控制单层1T-WTe2边缘态自旋行为中的作用，并探索其在自旋电子器件中的应用潜力。

Method: 通过比较沿着晶体y轴和x轴边缘的纳米带的电子传输，进行了全面的磁输运分析，并结合了能量分辨的电流密度和角度传输分析。

Result: 研究发现y轴边缘的纳米带在能量和动量空间中表现出显著的自旋分裂和强角度依赖性，而x轴边缘的纳米带在磁场下几乎没有自旋分裂。证实了非密性对称性保护了沿Gamma-X方向的简并性。

Conclusion: 本研究通过磁输运分析揭示了单层1T-WTe2中的非密性对称性在控制边缘态自旋行为中的作用，并证实了其在自旋电子器件中的应用潜力。

Abstract: We present a comprehensive magnetotransport analysis of monolayer 1T WTe2,
highlighting the role of nonsymmorphic symmetries in governing edge-state spin
behavior. By comparing the electronic transmission in nanoribbons with edges
along the crystallographic y and x directions, our analysis reveals a
pronounced anisotropy in the magnetic field response. The y-edge ribbon
exhibits significant spin splitting of edge-state bands in both energy and
momentum space, along with a strong angular dependence of the conductance. The
observed magnetotransport response indicates a spin quantization axis that
aligns with the out-of-plane spin quantization axis reported in previous
experimental studies. In contrast, the x edge ribbon shows negligible spin
splitting under magnetic fields, which is attributed to nonsymmorphic
symmetries such as glide mirror and screw rotation, that protects degeneracies
along the Gamma X direction, even when time-reversal symmetry is broken. The
energy-resolved current density and angular transmission analyses confirm that
this anisotropy originates from edge states, while bulk states remain largely
insensitive to the field orientation. Our results establish direct
transport-spectroscopy based evidence of nonsymmorphic-symmetry-protected spin
degeneracy in the 1T WTe2, and underscores its promise for spintronic devices
that leverage symmetry-protected and directionally selective transport
channels.

</details>


### [106] [Ultrafast coherent magnon spin currents in antiferromagnets](https://arxiv.org/abs/2507.18563)
*Torstein Hegstad,Johan H. Mentink*

Main category: cond-mat.mes-hall

TL;DR: 在某些反铁磁体中，利用超快相干技术可以产生净自旋流，并可控。


<details>
  <summary>Details</summary>
Motivation: 在超快自旋电子学和准磁振子学中，产生具有最高频率和最短波长的相干准磁振子自旋流是一个关键挑战。

Method: 使用线偏振光激发超快相干自旋流，并通过控制光偏振方向来控制自旋流方向。通过叠加两个正交的自旋流，可以产生圆偏振自旋流。

Result: 展示了在某些反铁磁体中，通过相干叠加多个准磁振子对模式可以产生净自旋流，并且可以通过光偏振控制其方向，还可以产生圆偏振自旋流。

Conclusion: 可以从多重太ईआरटी-时间对称反铁磁体中的多重准磁振子对模式相干叠加中产生净自旋流。

Abstract: Generating coherent magnon spin currents with the highest frequencies and
shortest wavelengths is a key challenge in ultrafast spintronics and magnonics.
A promising route is to excite counter-propagating magnon pairs. In
antiferromagnets, such pairs can be accessed in the ultrafast regime, where
coherent dynamics are dominated by magnons at the edge of the Brillouin zone.
However, it has seemed impossible to generate a net spin current from coherent
magnon pairs. Here we show that a coherent superposition of multiple
magnon-pair modes can produce such a current in parity-time symmetric
antiferromagnets. The ultrafast coherent spin currents are excited with
linearly polarized light, with the light polarization steering the current
direction. Finally, by superposing two orthogonal spin currents, circular spin
currents can be generated, which have not been discussed for steady-state
currents.

</details>


### [107] [Superconductivity from dual-surface carriers in rhombohedral graphene](https://arxiv.org/abs/2507.18598)
*Manish Kumar,Derek Waleffe,Anna Okounkova,Raveel Tejani,Vo Tien Phong,Kenji Watanabe,Takashi Taniguchi,Cyprian Lewandowski,Joshua Folk,Matthew Yankowitz*

Main category: cond-mat.mes-hall

TL;DR: 本研究发现了多层石墨烯中的一种新颖超导机制，并通过实验手段对其进行了表征，为超导和拓扑态的进一步研究提供了新的方向。


<details>
  <summary>Details</summary>
Motivation: 探究本征菱形石墨烯中存在的不同寻常的低能电子波函数及其对对称性破缺相的影响，并在此基础上发现新的超导现象。

Method: 通过实验研究了本征菱形石墨烯的低能电子波函数特性，以及在不同层数和超晶格结构下，外部电场对超导电性的影响。

Result: 在本研究中，我们发现了菱形石墨烯中的超导电性，其来源于一种不同寻常的电荷离域半金属正常态。在八层石墨烯中，超导电性出现在每个符号的外部电位移场（D）下，五个明显不同的口袋中。在七层石墨烯的莫尔超晶格样品中，超导电性从一个单一尖锐的电阻特征中出现两个口袋。在更高的D下，相同的电阻特征还在接近每个莫尔单位细胞一个电子的掺杂附近诱导了h/e^2量子化的反常霍尔态。 发现了一种新颖的超导区域，并为与相邻拓扑态的耦合创造了机会。

Conclusion: 本研究揭示了多层石墨烯中一种新颖的超导机制，并为与相邻拓扑态的耦合提供了机会。

Abstract: Intrinsic rhombohedral graphene hosts an unusual low-energy electronic
wavefunction, predominantly localized at its outer crystal faces with
negligible presence in the bulk. Increasing the number of graphene layers
amplifies the density of states near charge neutrality, greatly enhancing the
susceptibility to symmetry-breaking phases. Here, we report superconductivity
in rhombohedral graphene arising from an unusual charge-delocalized
semimetallic normal state, characterized by coexisting valence- and
conduction-band Fermi pockets split to opposite crystal surfaces. In octalayer
graphene, the superconductivity appears in five apparently distinct pockets for
each sign of an external electric displacement field ($D$). In a moir\'e
superlattice sample where heptalayer graphene is aligned on one side to
hexagonal boron nitride, two pockets of superconductivity emerge from a single
sharp resistive feature. At higher $D$ the same resistive feature additionally
induces an $h/e^{2}$-quantized anomalous Hall state at dopings near one
electron per moir\'e unit cell. Our findings reveal a novel superconducting
regime in multilayer graphene and create opportunities for coupling to nearby
topological states.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [108] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃衍接器在多芯片系统中具有电气性能优势，但尺寸增大时易产生翘曲问题。本研究提出了一种热-翘曲-性能感知设计框架，通过架构和封装协同优化，有效解决了翘曲问题，并显著提升了性能、降低了功耗，同时降低了成本。


<details>
  <summary>Details</summary>
Motivation: 玻璃衍接器在多芯片系统中虽有优势，但尺寸增大时会产生翘曲问题，影响机械应力和可靠性，现有技术难以有效管理，需要新的方法来解决。

Method: 提出一个结合架构和封装协同优化的设计框架，通过分解表面和嵌入式芯片来平衡设计目标，以减轻翘曲引起的弯曲，并实现可扩展的性能。

Result: 优化的多芯片架构在深度神经网络工作负载方面，与传统的2.5D系统相比，性能提高了64.7%，功耗降低了40%，同时制造成本更低。

Conclusion: 本研究提出的热-翘曲-性能感知设计框架通过架构和封装协同优化，能够平衡性能、功耗和结构可靠性之间的冲突，为玻璃衍接器多芯片系统提供了有效的解决方案。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [109] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一个新的CPU服务引擎，它为LLM的预填充和解码阶段采用了不同的优化策略，显著提高了吞吐量和降低了延迟，同时减少了对资源的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CPU的解决方案忽略了LLM推理的预填充和解码阶段之间的工作负载差异，采用静态的每个NUMA节点模型分区，并利用供应商库进行算子级执行，这是次优的。

Method: Sandwich是一个以硬件为中心的、基于CPU的LLM服务引擎，它为LLM推理的预填充和解码阶段采用了不同的执行计划，并分别进行优化。

Result: Sandwich在x86（支持AVX-2和AVX-512）和ARM（支持NEON）等五个CPU平台上，在不同基线和数据集上进行了评估。

Conclusion: Sandwich在CPU上实现了LLM服务，平均吞吐量提高了2.01倍，在单序列服务中，令人满意的首次令牌时间和每个输出令牌时间的延迟降低了90%，并且对计算资源的需求降低了3.40倍。在连续批处理服务中，Goodput也得到了显著提升。Sandwich生成的GEMM内核性能优于代表性厂商内核和其他动态形状解决方案，并且在内核调优成本仅为静态编译器的三百分之一的情况下，取得了与之相当的性能。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [110] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical优化了DDR5的Rowhammer防护机制，在不牺牲安全性的情况下提高了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: DDR5标准的PRAC和ABO机制虽然解决了Rowhammer问题，但会带来性能开销和恢复刷新延迟。PRACtical旨在优化这些机制，在保持相同安全性的前提下提高性能。

Method: PRACtical通过引入集中式增量电路来减少计数器更新延迟，使计数器更新与其他子阵列的后续行激活重叠。此外，通过DRAM内部寄存器识别受攻击银行，实现了银行级粒度的恢复刷新。

Result: PRACtical平均性能提升8%（最高20%），能耗降低19%，并将攻击对性能的影响限制在6%以内，同时保留了Rowhammer防护能力。

Conclusion: PRACtical在提高性能的同时，将DDR5标准的PRAC和ABO机制的性能开销和恢复刷新延迟降低了8%-20%，并减少了19%的能耗，同时保持了Rowhammer防护能力。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>


### [111] [Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search](https://arxiv.org/abs/2507.17953)
*Chang Eun Song,Weihong Xu,Keming Fan,Soumil Jain,Gopabandhu Hota,Haichao Yang,Leo Liu,Kerem Akarvardar,Meng-Fan Chang,Carlos H. Diaz,Gert Cauwenberghs,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: Clo-HDnn是一种为端上设备学习（ODL）的持续学习（CL）任务设计的硬件加速器，它结合了超维度计算（HDC）和优化的特征提取与更新策略，显著提高了能效和性能。


<details>
  <summary>Details</summary>
Motivation: 为了应对新兴的持续学习（CL）任务，特别是在端上设备学习（ODL）场景下对准确性和能效的更高要求，需要设计专门的硬件加速器。

Method: Clo-HDnn采用超维度计算（HDC）、低成本克罗内克HD编码器（Kronecker HD Encoder）和权重聚类特征提取（WCFE）技术，并结合无梯度持续学习（CL）策略，利用类超向量（class hypervectors）的形式存储和更新知识。此外，它还采用了双模式操作以适应不同复杂度的数据集，并通过渐进搜索（progressive search）来降低计算复杂度。

Result: Clo-HDnn在能效方面表现出色，实现了4.66 TFLOPS/W（特征提取）和3.78 TOPS/W（分类器），分别比现有最优的ODL加速器高出7.77倍和4.85倍。

Conclusion: Clo-HDnn通过集成HDC、Kronecker HD Encoder和WCFE，并采用无梯度CL、双模式操作和渐进搜索，在准确性和能效方面进行了优化，实现了优于SOTA ODL加速器的性能。

Abstract: Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging
continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing
(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature
extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts
gradient-free CL to efficiently update and store the learned knowledge in the
form of class hypervectors. Its dual-mode operation enables bypassing costly
feature extraction for simpler datasets, while progressive search reduces
complexity by up to 61% by encoding and comparing only partial query
hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),
Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL
accelerators.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [112] [Sb2S3 and GaAs Absorber Layer-based Quantum Dot Solar Cells with Cadmium Telluride-based HTL: A Comparative Study](https://arxiv.org/abs/2507.17877)
*Sayak Banerjee,Anupam Chetia,Satyajit Sahu*

Main category: cond-mat.mtrl-sci

TL;DR: GaAs is a better absorber layer material for QDSC than Sb2S3, achieving higher efficiency.


<details>
  <summary>Details</summary>
Motivation: To identify the better absorber layer material between Sb2S3 and GaAs for QDSC by comparing their operational efficiency.

Method: Numerical analysis using SCAPS-1D.

Result: The maximum efficiency attained by Sb2S3 and GaAs based QDSC is 15.94% and 26.95% respectively, indicating GaAs as the superior material.

Conclusion: GaAs is a better absorber layer material for QDSC compared to Sb2S3.

Abstract: Quantum dot solar cells (QDSC) are widely acknowledged to be one of the best
solar energy harvesting devices in the present world. Absorber layer is a core
component of a QDSC with a strong influence on its operational efficiency.
Hence, we choose to undertake a comparative study of two QDSC having different
QD absorber layers: Sb2S3 and GaAs with the motive to identify the better
absorber layer material. The numerical analysis has been carried out using
SCAPS-1D (Solar Cell Capacitance Simulator-1D). The structure of the QDSCs
under study are: FTO/TiO2/CdS/Sb2S3/CuI/C and FTO/TiO2/CdS/GaAs/CuI/C. Critical
parameters, including temperature, back contact work function, series and shunt
resistances, were meticulously adjusted in the simulations, demonstrating that
the maximum efficiency attained by Sb2S3 and GaAs absorber layer based QDSC is
15.94% and 26.95% respectively indicating GaAs-QD to be a better absorber layer
material for a QDSC.

</details>


### [113] [Ultra-clean interface between high k dielectric and 2D MoS2](https://arxiv.org/abs/2507.18010)
*Han Yan,Yan Wang,Yang Li,Dibya Phuyal,Lixin Liu,Hailing Guo,Yuzheng Guo,Tien-Lin Lee,Min Hyuk Kim,Hu Young Jeong,Manish Chhowalla*

Main category: cond-mat.mtrl-sci

TL;DR: 氧化锆（ZrO2）是一种高k电介质，可与2D TMDs（如MoS2）形成无缺陷界面，解决了传统电介质带来的掺杂和性能问题，实现了高性能、高迁移率的晶体管，并可通过功函数工程调控阈值电压，是未来2D电子器件的理想选择。


<details>
  <summary>Details</summary>
Motivation: 传统的氧化物电介质（如SiO2、Al2O3、HfO2）在与2D TMDs（如MoS2）结合时，会引入非预期的掺杂和界面缺陷态，导致场效应晶体管（FET）性能不佳和阈值电压不稳定。因此，需要开发一种与半导体工艺兼容且能形成超净界面的高k电介质，以实现高性能、可扩展的2D TMDs电子器件。

Method: 通过软X射线光电子能谱、硬X射线光电子能谱和密度泛函理论分析，研究了ZrO2与单层MoS2的界面相互作用。制备了基于ZrO2电介质的背栅和顶栅MoS2及WSe2场效应晶体管，并测试了其电学性能，如阈值电压、亚阈值摆幅和导通电流。原子分辨率成像用于确认界面质量。

Result: ZrO2与单层MoS2形成了超净界面，没有引起可测量的相互作用，与SiO2和HfO2形成了鲜明对比。基于ZrO2电介质的背栅单层MoS2 FETs表现出稳定的正阈值电压（0.36 ± 0.3 V）、低的亚阈值摆幅（75 mV/decade）和超过400 µA的导通电流。此外，ZrO2电介质也成功抑制了WSe2 FETs中的电子掺杂，实现了超过200 µA/µm的导通电流。原子分辨率成像证实了ZrO2/MoS2界面的无缺陷特性，使得顶栅FETs具有0.86 nm的等效氧化层厚度和80 mV/decade的亚阈值摆幅。顶栅FETs的阈值电压还可以通过栅金属功函数工程进行有效调控。

Conclusion: 氧化锆（ZrO2）作为一种高k电介质，可与2D TMDs（如MoS2和WSe2）形成超净界面，抑制了非预期的掺杂和缺陷态的引入，从而实现了具有稳定阈值电压、低亚阈值摆幅和高导通电流的场效应晶体管（FET）。通过工作函数工程，还可以有效地调节阈值电压，为可扩展的2D TMDs电子器件提供了有前景的工业兼容解决方案。

Abstract: Atomically thin transition metal dichalcogenides (TMDs) are promising
candidates for next-generation transistor channels due to their superior
scaling properties. However, the integration of ultra-thin gate dielectrics
remains a challenge, as conventional oxides such as SiO2, Al2O3, and HfO2 tend
to unintentionally dope 2D TMDs and introduce interfacial defect states,
leading to undesirable field-effect transistor (FET) performance and unstable
threshold voltages. Here, we demonstrate that zirconium oxide (ZrO2), a high-k
dielectric compatible with semiconductor processing, forms an ultra-clean
interface with monolayer MoS2. Using soft and hard X-ray photoelectron
spectroscopy and density functional theory, we find that ZrO2 does not
measurably interact with MoS2, in contrast to significant doping observed for
SiO2 and HfO2 substrates. As a result, back-gated monolayer MoS2 FETs
fabricated with ZrO2 dielectrics exhibit stable and positive threshold voltages
(0.36 plus/minus 0.3 V), low subthreshold swing (75 mV per decade), and high ON
currents exceeding 400 microamperes. We further demonstrate p-type WSe2 FETs
with ON currents greater than 200 microamperes per micrometer by suppressing
electron doping with ZrO2 dielectrics. Atomic-resolution imaging confirms a
defect-free ZrO2/MoS2 interface, which enables top-gate FETs with an equivalent
oxide thickness of 0.86 nanometers and subthreshold swing of 80 mV per decade.
Moreover, the ultraclean ZrO2/MoS2 interface allows for effective threshold
voltage modulation in top-gate FETs via gate metal work function engineering.
These findings establish ZrO2 as a highly promising, industry-compatible high-k
dielectric for scalable 2D TMD-based electronics.

</details>


### [114] [Analysis of Fe and Co binary catalysts in chemical vapor deposition growth of single-walled carbon nanotubes](https://arxiv.org/abs/2507.17891)
*Qingmei Hu,Ya Feng,Wanyu Dai,Daisuke Asa,Daniel Hedman,Aina Fito Parera,Yixi Yao,Yongjia Zheng,Kaoru Hisama,Gunjan Auti,Hirofumi Daiguji,Christophe Bichara,Shohei Chiashi,Yan Li,Wim Wenseleers,Dmitry Levshov,Sofie Cambre,Keigo Otsuka,Rong Xiang,Shigeo Maruyama*

Main category: cond-mat.mtrl-sci

TL;DR: Fe-Co比例和CVD条件影响SWCNT生长。Fe$_{0.75}$Co$_{0.25}$在850°C下生长较粗的SWCNT，Fe$_{0}$Co$_{1}$在600°C下生长较细的SWCNT。表面富Co的Fe-Co催化剂颗粒能提高SWCNT产率。MD模拟也支持这些结果。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要使用固定原子比例的Fe-Co合金催化剂和不变的CVD条件，对于Fe-Co比例和CVD生长参数对SWCNT生长影响的理解仍然不足。本研究旨在阐明Fe和Co原子在SWCNT生长中的不同作用，重点研究Fe-Co催化剂比例的作用。

Method: 本研究通过系统地探索不同Fe-Co比例和生长条件，结合透射电子显微镜(TEM)、能量色散X射线光谱(EDS)分析以及分子动力学(MD)模拟，来研究Fe-Co催化剂比例对SWCNT生长的影响。

Result: Fe$_{0.75}$Co$_{0.25}$在850°C下是一种高效的二元催化剂，主要形成直径为2.5-6 nm的催化剂团簇，产物为直径为0.9-1.1 nm的SWCNT。Fe$_{0}$Co$_{1}$在600°C下表现出更高的催化活性，生成直径为1.5-5 nm的较小催化剂团簇，产物为直径为0.6-0.9 nm的SWCNT。TEM和EDS分析表明，高SWCNT产率与尺寸均匀且表面富Co的Fe-Co催化剂颗粒的形成有关，这优化了碳溶解度。MD模拟进一步证实了这些发现，表明Fe$_x$Co$_{1-x}$团簇的结构和熔化行为取决于团簇尺寸和组成。

Conclusion: Fe-Co合金的比例和CVD生长条件对单壁碳纳米管(SWCNT)的生长有显著影响。Fe$_{0.75}$Co$_{0.25}$在850°C下表现出高催化效率，生成直径为2.5-6 nm的催化剂团簇，进而生长出直径为0.9-1.1 nm的SWCNT。Fe$_{0}$Co$_{1}$在600°C下催化活性更高，生成直径为1.5-5 nm的催化剂团簇，并生长出直径为0.6-0.9 nm的SWCNT。高产率的SWCNT与尺寸均匀且表面富Co的Fe-Co催化剂颗粒的形成有关，这优化了碳溶解度。

Abstract: Metal catalysts play a pivotal role in the growth of single-walled carbon
nanotubes (SWCNTs), with binary metallic catalysts emerging as an efficient
SWCNT synthesis strategy. Among these, iron (Fe), cobalt (Co), and their alloys
are particularly effective. However, prior studies have predominantly employed
Fe--Co alloy catalysts with fixed atomic ratios as well as unchanged chemical
vapor deposition (CVD) conditions, leaving the influence of variable Fe--Co
compositions and CVD growth parameters on SWCNT synthesis poorly understood.
This study focuses on the role of Fe--Co catalyst ratios, with the aim of
elucidating the distinct contributions of Fe and Co atoms in the growth of
SWCNTs. By systematically exploring a wide range of Fe--Co ratios and growth
conditions, we identified Fe$_{0.75}$Co$_{0.25}$ as a highly efficient binary
catalyst at 850~$^\circ$C, primarily forming catalyst clusters with diameters
of 2.5--6~nm and yielding SWCNTs with diameters ranging from 0.9--1.1~nm. On
the other hand, Fe$_{0}$Co$_{1}$ exhibited higher catalytic activity at
600~$^\circ$C, generating smaller catalyst clusters of 1.5--5~nm and producing
SWCNTs with reduced diameters of about 0.6--0.9~nm. Transmission electron
microscope (TEM) and energy dispersive X-ray spectroscopy (EDS) analyses reveal
that high SWCNT yields correlate with the formation of uniformly sized Fe--Co
catalyst particles with surface-segregated Co that optimizes carbon solubility.
Molecular dynamics (MD) simulations further corroborate these findings,
demonstrating that the structure and melting behavior of Fe$_x$Co$_{1-x}$
clusters depend on cluster size and composition.

</details>


### [115] [Computation and Sensitivity Analysis of the Deformation-Gradient Tensor Reconstruction in Dark-Field X-ray Microscopy](https://arxiv.org/abs/2507.17929)
*Brinthan Kanesalingam,Darshan Chalise,Carsten Detlefs,Leora Dresselhaus-Marais*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一个用于暗场X射线显微镜（DFXM）的逆建模框架，能够精确测量材料内部的应变和晶格旋转，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的X射线衍射技术在同时实现高空间分辨率和大视场方面存在挑战，而DFXM有潜力解决这个问题，但缺乏将角度偏移显式关联到应变和晶格旋转张量的逆建模框架。

Method: 提出了一种新的逆建模形式主义，利用倾斜衍射几何来重构变形梯度张量F(g)；开发了计算框架以进行正向计算和像素级重构；建立了敏感性分析方法以关联特定角度偏移和应变/旋转张量分量，并计算重建误差。

Result: 成功开发了DFXM的逆建模形式主义，能够重构变形梯度张量F(g)；建立了计算框架以支持实验；提出了敏感性分析方法，能够量化应变和旋转张量分量与DFXM角度测量之间的关系以及重建误差。

Conclusion: 该研究为暗场X射线显微镜（DFXM）的逆建模提供了一个完整的框架，实现了从角度偏移到应变和晶格旋转张量的显式关联，并开发了相应的计算框架和敏感性分析方法，为过去和未来的DFXM实验提供了指导。

Abstract: Spatially resolved strain measurements are crucial to understanding the
properties of engineering materials. Although strain measurements utilizing
techniques such as transmission electron microscopy and electron backscatter
diffraction offer high spatial resolution, they are limited to surface or thin
samples. X-ray diffraction methods, including Bragg Coherent Diffraction
Imaging and X-ray topography, enable strain measurements deep inside bulk
materials but face challenges in simultaneously achieving both high spatial
resolution and large field-of-view. Dark-field X-ray Microscopy (DFXM) offers a
promising solution with its ability to image bulk crystals at the nanoscale
while offering a field-of-view approaching a few hundred $\mu$m. However, an
inverse modeling framework to explicitly relate the angular shifts in DFXM to
the strain and lattice rotation tensors is lacking. In this paper, we develop
such an inverse modeling formalism. Using the oblique diffraction geometry,
enabling access to noncoplanar symmetry-equivalent reflections, we demonstrate
that the reconstruction of the full deformation gradient tensor
($\mathbf{F^{(g)}}$) is possible. We also develop the computational framework
to both forward calculate the anticipated angular shifts and reconstruct the
average $\mathbf{F^{(g)}}$ for an individual pixel from DFXM experiments.
Finally, utilizing the established formalism and computational framework, we
present methods for sensitivity analysis to relate individual components of the
rotation or strain tensor to specific angles of DFXM. The developed sensitivity
analysis also enables explicit computation of the errors associated with the
reconstruction of each component. The formalism, the computational framework,
and the sensitivity analysis established in this paper should assist both the
interpretation of past DFXM experiments and the design of future DFXM
experiments.

</details>


### [116] [Nucleation of magnetic textures in stripe domain bifurcations for reconfigurable domain wall racetracks](https://arxiv.org/abs/2507.18356)
*V. V. Fernández,S. Ferrer,A. Hierro-Rodríguez,M. Vélez*

Main category: cond-mat.mtrl-sci

TL;DR: 本文研究了赛道内存中的磁化反转过程，重点关注磁拓扑电荷交换在控制涡旋和反涡旋形成中的作用，这对器件功能至关重要。


<details>
  <summary>Details</summary>
Motivation: 为了增强最终器件的多功能性，增加了磁可重构能力，利用了利用磁引导势而非几何势的赛道内存范式。

Method: 利用包括天量子和磁涡度线在内的概念，通过微磁模拟研究了 NdCo5/Py 可重构赛道中的磁化反转过程。

Result: 研究表明，磁拓扑电荷交换控制着涡旋、反涡旋、布洛赫线和布洛赫点的形核。

Conclusion: 磁拓扑电荷交换是形成具有相反极性的涡旋/反涡旋对的关键，这对于通过条纹模式进行引导传播至关重要。

Abstract: Within the racetrack memory paradigm, systems exploiting magnetic guiding
potentials instead of geometrical ones, allow for enhancing the versatility of
the final devices adding magnetic reconfigurable capabilities. Hard/soft
magnetic multilayers with stripe domain configurations fulfill these
requirements. In these systems, the topology of the generated textures that
would act as information carriers, is strongly conditioned by the stripe
lattice configuration. Micromagnetic simulations have been used to study the
magnetization reversal process in NdCo$_5$/Py reconfigurable racetracks. By
using skyrmionic charges and magnetic vorticity lines, the topological
transformations controlling the nucleation of vortices, antivortices, Bloch
lines and Bloch points has been analyzed. It has been shown that magnetic
topological charge exchanges between textures rule the formation of
vortex/antivortex pairs with opposite polarities, key for the guided
propagation through the stripe pattern.

</details>


### [117] [Efficient $G_0W_0$ and BSE calculations of heterostructures within an all-electron framework](https://arxiv.org/abs/2507.17960)
*Maximilian Schebek,Ignacio Gonzalez Oliva,Claudia Draxl*

Main category: cond-mat.mtrl-sci

TL;DR: 通过改进计算方法，可以高效地计算二维材料异质结的光电特性。


<details>
  <summary>Details</summary>
Motivation: 二维材料异质结在设计具有可调光电特性的光电器件方面提供了新的机会，但由于其大的单位晶胞，使用现有方法计算这些系统的电子和光学性质非常困难，特别是对于多体微扰理论框架下的高精度全电子计算。

Method: 本文将先前用于平面波基组的计算非相互作用极化率的方法扩展到（线性化）增强平面波（L）APW 方法，该方法基于加性模型，可以计算并叠加各个组分在其各自单位晶胞中的极化率。在 exciting 代码的 G0W0 模块中实现了该形式主义，并为 BSE 计算实现了一个类似的方法。

Result: 所提出的方法能够以较低的计算成本实现高精度的光学光谱计算，并成功应用于双层 WSe2 和吡啶@MoS2 材料，计算结果与精确参考计算结果吻合良好。

Conclusion: 可以高效计算双层 WSe2 和吡啶@MoS2 的准粒子带隙和光学光谱，结果与精确参考计算一致。

Abstract: The combination of two-dimensional materials into heterostructures offers new
opportunities for the design of optoelectronic devices with tunable properties.
However, computing electronic and optical properties of such systems using
state-of-the-art methodology is challenging due to their large unit cells. This
is in particular so for highly-precise all-electron calculations within the
framework of many-body perturbation theory, which come with high computational
costs. Here, we extend an approach that allows for the efficient calculation of
the non-interacting polarizability, previously developed for planewave basis
sets, to the (linearized) augmented planewave (L)APW method. This approach is
based on an additive ansatz, which computes and superposes the polarizabilities
of the individual components in their respective unit cells. We implement this
formalism in the $G_0W_0$ module of the exciting code and implement an
analogous approach for BSE calculations. This allows the calculation of
highly-precise optical spectra at low cost. So-obtained results of the
quasi-particle band structure and optical spectra are demonstrated for bilayer
WSe$_2$ and pyridine@MoS$_2$ in comparison with exact reference calculations.

</details>


### [118] [Tuning chiral anomaly signature in a Dirac semimetal via fast-ion implantation](https://arxiv.org/abs/2507.17972)
*Manasi Mandal,Eunbi Rha,Abhijatmedhi Chotrattanapituk,Denisse Córdova Carrizales,Alexander Lygo,Kevin B. Woller,Mouyang Cheng,Ryotaro Okabe,Guomin Zhu,Kiran Mak,Chu-Liang Fu,Chuhang Liu,Lijun Wu,Yimei Zhu,Susanne Stemmer,Mingda Li*

Main category: cond-mat.mtrl-sci

TL;DR: 通过离子注入技术成功增强了Cd3As2薄膜中的负纵向磁阻效应，为手征电子学应用提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 为了测试高能物理学假说以及实现节能应用，利用Cd3As2作为平台，研究其手征反常特性。

Method: 结合使用加速器驱动的快离子注入和驱动理论的规划，以增强铌掺杂Cd3As2薄膜中的负纵向磁阻（NLMR），这是手征反常的标志。通过输运测量和透射电子显微镜表征NLMR和铌掺杂Cd3As2薄膜的结晶度。

Result: 表面掺杂和体掺杂的铌镉砷薄膜均保持了结晶度，其中表面掺杂薄膜在B=7T时表现出最大的NLMR，体掺杂薄膜在B=9T时表现出最大的NLMR，相比于原始Cd3As2薄膜（B=4T），最大NLMR的相对增强超过了100%。

Conclusion: 该研究证明了高能离子注入作为实现拓扑半金属手征电子学功能的实用途径的潜力。

Abstract: Cd$_3$As$_2$ is a prototypical Dirac semimetal that hosts a chiral anomaly
and thereby functions as a platform to test high-energy physics hypotheses and
to realize energy efficient applications. Here we use a combination of
accelerator-based fast ion implantation and theory-driven planning to enhance
the negative longitudinal magnetoresistance (NLMR)--a signature of a chiral
anomaly--in Nb-doped Cd$_3$As$_2$ thin films. High-energy ion implantation is
commonly used to investigate semiconductors and nuclear materials but is rarely
employed to study quantum materials. We use electrical transport and
transmission electron microscopy to characterize the NLMR and the crystallinity
of Nb-doped Cd$_3$As$_2$ thin films. We find surface-doped Nb-Cd$_3$As$_2$ thin
films display a maximum NLMR around $B = 7$ T and bulk-doped Nb-Cd$_3$As$_2$
thin films display a maximum NLMR over $B = 9$ T--all while maintaining
crystallinity. This is more than a 100% relative enhancement of the maximum
NLMR compared to pristine Cd$_3$As$_2$ thin films ($B = 4$ T). Our work
demonstrates the potential of high-energy ion implantation as a practical route
to realize chiralitronic functionalities in topological semimetals.

</details>


### [119] [Compositional Tuning in NaxAlB14 via Diffusion Control](https://arxiv.org/abs/2507.18008)
*Mihiro Hoshino,Suguru Iwasaki,Shigeto Hirai,Yoshihiko Ihara,Tohru Sugahara,Haruhiko Morito,Masaya Fujioka*

Main category: cond-mat.mtrl-sci

TL;DR: 通过高压扩散控制和后退火技术，成功制备了在宽钠浓度范围内具有均匀钠分布的NaxAlB14材料。研究发现，降低钠含量可提高电导率并缩小光学带隙，这与硼相关态和硼空位有关。该方法为调控硼化物材料的电子性质提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统方法在制备非化学计量NaxAlB14时常遇到的问题，如形成化学计量NaxAlB14和副产物，以及高压扩散控制（HPDC）可能产生的浓度梯度问题，本研究旨在探索一种新的方法来获得均匀的钠分布，并研究其对材料性质的影响。

Method: 使用高压扩散控制（HPDC）结合后退火处理，在NaxAlB14中实现均匀的钠分布，并通过控制钠的提取和诱导缺陷来调节其组成和电子性质。

Result: 随着钠含量的减少，NaxAlB14的电导率增加，光学带隙变窄。核磁共振（NMR）测量和密度泛函理论（DFT）计算表明，费米能级处的态密度增加，这与硼相关的带隙内态有关。硼空位是导致光学带隙缩小的关键因素。该研究成功地合成了具有可调组成和电子性质的亚稳态化合物。

Conclusion: 通过结合高压扩散控制（HPDC）和后退火，实现了NaxAlB14中Na的均匀分布，从而能够在宽范围的Na浓度下研究其结构和电子性质。该研究表明，扩散控制方法可用于合成具有可调组成和可调电子性质的亚稳态化合物，为设计功能性硼化物材料提供了基础。

Abstract: A uniform Na distribution in NaxAlB14 was achieved using high-pressure
diffusion control (HPDC), which promotes Na deintercalation through enhanced
diffusion under high pressure, combined with post-annealing. NaxAlB14 with a
non-stoichiometric Na composition is thermodynamically metastable, and
conventional solid-state reactions with adjusted starting compositions
typically result in the formation of stoichiometric NaAlB14 and side products.
While HPDC alone typically leads to concentration gradients, intentionally
halting the Na removal process before complete extraction, followed by
annealing, enabled a uniform composition across the bulk. This allowed
structural and electronic properties to be examined over a wide range of Na
concentrations. As Na content decreased, electrical conductivity increased, and
the optical band gap narrowed. NMR measurements showed an increase in the
density of states at the Fermi level, consistent with DFT calculations
predicting boron-related in-gap states. Boron vacancies at specific sites were
found to generate deep levels near the band gap center, which can explain
experimentally observed optical gap reduction. These results demonstrate that
diffusion-controlling methods can be effectively applied to synthesize
metastable compounds with tunable compositions in covalent frameworks.
Furthermore, they provide a foundation for designing functional boride-based
materials with adjustable electronic properties by controlling Na extraction
and inducing defect formation.

</details>


### [120] [Defect-Assisted Recombination in Semiconductors and Photovoltaic Device Parameters from First Principles](https://arxiv.org/abs/2507.18011)
*Jiban Kangsabanik,Kristian S. Thygesen*

Main category: cond-mat.mtrl-sci

TL;DR: 一种从第一性原理计算缺陷辅助 SRH 重组速率的新方法，用于评估缺陷对光伏材料的影响，并为开发缺陷耐受半导体提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 为了解决不完美半导体中缺陷辅助 Shockley-Read-Hall (SRH) 重组速率的计算问题，并为理解缺陷对光伏器件参数的影响提供理论基础，从而推动缺陷耐受半导体的发展和高性能光伏材料的计算发现。

Method: 提出了一种从第一性原理计算缺陷辅助 Shockley-Read-Hall (SRH) 重组速率的方法，该方法通过调用缺陷所有可能电荷态之间跃迁的速率方程的完整解，来考虑给定非平衡条件（分离的准费米能级）下的稳态重组动力学。同时，从第一性原理计算了由辐射和非辐射多声子发射过程引起的跃迁速率。

Result: 该方法已被应用于评估七种新兴光伏半导体中特定缺陷对光伏器件参数的影响，结果清晰地揭示了常用的重组动力学近似方法的局限性。

Conclusion: 该研究通过考虑缺陷的所有可能电荷态，为在非平衡条件下计算不完美半导体的缺陷辅助 Shockley-Read-Hall (SRH) 重组速率提供了一种新方法，从而对材料的太阳能电池器件参数进行评估，并为开发缺陷耐受半导体和计算发现高性能光伏材料提供了理论基础。

Abstract: We introduce a method to calculate defect-assisted Shockley-Read-Hall (SRH)
recombination rates in imperfect semiconductors from first principles. The
method accounts for the steady state recombination dynamics under given
non-equilibrium conditions (split quasi Fermi levels), by invoking a full
solution to the rate equations describing transitions across the band gap via
all possible charge states of the defect. Transition rates due to radiative and
non-radiative multi-phonon emission processes are calculated from first
principles. The method is used to evaluate the effect of selected defects on
the photovoltaic device parameters of seven emergent photovoltaic
semiconductors. These examples clearly highlight the limitations of commonly
employed approximations to the recombination dynamics. Our work advances the
description and understanding of defect-induced losses in photovoltaics and
provides a basis for developing the important concept of defect tolerant
semiconductors and to discover high-performance photovoltaic materials
computationally.

</details>


### [121] [Out-of-plane ferroelectricity, magnetoelectric coupling and persistent spin texture in two-dimensional multiferroics](https://arxiv.org/abs/2507.18018)
*Ying Zhou,Cheng-Ao Ji,Shuai Dong,Xuezeng Lu*

Main category: cond-mat.mtrl-sci

TL;DR: 通过外延应变诱导二维 Ruddlesden Popper 衍生物的面外铁电性，并实现铁电畴翻转驱动的磁有序旋转和自旋纹理变化，为下一代自旋电子器件提供了具有垂直极化、磁电耦合和可切换自旋纹理的二维多鐵材料。


<details>
  <summary>Details</summary>
Motivation: 现有二维多鐵材料中，结合鲁棒铁电性和强磁电耦合的体系非常罕见。为了开发适用于小型化磁电自旋轨道晶体管的二维多鐵材料，需要探索能够实现面外铁电性的新体系。

Method: 通过外延应变稳定剥离的二维 Ruddlesden Popper 衍生物中的面外铁电性，并研究了其铁电畴翻转对磁有序（包括磁相、磁耦合和自旋纹理）的影响。

Result: 外延应变能够稳定二维 Ruddlesden Popper 衍生物中的面外铁电性。在铁电畴翻转过程中，观察到伴随弱铁磁性的 90 度旋转。面外铁电相（Pc 相）表现出交替磁性，而面内铁电相（P21 相）则表现出全布里渊区带分裂，并伴有持续的自旋纹理，在相边界发生 90 度旋转。

Conclusion: 本研究展示了一种通过外延应变稳定剥离的二维 Ruddlesden Popper 衍生物中的面外铁电性的方法。在此基础上，实现了铁电畴与磁畴的耦合，并发现了由铁电畴翻转驱动的磁有序的 90 度旋转。此外，还揭示了两种铁电相（Pc 相和 P21 相）分别对应的磁序（交替磁性相和布里渊区带分裂）以及持续的自旋纹理。该研究为构建集成垂直极化、磁电耦合和可切换自旋纹理的二维多鐵材料提供了新途径，这些特性对于下一代自旋电子器件至关重要。

Abstract: Two dimensional multiferroics with out of plane ferroelectricity hold
significant promise for miniaturized magnetoelectric spin-orbit transistors,
yet systems combining robust ferroelectricity and strong magnetoelectric
coupling are exceedingly rare. Here, we demonstrate that epitaxial strain
stabilizes out of plane ferroelectricity in exfoliated two dimensional
Ruddlesden Popper derivatives. The hybrid improper ferroelectric Pc phase
transitions to a competing P21 phase with purely in plane polarization upon
switching, accompanied by a 90 degree rotation of weak ferromagnetism.
Crucially, the Pc phase exhibits altermagnetism, while P21 displays full
Brillouin zone band splitting, with persistent spin textures rotating 90 degree
at the phase boundary. This work establishes a pathway to engineer two
dimensional multiferroics that integrate vertical polarization, magnetoelectric
coupling, and switchable spin textures, key features for next generation
spintronic devices.

</details>


### [122] [Nitrogen-vacancy centre formation via local femto-second laser annealing of diamond](https://arxiv.org/abs/2507.18027)
*Davin Yue Ming Peng,Alexander J Healey,Rebecca Griffin,Benjamin Cumming,Hiroshi Abe,Takeshi Ohshima,Alastair Stacey,Brant C Gibson,Brett C Johnson,Philipp Reineck*

Main category: cond-mat.mtrl-sci

TL;DR: Femtosecond lasers can be used to fine-tune the creation of NV centers in diamond by controlling vacancy diffusion, which is important for quantum technologies.


<details>
  <summary>Details</summary>
Motivation: To understand and control the formation of nitrogen-vacancy (NV) centers in diamond, which are crucial for emerging quantum technologies, by investigating the role of femtosecond laser processing and its parameters.

Method: The research used femtosecond laser processing on nitrogen-doped diamond, systematically varying laser pulse energy and examining the effects on NV center formation and quality through photoluminescence and optically detected magnetic resonance measurements. They also investigated the influence of pre-existing defects from electron irradiation and defect evolution over time.

Result: The study examined the impact of laser pulse energy on NV production and quality, probed the role of pre-existing defects, and considered defect evolution. A key finding is the identification of a regime where the laser diffuses vacancies, acting as a local annealing mechanism.

Conclusion: The study identifies a processing regime where femtosecond lasers primarily diffuse vacancies, enabling local annealing and fine-tuning of defect populations for tailored nitrogen-vacancy (NV) center production.

Abstract: Emerging quantum technologies based on the nitrogen-vacancy (NV) centre in
diamond require carefully engineered material with controlled defect density,
optimised NV formation processes, and minimal crystal strain. The choice of NV
generation technique plays a crucial role in determining the quality and
performance of these centres. In this work, we investigate NV centre formation
in nitrogen-doped diamond using femtosecond (fs) laser processing. We
systematically examine the effect of laser pulse energy on NV production and
quality using photoluminescence and optically detected magnetic resonance
measurements. We also probe the role of pre-existing lattice defects formed by
electron irradiation and consider defect evolution over extended dwell times.
Finally, we are able to identify a regime where the main action of the fs-laser
is to diffuse rather than create vacancies. This local annealing capability
expands the toolkit for tailored NV production and presents opportunities for
fine tuning defect populations.

</details>


### [123] [Anomalous magnetoresistance in an antiferromagnetic Kagome semimetal heterostructures](https://arxiv.org/abs/2507.18069)
*Xionghua Liu,Qiyuan Feng,Weibin Cui,Hanjie Guo,Yubin Hou,Xiaomin Zhang,Yongcheng Deng,Dong Zhang,Jing Zhang,Qingyou Lu,Kaiyou Wang*

Main category: cond-mat.mtrl-sci

TL;DR: FeSn薄膜与Pt层结合，通过界面相互作用实现了自旋织构的调控，并发现了与拓扑相关的磁阻振荡现象。


<details>
  <summary>Details</summary>
Motivation: 为了在反铁磁Kagome半金属中实现拓扑自旋电子学的潜在应用，以及探索由拓扑、自旋和关联之间的量子相互作用引起的丰富物理现象。

Method: 通过引入重金属Pt层的界面Dzyaloshinskii Moriya相互作用，实现了FeSn薄膜中可调谐的自旋织构。

Result: 随着FeSn厚度的增加，可变的自旋织构导致霍尔电阻率和磁阻发生渐变。在较小的磁场下，薄FeSn-Pt样品中观察到一种非常规的、类似阻尼振荡的磁阻行为，这与通过磁力显微镜测量的特殊拓扑自旋织构有关。

Conclusion: FeSn薄膜中的可调谐自旋织构以及反常的磁阻行为，为理解Kagome反铁磁体中的新奇现象提供了新的视角。

Abstract: Antiferromagnetic Kagome semimetals have attracted tremendous attentions for
their potential application in antiferromagnetic topological spintronics.
Effectively manipulating Kagome antiferromagnetic states could reveal abundant
physical phenomena induced from quantum interactions between topology, spin,
and correlation. Here, we achieved tunable spin textures of FeSn thin films via
introducing interfacial Dzyaloshinskii Moriya interaction from heavy-metal Pt
overlayer. With increasing FeSn thickness, the variable spin textures result in
gradual change in Hall resistivity and magnetoresistance. Importantly, an
unconventional damped oscillatory-like behavior of magnetoresistance at
relatively low magnetic field can be observed in thin FeSn-Pt samples. This
oscillatory like magnetoresistance feature was confirmed to be related to the
special topological spin textures revealed by magnetic force microscopy
measurements. The formation of rich variety of topological spin textures in
association with exotic magneto-transport properties in antiferromagnetic
Kagome FeSn heterostructures offers new perspectives for understanding the
novel emergent phenomena in Kagome antiferromagnets.

</details>


### [124] [Exploring the functional properties of diamond-like quaternary compound Li$_2$ZnGeS$_4$ for potential energy applications: A theoretical approach](https://arxiv.org/abs/2507.18136)
*Celestine Lalengmawia,Michael T. Nunsanga,Saurav Suman,Zosiamliana Renthlei,Lalruat Sanga,Hani Laltlanmawii,Lalhriat Zuala,Shivraj Gurung,Amel Laref,Dibya Prakash Rai*

Main category: cond-mat.mtrl-sci

TL;DR: DFT研究表明Li2ZnGeS4具有优异的光电和压电性能，是光电器件和压电器件的理想候选材料


<details>
  <summary>Details</summary>
Motivation: 为了寻找适合能源生产和储存的宽带隙半导体材料，研究了金刚石类四元半导体Li2ZnGeS4

Method: 研究人员利用GGA和mGGA泛函，在密度泛函理论（DFT）框架内，研究了Li2ZnGeS4的电子、光学、力学和压电-力学性能

Result: 计算结果在定性方面与一些先前报道的数据一致，并通过了Born稳定性判据和分子动力学（MD）模拟的结构稳定性验证

Conclusion: Li2ZnGeS4是一种很有潜力的光电器件和压电器件材料

Abstract: It is anticipated that wide-bandgap semiconductors (WBGSs) would be useful
materials for energy production and storage. A well-synthesized, yet, scarcely
explored diamond-like quaternary semiconductor-Li$_2$ZnGeS$_4$ has been
considered for this work. Herein, we have employed two well-known functionals
GGA and mGGA within a frame-work of density functional theory (DFT). We have
explored the electronic, optical, mechanical, and piezo-electromechanical
properties. Our results are in qualitative agreement with some of the
previously reported data. The structural stabilities have been confirmed using
the Born stability criteria and Molecular-dynamic (MD) simulations. Based on
our findings, we claim that Li$_2$ZnGeS$_4$ is the most probable candidate for
optoelectronics and piezoelectric applications.

</details>


### [125] [Theory of Magnetization Temperature Dependence in Ferrimagnetics](https://arxiv.org/abs/2507.18209)
*Rostyslav O. Serha,Anna Pomyalov,Andrii V. Chumak,Victor S. L'vov*

Main category: cond-mat.mtrl-sci

TL;DR: 我们已将描述铁磁体的 M(T) 的理论框架扩展到亚铁磁体，并发现它与实验数据吻合良好。


<details>
  <summary>Details</summary>
Motivation: 最近，人们提出了一个成功的框架，用于在简单立方铁磁体 EuO 和 EuS 的整个温度范围内（从零到居里温度）从理论上描述 M(T)。我们已将此方法扩展到计算和分析多亚晶格共线亚铁磁体（如钇铁石榴石 Y3Fe5 O12）的 M(T)。

Method: 我们分析并推广了两种描述 M(T) 的近似方法，即布洛赫 3/2 定律（在低温极限下有效）和维斯的平均场近似（在 Tc 附近提供合理的描述）。使用单一调优参数，我们结合了这两种方法来描述从 0 到 Tc 的 M(T)。

Result: 我们计算并分析了 YIG 的 M(T)，发现它几乎在所有温度下都遵循平均场预测 $\sqrt{T_c - T }$。

Conclusion: 该理论结果与我们的测量结果以及先前可获得的整个温度范围内的实验数据吻合良好，并且表明实验和理论依赖关系 M(T) 几乎在所有温度下都遵循平均场预测 $\sqrt{T_c - T }$。

Abstract: Recent advancements in spintronics and fundamental physical research have
brought increased attention to the rare-earth-based magnetically ordered
materials. One of the important properties of these materials is the
temperature dependence of the spontaneous magnetization $M(T)$. Recently, a
successful framework was proposed for the theoretical description of M(T)
across the entire temperature range from zero to the Curie temperature in
simple cubic ferromagnetics, EuO and EuS. We extend this approach to compute
and analyze $M(T)$ for multi-sublattice collinear ferrimagnetics such as
Yttrium Iron Garnet $Y_3Fe_5 O_{12}$. We analyzed and generalized for
multi-sublattice collinear ferrimagnetics two well-known approximations
describing $M(T)$. The first approach is the Bloch-3/2 law, which describes the
suppression of $M(T)$ due to spin-wave excitation, and is valid in the
low-temperature limit $T << T_c$. The second one is Weiss's mean-field
approximation, which provides a reasonable description of $M(T)$ near $T_c$.
Using a single tuning parameter, we combine these two approaches to describe
$M(T)$ for any $0<T<T_c$. The theoretical result for $M(T)$ aligns well with
our measurements and the previously available experimental data across the
entire temperature range. We also demonstrate that experimental and theoretical
dependences $M(T)$ follow the mean-field prediction $\sqrt{T_c - T }$ for
almost all temperatures.

</details>


### [126] [Dis-GEN: Disordered crystal structure generation](https://arxiv.org/abs/2507.18275)
*Martin Hoffmann Petersen,Ruiming Zhu,Haiwen Dai,Savyasanchi Aggarwal,Nong Wei,Andy Paul Chen,Arghya Bhowmik,Juan Maria Garcia Lastra,Kedar Hippalgaonkar*

Main category: cond-mat.mtrl-sci

TL;DR: Dis-GEN是一个新的生成模型，可以处理无序无机晶体材料，并保持对称性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型无法处理无序无机晶体结构建模的复杂性，因为材料的物理和化学性质取决于原子如何分布在对称位点上。

Method: Dis-GEN是一个基于经验等变表示的生成模型，该表示源于晶体学理论方法论。它能够生成对称性一致且包含成分无序和空位的结构。

Result: Dis-GEN能够有效生成无序无机材料，并在生成过程中保持晶体学对称性。

Conclusion: Dis-GEN成功生成了无序无机材料，并在生成过程中保持了晶体学对称性。该方法为系统性地探索和发现无序功能材料提供了关键的检查点，扩大了生成模型在材料科学中的应用范围。

Abstract: A wide range of synthesized crystalline inorganic materials exhibit
compositional disorder, where multiple atomic species partially occupy the same
crystallographic site. As a result, the physical and chemical properties of
such materials are dependent on how the atomic species are distributed among
the corresponding symmetrical sites, making them exceptionally challenging to
model using computational methods. For this reason, existing generative models
cannot handle the complexities of disordered inorganic crystals. To address
this gap, we introduce Dis-GEN, a generative model based on an empirical
equivariant representation, derived from theoretical crystallography
methodology. Dis-GEN is capable of generating symmetry-consistent structures
that accommodate both compositional disorder and vacancies. The model is
uniquely trained on experimental structures from the Inorganic Crystal
Structure Database (ICSD) - the world's largest database of identified
inorganic crystal structures. We demonstrate that Dis-GEN can effectively
generate disordered inorganic materials while preserving crystallographic
symmetry throughout the generation process. This approach provides a critical
check point for the systematic exploration and discovery of disordered
functional materials, expanding the scope of generative modeling in materials
science.

</details>


### [127] [Antiferromagnetic Hall-Memristors](https://arxiv.org/abs/2507.18388)
*Gaspar De la Barrera,Alvaro S. Nunez*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了一种基于反铁磁材料和霍尔-忆阻效应的四端自旋忆阻器，利用非线性爱德尔斯坦效应进行读写，为实现更智能、更节能的计算系统提供了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 为了克服标准硅电子器件的局限性，并推动更智能、更节能的计算系统的发展，探索了基于自旋的记忆存储新材料。

Method: 通过对称性分析支持了非线性爱德尔斯坦效应的可行性，并以铜锰砷（CuMnAs）作为具体实例，研究了其可控的非线性霍尔效应。

Result: 成功将自旋忆阻器从两端器件扩展到四端器件，并利用非线性爱德尔斯坦效应实现了记忆的读写。

Conclusion: 所提出的基于反铁磁材料和霍尔-忆阻效应的自旋忆阻器，并利用非线性爱德尔斯坦效应作为读写器，将忆阻器从传统的两端器件扩展到四端器件，为更智能、更节能的计算系统提供了新的方向。

Abstract: Spin-memristors are a class of materials that can store memories through the
control of spins, potentially leading to novel technologies that address the
constraints of standard silicon electronics, thereby facilitating the
advancement of more intelligent and energy-efficient computing systems. In this
work, we present a spin-memristor based on antiferromagnetic materials that
exhibit Hall-memresistance. Moreover, the nonlinear Edelstein effect acts as
both a writer and eraser of memory registers. We provide a generic
symmetry-based analysis that supports the viability of the effect. To achieve a
concrete realization of these ideas, we focus on CuMnAs, which has been shown
to have a controllable nonlinear Hall effect. Our results extend the
two-terminal spin-memristor setting, which is customarily the standard type of
device in this context, to a four-terminal device.

</details>


### [128] [Efficient $GW$ band structure calculations using Gaussian basis functions and application to atomically thin transition-metal dichalcogenides](https://arxiv.org/abs/2507.18411)
*Rémi Pasquier,María Camarasa-Gómez,Anna-Sophia Hehn,Daniel Hernangómez-Pérez,Jan Wilhelm*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种新的 $GW$ 空间-时间算法，用于计算原子尺度二维材料的能带结构，计算精度高且速度快。


<details>
  <summary>Details</summary>
Motivation: 旨在实现周期性体系中精确且计算高效的准粒子能带结构计算，特别关注原子尺度二维过渡金属硫化物。

Method: 本文采用 $GW$ 空间-时间算法，并结合高斯基组和自旋-轨道耦合。

Result: 计算得到的单层 MoS$_{2}$、MoSe$_{2}$、WS$_{2}$ 和 WSe$_{2}$ 的 $GW$ 带隙与基于平面波的参考计算结果平均相差在 50 meV 以内，且计算速度快。

Conclusion: 该算法为原子尺度二维材料的 $GW$ 计算提供了一个高效且可扩展的框架。

Abstract: We present a $GW$ space-time algorithm for periodic systems in a Gaussian
basis including spin-orbit coupling. We employ lattice summation to compute the
irreducible density response and the self-energy, while we employ $k$-point
sampling for computing the screened Coulomb interaction. Our algorithm enables
accurate and computationally efficient quasiparticle band structure
calculations for atomically thin transition-metal dichalcogenides. For
monolayer MoS$_\text{2}$, MoSe$_\text{2}$, WS$_\text{2}$, and WSe$_\text{2}$,
computed $GW$ band gaps agree on average within 50~meV with plane-wave-based
reference calculations. $G_0W_0$ band structures are obtained in less than two
days on a laptop (Intel i5, 192 GB RAM) or in less than 30 minutes using 1024
cores. Overall, our work provides an efficient and scalable framework for $GW$
calculations on atomically thin materials.

</details>


### [129] [2D ferroelectricity accompanying antiferro-orbital order in semi-metallic WTe$_2$](https://arxiv.org/abs/2507.18438)
*Fangyuan Gu,Ruoshi Jiang,Wei Ku*

Main category: cond-mat.mtrl-sci

TL;DR: WTe2双层和三层结构中的铁电性得到了解释，这是一种伴随着强面外反常轨道序的弱面外铁电性。


<details>
  <summary>Details</summary>
Motivation: 解释了在WTe2双层和三层结构中，尽管极化很小，铁电性却能高达350 K的现象。

Method: 通过基于密度泛函的、多能量尺度的对称性破缺分析。

Result: 识别出一种伴随着强面外反常轨道序的弱面外铁电性，这种低能量关联源于更高能量的反铁电结构，从而解释了实验中令人费解的现象。

Conclusion: 该研究揭示了一种新的电子铁电性范式，适用于具有超快开关极化的二维极性金属，可用于下一代非易失性存储器和其他设备。

Abstract: The first switchable electric polarization in metals was recently discovered
in bilayer and trilayer WTe2. Strangely, despite the tininess of the ordered
polarization, the ferroelectricity survives up to 350 K, rendering the
mechanism of such ferroelectricity challenging for standard understandings.
Here, via a density-functional-based multi-energy-scale analysis of the
system's broken symmetries, we identify a weak out-of-plane ferroelectricity
accompanying a strong in-plane antiferro-orbital order. This unusual low-energy
correlation, which emerges from an antiferroelectric structure formed at much
higher energy, naturally explains the above puzzling observation. This result
reveals an unprecedented paradigm of electronic ferroelectricity generally
applicable to 2D polar metals with ultrafast-switchable polarization ideal for
the next-generation non-volatile memory and other devices.

</details>


### [130] [Active Δ-learning with universal potentials for global structure optimization](https://arxiv.org/abs/2507.18485)
*Joe Pitfield,Mads-Peter Verner Christiansen,Bjørk Hammer*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Universal machine learning interatomic potentials (uMLIPs) have recently been
formulated and shown to generalize well. When applied out-of-sample, further
data collection for improvement of the uMLIPs may, however, be required. In
this work we demonstrate that, whenever the envisaged use of the MLIPs is
global optimization, the data acquisition can follow an active learning scheme
in which a gradually updated uMLIP directs the finding of new structures, which
are subsequently evaluated at the density functional theory (DFT) level. In the
scheme, we augment foundation models using a {\Delta}-model based on this new
data using local SOAP-descriptors, Gaussian kernels, and a sparse Gaussian
Process Regression model. We compare the efficacy of the approach with
different global optimization algorithms, Random Structure Search, Basin
Hopping, a Bayesian approach with competitive candidates (GOFEE), and a replica
exchange formulation (REX). We further compare several foundation models,
CHGNet, MACE-MP0, and MACE-MPA. The test systems are silver-sulfur clusters and
sulfur-induced surface reconstructions on Ag(111) and Ag(100). Judged by the
fidelity of identifying global minima, active learning with GPR-based
{\Delta}-models appears to be a robust approach. Judged by the total CPU time
spent, the REX approach stands out as being the most efficient.

</details>


### [131] [Deep learning-enabled large-scale analysis of particle geometry-lithiation correlations in battery cathode materials](https://arxiv.org/abs/2507.18530)
*Binbin Lin,Luis J. Carrillo,Xiang-Long Peng,Wan-Xin Chen,David A. Santosb,Sarbajit Banerjeeb,Bai-Xiang Xu*

Main category: cond-mat.mtrl-sci

TL;DR: 深度学习模型用于分割V2O5纳米颗粒，并分析其几何特征与锂化相图的关系，以优化电池材料性能。


<details>
  <summary>Details</summary>
Motivation: 解决V2O5纳米颗粒分割的挑战性问题，并探究锂化V2O5纳米颗粒的化学成分与其几何特征之间的相关性，以氧化钒纳米颗粒为例，作为相变电池正极材料。

Method: 利用结合了奇异值分解技术和光谱数据库的深度学习模型，对V2O5纳米颗粒进行分割，生成能够捕捉锂化异质性的成分图和相图。

Result: 揭示了颗粒尺寸、长宽比、圆度、凸度和方向性等几何特征对锂化相图的定量影响。  

Conclusion: 通过优化颗粒几何形状，为改善相变锂电池材料的锂化均匀性和减少应力提供了策略。

Abstract: A deep learning model is employed to address the challenging problem of V2O5
nanoparticle segmentation and the correlation between the chemical composition
and the geometrical features of lithiated V2O5 nanoparticles as an exemplar of
a phase-transforming battery cathode material. First, the deep learning-enabled
segmentation model is integrated with the singular value decomposition
technique and a spectral database to generate accurate composition and phase
maps capturing lithiation heterogeneities as imaged using scanning transmission
X-ray microscopy. These phase maps act as the output properties for correlation
analysis. Subsequently, the quantitative influences of the geometrical features
of nanoparticles such as the particle size (i.e., projected perimeter and
area), the aspect ratio, circularity, convexity, and orientation on the
lithiation phase maps are revealed. These findings inform strategies to improve
lithiation uniformity and reduce stress in phase-transforming lithium battery
materials via optimized particle geometry.

</details>


### [132] [Programmable phase selection between altermagnetic and non-centrosymmetric polymorphs of MnTe on InP via molecular beam epitaxy](https://arxiv.org/abs/2507.18592)
*An-Hsi Chen,Parul R. Raghuvanshi,Jacob Cook,Michael Chilcote,Jason Lapano,Alessandro R. Mazza,Qiangsheng Lu,Sangsoo Kim,Yueh-Chun Wu,T. Zac Ward,Benjamin Lawrie,Guang Bian,James Burns,Jonathan D. Poplawsky,Myung-Geun Han,Yimei Zhu,Lucas Lindsay,Hu Miao,Robert G. Moore,Gyula Eres,Valentino R. Cooper,Matthew Brahlek*

Main category: cond-mat.mtrl-sci

TL;DR: 通过改变InP衬底表面，可以控制MnTe的相选择性。


<details>
  <summary>Details</summary>
Motivation: 在电子外延生长过程中选择近简并的晶体多晶形对于实现特定应用的物理性质至关重要。

Method: 研究了通过分子束外延（MBE）在InP衬底上生长MnTe的相选择性。

Result: 成功地在InP衬底的(111)A表面（In端 termination）上合成了六方NiAs结构（反铁磁性）的MnTe，在(111)B表面（P端 termination）上合成了立方ZnS结构（非中心对称，宽带隙）的MnTe。

Conclusion: 通过改变InP衬底的表面，可以控制MnTe的相选择性，从而实现具有不同物理性质的材料。

Abstract: Phase selecting nearly degenerate crystalline polymorphs during epitaxial
growth can be challenging yet is critical to targeting physical properties for
specific applications. Here, we establish how phase selectivity of
altermagnetic and non-centrosymmetric polymorphs of MnTe with high structural
quality and phase purity can be programmed by subtle changes to the surface of
lattice-matched InP substrates in molecular beam epitaxial (MBE) growth. Bulk
altermagnetic MnTe is thermodynamically stable in the hexagonal NiAs-structure
and is synthesized here on the (111)A surface (In-terminated) of InP, while the
non-centrosymmetric, cubic ZnS-structure with wide band gap (> 3eV) is
stabilized on the (111)B surface (P-terminated). Here we use electron
microscopy, photoemission spectroscopy, and reflection high-energy electron
diffraction, which together indicate that the phase selection is triggered at
the interface and proceeds along the growing surface. First principles
calculations suggest that interfacial termination and strain have a significant
effect on the interfacial energy; stabilizing the NiAs polymorph on the
In-terminated surface and the ZnS structure on the P-terminated surface.
Selectively grown, high-quality films of MnTe polymorphs are key platforms that
will enable our understanding of the novel properties of these materials,
thereby facilitating their use in new applications ranging from spintronics to
microelectronic devices.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [133] [PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy](https://arxiv.org/abs/2507.17846)
*Alison Bartsch,Arvind Car,Amir Barati Farimani*

Main category: cs.RO

TL;DR: A robotic system called PinchBot uses diffusion policy models to create pottery with pinch-based actions.


<details>
  <summary>Details</summary>
Motivation: To create a robotic system that can create simple pottery goals with only pinch-based actions, exploring the challenges of a highly multi-modal and long-horizon deformable manipulation task.

Method: PinchBot utilizes a goal-conditioned diffusion policy model combined with pre-trained 3D point cloud embeddings, task progress prediction, and collision-constrained action projection.

Result: The system is able to successfully create a variety of simple pottery goals.

Conclusion: PinchBot is a goal-conditioned diffusion policy model that can successfully create a variety of simple pottery goals using pinch-based actions.

Abstract: Pottery creation is a complicated art form that requires dexterous, precise
and delicate actions to slowly morph a block of clay to a meaningful, and often
useful 3D goal shape. In this work, we aim to create a robotic system that can
create simple pottery goals with only pinch-based actions. This pinch pottery
task allows us to explore the challenges of a highly multi-modal and
long-horizon deformable manipulation task. To this end, we present PinchBot, a
goal-conditioned diffusion policy model that when combined with pre-trained 3D
point cloud embeddings, task progress prediction and collision-constrained
action projection, is able to successfully create a variety of simple pottery
goals. For experimental videos and access to the demonstration dataset, please
visit our project website:
https://sites.google.com/andrew.cmu.edu/pinchbot/home.

</details>


### [134] [A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.17856)
*Dennis Benders,Laura Ferranti,Johannes Köhler*

Main category: cs.RO

TL;DR: 该报告提供了一个关于如何为移动机器人实现安全导航的NMPC方案的指南。


<details>
  <summary>Details</summary>
Motivation: 为机器人安全导航设计模型预测控制（MPC）方案，确保机器人遵守约束并避开障碍物，这是一项复杂但重要的任务。

Method: 提出了一种逐步实施非线性模型预测控制（NMPC）方案的方法，以确保移动机器人在干扰和测量噪声存在的情况下，能够遵守状态和输入约束并避开障碍物。

Result: 提供了一个从理论概念到数学证明和实施的实际且易于理解的途径，重点关注安全性和性能保证。

Conclusion: 该报告旨在为机器人导航中的非线性模型预测控制（NMPC）提供一个实用的实施方案，强调安全性和性能保证。

Abstract: Designing a Model Predictive Control (MPC) scheme that enables a mobile robot
to safely navigate through an obstacle-filled environment is a complicated yet
essential task in robotics. In this technical report, safety refers to ensuring
that the robot respects state and input constraints while avoiding collisions
with obstacles despite the presence of disturbances and measurement noise. This
report offers a step-by-step approach to implementing Nonlinear Model
Predictive Control (NMPC) schemes addressing these safety requirements.
Numerous books and survey papers provide comprehensive overviews of linear MPC
(LMPC) \cite{bemporad2007robust,kouvaritakis2016model}, NMPC
\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},
and their applications in various domains, including robotics
\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.
This report does not aim to replicate those exhaustive reviews. Instead, it
focuses specifically on NMPC as a foundation for safe mobile robot navigation.
The goal is to provide a practical and accessible path from theoretical
concepts to mathematical proofs and implementation, emphasizing safety and
performance guarantees. It is intended for researchers, robotics engineers, and
practitioners seeking to bridge the gap between theoretical NMPC formulations
and real-world robotic applications.
  This report is not necessarily meant to remain fixed over time. If someone
finds an error in the presented theory, please reach out via the given email
addresses. We are happy to update the document if necessary.

</details>


### [135] [OpenNav: Open-World Navigation with Multimodal Large Language Models](https://arxiv.org/abs/2507.18033)
*Mingfeng Yuan,Letian Wang,Steven L. Waslander*

Main category: cs.RO

TL;DR: 利用MLLMs和鸟瞰图值图，实现机器人对开放集导航指令的理解和执行，并在真实世界中得到验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器人导航和规划任务中，语言描述与机器人实际动作之间的鸿沟，特别是处理开放世界中超出预定义运动原语的复杂指令。

Method: 提出了一种基于MLLMs的零样本视觉-语言导航框架，该框架能够与视觉-语言感知模型交互，生成鸟瞰图值图，以整合语义知识和空间信息，从而增强机器人的空间理解能力。

Result: 在大型自动驾驶车辆数据集（AVDs）上验证了该框架在户外导航任务中的零样本能力，证明其能执行各种自由形式的自然语言导航指令，并对目标检测错误和语言歧义具有鲁棒性。此外，在Husky机器人上进行了室内外场景的测试，验证了其真实世界的鲁棒性和适用性。

Conclusion: 该研究通过多模态大语言模型（MLLMs）和鸟瞰图值图，使机器人能够理解和执行开放集指令的导航任务，并在模拟和真实机器人环境中得到验证。

Abstract: Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/

</details>


### [136] [Modular Robot and Landmark Localisation Using Relative Bearing Measurements](https://arxiv.org/abs/2507.18070)
*Behzad Zamani,Jochen Trumpf,Chris Manzie*

Main category: cs.RO

TL;DR: 提出了一种模块化非线性最小二乘滤波方法，用于处理由独立子系统组成的系统，并通过集成CI算法来处理信息重叠。该方法特别应用于机器人-地标定位问题，并在仿真研究中与单片方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 为了解决由独立子系统组成的系统滤波问题，并处理子系统之间的信息重叠问题。

Method: 提出了一种模块化非线性最小二乘滤波方法，用于处理由独立子系统组成的系统。通过独立更新每个子系统的状态和误差协方差估计，即使在相对测量同时依赖于多个子系统状态的情况下也是如此。该方法集成了协方差交集（CI）算法，以防止在子系统共享估计时重复计算信息。所提出的方法特别应用于机器人-地标定位问题，其中机器人姿态和地标位置的估计问题通过机器人姿态的SE（2）相对于固定地标位置的测量噪声（方位角）耦合。

Result: 该方法能够与单片联合状态滤波器进行基准测试，并展示了在降低通信和带宽要求下的性能特点。

Conclusion: 所提出的模块化方法在通信和带宽要求降低的情况下，能够实现性能的良好下降。

Abstract: In this paper we propose a modular nonlinear least squares filtering approach
for systems composed of independent subsystems. The state and error covariance
estimate of each subsystem is updated independently, even when a relative
measurement simultaneously depends on the states of multiple subsystems. We
integrate the Covariance Intersection (CI) algorithm as part of our solution in
order to prevent double counting of information when subsystems share estimates
with each other. An alternative derivation of the CI algorithm based on least
squares estimation makes this integration possible. We particularise the
proposed approach to the robot-landmark localization problem. In this problem,
noisy measurements of the bearing angle to a stationary landmark position
measured relative to the SE(2) pose of a moving robot couple the estimation
problems for the robot pose and the landmark position. In a randomized
simulation study, we benchmark the proposed modular method against a monolithic
joint state filter to elucidate their respective trade-offs. In this study we
also include variants of the proposed method that achieve a graceful
degradation of performance with reduced communication and bandwidth
requirements.

</details>


### [137] [A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion](https://arxiv.org/abs/2507.18138)
*Min-Gyu Kim,Dongyun Kang,Hajun Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 这篇论文提出了一种结合基于模型和基于学习的方法的混合方法，以提高机器人的运动能力。该方法通过集成残差模块来弥补模型中的不足，并在具有挑战性的环境中取得了更好的控制性能和学习效率。在真实的四足机器人上的实验也证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了实现稳健的运动，弥补模型失配引起的性能下降，提高控制性能和学习效率。

Method: 该方法将残差模块集成到基于模型的框架中，该框架包括启发式设计的步态规划器和动态模型，以弥补模型失配引起的性能下降。通过利用模块化结构并为每个残差模块选择合适的基于学习的方法来改进控制性能。

Result: 残差模块与模型预测控制相结合，在真实四足机器人上进行了演示，机器人成功地保持了平衡并跟踪了命令速度。

Conclusion: 该方法通过结合基于模型和基于学习的框架的优点，在具有高不确定性的环境中实现了稳健的运动，提高了控制性能和学习效率，并且使标称控制器对参数调整更加鲁棒。在真实四足机器人上的实验表明，该框架在不确定的情况下也能保持平衡并跟踪命令速度。

Abstract: This paper presents a novel approach that combines the advantages of both
model-based and learning-based frameworks to achieve robust locomotion. The
residual modules are integrated with each corresponding part of the model-based
framework, a footstep planner and dynamic model designed using heuristics, to
complement performance degradation caused by a model mismatch. By utilizing a
modular structure and selecting the appropriate learning-based method for each
residual module, our framework demonstrates improved control performance in
environments with high uncertainty, while also achieving higher learning
efficiency compared to baseline methods. Moreover, we observed that our
proposed methodology not only enhances control performance but also provides
additional benefits, such as making nominal controllers more robust to
parameter tuning. To investigate the feasibility of our framework, we
demonstrated residual modules combined with model predictive control in a real
quadrupedal robot. Despite uncertainties beyond the simulation, the robot
successfully maintains balance and tracks the commanded velocity.

</details>


### [138] [Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks](https://arxiv.org/abs/2507.18160)
*Luka Šiktar,Branimir Ćaran,Bojan Šekoranja,Marko Švaco*

Main category: cs.RO

TL;DR: 提出一个基于UAV和ROS2的搜救子系统，利用CNN进行人脸识别和跟踪，并通过PD控制器实现自主导航，已在实验中验证了其实时有效性。


<details>
  <summary>Details</summary>
Motivation: 为搜救任务开发一个集成UAV、目标检测、人脸识别和跟踪的子系统，以提高搜救效率和准确性。

Method: 提出一个集成UAV和ROS2框架的子系统，利用YOLOv11、YOLOv11-pose和dlib库中的CNN模型进行目标检测、人脸识别和跟踪。通过系统识别和PD控制器实现自主导航，并使用YOLOv11-pose关键点进行精确跟踪。

Result: 在14名已知个体上进行的初步实验表明，该子系统能够成功地进行实时操作，并在跟踪特定个体方面表现出良好性能。

Conclusion: 该子系统在搜索和救援任务中成功用于实时识别、识别人脸和跟踪特定个体，实验结果表明其有效性。

Abstract: In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),
for search and rescue missions, focusing on people detection, face recognition
and tracking of identified individuals. The proposed solution integrates a UAV
with ROS2 framework, that utilizes multiple convolutional neural networks (CNN)
for search missions. System identification and PD controller deployment are
performed for autonomous UAV navigation. The ROS2 environment utilizes the
YOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN
for face recognition. The system detects a specific individual, performs face
recognition and starts tracking. If the individual is not yet known, the UAV
operator can manually locate the person, save their facial image and
immediately initiate the tracking process. The tracking process relies on
specific keypoints identified on the human body using the YOLOv11-pose CNN
model. These keypoints are used to track a specific individual and maintain a
safe distance. To enhance accurate tracking, system identification is
performed, based on measurement data from the UAVs IMU. The identified system
parameters are used to design PD controllers that utilize YOLOv11-pose to
estimate the distance between the UAVs camera and the identified individual.
The initial experiments, conducted on 14 known individuals, demonstrated that
the proposed subsystem can be successfully used in real time. The next step
involves implementing the system on a large experimental UAV for field use and
integrating autonomous navigation with GPS-guided control for rescue operations
planning.

</details>


### [139] [MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation](https://arxiv.org/abs/2507.18206)
*Arup Kumar Sahoo,Itzik Klein*

Main category: cs.RO

TL;DR: MoRPI-PINN是一种用于移动机器人导航的物理信息神经网络，通过模仿蛇的运动来提高惯性导航的准确性，在真实实验中准确性提高了85%。


<details>
  <summary>Details</summary>
Motivation: 为了在没有卫星导航或摄像头的情况下实现移动机器人的完全自主导航，需要一种能够应对惯性传感器固有噪声和误差导致的导航解决方案漂移的方法。其中一种解决方案是通过模仿蛇的逶藟运动来增加惯性信号信噪比，从而实现移动机器人位置的回归。

Method: 提出了一种名为MoRPI-PINN的物理信息神经网络框架，通过将物理定律和约束嵌入训练过程来为基于惯性的移动机器人导航提供准确且鲁棒的导航解决方案。

Result: 与其它方法相比，使用真实世界的实验表明，MoRPI-PINN的准确性提高了85%以上。

Conclusion: MoRPI-PINN是一个轻量级的框架，可用于边缘设备，并可用于任何典型的移动机器人应用。

Abstract: A fundamental requirement for full autonomy in mobile robots is accurate
navigation even in situations where satellite navigation or cameras are
unavailable. In such practical situations, relying only on inertial sensors
will result in navigation solution drift due to the sensors' inherent noise and
error terms. One of the emerging solutions to mitigate drift is to maneuver the
robot in a snake-like slithering motion to increase the inertial
signal-to-noise ratio, allowing the regression of the mobile robot position. In
this work, we propose MoRPI-PINN as a physics-informed neural network framework
for accurate inertial-based mobile robot navigation. By embedding physical laws
and constraints into the training process, MoRPI-PINN is capable of providing
an accurate and robust navigation solution. Using real-world experiments, we
show accuracy improvements of over 85% compared to other approaches. MoRPI-PINN
is a lightweight approach that can be implemented even on edge devices and used
in any typical mobile robot application.

</details>


### [140] [Evaluation of facial landmark localization performance in a surgical setting](https://arxiv.org/abs/2507.18248)
*Ines Frajtag,Marko Švaco,Filip Šuligoj*

Main category: cs.RO

TL;DR: 该研究评估了在手术照明和可变角度下使用 MediaPipe 进行面部标志物检测的有效性，发现其准确性有所提高，但存在一些不精确之处。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，包括神经外科、眼科和整形外科，机器人技术和计算机视觉的应用越来越广泛，但面部检测算法在可变的照明条件和检测位置灵活性方面面临挑战，难以精确定位患者。

Method: 实验测试了 MediaPipe 算法在受控环境中的面部标志物检测，其中机器人手臂会自动调整位置，同时手术灯和模型保持固定。

Result: 所提出的实验结果表明，在手术照明下，面部标志物检测的准确性有所提高，从而在较大的偏航和俯仰角度下显著提高了检测性能。标准偏差/离散度的增加是由于对面部标志物检测不精确造成的。

Conclusion: 该分析讨论了将 MediaPipe 算法集成到医疗程序中的潜力。

Abstract: The use of robotics, computer vision, and their applications is becoming
increasingly widespread in various fields, including medicine. Many face
detection algorithms have found applications in neurosurgery, ophthalmology,
and plastic surgery. A common challenge in using these algorithms is variable
lighting conditions and the flexibility of detection positions to identify and
precisely localize patients. The proposed experiment tests the MediaPipe
algorithm for detecting facial landmarks in a controlled setting, using a
robotic arm that automatically adjusts positions while the surgical light and
the phantom remain in a fixed position. The results of this study demonstrate
that the improved accuracy of facial landmark detection under surgical lighting
significantly enhances the detection performance at larger yaw and pitch
angles. The increase in standard deviation/dispersion occurs due to imprecise
detection of selected facial landmarks. This analysis allows for a discussion
on the potential integration of the MediaPipe algorithm into medical
procedures.

</details>


### [141] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: ReSem3D通过结合MLLMs和VFMs，实现了细粒度的视觉基础和动态构建的层次化三维空间约束，以克服现有方法的局限性，并在各种环境中实现了高效、鲁棒的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在约束建模中存在语义粒度粗糙、缺乏实时闭环规划以及在语义多样化环境中鲁棒性受损等三个主要限制。为了解决这些挑战，提出ReSem3D，一个用于语义多样化环境的统一操作框架。

Method: ReSem3D框架通过多模态大语言模型（MLLMs）和视觉基础模型（VFMs）的协同作用，实现了细粒度的视觉基础和动态构建的层次化三维空间约束，以实现实时操作。该框架由MLLMs中的层次化递归推理驱动，与VFMs交互，通过两个阶段（部件级提取和区域级精炼）从自然语言指令和RGB-D观测中自动构建三维空间约束。这些约束随后被编码为联合空间中的实时优化目标，实现了对动态干扰的响应。

Result: ReSem3D在语义丰富的家居和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下能够执行各种操作任务，展现出强大的适应性和泛化能力。

Conclusion: ReSem3D框架在零样本条件下能够执行各种操作任务，展现出强大的适应性和泛化能力，在大规模模拟和真实世界的实验中均表现优异。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>


### [142] [Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding](https://arxiv.org/abs/2507.18276)
*Xiaojie Zhang,Yuanfei Wang,Ruihai Wu,Kunqi Xu,Yu Li,Liuyu Xiang,Hao Dong,Zhaofeng He*

Main category: cs.RO

TL;DR: AdaRPG框架通过利用基础模型来提取零件，从而提高对关节对象的操纵能力，并在各种对象上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的适应性关节对象操纵方法在跨类别泛化方面存在两个主要挑战：(1)真实世界关节对象的几何多样性使视觉感知和理解复杂化；(2)对象功能和机制的变化阻碍了统一的适应性操纵策略的发展。

Method: AdaRPG框架利用基础模型提取零件，以增强功能原始技能的视觉可供性泛化。它还利用基础模型中嵌入的常识来推理复杂的机制，并根据零件可供性推断生成调用原始技能函数的高层控制码。

Result: AdaRPG框架通过提取零件来增强视觉可供性泛化，并利用基础模型的常识来推理复杂机制，从而克服了现有方法的局限性。

Conclusion: AdaRPG框架在模拟和真实世界实验中展示了其在新颖的关节对象类别上的强大泛化能力。

Abstract: Articulated objects pose diverse manipulation challenges for robots. Since
their internal structures are not directly observable, robots must adaptively
explore and refine actions to generate successful manipulation trajectories.
While existing works have attempted cross-category generalization in adaptive
articulated object manipulation, two major challenges persist: (1) the
geometric diversity of real-world articulated objects complicates visual
perception and understanding, and (2) variations in object functions and
mechanisms hinder the development of a unified adaptive manipulation strategy.
To address these challenges, we propose AdaRPG, a novel framework that
leverages foundation models to extract object parts, which exhibit greater
local geometric similarity than entire objects, thereby enhancing visual
affordance generalization for functional primitive skills. To support this, we
construct a part-level affordance annotation dataset to train the affordance
model. Additionally, AdaRPG utilizes the common knowledge embedded in
foundation models to reason about complex mechanisms and generate high-level
control codes that invoke primitive skill functions based on part affordance
inference. Simulation and real-world experiments demonstrate AdaRPG's strong
generalization ability across novel articulated object categories.

</details>


### [143] [AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments](https://arxiv.org/abs/2507.18317)
*Chenglong Qian,Yang Xu,Xiufang Shi,Jiming Chen,Liang Li*

Main category: cs.RO

TL;DR: AF-RLIO是一种自适应融合方法，通过结合4D毫米波雷达、激光雷达、IMU和GPS，提高了机器人在复杂环境下的导航鲁棒性，特别是在有烟雾或隧道等干扰的情况下。


<details>
  <summary>Details</summary>
Motivation: 为了解决在烟雾、隧道和恶劣天气等复杂动态环境中，单传感器（如激光雷达或GPS）性能下降导致机器人姿态估计和导航不稳定、不安全的问题。

Method: 提出了一种名为AF-RLIO的自适应融合方法，该方法包括预处理模块、动态感知多模态里程计模块和因子图优化模块。预处理模块利用雷达数据去除动态点并检测环境恶化；动态感知多模态里程计模块使用迭代误差状态卡尔曼滤波器将雷达、激光雷达和IMU数据进行紧耦合；因子图优化模块则通过平衡里程计和GPS数据的权重来优化姿态估计。

Result: AF-RLIO方法在数据集和真实机器人环境中进行了评估，证明了其在烟雾和隧道等挑战性条件下的有效性，并且相比现有方法具有明显优势。

Conclusion: 该方法在烟雾和隧道等挑战性条件下，通过融合毫米波雷达、激光雷达、IMU和GPS数据，实现了鲁棒的机器人姿态估计和导航，并在真实机器人环境中进行了测试，证明了其有效性和优越性。

Abstract: In robotic navigation, maintaining precise pose estimation and navigation in
complex and dynamic environments is crucial. However, environmental challenges
such as smoke, tunnels, and adverse weather can significantly degrade the
performance of single-sensor systems like LiDAR or GPS, compromising the
overall stability and safety of autonomous robots. To address these challenges,
we propose AF-RLIO: an adaptive fusion approach that integrates 4D
millimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to
leverage the complementary strengths of these sensors for robust odometry
estimation in complex environments. Our method consists of three key modules.
Firstly, the pre-processing module utilizes radar data to assist LiDAR in
removing dynamic points and determining when environmental conditions are
degraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects
appropriate point cloud data for scan-to-map matching and tightly couples it
with the IMU using the Iterative Error State Kalman Filter. Lastly, the factor
graph optimization module balances weights between odometry and GPS data,
constructing a pose graph for optimization. The proposed approach has been
evaluated on datasets and tested in real-world robotic environments,
demonstrating its effectiveness and advantages over existing methods in
challenging conditions such as smoke and tunnels.

</details>


### [144] [G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM](https://arxiv.org/abs/2507.18344)
*Gyuhyeon Pak,Hae Min Cho,Euntai Kim*

Main category: cs.RO

TL;DR: G2S-ICP SLAM 是一种新的 SLAM 系统，使用高斯盘进行高保真重建和鲁棒的姿态跟踪，并在准确性和完整性方面优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 为了在实时操作中实现高保真 3D 重建和鲁棒的相机姿态跟踪。

Method: 提出了一种新颖的几何感知 RGB-D 高斯泼溅 SLAM 系统 G2S-ICP SLAM，通过使用高斯分布表示场景元素，并将其约束在局部切平面上。该方法将表面对齐的高斯盘集成到广义 ICP 框架中，引入各向异性协方差先验，并提出了一种几何感知损失来监督光度、深度和法线一致性。

Result: G2S-ICP SLAM 在 Replica 和 TUM-RGBD 数据集上的实验表明，该系统在保持渲染质量的同时，在定位精度和重建完整性方面优于先前的方法。

Conclusion: G2S-ICP SLAM 在定位精度、重建完整性和渲染质量方面优于先前的 SLAM 系统。

Abstract: In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting
SLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D
reconstruction and robust camera pose tracking in real-time by representing
each scene element using a Gaussian distribution constrained to the local
tangent plane. This effectively models the local surface as a 2D Gaussian disk
aligned with the underlying geometry, leading to more consistent depth
interpretation across multiple viewpoints compared to conventional 3D
ellipsoid-based representations with isotropic uncertainty. To integrate this
representation into the SLAM pipeline, we embed the surface-aligned Gaussian
disks into a Generalized ICP framework by introducing anisotropic covariance
prior without altering the underlying registration formulation. Furthermore we
propose a geometry-aware loss that supervises photometric, depth, and normal
consistency. Our system achieves real-time operation while preserving both
visual and geometric fidelity. Extensive experiments on the Replica and
TUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems
in terms of localization accuracy, reconstruction completeness, while
maintaining the rendering quality.

</details>


### [145] [Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input](https://arxiv.org/abs/2507.18396)
*Yonghao Fu,Cheng Hu,Haokun Xiong,Zhangpeng Bao,Wenyuan Du,Edoardo Ghignone,Michele Magno,Lei Xie,Hongye Su*

Main category: cs.RO

TL;DR: RKMPC通过结合LMPC和神经网络残差模型，在保证模型可解释性的同时，提高了车辆轨迹跟踪的精度和稳定性，且所需的训练数据更少。


<details>
  <summary>Details</summary>
Motivation: 传统的纯跟踪（PP）控制器未能考虑车辆模型约束，影响驾驶安全；而模型预测控制（MPC）的性能依赖于车辆模型的准确性，但传统建模方法难以平衡非线性动力学与计算效率。

Method: 提出了一种残差核模型预测控制（RKMPC）框架，结合了线性模型预测控制（LMPC）和基于神经网络的残差核模型预测控制（RKMPC），分别计算基线控制输入和补偿输入，最后将两者相加得到最终控制指令。

Result: RKMPC仅需传统核模型预测控制（KMPC）20%的训练数据即可实现更高的跟踪性能。与传统LMPC相比，RKMPC能将侧向误差减少11.7%-22.1%，航向误差减少8.9%-15.8%，并将前轮转向稳定性提高高达27.6%。

Conclusion: RKMPC通过结合LMPC的可靠性和神经网络的补偿能力，在保持模型可解释性的同时，显著提高了轨迹跟踪性能，并减少了对训练数据的需求。

Abstract: In vehicle trajectory tracking tasks, the simplest approach is the Pure
Pursuit (PP) Control. However, this single-point preview tracking strategy
fails to consider vehicle model constraints, compromising driving safety. Model
Predictive Control (MPC) as a widely adopted control method, optimizes control
actions by incorporating mechanistic models and physical constraints. While its
control performance critically depends on the accuracy of vehicle modeling.
Traditional vehicle modeling approaches face inherent trade-offs between
capturing nonlinear dynamics and maintaining computational efficiency, often
resulting in reduced control performance. To address these challenges, this
paper proposes Residual Koopman Model Predictive Control (RKMPC) framework.
This method uses two linear MPC architecture to calculate control inputs: a
Linear Model Predictive Control (LMPC) computes the baseline control input
based on the vehicle kinematic model, and a neural network-based RKMPC
calculates the compensation input. The final control command is obtained by
adding these two components. This design preserves the reliability and
interpretability of traditional mechanistic model while achieving performance
optimization through residual modeling. This method has been validated on the
Carsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH
racing car. Experimental results show that RKMPC requires only 20% of the
training data needed by traditional Koopman Model Predictive Control (KMPC)
while delivering superior tracking performance. Compared to traditional LMPC,
RKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by
8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The
implementation code is available at: https://github.com/ZJU-DDRX/Residual
Koopman.

</details>


### [146] [Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning](https://arxiv.org/abs/2507.18436)
*David Blanco-Mulero,Júlia Borràs,Carme Torras*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Robotic-assisted dressing has the potential to significantly aid both
patients as well as healthcare personnel, reducing the workload and improving
the efficiency in clinical settings. While substantial progress has been made
in robotic dressing assistance, prior works typically assume that garments are
already unfolded and ready for use. However, in medical applications gowns and
aprons are often stored in a folded configuration, requiring an additional
unfolding step. In this paper, we introduce the pre-dressing step, the process
of unfolding garments prior to assisted dressing. We leverage imitation
learning for learning three manipulation primitives, including both high and
low acceleration motions. In addition, we employ a visual classifier to
categorise the garment state as closed, partly opened, and fully opened. We
conduct an empirical evaluation of the learned manipulation primitives as well
as their combinations. Our results show that highly dynamic motions are not
effective for unfolding freshly unpacked garments, where the combination of
motions can efficiently enhance the opening configuration.

</details>


### [147] [A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots](https://arxiv.org/abs/2507.18462)
*Alghalya Al-Hajri,Ejmen Al-Ubejdij,Aiman Erbad,Ali Safa*

Main category: cs.RO

TL;DR: 该研究提出了一种新方法，利用压缩感知和字典学习优化机器人数据收集的采样轨迹，显著减少了行驶距离并提高了数据重建精度。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用压缩感知（CS）测量矩阵的结构来为机器人环境数据收集设计优化的采样轨迹。

Method: 提出了一种新颖的蒙特卡洛优化框架，并结合字典学习（DL）来获得数据驱动的稀疏变换，以优化采样轨迹。

Result: 所提出的方法可以将机器人行驶距离减少到完整覆盖路径的10%以下，同时将重建精度与基于DCT和多项式字典的传统CS方法相比提高了五倍以上，与先前提出的信息路径规划（IPP）方法相比提高了两倍。

Conclusion: 所提出的方法通过优化压缩感知（CS）测量矩阵来设计用于机器人环境数据收集的采样轨迹，并在NO2污染图重建实验中展示了其有效性。

Abstract: In recent years, Compressed Sensing (CS) has gained significant interest as a
technique for acquiring high-resolution sensory data using fewer measurements
than traditional Nyquist sampling requires. At the same time, autonomous
robotic platforms such as drones and rovers have become increasingly popular
tools for remote sensing and environmental monitoring tasks, including
measurements of temperature, humidity, and air quality. Within this context,
this paper presents, to the best of our knowledge, the first investigation into
how the structure of CS measurement matrices can be exploited to design
optimized sampling trajectories for robotic environmental data collection. We
propose a novel Monte Carlo optimization framework that generates measurement
matrices designed to minimize both the robot's traversal path length and the
signal reconstruction error within the CS framework. Central to our approach is
the application of Dictionary Learning (DL) to obtain a data-driven sparsifying
transform, which enhances reconstruction accuracy while further reducing the
number of samples that the robot needs to collect. We demonstrate the
effectiveness of our method through experiments reconstructing $NO_2$ pollution
maps over the Gulf region. The results indicate that our approach can reduce
robot travel distance to less than $10\%$ of a full-coverage path, while
improving reconstruction accuracy by over a factor of five compared to
traditional CS methods based on DCT and polynomial dictionaries, as well as by
a factor of two compared to previously-proposed Informative Path Planning (IPP)
methods.

</details>


### [148] [Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces](https://arxiv.org/abs/2507.18502)
*Sait Sovukluk,Grazia Zambella,Tobias Egle,Christian Ott*

Main category: cs.RO

TL;DR: 该研究比较了两种人形机器人全身体控制方法（ID-WBC 和 PB-WBC）的性能，在实际机器人任务中评估了它们的鲁棒性和优缺点。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于，尽管现有的控制方法在理想条件下预测稳定，但它们在面对关节摩擦、传感器噪声、未建模的外部干扰以及不完美的接触条件时的鲁棒性尚不明确。

Method: 该研究通过在人形机器人平台上进行实验，比较了逆动力学全身体控制（ID-WBC）和基于被动性的全身体控制（PB-WBC）这两种全身体控制方法。实验内容包括：摆荡脚的位置和方向控制、有无附加模型未知重量的下蹲、以及跳跃。

Result: 实验结果将两种控制方法的性能和特性差异与其控制方法联系起来，并强调了各自的优缺点。

Conclusion: 该研究实验性地比较了两种全身体控制方法（逆动力学全身体控制和基于被动性的全身体控制），并分析了它们在不同机器人任务中的性能。

Abstract: This paper studies the experimental comparison of two different whole-body
control formulations for humanoid robots: inverse dynamics whole-body control
(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers
fundamentally differ from each other as the first is formulated in task
acceleration space and the latter is in task force space with passivity
considerations. Even though both control methods predict stability under ideal
conditions in closed-loop dynamics, their robustness against joint friction,
sensor noise, unmodeled external disturbances, and non-perfect contact
conditions is not evident. Therefore, we analyze and experimentally compare the
two controllers on a humanoid robot platform through swing foot position and
orientation control, squatting with and without unmodeled additional weights,
and jumping. We also relate the observed performance and characteristic
differences with the controller formulations and highlight each controller's
advantages and disadvantages.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [149] [Squeezing enhanced sensing at an exceptional point](https://arxiv.org/abs/2507.17961)
*Changqing Wang,Deyuan Hu,Silvia Zorzetti,Anna Grassellino,Alexander Romanenko,Zheshen Zhang*

Main category: quant-ph

TL;DR: 将量子压缩态和非厄米简并结合起来，在特定条件下可实现超高精度测量。


<details>
  <summary>Details</summary>
Motivation: 为了推动测量精度的界限，将压缩态和非厄米简并这两种提高测量精度的非经典资源结合起来，但它们的结合具有挑战性。

Method: 提出一个统一压缩态和非厄米简并效应的通用量子传感框架，利用参数振荡阈值和奇异点来增强灵敏度。

Result: 在参数振荡阈值和奇异点处，传感精度与扰动强度的标度呈现出独特的四次方关系，该结果可推广到多模压缩态传感器和高阶奇异点。

Conclusion: 该研究提出了一个将压缩态和非厄米简并相结合的通用量子传感框架，在临界点实现了对扰动强度四次方的精度标度，并推广到多模压缩态传感器和高阶简并点。

Abstract: Pushing the boundaries of measurement precision is central for sensing and
metrology, pursued by nonclassical resources such as squeezing, and
non-Hermitian degeneracies with distinct spectral response. Their convergence,
however, remains challenging. We find extraordinary enhancement of sensitivity
by unifying both effects in a general framework for quantum sensing in open
systems. At the parametric oscillation threshold and an exceptional point, the
sensing precision exhibits a unique quartic scaling with the perturbation
strength. The result generalizes to multimode squeezed-state sensors with
higher-order exceptional points catered to various quantum sensing platforms.

</details>


### [150] [Improved Quantum Sensing by Spectral Design](https://arxiv.org/abs/2507.17828)
*Paul Aigner,Wolfgang Dür*

Main category: quant-ph

TL;DR: 研究通过设计印记哈密顿量的有效频谱来改进参数估计，并证明该方法可以显著提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 研究单一控制如何通过设计印记哈密顿量的有效频谱来改进参数估计

Method: 通过设计印记哈密顿量的有效频谱来实现，该方法将谱操作简化为一系列基本切换操作，并展示了可以实现任意期望的能级相对间距。

Result: 任何修改后的频谱都可以表示为原始特征值的凸组合，其中凸权重构成双随机矩阵，并通过几个单参数估计示例证明了光谱工程方法可显著提高估计精度

Conclusion: 通过光谱工程方法可显著提高参数估计的精度

Abstract: We investigate how unitary control can improve parameter estimation by
designing the effective spectrum of the imprinting Hamiltonian. We show that,
for commuting Hamiltonians, the general problem of spectral manipulation via
unitary control simplifies to a finite sequence of elementary switching
operations. Furthermore, we demonstrate that any desired relative spacing of
energy levels can be achieved. We also show that any modified spectrum can be
expressed as a convex combination of the original eigenvalues, with the convex
weights forming a bi-stochastic matrix. Through several single-parameter
estimation examples, we demonstrate that our spectral engineering method
substantially enhances estimation accuracy.

</details>


### [151] [Resource-Efficient Simulations of Particle Scattering on a Digital Quantum Computer](https://arxiv.org/abs/2507.17832)
*Yahui Chai,Joe Gibbs,Vincent R. Pascuzzi,Zoë Holmes,Stefan Kühn,Francesco Tacchino,Ivano Tavernelli*

Main category: quant-ph

TL;DR: Methods for simulating particle wave packet scattering in the Thirring model on quantum computers are developed, achieving significant circuit depth reduction and accurate simulations up to 80 qubits.


<details>
  <summary>Details</summary>
Motivation: To simulate the scattering of particle wave packets in the interacting Thirring model on digital quantum computers, addressing challenges with contemporary quantum processors.

Method: Develop and demonstrate methods for simulating scattering of particle wave packets in the interacting Thirring model on digital quantum computers using low-entanglement time slices and tensor networks. Exploit circuit compression based on matrix product state techniques and utilize zero-noise extrapolation with Pauli twirling for accurate simulations on quantum hardware.

Result: Achieve an average reduction of 3.2 in circuit depth compared to conventional approaches, enabling longer evolution times with higher fidelity. Accurately simulate full scattering dynamics on 40 qubits and demonstrate wave packet state-preparation on 80 qubits using zero-noise extrapolation and Pauli twirling.

Conclusion: We demonstrate accurate simulations of the interacting Thirring model

Abstract: We develop and demonstrate methods for simulating the scattering of particle
wave packets in the interacting Thirring model on digital quantum computers,
with hardware implementations on up to 80 qubits. We identify low-entanglement
time slices of the scattering dynamics and exploit their efficient
representation by tensor networks. Circuit compression based on matrix product
state techniques yields on average a reduction by a factor of 3.2 in circuit
depth compared to conventional approaches, allowing longer evolution times to
be evaluated with higher fidelity on contemporary quantum processors. Utilizing
zero-noise extrapolation in combination with Pauli twirling, on quantum
hardware we accurately simulate the full scattering dynamics on 40 qubits, and
further demonstrate the wave packet state-preparation on 80 qubits.

</details>


### [152] [Real-time analog circuit for auto-correlative weak-value amplification in the time domain](https://arxiv.org/abs/2507.18180)
*Jing-Hui Huang,Guang-Jun Wang,Xiang-Yun Hu*

Main category: quant-ph

TL;DR: AWVA技术通过使用AD835乘法器和NE5532运算放大器实现的实时模拟电路，相比SWVA在量子参数估计中具有更高的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了在实时参数估计中提高精度，AWVA需要额外的资源（实时乘法和积分模块）。

Method: 实现了AWVA的实时模拟电路，使用了AD835乘法器和NE5532运算放大器。

Result: 在-12 dB < SNR < -4 dB的信噪比下，AWVA相比SWVA具有更高的精度和优越的抗噪声能力，并且对于200 Hz < f < 20kHz的高斯指针具有足够的灵敏度。

Conclusion: AWVA技术相比SWVA技术在量子参数估计方面具有更高的精度和更优越的鲁棒性，并且该电路可应用于其他检测方案。

Abstract: The auto-correlative weak-value amplification (AWVA) technique demonstrates
distinct advantages over standard weak-value amplification (SWVA) for quantum
parameter estimation. To achieve enhanced precision in real-time parameter
estimation, the AWVA requires additional resources compared to SWVA, namely
real-time multiplication and integrator modules. We implemented a real-time
analog circuit for AWVA using an AD835 multiplier and an NE5532 operational
amplifier for the integrator. The circuit was tested using Gaussian pointers in
the AWVA scheme, exhibiting sufficient sensitivity for Gaussian pointers with
frequencies 200 Hz < f < 20kHz. Compared to SWVA, AWVA achieves higher accuracy
and superior robustness against noise at signal-to-noise ratios (SNRs) of -12
dB < SNR < -4 dB. Beyond quantum metrology, the circuit is applicable to
diverse detection schemes for correlated signals.

</details>


### [153] [Dynamics of colored-noise-driven stochastic Schrödinger equations](https://arxiv.org/abs/2507.17864)
*Pietro De Checchi,Federico Gallina,Barbara Fresch,Giulio G. Giusteri*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, we study the effect of colored stochastic noise as a source of
fluctuations in the dynamics of a two-level system, e.g. the states of a qubit
system or two local sites in a transfer problem. We derive the stochastic
Schr\"odinger equations (SSE) and related quantum master equations (QME) for
the average density matrix for different stochastic potentials. We compare the
case of memory and memoryless processes, which reflect different short-time
behaviors and, in some cases, can lead to different stationary distributions of
the average system state. Focusing on the use of an Ornstein-Uhlenbeck coloured
noise driving the dynamics, in the same fashion as the white noise is the
formal derivative of a Wiener process, we shed light on how different
dissipative terms of a generic QME arise depending on the nature of the
stochastic potential involved, and their effect on the short and long time
evolution of the system. We rationalize the emergence of the different terms in
terms of the time- and frequency-dependent coefficients Redfield QME. Within
this framework, we explicitly derive a closure model for the open terms in the
QME derived from the SSE approach, and clarify how colored noise impacts the
coherence relaxation time scales

</details>


### [154] [Shallow quantum circuit for generating O(1)-entanged approximate state designs](https://arxiv.org/abs/2507.17871)
*Wonjun Lee,Minki Hhan,Gil Young Cho,Hyukjoon Kwon*

Main category: quant-ph

TL;DR: 研究发现了低纠缠、低魔量、低相干性的近似状态 $t$-设计量子态，并提出了一种高效的无辅助比特浅层量子电路生成方法，在经典影拓扑学等应用中具有优势。


<details>
  <summary>Details</summary>
Motivation: 随机量子态在量子信息科学中有多种应用，包括量子密码学、量子模拟和量子设备基准测试。本研究旨在发现具有极低纠缠、魔量和相干性的 $\epsilon$-近似状态 $t$-设计，并提出一种高效的生成方法。

Method: 研究人员发现了一种新的量子态集合，它们是 $\epsilon$-近似状态 $t$-设计，同时具有极低的纠缠、魔量和相干性。他们证明了这些资源可以达到理论下界 $\Omegavert
u_e_g_i_o_n_t_a_l_(_	ext{log}vert
u_e_g_i_o_n_t_a_l_(_t/	extepsilon_)vert
u_e_g_i_o_n_t_a_l_(_)$，并且不随系统尺寸 $n$ 扩展（即 $O(1)$）。为了生成这些状态，研究人员开发了一种无辅助比特的浅层量子电路，该电路通过一系列多控制门将 $k$ 量子比特的近似状态设计转换为 $n$ 量子比特的设计，且不增加支持集大小。此外，研究人员还提出了一种使用 $O(1)$-纠缠估计器的经典影拓扑学。

Result: 该研究发现了新的量子态集合，其纠缠、魔量和相干性达到理论下界，并且不随系统尺寸扩展。成功构建了无辅助比特的浅层量子电路，实现了高效生成。将此方法应用于经典影拓扑学，可缩短运行时间。

Conclusion: 该研究发现了一种新的量子态集合，它们是 $\epsilon$-近似状态 $t$-设计，同时具有极低的纠缠、魔量和相干性。这些资源可以达到理论下界 $\Omegavert
u_e_g_i_o_n_t_a_l_(_	ext{log}vert
u_e_g_i_o_n_t_a_l_(_t/	extepsilon_)vert
u_e_g_i_o_n_t_a_l_(_)vert
u_e_g_i_o_n_t_a_l_(_)$，并且不随系统尺寸 $n$ 扩展。研究提出了一种无辅助比特的浅层量子电路来生成这些状态，其深度为 $Overt
u_e_g_i_o_n_t_a_l_(_tvert
u_e_g_i_o_n_t_a_l_(_[	ext{log}vert
u_e_g_i_o_n_t_a_l_(_tvert
u_e_g_i_o_n_t_a_l_(_)]^3vert
u_e_g_i_o_n_t_a_l_(_	ext{log}vert
u_e_g_i_o_n_t_a_l_(_nvert
u_e_g_i_o_n_t_a_l_(_)vert
u_e_g_i_o_n_t_a_l_(_	ext{log}vert
u_e_g_i_o_n_t_a_l_(_1/	extepsilon_)vert
u_e_g_i_o_n_t_a_l_(_)$。该方法比现有无辅助量子比特的算法更高效。最后，研究提出了使用 $O(1)$-纠缠估计器的经典影拓扑学，相比传统方案可以缩短运行时间。

Abstract: Random quantum states have various applications in quantum information
science, including quantum cryptography, quantum simulation, and benchmarking
quantum devices. In this work, we discover a new ensemble of quantum states
that serve as an $\epsilon$-approximate state $t$-design while possessing
extremely low entanglement, magic, and coherence. We show that those resources
such quantum states can reach their theoretical lower bounds, $\Omega\left(\log
(t/\epsilon)\right)$, which are also proven in this work. This implies that for
fixed $t$ and $\epsilon$, those resources do not scale with the system size,
i.e., $O(1)$ with respect to the total number of qubits $n$ in the system.
Moreover, we explicitly construct an ancilla-free shallow quantum circuit for
generating such states. To this end, we develop an algorithm that transforms
$k$-qubit approximate state designs into $n$-qubit ones through a sequence of
multi-controlled gates, without increasing the support size. The depth of such
a quantum circuit is $O\left(t [\log t]^3 \log n \log(1/\epsilon)\right)$,
which is the most efficient among existing algorithms without ancilla qubits. A
class of shallow quantum circuits proposed in our work offers reduced cost for
classical simulation of random quantum states, leading to potential
applications in various quantum information processing tasks. As a concrete
example for demonstrating utility of our algorithm, we propose classical shadow
tomography using an $O(1)$-entangled estimator, which can achieve shorter
runtime compared to conventional schemes.

</details>


### [155] [Stability of Continuous Time Quantum Walks in Complex Networks](https://arxiv.org/abs/2507.17880)
*Adithya L J,Johannes Nokkala,Jyrki Piilo,Chandrakala Meena*

Main category: quant-ph

TL;DR: 研究了量子行走在不同网络下的稳定性。结果发现，无标度网络比圈网络更稳定。中心性影响稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间量子行走（CTQWs）在不同网络拓扑和退相干机制下的稳定性，定义稳定性为系统随时间保持量子性质的能力。

Method: 我们采用节点占有率、相干性的l1范数、与初始状态的保真度、量子-经典距离和冯诺依曼熵等多种度量标准来评估量子稳定性。

Result: 研究结果表明，网络拓扑和退相干机制的相互作用会影响相干性的保持。内禀退相干导致相干性衰减最慢，其次是Haken-Strobl噪声，而量子随机游走（QSW）则导致相干性最快丧失。根据所使用的退相干模型和量化指标，网络拓扑的稳定性排序会有所不同。例如，在Haken-Strobl和内禀退相干下，量子-经典距离将圈网络排在比无标度网络更稳定的位置，尽管其他指标一致认为无标度网络更优。

Conclusion: 总的来说，异构网络（如图和无标度网络）表现出最高的稳定性，而像圈和Erd
H{o}s-R
uevo{}enyi网络这样的同质拓扑结构则更容易受到退相干的影响。然而，完全图尽管是同质的，但由于其密集的连接性，仍然非常稳定。此外，在异构网络中，初始化节点的中心性（通过度或紧密度衡量）对稳定性有显著影响，这突显了局部拓扑特征在量子动力学中的作用。

Abstract: We investigate the stability of continuous time quantum walks (CTQWs) in a
range of network topologies under different decoherence mechanisms, defining
stability as the system's ability to preserve quantum properties over time. The
networks studied range from homogeneous to heterogeneous structures, including
cycle, complete, Erd\H{o}s-R\'enyi, small-world, scale-free, and star
topologies. The decoherence models considered are intrinsic decoherence,
Haken-Strobl noise, and quantum stochastic walks (QSWs). To assess quantum
stability, we employ several metrics: node occupation probabilities, the
$\ell_1$-norm of coherence, fidelity with the initial state, quantum-classical
distance, and von Neumann entropy. Our results reveal that the interplay of
both network topology and decoherence model influences coherence preservation.
Intrinsic decoherence results in the slowest decay of coherence, followed by
Haken-Strobl noise, while QSW causes the most rapid loss of coherence. The
stability ranking among network topologies varies depending on the decoherence
model and quantifier used. For example, under Haken-Strobl and intrinsic
decoherence, the quantum-classical distance ranks the cycle network more stable
than scale-free networks, although other metrics consistently favour scale-free
topologies. In general, heterogeneous networks, such as star and scale-free
networks, exhibit the highest stability, whereas homogeneous topologies, such
as cycle and Erd\H{o}s-R\'enyi networks, are more vulnerable to decoherence.
The complete graph, despite its homogeneity, remains highly stable due to its
dense connectivity. Furthermore, in heterogeneous networks, the centrality of
the initialised node, measured by degree or closeness, has a pronounced impact
on stability, underscoring the role of local topological features in quantum
dynamics.

</details>


### [156] [Quantum Fisher Information in Curved Spacetime: Dirac Particles in Noisy Channels around a Schwarzschild Black Hole](https://arxiv.org/abs/2507.17901)
*Cookey Iyen,Muhammad Sanusi Liman,Benedict O. Ayomanor,Emem-obong Solomon James,Yame Mwanzang Philemon,Babatunde James Falaye*

Main category: quant-ph

TL;DR: 本研究评估了在黑洞时空中，压缩和阻尼噪声对三量子比特纠缠系统量子费舍尔信息（QFI）的影响。结果表明，强压缩可增强QFI对温度波动的鲁棒性，减缓退相干效应。


<details>
  <summary>Details</summary>
Motivation: 量子信息处理虽有优势但易受退相干影响，本研究旨在探究在弯曲时空中，不同噪声信道和压缩效应对量子Fisher信息（QFI）作为纠缠和参数估计诊断工具行为的影响。

Method: 研究了在史瓦西黑洞弯曲时空中，三量子比特纠缠狄拉克系统在耗散噪声信道（包括广义幅度阻尼SGAD、GAD和AD信道）影响下的量子费舍尔信息（QFI）行为，并分析了纠缠权重（θ）和相位（φ）参数的QFI。

Result: 强压缩（r=1）下，QFI随纠缠权重（θ）的变化对霍金温度（TH）不敏感，但随信道温度（TC）升高而衰减，且衰减速度慢于r=0。QFI随相位（φ）的变化在TC=2时出现瞬态尖峰，不受TH影响。GAD和AD信道也显示TC是主要的退相干来源。

Conclusion: 量子信息处理在克服退相干效应方面展现出一定潜力，特别是在强压缩（r=1）下，量子费舍尔信息（QFI）在面对霍金温度（TH）变化时表现出对纠缠比率的鲁棒性，并且相较于r=0，随着信道温度（TC）的升高，QFI的衰减速度显著减慢，表明压缩可作为一种误差缓解策略。对于相位参数，在TC=2时观察到的瞬态尖峰现象可能源于热共振或非单调退相干，且不受TH影响。GAD和AD信道也呈现相似的趋势，其中TC是主要的退相干来源。

Abstract: Quantum information processing promises significant advantages over classical
methods but remains vulnerable to decoherence induced by environmental
interactions and spacetime effects. This work investigates the behavior of
Quantum Fisher Information (QFI) as a diagnostic tool for entanglement and
parameter estimation in a three-qubit entangled Dirac system subjected to
dissipative noisy channels in the curved spacetime of a Schwarzschild black
hole. In particular, we examine the influence of the squeezed generalized
amplitude damping (SGAD) channel, along with its subchannels -- generalized
amplitude damping (GAD) and amplitude damping (AD) -- on the QFI with respect
to entanglement weight ($\theta$) and phase ($\phi$) parameters. Our results
show that under strong squeezing ($r = 1$), the QFI with respect to $\theta$
becomes completely resistant to variations in the Hawking temperature ($T_H$),
while still exhibiting degradation with increasing channel temperature ($T_C$).
The QFI decay is significantly slower at $r = 1$ compared to $r = 0$,
suggesting that squeezing can function as an error mitigation strategy. For QFI
with respect to $\phi$, a transient spike is observed at $T_C = 2$, potentially
due to thermal resonance or non-monotonic decoherence, and this behavior is
unaffected by $T_H$. Similar patterns are noted in the GAD and AD channels,
where $T_C$ consistently dominates as the principal source of decoherence.
Overall, the results highlight the intricate interplay between environmental
noise, relativistic effects, and quantum error resilience in curved spacetime.

</details>


### [157] [Quantum Machine Learning Playground](https://arxiv.org/abs/2507.17931)
*Pascal Debus,Sebastian Issel,Kilian Tscharke*

Main category: quant-ph

TL;DR: 本文介绍了一个QML可视化工具，旨在降低学习门槛，促进QML发展。


<details>
  <summary>Details</summary>
Motivation: 受到TensorFlow Playground等经典机器学习可视化工具的成功启发，旨在弥合QML领域可视化资源的差距，降低量子计算的入门门槛，并鼓励该领域的进一步创新。

Method: 通过结合数据重新上传通用量子分类器（作为代表性QML模型）的常见可视化隐喻，设计了一个具体的交互式Web应用程序实现。

Result: 开发了一个创新的交互式可视化工具，并设计了一个具体的交互式Web应用程序实现，作为第一个版本的量子机器学习游乐场，用于学习和探索QML模型。

Conclusion: 该文章介绍了一个创新的交互式可视化工具，旨在揭开量子机器学习（QML）算法的神秘面纱。该工具的开发受到了TensorFlow Playground等经典机器学习可视化工具的成功启发，旨在弥合QML领域可视化资源的差距。文章对量子计算和经典机器学习的相关可视化隐喻进行了全面概述，提出了算法可视化概念，并设计了一个具体的交互式Web应用程序实现。通过结合数据重新上传通用量子分类器（作为代表性QML模型）的常见可视化隐喻，该文章旨在降低量子计算的入门门槛，并鼓励该领域的进一步创新。附带的交互式应用程序是第一个版本的量子机器学习游乐场，用于学习和探索QML模型。

Abstract: This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.

</details>


### [158] [Tensorial Spin-Phonon Relaxation Reveals Mode-Selective Relaxation Pathways in a Single-Molecule Magnet](https://arxiv.org/abs/2507.17910)
*Roman Dmitriev,Nosheen Younas,Yu Zhang,Andrei Piryatinski,Eric R. Bittner*

Main category: quant-ph

TL;DR: 通过第一性原理计算，揭示了分子量子比特中退相干的模选择性，并成功预测了自旋弛豫时间，为分子量子比特的设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 理解和控制分子量子比特中的自旋弛豫对于开发化学可调量子信息平台至关重要。

Method: 本研究提出了一种全第一性原理框架，结合密度泛函理论和模分辨开系统形式主义，通过有限差分 g-张量评估自旋哈密顿量在振动法线模式下的展开以及线性和二次自旋-声子耦合张量，从而计算单分子磁体 VOPc(OH)8 的自旋弛豫张量。该理论捕捉了直接（单声子）和共振拉曼（双声子）弛豫过程，并将计算出的弛豫张量代入 Lindblad 型量子主方程。

Result: 本研究通过第一性原理计算，发现仅三个振动模式主导纵向（T1）弛豫，而单个模式贡献了大部分横向（T2）弛豫。计算出的弛豫时间与实验测量值高度一致，无需经验拟合，证明了该理论框架的准确性和预测能力。

Conclusion: 自旋弛豫的计算和控制对于开发化学可调量子信息平台至关重要。本研究提出的全第一性原理框架通过结合密度泛函理论和模分辨开系统形式主义，成功计算了单分子磁体 VOPc(OH)8 的自旋弛豫张量。该框架通过有限差分 g-张量评估了自旋哈密顿量在振动法线模式下的展开以及线性和二次自旋-声子耦合张量，构建了包含在 Lindblad 型量子主方程中的弛豫张量。该理论不仅包含了直接（单声子）和共振拉曼（双声子）弛豫过程，还揭示了高度的模选择性：仅三个振动模式主导纵向（T1）退相干，而单个模式则贡献了大部分横向（T2）弛豫。计算出的弛豫时间与实验测量值高度一致，无需任何经验拟合。这些结果证明了第一性原理自旋-声子张量能够提供对退相干途径的预测性见解，并指导分子量子比特的合理设计。

Abstract: Understanding and controlling spin relaxation in molecular qubits is
essential for developing chemically tunable quantum information platforms. We
present a fully first-principles framework for computing the spin relaxation
tensor in a single-molecule magnet, \ce{VOPc(OH)8}, by combining density
functional theory with a mode-resolved open-system formalism. By expanding the
spin Hamiltonian in vibrational normal modes and evaluating both linear and
quadratic spin-phonon coupling tensors via finite differences of the
$g$-tensor, we construct a relaxation tensor that enters a Lindblad-type
quantum master equation. Our formalism captures both direct (one-phonon) and
resonant-Raman (two-phonon) relaxation processes. Numerical analysis reveals a
highly mode-selective structure: only three vibrational modes dominate
longitudinal ($T_1$) decoherence, while a single mode accounts for the majority
of transverse ($T_2$) relaxation. The computed relaxation times show excellent
agreement with experimental measurements, without any empirical fitting. These
results demonstrate that first-principles spin-phonon tensors can provide
predictive insight into decoherence pathways and guide the rational design of
molecular qubits.

</details>


### [159] [Qubit encodings for lattices of dipolar planar rotors](https://arxiv.org/abs/2507.17952)
*Muhammad Shaeer Moeed,James Brown,Alexander Ibrahim,Estevao Vilas Boas De Oliveira,Pierre-Nicholas Roy*

Main category: quant-ph

TL;DR: 该研究提出了两种用于近期量子设备的平面转子晶格哈密顿量的量子比特编码方法，并通过实验验证了其有效性，并讨论了模拟资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了在近期量子设备上探索多体物理中难以探测的区域，需要针对二阶量化哈密顿量设计和优化量子比特编码方案。

Method: 研究了平面转子晶格哈密顿量的两种量子比特表示：一种是将转子哈密顿量投影分解为二元制并映射到自旋1/2投影，另一种是将平面转子格希尔伯特空间嵌入到更大的空间中，并将相关的量子比特编码系统恢复为投影到物理自由度上的商空间（通常称为一元映射）。

Result: 通过在小型链上进行稀疏对角化，验证了两种编码方法的正确性。

Conclusion: 该研究验证了两种编码方法，并讨论了在近期量子设备上模拟平面转子晶格的资源需求。

Abstract: Near term quantum devices have recently garnered significant interest as
promising candidates for investigating difficult-to-probe regimes in many-body
physics. To this end, various qubit encoding schemes targeting second quantized
Hamiltonians have been proposed and optimized. In this work, we investigate two
qubit representations of the planar rotor lattice Hamiltonian. The first
representation is realized by decomposing the rotor Hamiltonian projectors in
binary and mapping them to spin-1/2 projectors. The second approach relies on
embedding the planar rotor lattice Hilbert space in a larger space and
recovering the relevant qubit encoded system as a quotient space projecting
down to the physical degrees of freedom. This is typically called the unary
mapping and is used for bosonic systems. We establish the veracity of the two
encoding approaches using sparse diagonalization on small chains and discuss
quantum phase estimation resource requirements to simulate small planar rotor
lattices on near-term quantum devices.

</details>


### [160] [Spatial correlations in four-wave mixing with structured light](https://arxiv.org/abs/2507.17964)
*Mateus R. L. da Motta,Sandra S. Vianna*

Main category: quant-ph

TL;DR: 在量化的近轴框架中，对四波混频（FWM）进行了详细的理论分析，考虑了双光子态的多空间模式性质。在位置和动量表示下分析了双光子态，并确定了这些表示等效的条件。研究了FWM和自发参量下转换（SPDC）之间的相似性，以及泵浦结构如何转移到双光子态的空间符合度量。该研究还包括了从近场位置相关性到远场动量相关性的过渡，并讨论了纠缠度量，如螺旋带宽和施密特秩。


<details>
  <summary>Details</summary>
Motivation: 为了分析双光子态在位置和动量表示下的情况，并确定这些描述等效的条件。

Method: 对四波混频（FWM）进行了量化的近轴框架的详细理论处理，并捕获了该过程中产生的双光子态的多空间模式性质。

Result: 确定了双光子态的位置和动量表示的等效性条件，并展示了FWM和自发参量下转换（SPDC）之间的相似性，研究了纠缠度量，例如螺旋带宽和施密特秩。

Conclusion: 该理论框架巩固了有关FWM空间相关性的已知和新结果，并为未来结构光在非线性光学和量子光学中的研究提供了理论基础。

Abstract: We present a detailed theoretical treatment of four-wave mixing (FWM) in a
quantized paraxial framework, capturing the multi-spatial-mode nature of the
biphoton state generated in the process. By analyzing the biphoton state both
in position and momentum representations, we identify the conditions under
which these descriptions become equivalent. We also highlight formal and
physical similarities between FWM and spontaneous parametric down-conversion
(PDC), showing that the transfer of pump structure to the spatial coincidence
profile, an important and well-known characteristic of the biphoton state,
carries over naturally to FWM. In addition, our treatment captures the
transition from position correlations in the near field to momentum
correlations in the far field, reflecting the underlying spatial entanglement.
The measures of entanglement, including the spiral bandwidth and the Schmidt
rank, are discussed. Our work consolidates known and new results on spatial
correlations in FWM and provides a theoretical framework that may support
future studies in nonlinear and quantum optics with structured light.

</details>


### [161] [Molecular Properties in Quantum-Classical Auxiliary-Field Quantum Monte Carlo: Correlated Sampling with Application to Accurate Nuclear Forces](https://arxiv.org/abs/2507.17992)
*Joshua J. Goings,Kyujin Shin,Seunghyo Noh,Woomin Kyoung,Donghwi Kim,Jihye Baek,Martin Roetteler,Evgeny Epifanovsky,Luning Zhao*

Main category: quant-ph

TL;DR: QC-AFQMC通过最大化相关性来减少计算力的统计噪声，在各种化学系统中提高了准确性，并成功应用于碳捕获反应。


<details>
  <summary>Details</summary>
Motivation: 由于统计电子结构方法的计算梯度存在显著的统计噪声，因此需要一种能够进行精确核力计算的方法，这对于优化几何形状和反应动力学至关重要。

Method: 将相关的采样从经典的辅助场量子蒙特卡洛扩展到量子-经典（QC-AFQMC）框架，以实现精确的核力计算。通过同步随机数流、对齐轨道、使用确定性积分分解和在单一参考几何形状上定义一组一致的经典阴影测量来最大化邻近几何形状之间的相关性。重复使用单一的、参考定义的阴影系综，以消除在位移几何形状下进行额外量子测量的需要。

Result: 在氢链中验证了该方法，准确性涵盖了不同的相关区域。在N$_{2}$和拉伸线性H$_{4}$的力评估中，与单参考方法相比，该方法显示出显著的改进，特别是在强相关区域。轨道优化试探波函数提高了拉伸CO$_{2}$等情况的准确性。在MEA-CO$_{2}$碳捕获反应中成功应用了该方法。

Conclusion: QC-AFQMC框架通过最大化邻近几何形状之间的相关性，例如通过同步随机数流、对齐轨道、使用确定性积分分解以及在单一参考几何形状上定义一组一致的经典阴影测量，来实现精确的核力计算。这种方法通过在位移几何形状下重复使用单一的、参考定义的阴影系综，消除了额外的量子测量的需要，从而显著减少了计算力的统计方差。该方法已在氢链中得到验证，并在N$_{2}$和拉伸线性H$_{4}$的力评估中显示出比单参考方法有显著改进，特别是在强相关区域。轨道优化试探波函数进一步提高了具有挑战性案例（如拉伸CO$_{2}$）的准确性，而无需增加量子资源。该方法已成功应用于MEA-CO$_{2}$碳捕获反应，并使用量子信息度量进行活性空间选择和匹配门阴影以进行有效的重叠评估，确立了QC-AFQMC作为探索复杂反应路径的稳健框架。

Abstract: We extend correlated sampling from classical auxiliary-field quantum Monte
Carlo to the quantum-classical (QC-AFQMC) framework, enabling accurate nuclear
force computations crucial for geometry optimization and reaction dynamics.
Stochastic electronic structure methods typically encounter prohibitive
statistical noise when computing gradients via finite differences. To address
this, our approach maximizes correlation between nearby geometries by
synchronizing random number streams, aligning orbitals, using deterministic
integral decompositions, and employing a consistent set of classical shadow
measurements defined at a single reference geometry. Crucially, reusing this
single, reference-defined shadow ensemble eliminates the need for additional
quantum measurements at displaced geometries. Together, these methodological
choices substantially reduce statistical variance in computed forces. We
validate the method across hydrogen chains, confirming accuracy throughout
varying correlation regimes, and demonstrate significant improvements over
single-reference methods in force evaluations for N$_2$ and stretched linear
H$_4$, particularly in strongly correlated regions where conventional coupled
cluster approaches qualitatively fail. Orbital-optimized trial wave functions
further boost accuracy for demanding cases such as stretched CO$_2$, without
increasing quantum resource requirements. Finally, we apply our methodology to
the MEA-CO$_2$ carbon capture reaction, employing quantum information metrics
for active space selection and matchgate shadows for efficient overlap
evaluations, establishing QC-AFQMC as a robust framework for exploring complex
reaction pathways.

</details>


### [162] [On-Chip Laser-Driven Free-Electron Spin Polarizer](https://arxiv.org/abs/2507.17993)
*Clarisse Woodahl,Melanie Murillo,Charles Roques-Carmes,Aviv Karnieli,David A. B. Miller,Olav Solgaard*

Main category: quant-ph

TL;DR: 通过激光驱动的纳米光场在集成光子学芯片上产生自旋极化电子。


<details>
  <summary>Details</summary>
Motivation: 自旋极化电子束源能够研究纳米尺度上的自旋相关电和磁效应。

Method: 提出了一种在集成光子学芯片上利用激光驱动的纳米光场创建自旋极化电子的方法。该方法包括两个阶段的相互作用，中间有一个自由空间漂移长度。第一阶段和漂移长度在电子波函数的概率分布中引入了自旋依赖特性。第二阶段利用调整过的光学近场，通过利用自旋依赖的波包分布来旋转自旋态。

Result: 该平台提供了一种集成且紧凑的产生自旋极化电子的方法，可使用毫米级芯片和桌面激光器实现。

Conclusion: 该方法为在集成光子学芯片上通过激光驱动的纳米光场创建自旋极化电子提供了一种集成且紧凑的方法，能够实现具有高平均自旋期望值的电子的产生。

Abstract: Spin-polarized electron beam sources enable studies of spin-dependent
electric and magnetic effects at the nanoscale. We propose a method of creating
spin-polarized electrons on an integrated photonics chip by laser driven
nanophotonic fields. A two-stage interaction separated by a free space drift
length is proposed, where the first stage and drift length introduces
spin-dependent characteristics into the probability distribution of the
electron wavefunction. The second stage uses an adjusted optical near-field to
rotate the spin states utilizing the spin-dependent wavepacket distribution to
produce electrons with high ensemble average spin expectation values. This
platform provides an integrated and compact method to generate spin-polarized
electrons, implementable with millimeter scale chips and table-top lasers.

</details>


### [163] [Entanglement-based quantum key distribution with non-Gaussian continuous variables](https://arxiv.org/abs/2507.18000)
*Hao Jeng,Ping Koy Lam,Syed M. Assad*

Main category: quant-ph

TL;DR: 本文提出一种向纠缠态添加光子的技术，可提高量子密钥分发速率和距离，并增强其抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高量子密钥分发（QKD）的密钥生成速率和最大密钥分发距离，并解决从非高斯纠缠态提取密钥的复杂性问题。

Method: 本文提出了一种向纠缠态添加光子的技术，并通过实验验证了该技术在量子密钥分发中的应用。

Result: 添加单光子到双模压缩真空态可以提炼量子纠缠，从而提高量子密钥分发的密钥生成速率和最大分发距离。然而，这种方法产生的量子相关性具有高度非高斯性，使得传统的基于高斯性假设的密钥率计算方法失效。本文开发了新的分析方法，不依赖于对量子态的先验假设，并证明了添加单光子可以保护协议免受被动和主动退相干的影响。

Conclusion: 本文提出了一种向纠缠态添加光子的技术，该技术可以提高密钥生成速率和最大密钥分发距离，并且能够抵抗被动和主动的退相干。

Abstract: Addition of single photons to two-mode-squeezed-vacuum states has the effect
of distilling quantum entanglement, and, when deployed in quantum key
distribution, should lead also to an increase in the secret key rate. However,
the extraction of secret keys from non-Gaussian entangled states is a complex
issue and is at present not fully understood. In this paper we describe a
technique for adding photons to entangled states, and demonstrate how it leads
to an increase in secret key rates and the maximal distance for which keys can
be distributed assuming asymptotic conditions. The quantum correlations thus
produced were found to be of a highly non-Gaussian character, such that the
Gaussian extremity principle returns a negative keyrate and effectively kills
the protocol; we have therefore developed methods of analysis that do not
require prior assumptions about the state. Although it could have been that the
addition of single photons would make the system more fragile, this turned out
not to be the case. Rather, the addition of a single photon was found to
protect the protocol against both passive and active decoherence.

</details>


### [164] [Observation of period doubling and higher multiplicities in a driven single-spin system](https://arxiv.org/abs/2507.18387)
*Dhruv Deshmukh,Raúl B. González,Roberto Sailer,Fedor Jelezko,Ressa S. Said,Joachim Ankerhold*

Main category: quant-ph

TL;DR: 实验首次在氮-空位中心中观察到了自旋1/2系统的倍周期（k=2,...,5）响应，并发现了一种可用于改进传感的增强灵敏度域。


<details>
  <summary>Details</summary>
Motivation: 探索量子系统在外部时域周期场驱动下的亚谐波响应，特别是研究基本系统——单个自旋1/2——的倍周期现象。

Method: 通过理论分析和实验相结合，利用氮-空位中心的量子相干性，研究了在不同驱动参数下，自旋1/2系统（氮-空位中心）的亚谐波响应，特别是倍周期（k=2, ..., 5）现象。

Result: 首次在实验中展示了单个自旋1/2系统（氮-空位中心）的倍周期（k=2,...,5）响应，并验证了在特定驱动参数域内存在增强的灵敏度。

Conclusion: 实验证实了在驱动参数附近存在一个增强的灵敏度域，这为改进传感协议提供了新的手段。

Abstract: One of the prime features of quantum systems strongly driven by external
time-periodic fields is the subharmonic response with integer multiples of the
drive period $k\, T_d$. Here we demonstrate experimentally based on a careful
theoretical analysis period doubling and higher multiplicities ($k=2,\ldots 5$)
for one of the most fundamental systems, namely, an individual spin-$1/2$.
Realized as a nitrogen vacancy center in diamond, the particular coherence
properties under ambient conditions, strong field sensitivity, and optical
addressability allow to monitor coherent $k$-tupling oscillations over a broad
set of driving parameters in the vicinity of the ideal manifolds. We verify an
enhanced sensitivity within this domain which provides new means for improved
sensing protocols.

</details>


### [165] [Enhanced continuous-variable quantum key distribution protocol via adaptive signal processing](https://arxiv.org/abs/2507.18049)
*Ozlem Erkilic,Biveen Shajilal,Lorcan O. Conlon,Angus Walsh,Aritra Das,Sebastian Kish,Thomas Symul,Ping Koy Lam,Syed M. Assad,Jie Zhao*

Main category: quant-ph

TL;DR: 该研究提出了一种新的CV-QKD协议，通过引入概率性滤波器，在保持安全性的同时，显著提高了密钥生成速率，尤其在动态信道和低轨卫星通信场景下表现突出，且无需硬件改动。


<details>
  <summary>Details</summary>
Motivation: 为了克服连续变量量子密钥分发（CV-QKD）在光纤损耗和大气闪烁等信道限制下的性能瓶颈，并超越最优的高斯调制CV-QKD（GG02）协议。

Method: 提出了一种结合高斯滤波器和非高斯滤波器的量子密钥分发协议，并通过避免高斯极值进行安全分析，以精确界定窃听者信息。该协议能够动态优化密钥生成速率，并能在参数估计认为不安全的区域提取密钥。

Result: 实验结果显示，该协议的密钥生成速率是GG02协议的三倍。在低地球轨道卫星量子通信仿真中，与未优化的对应协议相比，密钥生成速率提高了400倍。

Conclusion: 该量子密钥分发协议通过使用非高斯滤波器和优化调制方差，在保持安全性的前提下，显著提高了密钥生成速率，并能适应动态变化信道，无需硬件修改，在实验和仿真中均表现出优于现有协议的性能。

Abstract: Quantum key distribution (QKD) provides a promising approach to secure
communications, with continuous-variable QKD (CV-QKD) offering compatibility
with existing telecommunication infrastructure. Despite this advantage, CV-QKD
is limited by challenges such as losses in terrestrial fibres and atmospheric
scintillation in free-space channels. We introduce a QKD protocol that
surpasses the optimal Gaussian modulated CV-QKD (GG02) protocol by utilising
probabilistic filters without known physical representation. Our approach
employs a Gaussian filter at Alice's station and a non-Gaussian notch-like
filter at Bob's station. Alice's filter optimises modulation variance to
achieve key rates near the optimal GG02 performance, while Bob's filter adapts
the effective channel conditions, which can result in higher key rates than the
optimal GG02 protocol. Our security analysis avoids Gaussian extremality,
accurately bounding Eve's information. The protocol dynamically optimises the
secret-key rate for rapidly changing channels, such as terrestrial links and
satellite-to-ground communications, and can extract keys in regions deemed
non-secure by parameter estimation. Implemented at software level, our protocol
requires no hardware modifications and can be integrated into existing QKD
systems. Experimental results show a threefold increase in key rates over the
optimal GG02 protocol, while simulations for Low Earth Orbit satellite quantum
communications indicate a 400-fold increase compared to the non-optimised
counterpart.

</details>


### [166] [Entanglement certification by measuring nonlocality](https://arxiv.org/abs/2507.18066)
*Xuan Du Trinh,Zhengyu Wu,Junlin Bai,Huan-Hsin Tseng,Nengkun Yu,Aruna Balasubramanian*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reliable entanglement manipulation is key to quantum networks. This paper
presents a practical approach to entanglement verification based on
Clauser-Horne-Shimony-Holt (CHSH) inequality violation. We establish
mathematical bounds relating the CHSH measure to entanglement fidelity and
develop statistical methods that optimize resource usage while maintaining
verification reliability. Our main contributions include tight bounds on
entanglement fidelity using the CHSH measure, a statistical framework for
estimating sample complexity, and practical verification protocols with
quantifiable confidence. Using NetSquid simulations, we comprehensively
evaluated our protocols under various network conditions, demonstrating the key
trade-offs between verification accuracy, resource efficiency, and operational
parameters. Our results provide concrete guidance for implementing efficient
entanglement verification in resource-constrained quantum networks, balancing
security requirements with practical limitations.

</details>


### [167] [Magnetic Memory and Hysteresis from Quantum Transitions: Theory and Experiments on Quantum Annealers](https://arxiv.org/abs/2507.18079)
*Frank Barrows,Elijah Pelofske,Pratik Sathe,Francesco Caravelli,Cristiano Nisoli*

Main category: quant-ph

TL;DR: 本研究提出了一个结合朗道-泽纳跃迁和畴壁动力学的框架，成功解释了量子退火中的量子迟滞现象，并证明了量子退火器在研究量子多体系统非平衡动力学方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现在D-Wave的量子硬件上，大规模横向场伊辛系统中存在鲁棒的迟滞现象，其量子本质需要更深入的理解。

Method: 本研究结合了二级朗道-泽纳跃迁（通过一阶分段常数传播子）和半经典畴壁动力学来解释观察到的行为。在量子退火器上进行了实验验证。

Result: 研究提出的框架能够重现实验中观察到的量子迟滞现象，包括扭结密度、迟滞回线形状以及纵向扫描速率依赖趋势，并能捕捉到诸如负磁化率等真实的量子记忆效应。

Conclusion: 本研究提出的框架成功解释了量子退火中观察到的迟滞现象，并表明可编程量子退火器是探索具有记忆的非平衡量子多体系统动力学的有力平台。

Abstract: Quantum annealing leverages quantum tunneling for non-local searches, thereby
minimizing memory effects that typically arise from metastabilities.
Nonetheless, recent work has demonstrated robust hysteresis in large-scale
transverse-field Ising systems implemented on D-Wave's analog quantum hardware.
The quantum nature of these intriguing results remains to be understood at a
deeper level. Here, we present a conceptual framework that explains the
observed behavior by combining two-level Landau-Zener transitions via a
first-order piecewise-constant propagator with semiclassical domain-wall
kinetics. We test this approach experimentally on a quantum annealer, where we
observe clear coercivity even in one-dimensional rings with periodic boundary
conditions comprising up to 4,906 qubits-regimes where classical hysteresis is
forbidden, but quantum hysteresis is not. Our framework reproduces the measured
kink densities, hysteresis loop shapes, and longitudinal sweep-rate scaling
trends observed in data from three different D-Wave quantum annealers. In
particular, it captures striking non-monotonic features and transiently
negative susceptibilities, identifying them as genuine quantum memory effects.
These results establish programmable quantum annealers as powerful testbeds for
exploring memory-endowed non-equilibrium dynamics in quantum many-body systems.

</details>


### [168] [An Initialization-free Quantum Algorithm for General Abelian Hidden Subgroup Problem](https://arxiv.org/abs/2507.18088)
*Sekang Kwon,Jeong San Kim*

Main category: quant-ph

TL;DR: 提出了一种无需初始化、能恢复辅助寄存器状态的量子算法，用于解决有限阿贝尔群的隐藏子群问题，提高了算法效率。


<details>
  <summary>Details</summary>
Motivation: 旨在解决隐藏子群问题（HSP），特别是当G是有限阿贝尔群时，利用量子计算的优势。

Method: 提出了一种无需初始化的量子算法，该算法能够处理任意未知混合态作为辅助寄存器，并能在计算完成后恢复其原始状态。

Result: 算法在计算成本上可与现有方法媲美，并通过恢复辅助寄存器状态，减少了初始化操作时间，提高了量子算法的效率。 此外，由于恢复的状态可用于其他操作，因此只需制备一次辅助寄存器即可执行迭代过程。 

Conclusion: 该算法通过恢复辅助寄存器的状态，并允许其被重复利用于后续运算，单个辅助寄存器的制备即可满足隐藏子群问题的迭代求解需求，为提升量子算法效率提供了新的方向。

Abstract: Hidden Subgroup Problem(HSP) seeks to identify an unknown subgroup H of a
group G for a given injective function f defined on cosets of H. Here we
present an initialization-free quantum algorithm for solving HSP in the case
where G is a finite abelian group. Our algorithm can adopt an arbitrary unknown
mixed state as the auxiliary register and removes the need for initialization
while preserving computational cost comparable to existing methods. Our
algorithm also restores the state of the auxiliary register to its original
form after completing the computations. Since the recovered state can be
utilized for other operations, a single preparation of the auxiliary register
in an arbitrarily unknown mixed state is sufficient to execute the iterative
procedure in solving hidden subgroup problems. This approach provides a
promising direction for improving quantum algorithm efficiency by reducing
operational time of initialization.

</details>


### [169] [Advancing the hBN Defects Database through Photophysical Characterization of Bulk hBN](https://arxiv.org/abs/2507.18093)
*Chanaprom Cholsuk,Sujin Suwanna,Tobias Vogl*

Main category: quant-ph

TL;DR: 该研究扩展了hBN缺陷数据库，包含了体相hBN缺陷及其光物理性质，旨在弥合理论与实验之间的差距，并支持机器学习在量子材料研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 大多数理论研究集中在单层hBN，而大多数实验研究则针对多层或体相hBN，这导致理论与实验之间存在差异。因此，需要一个包含体相hBN缺陷及其光物理性质的扩展数据库。

Method: 研究人员扩展了六方氮化硼（hBN）缺陷数据库，纳入了包括-2到2电荷态的120个以上中性缺陷（共600个缺陷）的体相hBN缺陷及其光物理性质。研究人员计算了零声子线、光致发光光谱、吸收光谱、黄瑞兹因子、辐射寿命、跃迁偶极矩和极化特性，并评估了电子-声子耦合强度与组态坐标的关联性。

Result: 研究结果显示，空位对电子-声子耦合强度有显著影响，会导致更强的晶格畸变和展宽的声子侧带。黄瑞兹因子与组态坐标高度相关。所有数据均可通过https://h-bn.info公开访问，并提供了一个新的API以方便集成到机器学习工作流中。

Conclusion: 该数据库旨在弥合理论与实验之间的差距，有助于可靠地识别量子发射器，并支持量子材料研究中机器学习驱动的方法的发展。

Abstract: Quantum emitters in hexagonal boron nitride (hBN) have gained significant
attention due to a wide range of defects that offer high quantum efficiency and
single-photon purity at room temperature. Most theoretical studies on hBN
defects simulate monolayers, as this is computationally cheaper than
calculating bulk structures. However, most experimental studies are carried out
on multilayer to bulk hBN, which creates additional possibilities for
discrepancies between theory and experiment. In this work, we present an
extended database of hBN defects that includes a comprehensive set of bulk hBN
defects along with their excited-state photophysical properties. The database
features over 120 neutral defects, systematically evaluated across charge
states ranging from -2 to 2 (600 defects in total). For each defect, the most
stable charge and spin configurations are identified and used to compute the
zero-phonon line, photoluminescence spectrum, absorption spectrum, Huang-Rhys
(HR) factor, interactive radiative lifetimes, transition dipole moments, and
polarization characteristics. Our analysis reveals that the electron-phonon
coupling strength is primarily influenced by the presence of vacancies, which
tend to induce stronger lattice distortions and broaden phonon sidebands.
Additionally, correlation analysis shows that while most properties are
independent, the HR factor strongly correlates with the configuration
coordinates. All data are publicly available at https://h-bn.info, along with a
new application programming interface (API) to facilitate integration with
machine learning workflows. This database is therefore designed to bridge the
gap between theory and experiment, aid in the reliable identification of
quantum emitters, and support the development of machine-learning-driven
approaches in quantum materials research.

</details>


### [170] [Silicon single-photon detector achieving over 84% photon detection efficiency with flexible operation modes](https://arxiv.org/abs/2507.18172)
*Dong An,Chao Yu,Ming-Yang Zheng,Anran Guo,Junsong Wang,Ruizhi Li,Huaping Ma,Xiu-Ping Xie,Xiao-Hui Bao,Qiang Zhang,Jun Zhang,Jian-Wei Pan*

Main category: quant-ph

TL;DR: 这项工作提出了一种创新的硅单光子探测器（Si SPD），其在785nm波长下实现了84.4%的卓越光子探测效率（PDE），并支持多种操作模式。该探测器通过优化的厚结SPAD设计和读出电路实现高性能，为需要高效率和多功能性的应用提供了理想解决方案。


<details>
  <summary>Details</summary>
Motivation: 为了在可见光谱中检测单光子，硅单光子探测器（Si SPDs）至关重要，而光子探测效率（PDE）是收集光子的关键特性。

Method: 通过设计和制造厚结硅单光子雪崩二极管（SPAD），利用背部入射结构增强雪崩概率，并通过掺杂补偿雪崩区来最小化噪声。采用50V淬灭电压的读出电路，实现自由运行、门控或混合模式下的操作。

Result: 所提出的硅单光子探测器在785nm处实现了高达84.4%的PDE，在268K的自由运行模式下，暗计数率为260cps，重合脉冲概率为2.9%。

Conclusion: 这项工作为需要超高效率、多工作模式的硅单光子探测器的应用提供了一个实用的解决方案。

Abstract: Silicon single-photon detectors (Si SPDs) play a crucial role in detecting
single photons in the visible spectrum. For various applications, photon
detection efficiency (PDE) is the most critical characteristic for effectively
collecting photons. Here, we present a Si SPD with a remarkable PDE of up to
84.4% at 785 nm, supporting multiple operation modes. We design and fabricate a
thick-junction Si single-photon avalanche diode (SPAD) that enhances the
avalanche probability through a backside-illumination structure, while
minimizing noise through the design of a doping-compensated avalanche region.
To maximize PDE, we implement a readout circuit with a 50 V quenching voltage,
enabling operation in free-running, gating, or hybrid modes. The SPAD, along
with its readout circuits and affiliated circuits, is integrated into a compact
SPD module. In free-running mode, the module achieves a maximum PDE of 84.4%,
with a dark count rate of 260 cps, and an afterpulse probability of 2.9% at 268
K. This work provides a practical solution for applications requiring
ultra-high-efficiency Si SPD with multiple operation modes.

</details>


### [171] [Optimal Quantum $(r,δ)$-Locally Repairable Codes via Classical Ones](https://arxiv.org/abs/2507.18175)
*Kun Zhou,Meng Cao*

Main category: quant-ph

TL;DR: 本文为最优$(r,\delta)$-LRCs提供了统一的理论框架，解决了其局部保护码和最小汉明距离问题，并推广到量子LRCs，构造了三族新的量子LRCs。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模分布式和云存储系统中数据丢失问题，需要研究局部可修复码（LRCs）。本文旨在为最优$(r,\delta)$-LRCs提供一个统一的理论框架，并探索其在量子信息存储中的应用。

Method: 本文基于建立的统一分解定理，推导了最优$(r,\delta)$-LRCs的局部保护码性质，并证明了最小汉明距离$d$与$\delta$的关系。然后，通过对经典最优$(r,\delta)$-LRCs进行分析，表征了最优量子$(r,\delta)$-LRCs，并给出了构造方法。

Result: 本文成功建立了通用最优$(r,\delta)$-LRCs的统一分解定理，并证明了其局部保护码为MDS码，最小汉明距离为$\delta$。同时，证明了$d\geq \delta$。此外，还表征了最优量子$(r,\delta)$-LRCs，并构造了三族具有灵活参数的最优量子$(r,\delta)$-LRCs。

Conclusion: 本文为通用的最优$(r,\delta)$-LRCs建立了统一的分解定理，并证明了其局部保护码是最小汉明距离为$\delta$的MDS码。同时，文章还表明，对于通用的最优$(r,\delta)$-LRCs，其最小汉明距离$d$总是满足$d\geq \delta$。此外，本文还完全表征了由满足最小分解的经典最优$(r,\delta)$-LRCs诱导的最优量子$(r,\delta)$-LRCs，并构造了三个具有灵活参数的最优量子$(r,\delta)$-LRCs的无限族。

Abstract: Locally repairable codes (LRCs) play a crucial role in mitigating data loss
in large-scale distributed and cloud storage systems. This paper establishes a
unified decomposition theorem for general optimal $(r,\delta)$-LRCs. Based on
this, we obtain that the local protection codes of general optimal
$(r,\delta)$-LRCs are MDS codes with the same minimum Hamming distance
$\delta$. We prove that for general optimal $(r,\delta)$-LRCs, their minimum
Hamming distance $d$ always satisfies $d\geq \delta$. We fully characterize the
optimal quantum $(r,\delta)$-LRCs induced by classical optimal
$(r,\delta)$-LRCs that admit a minimal decomposition. We construct three
infinite families of optimal quantum $(r,\delta)$-LRCs with flexible
parameters.

</details>


### [172] [Data Transmission over a Bosonic Arbitrarily Varying Quantum Channel](https://arxiv.org/abs/2507.18259)
*Janis Nötzel,Florian Seitz*

Main category: quant-ph

TL;DR: 我们为半经典攻击下的有损玻色信道提供了容量公式，该公式与量子熵功率不等式有关。


<details>
  <summary>Details</summary>
Motivation: 分析量子通信系统的鲁棒性，特别是经典-量子模型，以揭示特定信号星座在通用攻击下的优缺点。

Method: 通过将半经典攻击建模为光束分裂器设置，并研究接收方观察一个输出端口的情况，我们推导出了有损玻色信道在半经典攻击下的容量公式。

Result: 得到了有损玻色信道在半经典攻击下的显式容量公式，并展示了该公式与最新的量子熵功率不等式猜想之间的关系。

Conclusion: 该研究为一类重要的任意变化信道模型提供了编码定理，特别是针对有损玻色信道在半经典攻击下的情况，并给出了明确的容量公式。

Abstract: Arbitrarily varying channels offer a powerful framework for analyzing the
robustness of quantum communication systems, especially for classical-quantum
models, where the analysis displays strengths or weaknesses of specific signal
constellations under generic attacks. In this work, we provide a coding theorem
for a large class of practically relevant arbitrarily varying channel models.
Namely, we give an explicit capacity formula for the lossy bosonic channel
subject to semi-classical attacks, where an adversary injects semi-classical
states into the transmission line. Mathematically, this is modeled via a
beam-splitter setup, with transmitter and jammer controlling different input
ports and the receiver observing one output port. We show how a recently
conjectured new quantum entropy power inequality relates to our capacity
formula.

</details>


### [173] [Non-equilibrium Dynamics of Three-Level Absorption Refrigerator at Third-Order Liouvillian Exceptional Points](https://arxiv.org/abs/2507.18261)
*Jingyi Gao,Naomichi Hatano*

Main category: quant-ph

TL;DR: 研究了三能级量子吸收制冷机中的利奥维尔例外点（LEP），发现在三阶LEP处系统状态和热流的阻尼最快，并且可以提高制冷机的性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高制冷机的性能，研究了利奥维尔例外点（LEP）在三能级量子吸收制冷机中的作用，特别是系统在达到稳态之前的非平衡过程。

Method: 分析了三能级量子吸收制冷机中利奥维尔例外点（LEP）的影响，特别关注了收敛到稳态之前的非平衡过程。研究了具有两种耦合方式的系统中的二阶和三阶LEP。重点分析了三阶LEP系统状态和热流的阻尼，以及非平衡过程对从冷库提取热量产生的影响。

Result: 在三阶LEP处实现了系统状态和热流的关键阻尼，表明其收敛速度最快。在非平衡过程中，三阶LEP可以实现从冷库到热库的热量传递，同时降低了功库的能量成本，从而提高了制冷机的性能。

Conclusion: 在三阶利奥维尔例外点（LEP）处，系统状态和热电流的关键阻尼实现了最快的收敛到平衡态。在非平衡过程中，与热库相比，从冷库转移到热库的热量更多，而功库的能量成本更低，从而提高了制冷机的性能。

Abstract: We analyze the influence of Liouvillian exceptional points (LEPs) in the
three-level quantum absorption refrigerator, putting emphasis on the
non-equilibrium process before the convergence to the steady state. We search
for the second-order and third-order LEPs in the system with two types of
couplings. Focusing on the third-order LEPs, we analyze the damping of the
system state in the long term analytically and numerically. In addition, we
analyze the damping of heat currents and the influence of the non-equilibrium
process in the heat extraction from the cold bath. Critical damping at LEPs of
both the system state and the heat currents is achieved, implying the fastest
convergence to the equilibrium system. During the non-equilibrium process, we
find that much heat transfer from the cold bath to the hot bath with less
energy cost of the work bath is achieved at the third-order LEP, leading to
better performance of the refrigerator.

</details>


### [174] [Partial trace relations beyond normal matrices](https://arxiv.org/abs/2507.18278)
*Pablo Costa Rico,Michael M. Wolf*

Main category: quant-ph

TL;DR: 本研究探讨了部分迹及其扩张的关系，特别是扩张的存在性和范数不等式，并考虑了秩约束。研究结果包括：1. 存在大于一的任何秩的扩张。2. 推广了Audenaert的次可加性不等式。3. 证明了Kronecker和的新主张关系。4. 应用于扩展Werner状态的2-不可蒸馏区间。5. 证明了新的Schmidt数证人和k-正映射。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究部分迹及其扩张的关系，并对秩约束给予特别关注。

Method: 本研究研究了部分迹及其扩张之间的关系，重点关注（联合）扩张的存在性以及部分迹及其扩张之间的范数不等式。在整个分析过程中，特别关注了秩约束。

Result: 研究发现，每对具有相同大小和迹的矩阵都允许大于一的任何秩的扩张。将Audenaert的次可加性不等式推广到一般矩阵、多个张量因子和不同范数，证明了Kronecker和的新主张关系。

Conclusion: 本研究将Audenaert的次可加性不等式推广到一般矩阵、多个张量因子和不同的范数，并证明了Kronecker和的新主张关系。作为应用，研究将Werner状态在可证明2-不可蒸馏的区间扩展到任何维度$d\geq4$，还证明了新的Schmidt数证人以及k-正映射。

Abstract: We investigate the relationship between partial traces and their dilations
for general complex matrices, focusing on two main aspects: the existence of
(joint) dilations and norm inequalities relating partial traces and their
dilations. Throughout our analysis, we pay particular attention to rank
constraints. We find that every pair of matrices of equal size and trace admits
dilations of any rank larger than one. We generalize Audenaert's subadditivity
inequality to encompass general matrices, multiple tensor factors, and
different norms. A central ingredient for this is a novel majorization relation
for Kronecker sums. As an application, we extend the interval of Werner states
in which they are provably 2-undistillable in any dimension $d\geq4$. We also
prove new Schmidt-number witnesses and $k$-positive maps.

</details>


### [175] [Certifying non-classicality and non-Gaussianity through optical parametric amplification](https://arxiv.org/abs/2507.18296)
*Mahmoud Kalash,Marcello H. M. Passos,Éva Rácz,László Ruppert,Radim Filip,Maria V. Chekhova*

Main category: quant-ph

TL;DR: 提出了一种结合光学参量放大和常规强度探测器的新方法，无需光子数分辨即可认证非高斯态，并为高维量子技术奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 为了实现量子信息协议，对非高斯态的需求日益增长，因此，认证非高斯态至关重要。然而，传统方法（如量子态层析成像）需要测量韦尔函数负值，这需要高效的探测器。而通过测量光子数概率来认证非高斯态虽然更简单，但仍需要光子数分辨探测器或特定的测量技术。

Method: 提出了一种结合光学参量放大和常规强度探测器的新方法，通过测量放大态的平均光子数和二阶关联函数来认证非高斯态。

Result: 成功认证了受激拟单光子态的非高斯性，证明了该方法的可行性。

Conclusion: 该方法提供了一种无需光子数分辨即可认证非高斯态的新途径，并为利用多模非高斯态的高维量子技术发展奠定了基础。

Abstract: Non-Gaussian states of light are essential for numerous quantum information
protocols; thus, certifying non-Gaussianity is crucial. Full quantum state
tomography achieves this, but it implies observing the Wigner function
negativity, which requires efficient detection. Certifying non-Gaussianity
through directly measurable parameters is a simpler alternative, typically
achieved by measuring photon-number probabilities - either directly using
photon-number resolving detectors or through Hanbury Brown-Twiss type
measurements with single-photon detectors. Here, we demonstrate theoretically
and experimentally that optical parametric amplification combined with
conventional intensity detectors can effectively replace this approach without
the need for photon-number resolution. In our method, we measure the mean
photon number and the second-order correlation function for the amplified
state. Using it, we successfully certify the non-Gaussianity of a heralded
quasi-single-photon state. Since optical parametric amplification is a
multimode process, our method provides a foundation for developing
high-dimensional quantum technologies utilizing multimode non-Gaussian states.

</details>


### [176] [Probing metric fluctuations with the spin of a particle: a quantum simulation with bimodal optical cavities](https://arxiv.org/abs/2507.18351)
*Jiannis K. Pachos,Patricio Salgado-Rebolledo,Martine Schut*

Main category: quant-ph

TL;DR: 该研究使用（2+1）D 大质量引力玩具模型与狄拉克费米子在晶格上进行交互，研究费米子自旋在时空涨落影响下的演化，并提出了一个可通过原子-光学腔系统实现的实验方案，观察到了多样化的自旋演化模式。


<details>
  <summary>Details</summary>
Motivation: 探索量子引力的潜在经验表现形式，并为量子引力与物质的相互作用提供一个可行的模拟和探测途径。

Method: 利用（2+1）D 大质量引力玩具模型与狄拉克费米子在晶格上进行交互，研究费米子自旋在时空涨落影响下的演化。

Result: 观察到费米子自旋根据其与时空涨落的相互作用表现出多样化的演化模式。

Conclusion: 该研究提出了一个新颖的方法来模拟量子引力与物质之间相互作用的影响，并提出了一个可以通过当前技术进行探测的实验方案。

Abstract: Exploring potential empirical manifestations of quantum gravity is a
challenging pursuit. In this study, we utilise a lattice representation of a
(2+1)D massive gravity toy model interacting with Dirac fermions that can
support specific spacetime fluctuations. We focus on the evolution of the
fermion's spin due to its coupling to spacetime fluctuations. To monitor their
dynamics a minimal model is required that comprises two bosonic modes
describing spacetime geometry fluctuations coupled with the spin of the
fermion. A possible emulation of this system involves encoding spin degrees of
freedom in an atom coupled with a bimodal optical cavity that provides the two
bosonic modes. We observe diverse spin evolution patterns based on its
interaction with fluctuating spacetime geometry. Our proposal introduces a
novel approach for modelling the effect of interactions between quantum gravity
and matter that can be probed with current technology.

</details>


### [177] [The Quantum Foucault Modes](https://arxiv.org/abs/2507.18420)
*Samuel Alperin*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The driven quantum harmonic oscillator is fundamental to a number of
important physical systems. Here, we consider the quantum harmonic oscillator
under non-Hermitian, PT-symmetric driving, showing that the resulting set of
Wigner-space trajectories of an initial coherent state is identical to the set
of real-space trajectories of the classical Foucault pendulum. Remarkably, in
the case mapped from the trivial 1D pendulum, the corresponding quantum
dynamics are those of an oscillator with periodically evolving momentum but
fixed position, a novel type of dynamics which are forbidden in classical
systems.

</details>


### [178] [Quantum Machine Learning for Predicting Binding Free Energies in Structure-Based Virtual Screening](https://arxiv.org/abs/2507.18425)
*Pei-Kun Yang*

Main category: quant-ph

TL;DR: 一种用于虚拟筛选的量子机器学习方法，可在近海量子硬件上实现可接受的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了克服经典计算在处理结构基础虚拟筛选中分子构象、空间位移和旋转的组合时面临的挑战，并利用量子计算的内在并行性。

Method: 该模型使用 PyTorch 实现和训练，并在三种设置下进行评估：理想模拟、有限次采样和量子噪声模拟。

Result: 使用六个量子电路单元，该模型实现了 2.37 kcal/mol 的 RMSD 和 0.650 的皮尔逊相关系数。即使在 100,000 次采样的情况下，预测也保持一致，并且在量子噪声下，配体亲和力的排名基本保持不变。

Conclusion: 这项研究提出了一种量子机器学习方法，该方法将分子信息编码到量子态，并通过参数化量子门进行处理，以评估蛋白质-配体复合物的结合自由能。

Abstract: In structure-based virtual screening, it is often necessary to evaluate the
binding free energy of protein-ligand complexes by considering not only
molecular conformations but also how these structures shift and rotate in
space. The number of possible combinations grows rapidly and can become
overwhelming. While classical computing has limitations in this context,
quantum computing offers a promising alternative due to its inherent
parallelism. In this study, we introduce a quantum machine learning approach
that encodes molecular information into quantum states and processes them using
parameterized quantum gates. The model is implemented and trained using
PyTorch, and its performance is evaluated under three settings: ideal
simulation, limited-shot sampling, and simulations with quantum noise. With six
quantum circuit units, the model achieves an RMSD of 2.37 kcal/mol and a
Pearson correlation of 0.650. Even when using 100,000 shots, the predictions
remain consistent, indicating that the model is compatible with near-term
quantum hardware. Although noise slightly reduces accuracy, the ranking of
ligand affinities remains largely unchanged. These findings point to a
practical and scalable strategy that balances robustness and predictive power,
offering a viable path to accelerate virtual screening through moderately deep
quantum circuits.

</details>


### [179] [Three-qubit encoding in ytterbium-171 atoms for simulating 1+1D QCD](https://arxiv.org/abs/2507.18426)
*William Huie,Cianan Conefrey-Shinozaki,Zhubing Jia,Patrick Draper,Jacob P. Covey*

Main category: quant-ph

TL;DR: 通过使用镥原子实现量子色动力学模拟，效率更高。


<details>
  <summary>Details</summary>
Motivation: 模拟由量子色动力学描述的核物质，由于夸克自由度（如物质/反物质、味、色和自旋）的种类繁多，效率低下。

Method: 通过在单个镥-171原子的中性原子量子处理器中编码三个量子比特，并开发一组复合边带脉冲，为该三量子比特系统演示了一组通用的门和读出协议。然后，将其应用于单味量子色动力学，其中三个量子比特直接表示三种颜色中夸克的占用情况。

Result: 使用两个原子足以模拟真空持久性振荡和链断裂。我们考虑了资源需求以及与错误检测/校正的联系。

Conclusion: 该工作是实现核物质资源高效数字模拟的一步，并为中性原子量子处理器中通用的量子比特编码开辟了新的机会。

Abstract: Simulating nuclear matter described by quantum chromodynamics using quantum
computers is notoriously inefficient because of the assortment of quark degrees
of freedom such as matter/antimatter, flavor, color, and spin. Here, we propose
to address this resource efficiency challenge by encoding three qubits within
individual ytterbium-171 atoms of a neutral atom quantum processor. The three
qubits are encoded in three distinct sectors: an electronic "clock" transition,
the spin-1/2 nucleus, and the lowest two motional states in one radial
direction of the harmonic trapping potential. We develop a family of composite
sideband pulses and demonstrate a universal gate set and readout protocol for
this three-qubit system. We then apply it to single-flavor quantum
chromodynamics in 1+1D axial gauge for which the three qubits directly
represent the occupancy of quarks in the three colors. We show that two atoms
are sufficient to simulate both vacuum persistence oscillations and string
breaking. We consider resource requirements and connections to error
detection/correction. Our work is a step towards resource-efficient digital
simulation of nuclear matter and opens new opportunities for versatile qubit
encoding in neutral atom quantum processors.

</details>


### [180] [Geometric Measures of Complexity for Open and Closed Quantum Systems](https://arxiv.org/abs/2507.18440)
*Alberto Acevedo,Antonio Falco*

Main category: quant-ph

TL;DR: Quantum complexity can be geometric, but only for unitary dynamics. We extend this to non-unitary dynamics by defining a notion of geometric complexity for quantum channels that model noise.


<details>
  <summary>Details</summary>
Motivation: The unitary dynamics of quantum systems can be modeled as a trajectory on a Riemannian manifold. This theoretical framework naturally yields a purely geometric interpretation of computational complexity for quantum algorithms, a notion originally developed by Michael Nielsen (Circa, 2007). However, for nonunitary dynamics, it is unclear how one can recover a completely geometric characterization of Nielsen-like geometric complexity.

Method: Building on Nielsen's work, we present a definition of geometric complexity for a fairly generic family of quantum channels. These channels are useful for modeling noise in quantum circuits, among other things, and analyze the geometric complexity of these quantum channels.

Result: We present a definition of geometric complexity for a fairly generic family of quantum channels and analyze the geometric complexity of these quantum channels.

Conclusion: For nonunitary dynamics, it is unclear how one can recover a completely geometric characterization of Nielsen-like geometric complexity. The main obstacle to overcome is that nonunitary dynamics cannot be characterized by Lie groups (which are Riemannian manifolds), as is the case for unitary dynamics. Building on Nielsen's work, we present a definition of geometric complexity for a fairly generic family of quantum channels. These channels are useful for modeling noise in quantum circuits, among other things, and analyze the geometric complexity of these quantum channels.

Abstract: The unitary dynamics of quantum systems can be modeled as a trajectory on a
Riemannian manifold. This theoretical framework naturally yields a purely
geometric interpretation of computational complexity for quantum algorithms, a
notion originally developed by Michael Nielsen (Circa, 2007). However, for
nonunitary dynamics, it is unclear how one can recover a completely geometric
characterization of Nielsen-like geometric complexity. The main obstacle to
overcome is that nonunitary dynamics cannot be characterized by Lie groups
(which are Riemannian manifolds), as is the case for unitary dynamics. Building
on Nielsen's work, we present a definition of geometric complexity for a fairly
generic family of quantum channels. These channels are useful for modeling
noise in quantum circuits, among other things, and analyze the geometric
complexity of these quantum channels.

</details>


### [181] [Generalised state space geometry in Hermitian and non-Hermitian quantum systems](https://arxiv.org/abs/2507.18486)
*Kunal Pal*

Main category: quant-ph

TL;DR: 该研究在量子信息几何中引入了修改厄米张量结构的方法，找到了对偶联络，并分析了非厄米哈密顿量的情况，最后将其应用于量子梯度下降。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于经典信息几何中的度量结构和联络族在量子系统中无法直接推广，而这种推广可能有助于定义与经典概率分布函数对应的对偶联络，并处理非厄米哈密顿量的情况。

Method: 本研究使用双正交形式主义，在实值Fubini-Study张量下，系统地对非厄米哈密顿量动力学产生的四种张量进行了分类，并确定了复值度量和贝里曲率。

Result: 研究表明，可以构建一个在广义意义下相互对偶的联络族，并确定了非厄米哈密顿量动力学产生的四种张量、复值度量和贝里曲率。最后，阐明了度量在量子自然梯度下降优化问题中的作用。

Conclusion: 本工作通过修改投影空间上的厄米张量结构，探索了纯量子态几何的可能变化，并尝试将这些推广用于定义与经典概率分布函数直接对应的对偶联络，该函数由非平凡相位修改。

Abstract: One of the key features of information geometry in the classical setting is
the existence of a metric structure and a family of connections on the space of
probability distributions. The uniqueness of the Fisher--Rao metric and the
duality of these connections is at the heart of classical information geometry.
However, these features do not carry over straightforwardly to quantum systems,
where a Hermitian inner product structure on the Hilbert space induces a metric
on the complex projective space of pure states -- the Fubini-Study tensor,
which is preserved under the unitary evolution. In this work, we explore how
modifying the Hermitian tensor structure on the projective space may affect the
geometry of pure quantum states, and whether such generalisations can be used
to define dual connections with a direct correspondence to classical
probability distribution functions, modified by the presence of a non-trivial
phase. We show that it is indeed possible to construct a family of connections
that are dual to each other in a generalised sense with respect to the
real-valued sector of the Fubini--Study tensor. Using this biorthogonal
formalism, we systematically classify the four types of tensors that can arise
when the dynamics of a quantum system are governed by a non-Hermitian
Hamiltonian, identifying both the complex-valued metric and the Berry
curvature. Finally, we elucidate the role of the metric in a quantum natural
gradient descent optimisation problem, generalised to the non-Hermitian case
for a suitable choice of cost function.

</details>


### [182] [The hidden subgroup problem for infinite groups](https://arxiv.org/abs/2507.18499)
*Greg Kuperberg*

Main category: quant-ph

TL;DR: 探索离散无限群中的隐藏子群问题，证明其在某些群中是NP难的，并提出针对特定情况的算法，包括将Shor-Kitaev算法推广到秩亏缺或无限索引情况，以及为阿贝尔隐藏移位问题设计了伸展指数时间算法。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于扩展Shor算法在整数周期查找中的成功案例，探索更广泛的数学结构——离散无限群——中的隐藏子群问题。研究者旨在理解该问题的计算复杂性（硬度），并开发更有效的算法来解决它。

Method: 本文采用了计算复杂性理论的方法，包括NP难性证明、还原以及算法设计。研究人员通过泛化Shor-Kitaev算法来处理具有秩亏缺或无限索引的隐藏子群问题，并基于先前工作设计了针对阿贝尔隐藏移位问题的伸展指数时间算法。

Result: 研究结果表明，隐藏子群问题在有理数加法群和非阿贝尔自由群的法子群中是NP难的。同时，也间接将短向量问题的某个版本归约到伪多项式查询成本的HSP in $\mathbb{Z}^k$。在算法方面，成功地将Shor-Kitaev算法推广到秩亏缺或无限索引的HSP in $\mathbb{Z}^k$。最后，为阿贝尔隐藏移位问题（AHShP）设计了一个伸展指数时间算法，并证明了任意有限生成、虚拟阿贝尔群的HSP也具有伸展指数时间算法。

Conclusion: 该研究为离散无限群的隐藏子群问题（HSP）提供了新的算法和硬度结果。具体来说，证明了HSP在有理数加法群和非阿贝尔自由群的法子群中是NP难的，并展示了隐藏子群问题在某些情况下（例如秩亏缺或无限索引）可以被推广。此外，还提出了一个针对阿贝尔隐藏移位问题（AHShP）的算法，该算法的时间复杂度为伸展指数，并将其推广到任意有限生成、虚拟阿贝尔群的HSP。

Abstract: Following the example of Shor's algorithm for period-finding in the integers,
we explore the hidden subgroup problem (HSP) for discrete infinite groups. On
the hardness side, we show that HSP is NP-hard for the additive group of
rational numbers, and for normal subgroups of non-abelian free groups. We also
indirectly reduce a version of the short vector problem to HSP in
$\mathbb{Z}^k$ with pseudo-polynomial query cost. On the algorithm side, we
generalize the Shor-Kitaev algorithm for HSP in $\mathbb{Z}^k$ (with standard
polynomial query cost) to the case where the hidden subgroup has deficient rank
or equivalently infinite index. Finally, we outline a stretched exponential
time algorithm for the abelian hidden shift problem (AHShP), extending prior
work of the author as well as Regev and Peikert. It follows that HSP in any
finitely generated, virtually abelian group also has a stretched exponential
time algorithm.

</details>


### [183] [Tunable Non-Gaussian Mechanical States in a Strongly Coupled Hybrid Quantum System](https://arxiv.org/abs/2507.18571)
*Jugal Talukdar,Scott E. Smart,Prineha Narang*

Main category: quant-ph

TL;DR: 通过在混合量子系统中应用特定的时变驱动，成功制备了非高斯运动态，该态具有优越的量子特性，可用于量子传感和信息处理。


<details>
  <summary>Details</summary>
Motivation: 量子运动态是第二次量子革命的关键组成部分。本研究旨在探索在混合量子系统中生成和控制非高斯运动态的方法，并评估其作为量子信息处理平台的潜力。

Method: 研究了一个包含量子比特集合、机械谐振子和光子腔的三方混合量子系统。在强耦合区，利用具有方波剖面的时变外部驱动，分析了瞬态动力学行为，并研究了量子比特相位、相互作用强度和驱动参数对非高斯性的影响。

Result: 该三方混合量子系统能够生成高度非高斯运动态，该状态在Wigner拟态分布中表现为显著的负体积，并在量子Fisher信息方面有所增强，表明了其在量子信息任务中的效用。

Conclusion: 该混合量子系统通过利用时变驱动和强耦合机制，成功制备了高度非高斯运动态，并论证了其在量子传感、计量和信息处理等领域的应用潜力。

Abstract: Quantum states of motion are critical components in the second quantum
revolution. We investigate the generation and control of non-Gaussian motional
states in a tripartite hybrid quantum system consisting of a collection of
qubits coupled to a mechanical resonator, which in turn interacts with an
externally driven photonic cavity. This hybrid architecture provides a
versatile platform for quantum control by integrating nonlinear interactions
and multiple control parameters. Operating in the strong coupling regime, we
study the transient dynamics resulting from a time-dependent external drive
that has a boxcar profile. Starting from coherent states in both the mechanical
and cavity subsystems, we show that this drive protocol, combined with
time-independent interaction and frequency configurations, leads to the
emergence of highly non-Gaussian quantum states in the intermediary mechanical
degree of freedom. These states are characterized by a pronounced negative
volume in the Wigner quasi-probability distribution and enhanced quantum Fisher
information, indicative of their quantum utility. We systematically analyze the
impact of the qubit phase, interaction strengths, and drive parameters on the
degree of non-Gaussianity. Our findings underscore the tunability and richness
of this hybrid platform, paving the way for advanced quantum state engineering
and applications in quantum sensing, metrology, and information processing.

</details>


### [184] [Hybrid quantum-classical algorithm for near-optimal planning in POMDPs](https://arxiv.org/abs/2507.18606)
*Gilberto Cunha,Alexandra Ramôa,André Sequeira,Michael de Oliveira,Luís Barbosa*

Main category: quant-ph

TL;DR: QBRL是一种结合量子计算的强化学习算法，能更快地解决部分可观测环境下的决策问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决部分可观测环境下的决策问题，并加速基于稀疏贝叶斯网络的马尔可夫决策过程的推理过程。

Method: 提出了一种名为QBRL（Quantum Bayesian Reinforcement Learning）的混合量子-经典前瞻算法，用于基于模型的强化学习。该算法利用量子拒绝采样和幅度放大技术来加速稀疏贝叶斯网络上的推理。

Result: QBRL在稀疏贝叶斯网络的动态环境下，实现了比经典方法更快的近优规划。数值实验表明，量子计算优势在不同部署场景下对决策性能的影响程度存在显著差异。

Conclusion: QBRL通过量子增强的信念更新，在稀疏贝叶斯网络的动态环境下，实现了比经典方法更快的近优规划。

Abstract: Reinforcement learning (RL) provides a principled framework for
decision-making in partially observable environments, which can be modeled as
Markov decision processes and compactly represented through dynamic decision
Bayesian networks. Recent advances demonstrate that inference on sparse
Bayesian networks can be accelerated using quantum rejection sampling combined
with amplitude amplification, leading to a computational speedup in estimating
acceptance probabilities.\\ Building on this result, we introduce Quantum
Bayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead
algorithm for model-based RL in partially observable environments. We present a
rigorous, oracle-free time complexity analysis under fault-tolerant assumptions
for the quantum device. Unlike standard treatments that assume a black-box
oracle, we explicitly specify the inference process, allowing our bounds to
more accurately reflect the true computational cost. We show that, for
environments whose dynamics form a sparse Bayesian network, horizon-based
near-optimal planning can be achieved sub-quadratically faster through
quantum-enhanced belief updates.
  Furthermore, we present numerical experiments benchmarking QBRL against its
classical counterpart on simple yet illustrative decision-making tasks. Our
results offer a detailed analysis of how the quantum computational advantage
translates into decision-making performance, highlighting that the magnitude of
the advantage can vary significantly across different deployment settings.

</details>


### [185] [Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis](https://arxiv.org/abs/2507.16641)
*Sara Giordano,Kornikar Sen,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: 通过强化学习和状态空间离散化，利用混合奖励机制，在合成量子电路方面取得了效率和优化。


<details>
  <summary>Details</summary>
Motivation: 为了解决在NISQ时代和未来容错量子计算中合成具有指定目标状态的量子电路所面临的挑战，该研究引入了一种强化学习框架。

Method: 本研究提出了一种基于强化学习（RL）的框架，利用表格型Q学习，基于动作序列，在离散化的量子态空间中进行操作，并通过结合静态的、领域信息奖励和可定制的动态惩罚（抑制门阻塞和冗余状态访问等低效电路结构）的混合奖励机制来合成量子电路。该方法利用稀疏矩阵表示和状态空间离散化来处理高维环境，并最小化计算开销。

Result: 在高达七个量子比特的图态制备任务的基准测试中，证明了该算法能够持续发现具有最优门数量的最小深度电路。此外，将该框架扩展到通用的量子态制备，它仍然能生成最小深度电路，凸显了该算法的鲁棒性和适应性。

Conclusion: 该强化学习驱动的方法有效地探索了复杂的量子态空间并合成了接近最优的量子电路，为量子电路优化提供了资源高效的基础。

Abstract: A reinforcement learning (RL) framework is introduced for the efficient
synthesis of quantum circuits that generate specified target quantum states
from a fixed initial state, addressing a central challenge in both the NISQ era
and future fault-tolerant quantum computing. The approach utilizes tabular
Q-learning, based on action sequences, within a discretized quantum state
space, to effectively manage the exponential growth of the space dimension. The
framework introduces a hybrid reward mechanism, combining a static,
domain-informed reward that guides the agent toward the target state with
customizable dynamic penalties that discourage inefficient circuit structures
such as gate congestion and redundant state revisits. By leveraging sparse
matrix representations and state-space discretization, the method enables
scalable navigation of high-dimensional environments while minimizing
computational overhead. Benchmarking on graph-state preparation tasks for up to
seven qubits, we demonstrate that the algorithm consistently discovers
minimal-depth circuits with optimized gate counts. Moreover, extending the
framework to a universal gate set for arbitrary quantum states, it still
produces minimal depth circuits, highlighting the algorithm's robustness and
adaptability. The results confirm that this RL-driven approach efficiently
explores the complex quantum state space and synthesizes near-optimal quantum
circuits, providing a resource-efficient foundation for quantum circuit
optimization.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [186] [$k$-Approval Veto: A Spectrum of Voting Rules Balancing Metric Distortion and Minority Protection](https://arxiv.org/abs/2507.17981)
*Fatih Erdem Kizilkaya,David Kempe*

Main category: cs.GT

TL;DR: 本文分析了 $k$-Approval Veto 投票规则在社会福利和少数群体保护之间的权衡。该规则在不同参数下表现出不同的性质，例如，在均衡目标下具有最优的社会福利，但在某些情况下效用目标下的社会福利可能很差。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨民主制度中最大化社会福利（多数原则）和保护少数群体（少数原则）这两个相互竞争的目标之间的权衡。

Method: 本文研究了单票否决制$k$-Approval Veto 投票规则，分析了其在社会福利（以指标扭曲衡量）和少数群体保护（以 $\ell$-相互少数群体标准衡量）之间的权衡。

Result: 10-Approval Veto 规则具有不低于 10 的少数群体保护能力。在效用目标下，其指标扭曲随 $k$ 线性增加。在 $\alpha$-百分位目标下，当 $\alpha 
b 
 >= k/(k+1)$ 时，指标扭曲为 5，当 $\alpha < k/(k+1)$ 时，指标扭曲为无限大。在均衡目标下，对于所有 $k$，指标扭曲均为 3。

Conclusion: 10-Approval Veto 在所有 $\alpha$ 值下都具有最低的均衡代价扭曲（3）。然而，当 $\alpha 
b 
 >= k/(k+1)$ 时，其效用代价扭曲为 5，当 $\alpha < k/(k+1)$ 时，其效用代价扭曲是无限的。

Abstract: In the context of single-winner ranked-choice elections between $m$
candidates, we explore the tradeoff between two competing goals in every
democratic system: the majority principle (maximizing the social welfare) and
the minority principle (safeguarding minority groups from overly bad
outcomes).To measure the social welfare, we use the well-established framework
of metric distortion subject to various objectives: utilitarian (i.e., total
cost), $\alpha$-percentile (e.g., median cost for $\alpha = 1/2$), and
egalitarian (i.e., max cost). To measure the protection of minorities, we
introduce the $\ell$-mutual minority criterion, which requires that if a
sufficiently large (parametrized by $\ell$) coalition $T$ of voters ranks all
candidates in $S$ lower than all other candidates, then none of the candidates
in $S$ should win. The highest $\ell$ for which the criterion is satisfied
provides a well-defined measure of mutual minority protection (ranging from 1
to $m$).
  Our main contribution is the analysis of a recently proposed class of voting
rules called $k$-Approval Veto, offering a comprehensive range of trade-offs
between the two principles. This class spans between Plurality Veto (for $k=1$)
- a simple voting rule achieving optimal metric distortion - and Vote By Veto
(for $k=m$) which picks a candidate from the proportional veto core. We show
that $k$-Approval Veto has minority protection at least $k$, and thus, it
accommodates any desired level of minority protection. However, this comes at
the price of lower social welfare. For the utilitarian objective, the metric
distortion increases linearly in $k$. For the $\alpha$-percentile objective,
the metric distortion is the optimal value of 5 for $\alpha \ge k/(k+1)$ and
unbounded for $\alpha < k/(k+1)$. For the egalitarian objective, the metric
distortion is the optimal value of 3 for all values of $k$.

</details>


### [187] [On Pareto-Optimal and Fair Allocations with Personalized Bi-Valued Utilities](https://arxiv.org/abs/2507.18251)
*Jiarong Jin,Biaoshuai Tao*

Main category: cs.GT

TL;DR: 研究了个性化双值效用下的公平分配问题，提出了帕累托最优分配的刻画，并对判定问题和EFX分配进行了研究，证明了EFX分配总是存在且可高效计算。


<details>
  <summary>Details</summary>
Motivation: 为了解决公平分配问题，特别是处理代理具有个性化双值效用的情况，并与现有关于三值效用和普通双值效用的结果进行比较和扩展。

Method: 通过对具有个性化双值效用的$m$个不可分割物品分配给$n$个代理的公平分配问题进行研究，提出一种刻画所有帕累托最优分配的方法。针对不同情况（$r_i$为整数或分数）设计了判定算法，并证明了EFX分配的存在性和计算复杂度。

Result: 1. 提出了个性化双值效用下公平分配的帕累托最优分配的刻画。2. 证明了在$r_i$为整数时，判断分配是否为帕累托最优可以在多项式时间内完成。3. 证明了一般情况下（$r_i$为分数），该判定问题是coNP完全的。4. 证明了EFX分配在个性化双值效用下总是存在且可在多项式时间内计算。

Conclusion: 该研究对具有个性化双值效用的$m$个不可分割物品分配给$n$个代理的公平分配问题进行了研究，并给出了所有帕累托最优分配的刻画。研究表明，在每个$r_i$为整数的情况下，存在一个多项式时间算法来判断给定分配是否为帕累托最优。对于一般情况（$r_i$可能为分数），该判定问题是coNP完全的。此外，研究证明了在个性化双值效用设置下，EFX分配总是存在且可以在多项式时间内计算，这扩展了先前关于双值效用的结果。最后，提出了一个开放性问题：是否存在（且是否能在多项式时间内计算）一个同时满足EFX和帕累托最优的分配。

Abstract: We study the fair division problem of allocating $m$ indivisible goods to $n$
agents with additive personalized bi-valued utilities. Specifically, each agent
$i$ assigns one of two positive values $a_i > b_i > 0$ to each good, indicating
that agent $i$'s valuation of any good is either $a_i$ or $b_i$. For
convenience, we denote the value ratio of agent $i$ as $r_i = a_i / b_i$.
  We give a characterization to all the Pareto-optimal allocations. Our
characterization implies a polynomial-time algorithm to decide if a given
allocation is Pareto-optimal in the case each $r_i$ is an integer. For the
general case (where $r_i$ may be fractional), we show that this decision
problem is coNP-complete. Our result complements the existing results: this
decision problem is coNP-complete for tri-valued utilities (where each agent's
value for each good belongs to $\{a,b,c\}$ for some prescribed $a>b>c\geq0$),
and this decision problem belongs to P for bi-valued utilities (where $r_i$ in
our model is the same for each agent).
  We further show that an EFX allocation always exists and can be computed in
polynomial time under the personalized bi-valued utilities setting, which
extends the previous result on bi-valued utilities. We propose the open problem
of whether an EFX and Pareto-optimal allocation always exists (and can be
computed in polynomial time).

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [188] [Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling](https://arxiv.org/abs/2507.17886)
*James B Aimone*

Main category: cs.NE

TL;DR: 神经拟态计算（NMC）是一种低功耗的计算范式，与传统CPU和GPU不同，它的能量消耗与算法状态的变化率有关，因此特别适合处理稀疏、可扩展的算法，如蒙特卡洛模拟。


<details>
  <summary>Details</summary>
Motivation: 尽管NMC被认为是低功耗的替代方案，但其计算价值尚不明确，需要界定其与传统计算架构的异同以及适用场景。

Method: 提出一种将NMC视为通用且可编程的计算范式，并分析其在时间和空间上的可扩展性，将其与理论上无限处理器系统进行比较。

Result: NMC的时间和空间扩展性等同于理论上无限处理器的传统系统，但能量扩展性显著不同：传统系统随绝对算法工作量扩展，而NMC随算法状态的导数扩展。NMC适用于稀疏、可扩展且活动与目标函数成比例的算法，例如迭代优化和大规模采样（如蒙特卡洛），而GPU则更适合线性代数等密集数值应用。

Conclusion: 神经拟态计算（NMC）与传统冯·诺依曼架构在能量扩展方面存在显著差异，适用于与传统多核系统（如GPU）不同的算法类别。

Abstract: Neuromorphic computing (NMC) is increasingly viewed as a low-power
alternative to conventional von Neumann architectures such as central
processing units (CPUs) and graphics processing units (GPUs), however the
computational value proposition has been difficult to define precisely.
  Here, we explain how NMC should be seen as general-purpose and programmable
even though it differs considerably from a conventional stored-program
architecture. We show that the time and space scaling of NMC is equivalent to
that of a theoretically infinite processor conventional system, however the
energy scaling is significantly different. Specifically, the energy of
conventional systems scales with absolute algorithm work, whereas the energy of
neuromorphic systems scales with the derivative of algorithm state. The unique
characteristics of NMC architectures make it well suited for different classes
of algorithms than conventional multi-core systems like GPUs that have been
optimized for dense numerical applications such as linear algebra. In contrast,
the unique characteristics of NMC make it ideally suited for scalable and
sparse algorithms whose activity is proportional to an objective function, such
as iterative optimization and large-scale sampling (e.g., Monte Carlo).

</details>


### [189] [Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers](https://arxiv.org/abs/2507.18179)
*Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli*

Main category: cs.NE

TL;DR: 通过将定点乘法器分解为子组件（如编码器和乘法器模块），利用带符号绝对值编码的功率效率，实现AI工作负载中常见的以零为中心的值的功率效率提升。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将定点乘法器分解为子组件来最大化其功率效率。

Method: 提出一种将定点乘法器分解为子组件的方法。首先，使用编码器块将操作数从二的补码转换为带符号的绝对值表示；然后，乘法器模块执行计算并将结果以原始格式输出。该方法利用带符号绝对值编码的功率效率。为确保计算格式不变，对这两个组件进行单独综合和优化。

Result: 在标准差为3.0的正常分布输入下，4位乘法器设计的开关活动比未分解的综合设计低12.9%。如果放宽合规性要求并使用-7到+7的输入范围，开关活动降低可达33%。基于开关活动的综合优化方法可进一步提高5-10%的功率效率。

Conclusion: 该方法通过将定点乘法器分解为子组件，显著提高了功率效率，尤其是在人工智能工作负载中常见的以零为中心的值。4位乘法器设计在标准差为3.0的正常分布输入下，与未分解的综合相比，开关活动降低了12.9%。如果放宽合规性要求并使用-7到+7的输入范围，开关活动降低可达33%。此外，基于开关活动的综合优化方法可进一步提高5-10%的功率效率。

Abstract: This work presents a method to maximize power-efficiency of fixed point
multiplier units by decomposing them into sub-components. First, an encoder
block converts the operands from a two's complement to a sign magnitude
representation, followed by a multiplier module which performs the compute
operation and outputs the resulting value in the original format. This allows
to leverage the power-efficiency of the Sign Magnitude encoding for the
multiplication. To ensure the computing format is not altered, those two
components are synthesized and optimized separately. Our method leads to
significant power savings for input values centered around zero, as commonly
encountered in AI workloads. Under a realistic input stream with values
normally distributed with a standard deviation of 3.0, post-synthesis
simulations of the 4-bit multiplier design show up to 12.9% lower switching
activity compared to synthesis without decomposition. Those gains are achieved
while ensuring compliance into any production-ready system as the overall
circuit stays logic-equivalent. With the compliance lifted and a slightly
smaller input range of -7 to +7, switching activity reductions can reach up to
33%. Additionally, we demonstrate that synthesis optimization methods based on
switching-activity-driven design space exploration can yield a further 5-10%
improvement in power-efficiency compared to a power agnostic approach.

</details>


### [190] [Contraction, Criticality, and Capacity: A Dynamical-Systems Perspective on Echo-State Networks](https://arxiv.org/abs/2507.18467)
*Pradeep Singh,Lavanya Sankaranarayanan,Balasubramanian Raman*

Main category: cs.NE

TL;DR: 一项新的研究为回声状态网络（ESNs）提供了统一的数学和神经科学视角，解决了稳定性、记忆和表达能力问题，并提供了工程设计指南。


<details>
  <summary>Details</summary>
Motivation: 尽管回声状态网络（ESNs）在处理时间序列方面效率显著，但在稳定性、记忆和表达能力方面的基本问题仍然分散在各个学科中。

Method: 该研究提出了一个统一的动力学系统处理方法，结合了泛函分析、随机吸引子理论和神经科学的最新发现。具体来说，它解决了ESNs的稳定性、记忆和表达能力问题。

Result: 研究证明了回声状态属性（初始条件的洗出）与全局Lipschitz动力学相结合，必然产生渐近记忆属性（对远程输入的几何遗忘）。通过Stone-Weierstrass策略，研究提供了一个简化的证明，表明具有多项式Reservoir和线性读出的ESNs在因果、时不变渐近记忆滤波器巴拿赫空间中是稠密的，将通用性扩展到随机输入。此外，通过记忆容量谱量化了计算资源，并展示了拓扑结构和泄漏率如何重新分配延迟特定的容量，并将这些权衡与“混沌边缘”的Lyapunov谱联系起来。

Conclusion: ESNs可以视为斜积随机动力系统，它们具有单点回拉吸引子，并具有条件Lyapunov界限，这与皮层关键性相吻合。该分析提供了具体的工程设计规则——谱半径、输入增益、激活选择——这些规则同时基于数学和神经科学，并阐明了为什么中等大小的Reservoir在实践中常常能与完全训练的循环网络相媲美。

Abstract: Echo-State Networks (ESNs) distil a key neurobiological insight: richly
recurrent but fixed circuitry combined with adaptive linear read-outs can
transform temporal streams with remarkable efficiency. Yet fundamental
questions about stability, memory and expressive power remain fragmented across
disciplines. We present a unified, dynamical-systems treatment that weaves
together functional analysis, random attractor theory and recent
neuroscientific findings. First, on compact multivariate input alphabets we
prove that the Echo-State Property (wash-out of initial conditions) together
with global Lipschitz dynamics necessarily yields the Fading-Memory Property
(geometric forgetting of remote inputs). Tight algebraic tests translate
activation-specific Lipschitz constants into certified spectral-norm bounds,
covering both saturating and rectifying nonlinearities. Second, employing a
Stone-Weierstrass strategy we give a streamlined proof that ESNs with
polynomial reservoirs and linear read-outs are dense in the Banach space of
causal, time-invariant fading-memory filters, extending universality to
stochastic inputs. Third, we quantify computational resources via
memory-capacity spectrum, show how topology and leak rate redistribute
delay-specific capacities, and link these trade-offs to Lyapunov spectra at the
\textit{edge of chaos}. Finally, casting ESNs as skew-product random dynamical
systems, we establish existence of singleton pullback attractors and derive
conditional Lyapunov bounds, providing a rigorous analogue to cortical
criticality. The analysis yields concrete design rules-spectral radius, input
gain, activation choice-grounded simultaneously in mathematics and
neuroscience, and clarifies why modest-sized reservoirs often rival fully
trained recurrent networks in practice.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [191] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 符号回归（SR）结合答案集编程（ASP）用于流体动力学建模，生成了可解释的流动方程，并提高了模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机源于认识到在流体动力学领域，对流动物理学的理解与准确预测同样重要。因此，研究旨在利用符号回归（SR）这一无需预设模型结构即可揭示复杂物理系统数学关系的方法，来处理流体动力学建模中的“黑箱”问题。

Method: 本研究采用符号回归（SR）方法，并结合了知识表示框架——答案集编程（ASP）。具体而言，研究人员使用PySR库从数值模拟数据中直接生成了用于模拟三维不可压缩流的紧凑符号方程。然后，将这些生成的方程与ASP相结合，以确保其统计准确性和物理合理性。

Result: 研究成功地从数值模拟数据中推导出了能够近似模拟三维直角通道层流轴向速度和压力场的紧凑符号方程。这些方程不仅准确地再现了抛物线速度分布和压力降等流动特征，而且与文献中的解析解完全一致。此外，提出的SR/ASP混合框架证明了其能够生成既统计准确又符合物理规律的符号表达式。

Conclusion: 该研究展示了符号回归（SR）在流体动力学建模中的潜力，能够将复杂流动行为简化为简洁、可解释的方程。此外，通过将SR与ASP相结合，提高了SR模型的物理合理性和可靠性，为需要可解释预测和实时数据分析的应用领域（如工程和科学研究）开辟了道路。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [192] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 本研究提出了一个Agentic AI框架，通过一系列专用代理自动处理临床数据管道，以降低医疗保健领域机器学习解决方案的成本和劳动强度。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健领域构建和部署机器学习解决方案由于预处理工作流碎片化、模型兼容性问题以及严格的数据隐私限制，成本高昂且劳动密集。

Method: 介绍了一个Agentic AI框架，通过模块化的、特定任务的代理系统，自动处理从摄入到推理的整个临床数据管道。这些代理处理结构化和非结构化数据，自动进行特征选择、模型选择和预处理推荐，无需人工干预。

Result: 在老年病学、姑息治疗和结肠镜成像的公开数据集上评估了该系统。以结构化数据（焦虑数据）和非结构化数据（结肠镜息肉数据）为例，展示了从文件类型检测、数据匿名化、特征提取、模型选择、预处理到推理和可解释输出的自动化流程。

Conclusion: 通过自动化高摩擦环节，该框架为在临床环境中运行人工智能提供了一条可扩展、经济高效的途径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [193] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA通过模拟数据分析的认知步骤来改进基于LLM的代理，以实现更连贯的规划和更有意义的洞察。


<details>
  <summary>Details</summary>
Motivation: 以往的智能体系统在自动化见解生成方面虽然有效，但往往忽略了分析思维背后结构化的推理过程。然而，现实世界的数据分析需要一个一致的认知工作流，包括解释模糊的目标、在背景知识中确定它们、构建抽象计划以及根据中间结果调整执行。

Method: I2I-STRADA（Information-to-Insight via Structured Reasoning Agent for Data Analysis）是一个代理架构，通过模块化子任务来形式化分析过程，这些子任务反映了分析推理的认知步骤。

Result: I2I-STRADA在DABstep和DABench基准测试中，在规划一致性和洞察对齐方面优于以往的系统。

Conclusion: I2I-STRADA通过建模分析如何通过反映分析推理的认知步骤的模块化子任务来展开，在DABstep和DABench基准测试中表现优于以往的系统，在规划一致性和洞察对齐方面尤为突出，强调了在数据分析的代理设计中结构化认知工作流的重要性。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [194] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: SmartAPS是一个基于大语言模型的对话系统，它通过自然语言交互简化了高级规划系统（APS）的使用，解决了传统APS成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的高级规划系统（APS）虽然功能强大，但由于需要昂贵的顾问进行定制和维护，导致许多客户无法负担其使用成本。为了满足供应链规划人员对更易于使用的APS的需求，本研究旨在降低APS的使用门槛。

Method: 本研究提出并实现了一个名为SmartAPS的对话系统。该系统构建在一个工具增强的大语言模型之上，并提供了一个自然语言的聊天界面，用户可以通过该界面与APS进行交互。

Result: SmartAPS 成功地为操作规划人员提供了一个直观的自然语言聊天界面，使他们能够更方便地查询信息、进行反事实推理、接收推荐和执行场景分析，从而更好地管理其运营。

Conclusion: SmartAPS 作为一个基于工具增强的大语言模型的对话系统，通过提供自然语言的交互方式，降低了高级规划系统（APS）的使用门槛，解决了传统APS因定制和维护成本高昂而导致的用户受限问题。它使得操作规划人员能够更直观地查询信息、进行反事实推理、获取建议和执行场景分析。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [195] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 提出了一种定性时间线规划的子集，可以直接用确定性有限自动机合成规划策略，解决了现有方法的效率问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决在定性时间线规划中，由于需要进行昂贵的确定化步骤而无法直接使用非确定性自动机合成规划策略的问题。

Method: 通过识别定性时间线规划的一个特定片段，并确定包含在确定性片段中的最大艾伦关系（Allen's relations）子集，将规划存在性问题转化为确定性有限自动机的非空性问题。

Result: 识别了一个定性时间线规划的片段，其规划存在性问题可直接映射到确定性有限自动机的非空性问题，并确定了艾伦关系的最大确定性子集。

Conclusion: 确定了一个定性时间线规划（qualitative timeline-based planning）的子集，该子集的规划存在性问题可以直接映射到确定性有限自动机（deterministic finite automata）的非空性问题，从而能够直接合成规划策略。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [196] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO通过整合中心化指导和去中心化执行，以及使用自回归联合策略进行协调探索，来改进多智能体强化学习中的CTDE范式，并在广泛的任务中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CTDE方法在中心化训练利用不足或缺乏理论保证方面存在不足。本研究旨在提出一种新的框架，通过整合中心化指导和去中心化执行，更好地利用中心化训练。

Method: MAGPO框架整合了中心化指导和去中心化执行，通过自回归联合策略实现可扩展的协调探索，并显式地将其与去中心化策略对齐，以确保在部分可观测性下的可部署性。

Result: MAGPO在43个任务和6个环境中持续优于强CTDE基线方法，并能匹配或超越完全中心化的方法。

Conclusion: MAGPO是一个原则上可行且在实践中有效的去中心化多智能体学习解决方案，它在43个任务和6个环境中持续优于现有的CTDE基线方法，并能与完全中心化的方法相媲美或超越。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [197] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: AI研究通过E.A.R.T.H.框架，利用错误生成创意内容，在创造力、新颖性和相关性方面取得显著提升，并得到用户认可。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何超越模仿，实现真正的创造力。研究的核心观点是“创造潜力隐藏在失败之中”，并旨在通过一个结构化的框架来利用和放大模型生成过程中的错误，以提升AI的创造性输出。

Method: 本研究提出E.A.R.T.H.框架，一个包含错误生成、放大、精炼选择、转换和反馈利用五个阶段的生成流程。该框架借鉴认知科学和生成模型，利用结构化提示、语义评分和人工评估来处理模型生成中的错误。研究使用了LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion等模型，并通过新颖性、惊喜度和相关性等综合奖励函数进行优化。

Result: 在精炼阶段，创造力得分提高了52.5%（从1.179到1.898）。最终输出的创造力得分达到2.010，相比之下提高了70.4%。生成的口号更短（缩短40.7%）、更具新颖性（新颖性提高48.4%），而相关性仅下降4.0%。跨模态测试表明，口号与图像具有高度一致性（CLIPScore: 0.249; BERTScore F1: 0.816）。在人工评估中，60%的输出得分在4.0分及以上，其中比喻性口号（平均4.09分）优于字面性口号（3.99分）。用户反馈强调了风格精确性和情感共鸣。

Conclusion: 本研究提出E.A.R.T.H.框架，一个将模型错误转化为创意资产的五阶段生成流程，证明了以错误为中心、以反馈驱动的生成可以增强AI的创造力，为迈向自主进化、与人类对齐的创意AI提供了一条可扩展的路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [198] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究发现，AI系统（如GPT 4.1和Claude 3.5）在分析数据时，尤其是在处理复杂数据集时，可以从散点图等可视化中受益，从而提高数据描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨图表和图形不仅对人类，而且对AI系统在数据分析方面的潜在益处。

Method: 本研究通过在三种代表性分析任务中，对两个商业视觉语言模型（GPT 4.1和Claude 3.5）进行一系列实验来评估可视化对AI系统的影响。实验通过比较提供合成数据集及其散点图、提供空白图表以及提供数据不匹配图表三种情况下的模型表现来进行。

Result: 实验结果表明，当原始数据附带散点图时，两个AI系统（GPT 4.1和Claude 3.5）能够更精确和准确地描述合成数据集，特别是在数据集变得复杂的情况下。与提供空白图表和数据不匹配图表的基线相比，这种性能提升归因于图表的实际内容。

Conclusion: AI系统（例如GPT 4.1和Claude 3.5）在处理和分析数据时，尤其是在处理复杂数据集时，可以从可视化（如散点图）中受益，这表明AI像人类一样可以从图表中获取信息。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [199] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 均值GNN的表达能力研究：在非均匀设置下优于最大GNN，劣于求和GNN；在均匀设置下，若满足特定假设，则弱于求和GNN和最大GNN。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络（GNNs）在以均值为聚合函数时的表达能力。

Method: 在非均匀设置中，证明了均值GNN的表达能力与Ratio Modal Logic相同。在均匀设置中，假设组合函数连续且分类函数为阈值，证明了其表达能力与自由交替模态逻辑相当。

Result: 均值GNN在非均匀设置下的表达能力与Ratio Modal Logic相同，在均匀设置下与自由交替模态逻辑相当。

Conclusion: 均值GNN在非均匀设置下的表达能力与Ratio Modal Logic相同，高于最大聚合GNN，低于求和聚合GNN。在均匀设置下，其表达能力与自由交替模态逻辑相当，但低于求和GNN和最大GNN。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [200] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch 实现了人工智能在 AI 研究领域的自主创新，通过自动假设、实现、训练和验证新的神经网络架构，发现了超越人类设计的性能最优的架构，并建立了可计算扩展的科学发现定律。


<details>
  <summary>Details</summary>
Motivation: AI 研究的进展受限于人类认知能力，ASI-Arch 旨在打破这一限制，实现人工智能进行自身架构创新。

Method: ASI-Arch 是一个全自动系统，能够自主假设新颖的架构概念，将它们实现为可执行代码，并通过严格的实验和过往经验对它们的性能进行训练和经验验证。

Result: ASI-Arch 进行了 1,773 次自主实验，耗时 20,000 多个 GPU 小时，发现了 106 种创新的、最先进的线性注意力架构，这些架构的性能超越了人类设计的基线。

Conclusion: ASI-Arch 能够自主进行科学研究，在神经架构发现领域实现了人工智能对自身架构的创新，发现了 106 种创新的、最先进的线性注意力架构，并建立了科学发现的经验扩展定律，将研究进展从受人类限制转变为可计算扩展的过程。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [201] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 使用NLP和主动学习从急诊记录中检测疫苗安全问题。


<details>
  <summary>Details</summary>
Motivation: 为了应对COVID-19疫苗接种后安全数据收集窗口有限以及早期广泛实施的挑战，需要建立有效的上市后监管系统。急诊分诊记录包含关键的患者信息，能够为及时的疫苗安全信号监测做出贡献。

Method: 利用自然语言处理（NLP）和主动学习技术，开发了一个能够从急诊部门（ED）记录中检测潜在疫苗安全问题的分类器。该方法结合了主动学习、数据增强以及主动学习和评估技术，以优化注释过程和数据质量，从而加快模型实现并提高模型性能。

Result: 该研究旨在开发一个分类器，以提高从急诊分诊记录中进行疫苗安全监测的效率和准确性，并认为NLP和主动学习的结合可以实现这一目标。

Conclusion: 该研究结合了主动学习、数据增强和评估技术，开发了一个用于从急诊分诊记录中加强疫苗安全监测的分类器。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [202] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 提出认知归因框架，区分LLM的知识和推理。发现推理具有领域特异性，参数缩放影响推理和知识，且知识和推理在网络层级中有不同的分布。


<details>
  <summary>Details</summary>
Motivation: 区分大型语言模型（LLMs）的知识和推理能力对于模型分析、可解释性和开发至关重要。

Method: 受到双系统认知理论的启发，提出了一种认知归因框架，将大型语言模型的认知过程分解为知识检索（阶段1）和推理调整（阶段2）。通过在“快速思考”和“慢速思考”两种不同的认知模式下提示模型生成答案，并分析不同模式下的表现，来量化知识和推理的贡献。该框架应用于15个大型语言模型和3个数据集。

Result: 1.推理调整具有领域特异性，在需要大量推理的领域（如数学、物理和化学）有益，而在知识密集型领域可能产生负面影响。2.参数缩放能够同时提升知识和推理能力，但对知识的提升更为显著。参数缩放显著提高了模型的推理审慎性，同时适度提高了其智能性。3.知识主要驻留在网络的较低层，而推理则在较高层进行操作。

Conclusion: 该研究提出的认知归因框架能够有效分离大型语言模型中的知识和推理能力，并揭示了推理的领域特异性、参数缩放的影响以及知识和推理在网络层级中的分布。这些发现为理解和改进大型语言模型提供了新的视角。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [203] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: 本文比较了四种非模型最小化的ASP析取语义，发现其中三种（Forks、Justified Models、DI）实际上是相同的，并且它们提供比Strongly Supported Models更强的语义。


<details>
  <summary>Details</summary>
Motivation: 探讨在答案集编程中，不遵循模型最小化原则的析取语义。

Method: 比较了四种不同的析取语义（Justified Models、Strongly Supported Models、Forks、Determining Inference（DI）语义），这些语义与稳定模型不同，不遵循模型最小化原则。

Result: 证明了Forks、Justified Models和DI语义的合理放宽实际上是相同的，并且这种共同语义总是提供一个稳定模型集合的超集，严格强于Strongly Supported Models语义。

Conclusion: 三种非模型最小化语义（Forks、Justified Models和DI语义的合理放宽）实际上是相同的，它们构成了一个共同的单一方法，只是定义不同。这种共同的语义总是提供一个稳定模型集合的超集，并且严格强于第四种方法（Strongly Supported Models），后者将析取视为经典逻辑。

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [204] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 本研究提出了一个概念框架，用于在欧盟《人工智能法案》的背景下对人工智能进行定性风险评估，整合了定义性平衡和可废止推理，以解决法律合规性和基本权利保护的复杂性。该框架强调对人工智能部署场景的分析，并为人工智能风险分析的逻辑描述提供了哲学基础。


<details>
  <summary>Details</summary>
Motivation: 本章旨在为人工智能（AI）的定性风险评估提出一个概念框架，特别是在欧盟《人工智能法案》的背景下。该框架解决了法律合规性和基本权利保护的复杂性。

Method: 本研究提出了一种定性风险评估的概念框架，整合了定义性平衡和可废止推理，以解决欧盟《人工智能法案》背景下的法律合规性和基本权利保护的复杂性。定义性平衡采用比例分析来解决相互竞争的权利之间的冲突，而可废止推理则适应法律决策的动态性质。该方法强调需要对人工智能部署场景进行分析，并识别潜在的法律违规行为和对基本权利的多层次影响。

Result: 该研究为人工智能风险分析的逻辑描述提供了哲学基础，并考虑了在可废止推理中整合定义性平衡以及关于权利的背景促进或降级论证的基本构建块。

Conclusion: 该框架为高风险人工智能系统和通用人工智能（GPAI）系统提供了更具可操作性的评估模型，并强调了GPAI系统的广泛适用性。未来的工作旨在开发一个形式化模型和有效的算法，以加强人工智能风险评估，并连接理论见解与实际应用，以支持负责任的人工智能治理。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [205] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: This paper introduces an automated system for grading Physics exams using AI. It uses LLMs to understand student answers and then employs reasoning techniques to check correctness. The system was tested on over 1500 exam responses.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenging problem of automatically assessing the correctness of typed student answers in Physics exams with respect to a ground truth solution.

Method: The method involves using an LLM to interpret and correct student responses, followed by automated reasoning techniques (SMT solving and term rewriting systems) to assess correctness. The term rewrite system was specifically tailored for physics problems with trigonometric expressions.

Result: The system was evaluated on a large dataset of over 1500 student exam responses, demonstrating the effectiveness of the proposed approach.

Conclusion: The paper presents a novel method for automatically marking Physics exams by combining LLMs, CAS, SMT solvers, and term rewriting systems. The system was evaluated on over 1500 real-world student responses from the 2023 Australian Physics Olympiad.

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [206] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: ConDiFi基准评估LLM在金融领域的发散和汇聚推理能力，发现模型在创新性和可操作性方面存在差异。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理基准主要关注事实准确性或逐步逻辑，忽略了金融领域专业人士所需的创造性和未来预测能力。

Method: 引入了ConDiFi基准，包含607个用于发散推理的宏观金融提示和990个用于汇聚推理的多跳对抗选择题，评估了14个主流模型。

Result: 在评估的14个模型中，GPT-4o在创新性和可操作性方面表现不佳，而DeepSeek-R1和Cohere Command R+在生成可操作的、适合投资决策的见解方面表现突出。

Conclusion: ConDiFi提供了一个评估LLM在金融领域发散和汇聚推理能力的新视角，有助于LLM在金融领域的安全和战略部署。部分模型在指令遵循和多跳推理方面表现出色，但在创新性和可操作性方面仍有提升空间。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [207] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出了一种名为IB正则化的新方法，它基于信息瓶颈理论，可以帮助大型语言模型（LLM）进行更好的数学推理。该方法易于实现，只需修改一行代码，并能在现有框架上提升LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和可验证奖励（RLVR）的方法在引导LLM进行链式思考（CoT）推理方面取得了进展，但这些方法多是启发式和直觉驱动的，缺乏原则性的理论方法。本文旨在为LLM推理提供一个基于信息瓶颈原理的理论基础，以优化推理轨迹，使其既能提供关于最终正确答案的信息，又能在不同提示下具有普遍性。

Method: 提出了一种基于信息瓶颈（IB）原理的理论表征，并据此设计了IB-aware推理优化（IBRO）框架，包括一个token级别的代理目标和一种高效的近似方法，最终形成轻量级的IB正则化技术，该技术可直接嵌入现有基于强化学习的后训练框架，只需修改一行代码。

Result: 在多个数学推理基准和强化学习算法上验证了IB正则化，结果表明该方法能够持续提升LLM的推理性能。

Conclusion: 本文提出的IB正则化方法，通过信息瓶颈原理为LLM推理提供了理论基础，并能有效提升其在数学推理任务上的表现，且易于集成到现有框架中。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [208] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 本文使用基于模型（VI）和无模型（PPO）的方法来优化呼叫中心的呼叫路由，PPO 的效果更好，但训练时间更长。


<details>
  <summary>Details</summary>
Motivation: 本文旨在优化呼叫中心的呼叫路由，以最大限度地减少客户等待时间和员工空闲时间。

Method: 本文将价值迭代（VI）作为一种基于模型的方法与近端策略优化（PPO）作为一种无模型方法进行了比较，以优化呼叫中心的呼叫路由。

Result: PPO 策略在 1,000 次测试结束后，实现了最高的奖励、最短的客户等待时间和最短的员工空闲时间。

Conclusion: PPO 在此呼叫路由优化问题上取得了最佳结果，尽管其训练时间较长。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [209] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 该研究将CT算法移植到GPU上，以加速处理大规模的表约束。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的问题经常遇到包含大量取值选项的表约束，现有的基于CPU的CT算法难以有效处理这些大规模约束。

Method: 利用GPU的并行计算能力来优化CT算法，通过设计和实现GPU加速的CT算法，并将其集成到现有的约束求解器中，进行实验验证。

Result: GPU加速的CT算法能够有效处理大规模的表约束，并在实验中取得了良好的性能。

Conclusion: 该论文提出了一种利用GPU加速的紧凑表（CT）算法，以处理大规模的表约束，并验证了其在实际问题中的有效性。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [210] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: This paper examines how the data used to train concept probing models affects their performance in image classification and releases concept labels for two common datasets.


<details>
  <summary>Details</summary>
Motivation: Concept probing is a technique to interpret artificial neural networks by training additional classifiers to map internal representations into human-defined concepts. However, research has paid limited attention to the data used for training these probing models. This paper addresses this gap by examining the impact of data on probing model performance.

Method: The study focuses on concept probing in image classification tasks and investigates the effect of the data used to train probing models on their performance.

Result: The paper investigates the effect of data on the performance of concept probing models in image classification tasks. It also provides concept labels for two widely used datasets.

Conclusion: The paper investigates the effect of data used to train probing models on their performance in concept probing for image classification tasks and makes concept labels for two widely used datasets available.

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [211] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeLadder 框架通过安全导向的强化学习和多原则验证器，使 SafeWork-R1 模型在安全能力上取得显著提升（平均提高 46.54%），并且不牺牲通用能力，在安全性方面优于 GPT-4.1 和 Claude Opus 4。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够实现能力和安全协同进化的多模态推理模型，并解决以往对齐方法（如 RLHF）仅学习人类偏好的局限性。

Method: SafeLadder 框架，结合大规模、渐进式的安全导向强化学习后训练，并辅以一套多原则验证器。此外，还实现了两种不同的推理时干预方法和一种审议式搜索机制，以强制执行步级验证。

Result: SafeWork-R1 在安全相关基准测试上相比其基础模型 Qwen2.5-VL-72B 平均提高了 46.54%，且未损害通用能力。与 GPT-4.1 和 Claude Opus 4 等领先的专有模型相比，SafeWork-R1 表现出最先进的安全性能。研究还开发了 SafeWork-R1-InternVL3-78B、SafeWork-R1-DeepSeek-70B 和 SafeWork-R1-Qwen2.5VL-7B 等模型，证明了 SafeLadder 框架在构建鲁棒、可靠和可信赖的通用人工智能方面的通用性。

Conclusion: 该研究引入了 SafeWork-R1，一个先进的多模态推理模型，展示了能力和安全的协同进化。它由 SafeLadder 框架开发，该框架结合了大规模、渐进式的安全导向强化学习后训练，并辅以一套多原则验证器。SafeLadder 使 SafeWork-R1 能够发展出内在的安全推理和自我反思能力，产生安全上的“啊哈”时刻。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [212] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: IOTA 架构通过数据/流水线并行、激活压缩和公平激励机制，解决了现有分布式预训练模型在可扩展性、资源利用和奖励公平性方面的问题。


<details>
  <summary>Details</summary>
Motivation:  Bittensor 的 Subnet 9（SN9）虽然验证了基于区块链的分布式预训练的可行性，但存在矿工需要本地适应整个模型和“赢家通吃”奖励机制导致模型囤积的问题。本研究旨在解决这些限制。

Method: IOTA（Incentivized Orchestrated Training Architecture）提出了一种新的分布式预训练架构，通过数据和流水线并行，使模型层分布在异构的矿工之间，并通过流式传输激活来连接它们。该架构还采用了激励机制（如 CLASP）来根据矿工的边际效用进行公平分配，并通过激活压缩和 Butterfly All-Reduce 等技术优化了通信效率和可扩展性。

Result: 1. 实现了可扩展的模型大小，不再受限于单个机器的 VRAM；2. 实现了更公平、连续的激励机制，根据矿工的贡献分配奖励；3. 通过激活压缩将通信带宽降低了 128 倍；4. Butterfly All-Reduce 技术实现了 O(1) 带宽下的参数切片平均，提高了可扩展性和内置了欺诈检测；5. CLASP 机制能够公平地评估和分配贡献，并检测复杂的欺诈行为。

Conclusion: IOTA 架构通过数据并行和流水线并行将分散的竞争者转变为一个协作单元，实现了模型大小与参与者数量的任意扩展，并实现了公平的贡献者奖励。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [213] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe 是一种新的多 SLO 调度策略，可在大规模部署中保持高 SLO 满足率并最大化吞吐量。它通过将请求分箱、调度到服务器子集、创建负载梯度以实现自动扩展、允许请求共享实例以及使用分析数据来管理尾部延迟来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）应用的激增，它们对 token 生成延迟的要求多种多样。将工作负载简单地划分为延迟敏感（LS）或尽力而为（BE）会忽略 LS 类别中的细微差别，并导致次优的用户体验和调度机会。然而，有效服务具有多种 SLO 要求的请求带来了重大挑战，例如批次内请求同时生成 token、现有系统专注于整体请求率的自动扩展以及不同延迟敏感 SLO 之间无法容忍长时间延迟和需要控制尾部延迟。

Method: PolyServe 首先根据每个 token 的延迟要求将请求分组到多个桶中，然后将每个桶调度到服务器舰队的一个子集。PolyServe 将请求路由到负载最高但仍可达标的服务器，从而创建促进自动扩展的负载梯度。为了提高利用率，PolyServe 允许较松的 SLO 请求在自己的服务器饱和时共享较紧的 SLO 实例。PolyServe 使用分析数据来指导调度决策，并通过感知请求等待时间的调度、动态分块和连续分块预填充预测来管理尾部延迟。

Result: PolyServe 实现了 1.23 倍的吞吐量增益，达到了最优吞吐量的 92.5%。

Conclusion: PolyServe 相比现有策略实现了 1.23 倍的吞吐量增益，达到了最优吞吐量的 92.5%。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [214] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: PyTorch 在大规模QUBO问题求解方面表现最佳，能耗和运行时间均衡。


<details>
  <summary>Details</summary>
Motivation: 评估不同软件求解器在解决大规模二次无约束二元优化（QUBO）问题上的性能，重点关注解的质量（能量）和计算时间。

Method: 对Neal、PyTorch (CPU)、PyTorch (GPU)、JAX 和 SciPy 五种基于软件的QUBO求解器进行了基准测试，测试范围涵盖了1000x1000到45000x45000的随机生成的QUBO矩阵，并设置了从10^-1到10^-6的六个收敛阈值。

Result: Neal 求解器能达到最低能量值，但内存消耗高，仅限于最多6000个变量的问题。PyTorch 产生的能量值略高于 Neal，但扩展性更优，可解决最多45000个变量的问题，且通过GPU加速和CPU多线程显著缩短了运行时间。JAX 的能量值略高于 PyTorch，受限于25000个变量，运行时间与 PyTorch 的 GPU 相当。SciPy 的约束最大，只能处理最多6000个变量，能量值最高，计算时间最长。

Conclusion: 对于大规模QUBO问题，PyTorch是在计算资源允许的情况下最平衡的选择。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [215] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: RISC-V向量1.0在异构嵌入式SoC上显著提升深度学习性能和能效。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的嵌入式平台和异构架构上高效部署深度学习模型（特别是CNN），需要解决硬件集成、系统集成和编译执行模型等方面的挑战，以应对功耗/性能的权衡。

Method: 本文通过硬件集成和系统级分析，研究了异构架构中深度学习加速器的性能瓶颈，并利用RISC-V向量1.0扩展来优化预处理和CPU回退层执行。

Result: 研究结果表明，与CPU相比，使用RISC-V向量1.0可以将图像预处理速度提高高达9倍，YOLOv3的回退层执行速度提高高达3倍，同时功耗更低。

Conclusion: 该研究展示了RISC-V向量扩展1.0在加速富嵌入式SoC上的深度学习工作负载方面的潜力，通过优化硬件集成和利用其灵活的编程模型，可以实现比传统CPU更高的性能和能效。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [216] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 为了解决联邦学习（FL）通信成本高的问题，本研究引入了FIFO、LRU和基于优先级的缓存策略，通过只传输重要的模型更新来减少带宽使用，同时保持模型精度。实验证明该方法能有效降低通信成本，适用于智慧城市和医疗保健等场景。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在不集中化数据的情况下，允许多个分布式设备联合训练共享模型，但通信成本仍然是一个主要瓶颈，特别是在资源受限的环境中。

Method: 引入先进的缓存策略，如先进先出（FIFO）、最少最近使用（LRU）和基于优先级的缓存，以识别和传输最重要的模型更新，从而减少不必要的模型更新传输。

Result: 在CIFAR-10和医疗数据集上进行的实验表明，该方法能够减少通信量，同时只造成极小的精度损失。

Conclusion: 通过实验证明，智能缓存策略能够有效降低通信成本并保持模型精度，提高了联邦学习的可扩展性和内存效率，为边缘物联网网络的可靠部署提供了支持，尤其适用于智慧城市、医疗保健等延迟敏感型应用。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [217] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: 该研究提出了MultiKernelBench，一个用于评估LLM生成深度学习内核的多平台基准，解决了现有基准的局限性。该基准包含285个任务和14个内核类别，支持多种硬件平台。研究还提出了一种类别感知的一次性提示方法，并评估了七种先进LLM，揭示了LLM在不同平台上的性能差异和提示策略的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的用于评估LLM在深度学习内核生成领域的基准存在硬件支持有限、内核分类粗粒度以及任务覆盖不平衡等问题。为了解决这些局限性，需要一个更全面的基准。

Method: 提出MultiKernelBench基准，包含285个任务和14个内核类别，支持Nvidia GPU、Huawei NPU和Google TPU。设计了一个模块化的后端抽象层，以便轻松集成新硬件平台。提出了一种类别感知的一次性提示方法，通过提供类别内示例来提高生成质量。

Result: 通过评估七种先进LLM，MultiKernelBench揭示了任务难度的显著差异，表明LLM在训练暴露较少的平台上的泛化能力较差，并验证了目标提示策略的有效性。

Conclusion: 该研究提出了MultiKernelBench，一个全面的、多平台的基准，用于评估基于LLM的深度学习内核生成，解决了现有基准的局限性。通过对七种先进LLM的系统评估，揭示了任务难度的显著差异、在训练暴露较少的平台上的泛化能力差以及目标提示策略的有效性。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [218] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一个模块化的边缘AI平台，通过可热插拔的AI功能组件和FPGA加速器，为现场操作提供灵活、高性能的定制化生物识别和AI分析能力，具有良好的可扩展性和安全性。


<details>
  <summary>Details</summary>
Motivation: 为现场操作人员提供一种灵活、高性能、可快速适应的边缘AI系统，使其能够像搭积木一样组合定制化的生物识别和AI分析系统。

Method: CHAMP（Configurable Hot-swappable Architecture for Machine Perception）是一个模块化的边缘计算平台，利用低功耗FPGA加速器和自定义操作系统（VDiSK），允许动态更换AI功能组件（如面部识别、物体跟踪、文档分析），实现即插即用的AI管线和加密安全的数据集。

Result: 实验表明，CHAMP平台从1个扩展到5个神经网络计算加速器时，吞吐量实现了近乎线性的扩展，突显了基于USB3的总线在性能提升和饱和限制方面的表现。其中详细介绍了CHAMP的设计，包括其模块化扩展和VDiSK操作系统在运行时重构方面的能力，以及其加密功能如何保障模块上数据的安全和隐私。

Conclusion: CHAMP平台通过其模块化设计、低功耗FPGA加速器和VDiSK操作系统，实现了灵活、高性能的边缘AI系统，能够满足现场操作人员即时适应需求。实验证明了其近乎线性的吞吐量扩展能力，并展示了在现场生物识别、监控和灾难响应等领域的应用潜力，同时指出了未来在总线协议、功能组件和系统软件方面的改进方向。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [219] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 本论文提出了一种集成 NWDAF 和 UPF 的闭环架构，通过嵌入 AI 模型实现游戏分类和延迟估计，以优化移动边缘游戏的用户体验。


<details>
  <summary>Details</summary>
Motivation: 最新的游戏和通信技术对移动用户的服务管理和 SLA 合规性提出了挑战，现有的边缘游戏技术虽然提高了吞吐量、降低了延迟，但在非侵入式用户延迟测量等核心功能方面仍需改进。

Method: 提出了一种将网络数据分析功能（NWDAF）和用户平面功能（UPF）集成在一起的闭环架构，以实现对用户延迟的估计和延迟感知的5G控制平面。

Result: 研究结果表明，在 NWDAF 中嵌入人工智能模型能够实现游戏分类，并为移动边缘游戏研究开辟了新途径。

Conclusion: 本论文提出了一个将网络数据分析功能（NWDAF）和用户平面功能（UPF）相结合的闭环架构，用于估计用户延迟并使能延迟感知的5G控制平面，以应对最新的游戏和通信技术对移动用户服务管理和 SLA 合规性提出的挑战。研究结果表明，在 NWDAF 中嵌入人工智能模型能够实现游戏分类，为移动边缘游戏研究开辟了新途径。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [220] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: 由于大型AI模型对计算和功耗的需求巨大，训练工作负载被分散到地理上分散的站点。然而，这带来了通信开销的挑战。PowerTrip是一个新系统，通过动态选择站点并根据功率-成本启发式方法来优化功率-通信权衡，从而解决这一问题。评估显示，与现有方法相比，PowerTrip能将达到目标准确率的时间缩短多达50%。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型的计算和功耗需求已超出单个数据中心的处理能力，区域电网的有限供电能力限制了区域计算能力。因此，将训练工作负载分配到地理上分散的站点变得至关重要。然而，这种方法会带来通信开销的挑战，形成了访问更大聚合功率带来的性能提升与网络延迟增加带来的性能损失之间的根本权衡。现有方法侧重于减少通信量或使用分布式启发式方法，但它们假设供电能力恒定且同质，忽略了站点之间异质供电能力带来的挑战。

Method: PowerTrip是一个动态系统，通过功率-成本启发式方法，优先选择高功率可用性和低网络延迟的站点。它采用动态贪心方法，利用训练效率的边际收益（即每单位时间的准确率提升）来优化站点数量，以平衡增加计算能力的收益和网络开销带来的性能损失。

Result: 使用真实的Google电力追踪数据模拟实际的电力容量限制，评估结果表明，与现有的基线策略相比，PowerTrip能将达到目标准确率的时间缩短多达50%。

Conclusion: PowerTrip通过动态选择站点并根据功率-成本启发式方法优化功率-通信权衡，在功率受限、地理分布式环境中有效解决了训练大型模型的问题。与现有基线策略相比，它能将达到目标准确率的时间缩短多达50%。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [221] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 该论文提出了一种名为C-Koordinator的平台，用于解决微服务共置引起的性能干扰问题。通过使用CPI指标进行干扰预测，该平台能有效降低微服务延迟并提高性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着微服务在企业中的广泛应用和资源共享的需求，如何有效管理和缓解因微服务共置引起的资源竞争和性能干扰成为关键问题。

Method: 通过分析Alibaba的微服务集群特征，提出并实现了基于CPI（每指令周期）的干扰预测和缓解策略，并设计了C-Koordinator平台。

Result: C-Koordinator平台的干扰预测模型准确率超过90.3%，能够精确预测并快速缓解干扰，从而在各种系统负载下将应用延迟（P50、P90、P99响应时间）降低并稳定16.7%至36.1%。

Conclusion: 该研究展示了C-Koordinator平台在减少微服务延迟和稳定性能方面的有效性，在各种系统负载下实现了16.7%至36.1%的性能提升。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [222] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: CoCoServe是一个创新的弹性LLM服务系统，通过模块级动态扩展解决了资源管理和性能问题，显著降低成本并提升服务效率。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM服务系统中资源管理面临的挑战，即在满足服务需求、管理有限资源以及适应不可预测流量模式之间取得平衡，并克服静态部署导致的资源利用率低下和动态扩展成本高的问题。

Method: 提出CoCoServe系统，通过模块级（如解码器层、投影）的复制和迁移来实现LLM服务的动态和细粒度扩展，并开发了一种自动扩展机制来动态调节资源分配和性能优化。

Result: CoCoServe的扩展操作具有良好的可扩展性，成本降低46%，同时保持可用性。与Hugging Face Transformers和vLLM等先进系统相比，CoCoServe在不同模型大小和工作负载下，平均延迟降低14%-75%，吞吐量提高1.16倍-4倍。

Conclusion: CoCoServe通过模块级操作实现了LLM服务的弹性扩展，在降低成本（降低46%）和提升性能（延迟降低14%-75%，吞吐量提高1.16倍-4倍）方面优于现有系统。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [223] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: Cloud Native技术通过Kubernetes等手段优化了LLM云端推理的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在云环境中部署时面临的计算需求大、资源利用率低、成本高、延迟大和可扩展性受限等问题。

Method: 通过利用容器化、微服务和动态调度等Cloud Native技术，并结合基于Kubernetes的自动伸缩进行实际评估。

Result: 证明了Cloud Native系统能够实现更高效的资源分配，减少延迟，并在高需求场景下提高吞吐量。

Conclusion: Cloud Native技术通过容器化、微服务和动态调度等手段，能够显著提升LLM推理服务的效率、降低延迟并增强吞吐量，尤其是在高需求场景下。基于Kubernetes的自动伸缩等实际评估表明，Cloud Native架构能够动态适应工作负载变化，缓解性能瓶颈，优化LLM推理服务性能。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [224] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: FCPO是一种结合了持续强化学习（CRL）和联邦强化学习（FRL）的新型EVA系统，通过动态调整推理参数，显著提高了吞吐量、降低了延迟，并减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的EVA调度系统在异构设备上优化全局工作负载分布，但面临调度周期长、环境变化快导致次优处理的问题。局部强化学习（RL）虽能快速调整，但存在可扩展性、知识整合和适应性问题。

Method: FCPO结合了持续强化学习（CRL）和联邦强化学习（FRL），通过特定代理的聚合方案和面向多样性的经验缓冲区来动态调整推理批处理大小、输入分辨率和预/后处理的多线程。

Result: 实验在真实EVA测试环境中进行，结果显示FCPO在有效吞吐量方面提高了5倍以上，延迟降低了60%，收敛速度提高了20%，同时内存消耗减少了10倍以上，相比于现有的基于RL的方法。

Conclusion: FCPO通过结合持续强化学习（CRL）和联邦强化学习（FRL），并采用特定代理的聚合方案和面向多样性的经验缓冲区，解决了边缘视频分析（EVA）中实时推理服务系统的挑战。实验证明，FCPO在有效吞吐量、延迟和收敛速度方面均优于现有技术。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [225] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 通过异步监听、负载均衡、实体交互优化和空间哈希，新框架显著加速了并行离散事件模拟，提升了大规模和复杂模拟的效率。


<details>
  <summary>Details</summary>
Motivation: 传统的并行离散事件模拟（PDES）引擎在处理复杂模拟时存在可扩展性限制，特别是Warped2引擎在资源分配和实体交互方面存在效率问题。

Method: 本研究提出了一种优化框架，包含四个关键改进：1. 引入异步监听器线程以减少事件监控延迟。2. 采用METIS进行负载均衡，优化动态事件分配。3. 设计了包含约束满足机制的实体交互求解器以减少状态冲突。4. 集成了空间哈希算法以优化大规模邻近搜索。

Result: 实验证明，该框架相比基线实现有16倍的加速，并能在MPI和Pthreads实现中保持8倍于单线程配置的加速。负载均衡和LP迁移策略将同步开销降低了58.18%，其中负载均衡是主要的优化因素（占57%）。

Conclusion: 该框架通过异步监听器线程、基于METIS的负载均衡策略、具有约束满足机制的实体交互求解器以及空间哈希算法，显著提高了并行离散事件模拟（PDES）引擎的性能和效率，特别是在处理大规模仿真和复杂实体交互方面。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [226] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: Reinforcement learning is used to dynamically adapt data replication strategies to workload changes and system architectures, optimizing quality of service, profit, and environmental impact.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of global data volumes has created a demand for scalable distributed systems that can maintain a high quality of service. Data replication is a widely used technique that provides fault tolerance, improved performance and higher availability. Traditional implementations often rely on threshold-based activation mechanisms, which can vary depending on workload changes and system architecture. System administrators typically bear the responsibility of adjusting these thresholds.

Method: Reinforcement learning can be used to dynamically adapt to workload changes and different architectures. We present the architecture behind our solution and describe the reinforcement learning model by defining the states, actions and rewards.

Result: The strategy's aim is to provide satisfactory Quality of Service while optimizing a trade-off between provider profit and environmental impact.

Conclusion: We propose a novel data replication strategy for cloud systems that employs reinforcement learning to automatically learn system characteristics and adapt to workload changes. The strategy's aim is to provide satisfactory Quality of Service while optimizing a trade-off between provider profit and environmental impact.

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [227] [Program Logics via Distributive Monoidal Categories](https://arxiv.org/abs/2507.18238)
*Filippo Bonchi,Elena Di Lavore,Mario Román,Sam Staton*

Main category: cs.LO

TL;DR: 本文利用范畴论推导程序逻辑。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是为程序逻辑提供一个坚实的理论基础，并展示如何从范畴论的概念中推导出这些逻辑。

Method: 从命令式范畴的公理（即，统一追踪的分布式复制-丢弃范畴）中推导出多种程序逻辑，包括正确性、不正确性和关系型霍尔逻辑。在此基础上，引入了命令式多范畴的内部语言，并为其改编的 Dijkstra 保护命令语言推导了组合子。程序逻辑的规则是从该内部语言中推导出来的。

Result: 成功地从命令式范畴的公理中推导出了多种程序逻辑，并为 Dijkstra 的保护命令语言开发了组合子。

Conclusion: 程序逻辑可以从命令式范畴的公理中推导出来。

Abstract: We derive multiple program logics, including correctness, incorrectness, and
relational Hoare logic, from the axioms of imperative categories: uniformly
traced distributive copy-discard categories. We introduce an internal language
for imperative multicategories, on top of which we derive combinators for an
adaptation of Dijkstra's guarded command language. Rules of program logics are
derived from this internal language.

</details>


### [228] [Resourceful Traces for Commuting Processes](https://arxiv.org/abs/2507.18246)
*Matthew Earnshaw,Chad Nester,Mario Román*

Main category: cs.LO

TL;DR: Traces as transformations offer a new way to view effectful categories, leading to a graphical calculus and a method for combining systems where actions commute.


<details>
  <summary>Details</summary>
Motivation: To obtain a novel notion of presentation for effectful categories by considering actions of Mazurkiewicz traces not merely as atomic, but as transformations.

Method: Viewing Mazurkiewicz trace actions as transformations from specified input types to specified output types to obtain a novel notion of presentation for effectful categories.

Result: A novel notion of presentation for effectful categories, giving rise to a graphical calculus and a construction of the commuting tensor product of free effectful categories.

Conclusion: The presented novel notion of presentation for effectful categories, derived from viewing Mazurkiewicz trace actions as transformations, provides a graphical calculus and a construction for the commuting tensor product of free effectful categories.

Abstract: We show that, when the actions of a Mazurkiewicz trace are considered not
merely as atomic (i.e., mere names) but transformations from a specified type
of inputs to a specified type of outputs, we obtain a novel notion of
presentation for effectful categories (also known as generalised Freyd
categories), a well-known algebraic structure in the semantics of
side-effecting computation. Like the usual representation of traces as graphs,
our notion of presentation gives rise to a graphical calculus for effectful
categories. We use our presentations to give a construction of the commuting
tensor product of free effectful categories, capturing the combination of
systems in which the actions of each must commute with one another, while still
permitting exchange of resources

</details>


### [229] [Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures](https://arxiv.org/abs/2507.18418)
*Jean Goubault-Larrecq*

Main category: cs.LO

TL;DR: 研究了如何显式构造并验证由两个单子 S 和 T 结合而成的复合单子 U，揭示了弱分配律和分配反 the retractions 之间的一一对应关系，并通过三个具体应用展示了该理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何显式地构造由两个单子 S 和 T 结合而成的复合单子 U，以及如何验证 U 的正确性。

Method: 当已确定 U 的形式后，通过展示一个从 ST 到 U 的分配反 the retraction，即可证明 U 是由 S 和 T 结合而成的。

Result: 给出了三个应用实例，其中 S 分别是 Smyth、Hoare 或 Plotkin 极域单子，T 是连续估值单子，U 是依情况而定的预示或分叉单子。作为副产品，研究了超线性或次线性预示单子的代。

Conclusion: 弱分配律和分配反 the retractions 之间存在一一对应的关系，并且该对应关系在二维分类的设定下成立。

Abstract: Given two monads $S$, $T$ on a category where idempotents split, and a weak
distributive law between them, one can build a combined monad $U$. Making
explicit what this monad $U$ is requires some effort. When we already have an
idea what $U$ should be, we show how to recognize that $U$ is indeed the
combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a
distributing retraction of $ST$ onto $U$. We show that distributing retractions
and weak distributive laws are in one-to-one correspondence, in a 2-categorical
setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin
hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad
of previsions or of forks, depending on the case. As a byproduct, this allows
us to describe the algebras of monads of superlinear, resp. sublinear
previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace
monad is sometimes known as the Vietoris monad, the monad of probability
valuations coincides with the Radon monad, and we infer that the associated
combined monad is the monad of normalized forks.

</details>


### [230] [Well-Founded Coalgebras Meet König's Lemma](https://arxiv.org/abs/2507.18539)
*Henning Urbat,Thorsten Wißmann*

Main category: cs.LO

TL;DR: K"onig's lemma 的 coalgebraic 版本被提出，适用于更广泛的范畴，并具有实际应用。此外，还提供了构造初代数的新方法。


<details>
  <summary>Details</summary>
Motivation: 将 K"onig's lemma 从有限分支树推广到更广泛的范畴，并探索其在不同数学结构中的应用。

Method: 提出 K"onig's lemma 的 coalgebraic 版本，将有限分支树推广到 finitary endofunctor H 的 coalgebra，并将基础范畴从集合推广到局部有限可呈现范畴 C。

Result: 证明了在温和的假设下，每个良基的 H-coalgebra 都是其具有有限生成状态空间的良基子coalgebra 的有向连接，并且良基 coalgebra 的范畴是局部可呈现的。

Conclusion: K"onig's lemma 的一个 coalgebraic 版本被提出，该版本适用于更广泛的范畴，并且具有实际应用，例如在 topos、nominal 和 convex 转换系统中的 K"onig's lemma 版本。此外，我们还发现证明所依赖的关键构造可以用来构造初代数（最终递归的 coalgebra），其中一个构造是全新的，而另一个构造则提供了一个新的简短且清晰的正确性证明。

Abstract: K\"onig's lemma is a fundamental result about trees with countless
applications in mathematics and computer science. In contrapositive form, it
states that if a tree is finitely branching and well-founded (i.e. has no
infinite paths), then it is finite. We present a coalgebraic version of
K\"onig's lemma featuring two dimensions of generalization: from finitely
branching trees to coalgebras for a finitary endofunctor H, and from the base
category of sets to a locally finitely presentable category C, such as the
category of posets, nominal sets, or convex sets. Our coalgebraic K\"onig's
lemma states that, under mild assumptions on C and H, every well-founded
coalgebra for H is the directed join of its well-founded subcoalgebras with
finitely generated state space -- in particular, the category of well-founded
coalgebras is locally presentable. As applications, we derive versions of
K\"onig's lemma for graphs in a topos as well as for nominal and convex
transition systems. Additionally, we show that the key construction underlying
the proof gives rise to two simple constructions of the initial algebra
(equivalently, the final recursive coalgebra) for the functor H: The initial
algebra is both the colimit of all well-founded and of all recursive coalgebras
with finitely presentable state space. Remarkably, this result holds even in
settings where well-founded coalgebras form a proper subclass of recursive
ones. The first construction of the initial algebra is entirely new, while for
the second one our approach yields a short and transparent new correctness
proof.

</details>


### [231] [Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications](https://arxiv.org/abs/2507.18567)
*Ruben Gamboa,Panagiotis Manolios*

Main category: cs.LO

TL;DR: ACL2 is a theorem proving system.


<details>
  <summary>Details</summary>
Motivation: The ACL2 Workshop series is the major technical forum for users of the ACL2 theorem proving system to present research related to the ACL2 theorem prover and its applications.

Method: The ACL2 theorem proving system.

Result: The 2005 ACM Software System Award was awarded to Boyer, Kaufmann, and Moore for their work on ACL2 and the other theorem provers in the Boyer-Moore family.

Conclusion: ACL2 is an industrial-strength automated reasoning system, the latest in the Boyer-Moore family of theorem provers.

Abstract: The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.

</details>


### [232] [Approximate SMT Counting Beyond Discrete Domains](https://arxiv.org/abs/2507.18612)
*Arijit Shaw,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: pact是一种用于混合SMT公式的SMT模型计数器，它使用基于哈希的近似计数技术，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器在处理包含离散和连续变量的混合SMT公式的模型计数方面存在局限性，特别是当需要将解投影到离散域时。这促使了对能够处理这类混合公式的SMT计数器的需求。

Method: pact使用基于哈希的近似模型计数来估计解决方案，并具有理论保证。它利用优化的哈希函数，相对于投影变量，SMT求解器的调用次数是对数级别的。

Result: 在14,202个实例的大型基准测试中，pact成功完成了603个实例，而基线方法仅完成了13个实例，显示了pact的显著性能优势。

Conclusion: pact在混合SMT公式的SMT模型计数方面取得了显著的性能提升，能够处理比现有方法更广泛的公式。

Abstract: Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [233] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: MFNO是一种新型神经网络，能有效学习随机系统动力学，并处理非周期性输入，在准确性和生成速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够学习随机系统动力学的算子基神经网络，特别是处理非周期性输入的情况。

Method: MFNO通过引入镜像填充来扩展标准FNO，以处理非周期性输入。理论分析基于Wong--Zakai类型定理和各种逼近技术。

Result: MFNO展现出强大的分辨率泛化能力，并且在性能上可以与LSTM、TCN和DeepONet等基线模型相媲美或超越，同时样本路径生成速度远超经典的数值方案。

Conclusion: MFNO在处理非周期性输入方面表现出色，并能以任意精度逼近路径依赖随机微分方程的解以及分数布朗运动的Lipschitz变换。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [234] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [235] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets通过低秩适应（LoRA）技术降低了HyperDeepONets的参数数量和计算成本，同时提高了预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决HyperDeepONets在提高表达能力的同时，因需要大量输出参数而带来的高内存和计算成本问题，提出PI-LoRA-HyperDeepONets。

Method: 提出了一种名为PI-LoRA-HyperDeepONets的变体，它利用低秩适应（LoRA）技术来降低HyperDeepONets的复杂性。具体做法是将超网络的输出层权重矩阵分解为两个更小的低秩矩阵，以此减少可训练参数数量，并对主干网络的权重引入额外的正则化。

Result: 通过在常微分方程和偏微分方程上的大量实验表明，PI-LoRA-HyperDeepONets在参数数量上减少了高达70%，并且在预测准确性和泛化能力方面持续优于常规HyperDeepONets。

Conclusion: PI-LoRA-HyperDeepONets通过使用低秩适应（LoRA）来减少复杂性，通过将超网络的输出层权重矩阵分解为两个更小的低秩矩阵，从而在可训练参数数量上实现了高达70%的减少，并且在预测准确性和泛化能力方面持续优于常规HyperDeepONets。

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [236] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于生成对抗网络和因果不变性学习的AI模型，用于改进语义通信中的信道解码器数据重建。该模型通过提取因果表示来捕获通用知识，并采用稀疏更新协议来处理用户数据的时变性。实验证明了该模型在跨设备一致性、分类性能和数据重建的PSNR方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 旨在设计一个低复杂度、泛化的AI模型，以捕获通用知识，从而改进信道解码器的数据重建，以实现语义通信。

Method: 提出了一种生成对抗网络，利用因果不变性学习来提取数据的因果和非因果表示。

Result: 经验评估得出了三个关键观察：1. 因果不变性知识确保了跨不同设备的知识一致性；2. 不变性知识在分类任务中表现出有前景的性能；3. 所提出的基于知识的数据重建方法在峰值信噪比（PSNR）方面优于其他最先进的数据重建和语义压缩方法。

Conclusion: 该模型通过因果不变性学习提取因果和非因果表示，并设计稀疏更新协议以在保持知识不变性的同时最小化通信开销。实验结果表明，该模型在不同设备间具有一致性，在分类任务中表现良好，并且其基于知识的数据重建在PSNR方面优于现有方法。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [237] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GraphEXT是一个新颖的可解释性框架，利用合作博弈论和社会外部性理论，通过量化节点在联盟转移中的边际贡献来提高GNN的可解释性，其在真实数据集上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN可解释性方法在捕捉节点间复杂的交互模式方面存在不足，而GraphEXT旨在通过考虑节点交互和结构变化对GNN预测的影响来解决这一问题。

Method: GraphEXT框架利用合作博弈论和社会外部性理论，通过将图节点划分为联盟并将原始图分解为独立的子图，并整合图结构作为外部因素和外部因素下的Shapley值来量化节点的重要性。

Result: 实验研究表明，GraphEXT在忠实度方面优于现有的基线方法，并能显著提高GNN模型的可解释性。

Conclusion: GraphEXT通过整合图结构作为外部因素并将Shapley值纳入外部因素，在用户忠诚度方面表现优于现有方法，在提高GNN模型的可解释性方面表现出色。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [238] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 该研究提出了一种复杂网络建模方法，分析了深度神经网络中特征网络的自相似性，并发现通过约束自相似性可以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有初步研究表明权重分布的幂律分布会影响模型性能，但关于隐藏空间几何的自相似性如何影响模型权重优化，以及内部神经元的动态行为尚不清楚。

Method: 提出了一种基于隐藏层神经元输出特征的复杂网络建模方法，以研究不同隐藏层构建的特征网络的自相似性。

Result: 研究表明，特征网络的自相似性程度因模型架构而异。通过在训练过程中嵌入特征网络的自相似性约束，可以提高自相似深度神经网络（MLP架构和注意力架构）的性能。

Conclusion: 通过在训练过程中嵌入特征网络的自相似性约束，可以提高自相似深度神经网络（MLP架构和注意力架构）的性能，最高可达6个百分点。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [239] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 该研究提出了一种基于强化学习的降维气动形状优化算法，通过局部参数优化和CFD模拟加速全局优化，并能解释优化结果。


<details>
  <summary>Details</summary>
Motivation: 目的是最小化计算量，并利用观察到的优化结果来解释所发现的极值在实现期望流场中的作用。

Method: 介绍了一种基于强化学习（RL）的自适应优化算法，用于气动形状优化，并侧重于降维。该RL应用采用基于代理的、Actor-Critic策略评估MCMC方法，允许优化参数在时间上“冻结”。

Result: 通过一个简单的流体动力学问题示例，展示了该方法在特征重要性评分方面能够提供解释。

Conclusion: 该方法通过一系列局部优化的参数更改，并结合中间的CFD模拟作为“真值”，可以加速全局优化过程。但前提是局部参数邻域足够大，并且用于参数自适应的奖励和成本估计足够准确。所提出的方法能够对发现的极值进行解释，例如通过特征重要性评分。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [240] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 基础模型在LLM、VLM和多模态模型方面取得了成功，但存在表示能力、适应性和可扩展性方面的局限性。双曲几何空间提供了一种有前景的解决方案，能够以更少的维度嵌入数据，并在多个任务中提升模型性能。本文对双曲神经网络在基础模型中的应用进行了综述，并讨论了未来的挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 探究在基础模型中引入双曲几何空间是否能克服现有模型（如LLM、VLM）在表示能力、适应性和可扩展性方面的局限性，并提升其推理能力。

Method: 本文对双曲神经网络及其在基础模型上的最新进展进行了全面的综述。

Result: 双曲空间能够以更少的维度嵌入层次结构和幂律分布，在LLM的复杂推理、VLM的零样本泛化以及跨模态语义对齐方面展现出优越性，并能保持参数效率。

Conclusion: 总的来说，本文全面回顾了双曲神经网络在基础模型上的应用，并指出了该领域面临的挑战和未来的研究方向。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [241] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: MARL算法通过学习简单约定而非真正理解环境来取得成功，这导致了脆弱的策略。提议设计新的MARL环境，强调基于观测的行为和基于记忆的推理，以促进真正的技能发展。


<details>
  <summary>Details</summary>
Motivation: 当前模型无关的多智能体强化学习（MARL）算法的经验成功并非源于有效的马尔可夫信号恢复，而是源于学习绕过环境观测和记忆的简单约定。

Method: 通过有针对性的案例研究，我们展示了共同适应的智能体如何学会脆弱的约定，以及这些约定在与非适应性智能体配对时如何失败。我们还表明，相同的模型可以在任务设计需要时学会基础策略。

Result: 共同适应的智能体可以学会脆弱的约定，当与非适应性智能体配对时，这些约定会失败。相同的模型可以在任务设计需要时学会基础策略，这表明问题并非出在学习模型的根本限制，而是基准设计上的失败。现代MARL环境可能无法充分测试Dec-POMDP的核心假设。

Conclusion: 该位置论文认为，当前模型无关的多智能体强化学习（MARL）算法的经验成功并非源于有效的马尔可夫信号恢复，而是源于学习绕过环境观测和记忆的简单约定。通过有针对性的案例研究，我们发现共同适应的智能体可以学会脆弱的约定，而当与非适应性智能体合作时，这些约定就会失败。至关重要的是，当任务设计需要时，相同的模型可以学会基础策略，这表明问题并非出在学习模型的根本限制，而是基准设计上的失败。我们的分析还表明，现代MARL环境可能无法充分测试Dec-POMDP的核心假设。因此，我们提倡建立在两个核心原则基础上的新合作环境：（1）基于观测的基础行为和（2）基于记忆的关于其他智能体的推理，确保成功需要真正的技能，而不是脆弱的、共同适应的协议。

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [242] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: LLM在排序和评估任务中存在位置偏见和一致性问题，传统解决方法成本高。本研究提出动态早期停止方法，并结合置信度适应性调整，大幅降低了LLM调用次数，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，LLM在项目排序和答案评估中存在位置偏见（即候选项目在提示中的顺序会影响最终决策）和重复一致性低（即相同的输入可能导致不同的输出）的问题。传统的解决方法（如多次提示和多数投票）计算成本高昂。由于位置偏见的方向和幅度因实例而异，因此需要一种针对每个实例的缓解策略。

Method: 提出了一种动态早期停止方法，该方法能够自适应地确定每个实例所需的重复次数。此外，还提出了一种基于置信度的适应性调整方法，以进一步优化性能。

Result: 在三个不同大小的LLM和两个任务（重新排序和对齐）上的评估表明，所提出的动态早期停止方法能将LLM调用次数平均减少81%，同时保持准确性。基于置信度的适应性方法能将LLM调用次数平均减少87%，仅带来轻微的准确性损失。

Conclusion: 本研究提出了一种动态的早期停止方法，并通过置信度进行适应性调整，以解决LLM在项目排序和答案评估中的位置偏见和重复一致性问题。实验证明，该方法能在保证准确性的前提下，显著减少LLM调用次数（平均减少81%-87%），优于传统的静态重复策略。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [243] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: This paper introduces 'Moving Out,' a benchmark for human-AI collaboration in physically constrained environments. It also proposes BASS, a method that improves agent adaptability and performance in these scenarios.


<details>
  <summary>Details</summary>
Motivation: The ability to adapt to physical actions and constraints is crucial for embodied agents to effectively collaborate with humans, necessitating methods that account for complex continuous state-action spaces and constrained dynamics.

Method: BASS (Behavior Augmentation, Simulation, and Selection) enhances agent diversity and their understanding of action outcomes.

Result: The Moving Out benchmark was used to evaluate models' abilities to adapt to diverse human behaviors and unseen physical attributes, with BASS demonstrating superior performance.

Conclusion: BASS outperforms state-of-the-art models in AI-AI and human-AI collaboration.

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


### [244] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个开源Python框架，用于表格数据的可复现和可解释的机器学习，提供端到端功能和用户友好的界面，以促进数据科学的普及。


<details>
  <summary>Details</summary>
Motivation: 满足对透明、可复现和可解释的机器学习工作流日益增长的需求，特别是针对表格数据，并赋能没有数据科学背景的研究人员。

Method: Helix是一个软件框架，包含用于数据预处理、可视化、模型训练、评估、解释、结果检查和预测的模块，并提供用户友好的界面来设计实验和解释结果。

Result: Helix是一个开源软件框架，提供端到端的可复现和可解释的机器学习工作流，具有用户友好的界面和新颖的解释方法，支持社区开发并遵循FAIR原则。

Conclusion: Helix是一个开源、可扩展的Python软件框架，旨在促进表格数据的可复现和可解释的机器学习工作流。它通过文档化、可访问、可复现和可理解的整个分析过程来满足对透明实验数据分析来源日益增长的需求。该平台包括标准化的数据预处理、可视化、机器学习模型训练、评估、解释、结果检查和对未见过数据的模型预测模块。为了帮助没有数据科学正式培训的研究人员获得有意义和可行的见解，Helix提供了一个用户友好的界面，可以在集成环境中设计计算实验、检查结果，包括使用语言术语对机器学习决策进行的新颖解释方法。Helix是在MIT许可证下发布的，可通过GitHub和PyPI访问，支持社区驱动的开发并促进遵循FAIR原则。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [245] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: CICME is a new method to infer causal mechanisms from heterogeneous data, which can reliably detect domain-invariant causal mechanisms and outperforms baseline methods in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: To gain deeper insights into a complex sensor system through the lens of causality.

Method: CICME, a novel three-step approach to inferring causal mechanisms from heterogeneous data collected across multiple domains. By leveraging the principle of Causal Transfer Learning (CTL), CICME is able to reliably detect domain-invariant causal mechanisms when provided with sufficient samples. The identified common causal mechanisms are further used to guide the estimation of the remaining causal mechanisms in each domain individually.

Result: The performance of CICME is evaluated on linear Gaussian models under scenarios inspired from a manufacturing process.

Conclusion: CICME leverages the benefits of applying causal discovery on the pooled data and repeatedly on data from individual domains, and it even outperforms both baseline methods under certain scenarios.

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [246] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: LSDM是一种结合了LLM、扩散模型和Transformer的模型，用于提高移动流量预测的准确性和适应性，尤其是在处理不确定性和环境背景信息方面。


<details>
  <summary>Details</summary>
Motivation: 现有移动流量预测方法在不同城市环境下的适应性有限，且由于个人流量模式的高度不确定性、缺乏详细的环境背景信息以及不同网络服务间的复杂依赖关系，导致预测结果不准确。这些挑战需要能够捕捉动态交通分布和丰富环境特征的先进建模技术。

Method: 本文提出了一种名为LSDM（LLM-Enhanced Spatio-temporal Diffusion Model）的模型，该模型结合了扩散模型的生成能力、Transformer的自适应学习能力以及LLM对多模态环境信息的捕捉能力，用于建模服务级别的流量模式和动态。

Result: 通过LLM引入上下文信息后，性能在决定系数方面至少提高了2.83%。与CSDI等同类模型相比，均方根误差至少降低了8.29%。

Conclusion: LSDM模型在交通流量预测方面表现出色，具有出色的泛化能力和适应性。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [247] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: CoCAI 框架结合了生成式人工智能和基于 copula 的建模，用于多元时间序列分析，能够准确预测并检测异常。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列分析中的两个关键挑战：提供准确的预测和实现稳健的异常检测。

Method: 提出了一种利用生成式人工智能和基于 copula 的建模来解决多元时间序列分析中的预测和异常检测挑战的新颖框架，称为 CoCAI。该方法利用基于扩散的模型来捕捉数据中的复杂依赖关系，以实现高质量的预测，并使用共形预测技术进行校准，以生成具有所需置信水平的统计上有效的预测区域。然后，通过结合降维技术和基于 copula 的建模来进行稳健的异常检测，以提供具有统计依据的异常分数。

Result: CoCAI 能够准确预测数据目标序列，并在其中识别异常段。

Conclusion: CoCAI在水处理和污水处理系统等实际运行数据上的实证测试证明了其在准确预测数据目标序列和识别其中异常段方面的有效性。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [248] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [249] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: WGAN在降水降尺度预报中表现出更好的视觉真实感，并为评估和质量控制数据集提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预报对于减少局部强降雨造成的损害至关重要，但使用驱动模型进行此类预报仍然具有挑战性。

Method: 提出使用Wasserstein生成对抗网络（WGAN）结合最优传输成本来进行降水降尺度分析。

Result: WGAN生成的降水场具有视觉上逼真的细节结构，尽管在传统评估指标上表现略有下降；WGAN学习到的判别器与人类感知的真实度高度相关；基于案例的分析表明，判别器得分的较大差异有助于识别不真实的WGAN输出和参考数据中的潜在瑕疵。

Conclusion: WGAN框架不仅提高了降水降尺度预报的感知真实度，还为评估和质量控制降水数据集提供了新的视角。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [250] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 通过使用负面样本学习“属性方向”，然后反向操作以生成正面分子，分子任务算术在分子生成任务中提高了多样性和成功率，并具有数据效率和性能优势。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型设计中“正面”分子的稀缺性问题。

Method: 通过在多样且丰富的负面示例上训练模型来学习“属性方向”，然后朝着相反的属性方向移动模型以生成正面分子，从而避免了分子设计中的数据限制。

Result: 在20个零样本设计实验中，与在正面分子上训练的模型相比，分子任务算术生成的更具多样性且更成功。在双目标和少样本设计任务中，它也能在保持理想设计特性的同时，持续增加设计的多样性。

Conclusion: 分子任务算术有望成为从头分子设计的事实上的迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [251] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 差分私有机器学习在处理具有分布偏移的数据源时，其有效性会受到影响。研究结果表明，在偏移量小的情况下，需要大量数据；而在偏移量大的情况下，公共数据则没有益处。


<details>
  <summary>Details</summary>
Motivation: 在实践中，最有效的差分私有机器学习算法依赖于额外的、声称为公开的数据源。当两个数据源结合起来比它们各自的价值总和更有价值时，这种模式是最有趣的。

Method: 通过对高斯均值估计（两个分布具有不同的均值）和高斯线性回归（两个分布表现出参数偏移）的已知下界进行扩展，以涵盖分布偏移的情况。

Result: 该研究扩展了公共-私有学习的已知下界，以涵盖分布偏移的情况，并发现了在不同偏移量下的数据需求。

Conclusion: 当数据源之间的分布偏移很小时（相对于期望的精度），需要大量的公共或私有数据来估计私有参数。相反，当偏移量很大时，公共数据没有带来任何好处。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [252] [ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense](https://arxiv.org/abs/2502.18549)
*Jiyue Tao,Tongsheng Shen,Dexin Zhao,Feitian Zhang*

Main category: cs.LG

TL;DR: ARBoids是一个结合了DRL和Boids模型的框架，用于USV目标防御，能有效对抗高机动性攻击者。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决无人水面载具（USV）面临的目标防御问题（TDP），特别是在攻击者机动性优于防御者时，如何有效拦截的问题。

Method: 本研究提出了一种名为ARBoids的新型自适应残差强化学习框架，该框架将深度强化学习（DRL）与受生物启发的力学模型Boids相结合。Boids模型提供了一个计算高效的多智能体协调基线策略，而DRL则学习一个残差策略来优化防御者的行为。

Result: ARBoids框架在Gazebo仿真环境中得到验证，其性能优于纯力学方法和标准的DRL策略，并且能够很好地适应具有不同机动性特征的攻击者。

Conclusion: ARBoids框架在仿真环境中展示了优于传统拦截策略的性能，并能适应不同机动性的攻击者，证明了其鲁棒性和泛化能力。

Abstract: The target defense problem (TDP) for unmanned surface vehicles (USVs)
concerns intercepting an adversarial USV before it breaches a designated target
region, using one or more defending USVs. A particularly challenging scenario
arises when the attacker exhibits superior maneuverability compared to the
defenders, significantly complicating effective interception. To tackle this
challenge, this letter introduces ARBoids, a novel adaptive residual
reinforcement learning framework that integrates deep reinforcement learning
(DRL) with the biologically inspired, force-based Boids model. Within this
framework, the Boids model serves as a computationally efficient baseline
policy for multi-agent coordination, while DRL learns a residual policy to
adaptively refine and optimize the defenders' actions. The proposed approach is
validated in a high-fidelity Gazebo simulation environment, demonstrating
superior performance over traditional interception strategies, including pure
force-based approaches and vanilla DRL policies. Furthermore, the learned
policy exhibits strong adaptability to attackers with diverse maneuverability
profiles, highlighting its robustness and generalization capability. The code
of ARBoids will be released upon acceptance of this letter.

</details>


### [253] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习在云机器人操纵中的应用，讨论了其优势、挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前机器人操纵任务受限于机器人个体能力和速度以及计算资源限制的问题，云机器人学的概念应运而生，它允许机器人应用利用云计算的灵活性和可靠性，从而在云边缘计算连续体内有效缓解其计算需求。

Method: 本文介绍了联邦学习（FL）的基本概念及其与云机器人操纵的联系。

Result: 本文探讨了在分布式计算背景下，尤其是在云机器人操纵场景中，联邦学习（FL）的优势以及挑战和机遇。

Conclusion: 这项工作提出了联邦学习（FL）在云机器人操纵中的基本概念，并设想了通过FL实现高效可靠的云机器人操纵的机遇和挑战。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [254] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 本研究提出了一种名为pVAE的深度学习框架，用于通过反向设计生成具有特定水力特性的多孔超材料，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了探索利用深度学习进行多孔超材料的反向设计，以生成具有定制化水力特性（如孔隙率和渗透率）的超材料。

Method: 采用了一种结合了回归器的变分自编码器（pVAE）来生成结构超材料，并使用卷积神经网络（CNN）预测其有效水力特性，以降低计算成本。该方法在合成数据集和真实开孔泡沫数据集上进行了训练和验证。

Result: pVAE框架能够有效地捕捉微观结构特征，并在紧凑且可解释的潜在空间中进行映射，实现了结构-属性映射、插值和反向设计，从而能够生成具有所需特性的新型超材料。

Conclusion: 该研究成功开发了一种基于深度学习的生成框架，用于多孔超材料的反向设计，实现了具有特定水力特性的结构超材料的生成。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [255] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: 提出了一种半经验学习理论（SETOL），用以解释SOTA神经网络的优异性能。该理论通过统计力学、随机矩阵理论和量子化学等方法进行推导，并与现有的重尾自正则化（HTSR）理论在预测精度和层质量指标上表现出高度一致性。此外，SETOL还引入了ERG等新的学习指标，并在简单的MLP模型和复杂SOTA神经网络上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了解释SOTA神经网络的优异性能，并为HTSR理论提供形式化解释。

Method: 使用统计力学、随机矩阵理论和量子化学技术推导了SETOL理论，并通过计算权重矩阵的经验谱密度来估计层质量。

Result: SETOL理论与HTSR理论的alpha和alpha-hat指标高度一致，并且SETOL提出的ERG指标也表现出良好的性能。

Conclusion: SETOL理论与HTSR理论对齐，并且在MLP和SOTA NN模型上都得到了验证，SETOL为理想学习提供了新的数学先决条件，包括ERG指标。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [256] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: 为了评估文本到图像（T2I）模型的安全性，我们提出了一种名为Seed2Harvest的混合红队测试方法。该方法通过结合人类的创造力和机器的计算能力，成功地扩展了对抗性提示数据集，显著提高了其多样性和规模，同时保持了有效的攻击成功率。这表明人机协作是实现全面、可扩展的T2I模型安全评估的关键。


<details>
  <summary>Details</summary>
Motivation: 为了应对文本到图像（T2I）模型日益增长的应用需求，对其进行鲁棒的对抗性评估至关重要。现有的方法要么依赖于规模有限、代表性不均的人类创作提示，要么依赖于虽然规模大但缺乏真实性和创造性的合成提示。因此，需要一种结合两者优点的方法来生成多样化且大规模的对抗性提示。

Method: 提出了一种名为Seed2Harvest的混合红队测试方法，该方法通过引导性地扩展人类创作的对抗性提示种子，结合了人类的创造力和机器的计算能力，以生成更多样化、更大规模的对抗性提示。

Result: Seed2Harvest方法生成的对抗性提示数据集在保持人类提示特征和攻击模式的同时，达到了与人类提示相当的平均攻击成功率（NudeNet为0.31，SD NSFW为0.36，Q16为0.12）。此外，该数据集的地理位置多样性显著提高，从原始的58个地点增加到535个，香农熵从5.28提升至7.48。

Conclusion: 该研究强调了人机协作在文本到图像（T2I）模型安全评估中的重要性，通过Seed2Harvest方法成功地扩展了对抗性提示数据集，提高了数据集的多样性和规模，同时保持了与人类创作提示相当的攻击成功率。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [257] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse是一个创新的深度学习框架，通过将每个兴趣点视为节点并结合时间图卷积和Transformer来预测城市级人口流动。它采用迁移学习策略以实现跨城市泛化，并在大规模GPS数据上验证了其优越的准确性和可扩展性，为实际城市预测应用提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 准确的人口流动预测对于城市规划、交通管理和公共卫生至关重要。然而，现有方法面临关键限制：传统模型依赖静态空间假设，深度学习模型难以实现跨城市泛化，大型语言模型（LLMs）计算成本高昂且无法捕捉空间结构。此外，许多方法通过聚类兴趣点（POIs）或将覆盖范围限制在子区域来牺牲分辨率，限制了它们在城市范围分析中的应用。

Method: UrbanPulse是一个可扩展的深度学习框架，将每个兴趣点（POI）视为独立节点。它结合了时间图卷积编码器和基于Transformer的解码器来模拟多尺度时空依赖性。为了确保在不同城市环境中的稳健泛化能力，UrbanPulse采用了三阶段迁移学习策略：在大型城市图上进行预训练，冷启动适应，以及强化学习微调。

Result: 在对加利福尼亚三个大都市区的超过1.03亿条清理后的GPS记录进行评估后，UrbanPulse在准确性和可扩展性方面均达到了最先进的水平。

Conclusion: UrbanPulse通过其创新的深度学习框架、多尺度时空依赖建模以及三阶段迁移学习策略，在城市级OD流预测方面取得了最先进的准确性和可扩展性。通过有效的迁移学习，UrbanPulse为在高分辨率下进行人工智能驱动的城市预测在不同城市中的实际应用迈出了关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [258] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: MFTRR框架通过局部-全局语义相关性推理和多层次证据关系推理，利用多模态线索进行细粒度帖子质量评估，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的帖子质量评估研究存在三个主要局限性：1. 将任务视为单一模态分类，未能利用多模态线索和细粒度的质量区分；2. 在深度多模态融合过程中引入噪声，导致误导性信号；3. 缺乏捕捉相关性和全面性等复杂语义关系的能力。

Method: MFTRR框架包含两个关键模块：1. 局部-全局语义相关性推理模块，通过最大信息融合机制抑制噪声，对帖子和主题在局部和全局层面的细粒度语义交互进行建模；2. 多层次证据关系推理模块，探索宏观和微观层面的关系线索，以加强基于证据的推理。

Result: MFTRR框架在三个新构建的多模态主题-帖子数据集和公开的Lazada-Home数据集上进行了评估，实验结果表明MFTRR显著优于最先进的基线方法，在艺术史数据集上NDCG@3的提升最高可达9.52%。

Conclusion: MFTRR框架在三个新构建的多模态主题-帖子数据集和公开的Lazada-Home数据集上进行了评估，实验结果表明MFTRR显著优于最先进的基线方法，在艺术史数据集上NDCG@3的提升最高可达9.52%。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [259] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: 使用Tiny ML和IMU传感器，通过Edge Impulse平台训练DNN模型，实现了高精度的步态识别，适用于低功耗场景。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种低功耗、高精度的步态识别系统，能够用于电池供电或能量收集设备，并能实时识别用户的活动状态。

Method: 使用XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器捕获运动数据，通过Edge Impulse平台进行数据预处理（滑动窗口、数据归一化）和深度神经网络（DNN）分类器训练，最终将模型部署到微控制器上进行实时活动分类。

Result: 该Tiny ML步态识别系统在测试数据集上实现了超过80%的准确率，能够有效区分四种不同的活动（行走、静止、上楼、下楼），并且集成了异常检测功能，提高了系统的鲁棒性。

Conclusion: 该系统通过Tiny ML和IMU传感器实现了超过80%准确率的步态识别，能够有效区分行走、静止、上楼和下楼四种活动，并具备异常检测能力，适用于低功耗设备。

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


### [260] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一个两阶段Transformer模型，通过融合视频、音频和文本特征来预测fMRI活动，并在Algonauts 2025挑战赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了预测fMRI活动，研究人员提出了一种能够融合多模态信息（视频、音频、文本）的模型。

Method: 提出了一种名为VIBE的两阶段Transformer模型，该模型融合了视频、音频和文本的多模态特征，并利用了Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA等开源模型的表征。具体而言，首先通过一个模态融合Transformer合并多模态表征，然后通过一个带有旋转嵌入的预测Transformer进行时间解码。

Result: VIBE模型在CNeuroMod数据集的65小时电影数据上进行了训练，并在in-distribution的Friends S07数据集上达到了32.25的平均parcel-wise Pearson相关性，在六个out-of-distribution的电影数据集上达到了21.25的相关性。该模型的早期版本在Algonauts 2025挑战赛中获得了第一名和第二名的成绩。

Conclusion: VIBE模型在fMRI预测任务上取得了显著成果，在in-distribution和out-of-distribution测试中均表现优异，并在Algonauts 2025挑战赛中获奖。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [261] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: GeoAggregator通过优化流水线和引入模型集成及解释功能，提高了计算效率、预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地建模和解释地理空间表格数据（GTD），并理解地理空间现象及其潜在过程。

Method: 通过开发优化流水线来加速数据加载和GeoAggregator的正向传播，以提高计算效率；通过引入模型集成策略和基于GeoShapley框架的事后模型解释功能来增强模型的可解释性。

Result: 改进后的GeoAggregator模型在预测准确性和推理速度方面均优于原始实现，并且能够有效地捕捉数据中固有的空间效应。

Conclusion: GeoShapley框架能够有效地捕捉数据中固有的空间效应，同时，模型集成策略和事后模型解释功能的结合，增强了模型的可解释性。实验结果表明，改进后的GeoAggregator模型在预测准确性和推理速度方面均优于原始实现。

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [262] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL 是一种新的表格学习方法，它使用摘要统计信息和 XGBoost 模型来识别医疗保健数据中的数据转移因素，即使在存在噪声和隐私限制的情况下也能提供准确且可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 在以医疗保健为重点的分析和决策支持系统中，识别表格数据集中数据转移的驱动因素是一个重大挑战。隐私规则限制数据访问，复杂过程产生的噪声阻碍了分析。

Method: SIFOTL（统计信息保真度优化方法用于表格学习）通过以下方式解决此挑战：(i) 提取符合隐私规定的数据摘要统计信息，(ii) 采用双 XGBoost 模型，在 LLM 的辅助下将干预信号与噪声分离，(iii) 通过帕累托加权决策树合并 XGBoost 输出，以识别负责转移的可解释片段。

Result: SIFOTL 在 MEPS 倾向得分数据集上的 F1 分数为 0.85，显著优于 BigQuery 贡献分析 (F1=0.46) 和统计检验 (F1=0.20)。在 18 个基于 Synthea ABM 生成的 EHR 数据集上，SIFOTL 在无噪声情况下保持 0.86-0.96 的 F1 分数，在注入观察噪声的情况下仍 >= 0.75，而基线平均 F1 分数在相同测试下的范围为 0.19-0.67。

Conclusion: SIFOTL提供了一个可解释的、注重隐私的工作流程，并且在经验上对观察噪声具有鲁棒性。

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [263] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: 数据驱动的交通状态估计和预测 (TSEP) 面临隐私和安全挑战。本研究提出 TSEP-Machine Unlearning，允许模型遗忘敏感数据，以增强信任和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了解决数据驱动的交通状态估计和预测 (TSEP) 中数据隐私、网络安全和数据新鲜度问题，以及“被遗忘权”的要求，该研究旨在使模型能够“遗忘”。

Method: 提出了一种新颖的学习范式，称为 TSEP-Machine Unlearning，它使训练好的 TSEP 模型能够选择性地遗忘隐私敏感、被污染或过时的数据。

Result: 该研究通过 TSEP-Machine Unlearning 解决了这些挑战，使模型能够遗忘特定的数据。

Conclusion: 通过实现选择性遗忘，TSEP-Machine Unlearning 旨在提高数据驱动的交通状态估计和预测 (TSEP) 的可信度和可靠性。

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [264] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 这项研究通过一个预测框架和经验性标度律，为提高 GRPO 微调 LLMs 的效率提供了指导，识别出三个训练阶段，并建议提前停止训练以节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 为了解决使用像组相对策略优化（GRPO）这样的强化学习方法对大型语言模型（LLMs）进行微调以用于推理任务的计算成本高昂的问题。

Method: 提出一个预测框架，对训练动态进行建模，以优化资源使用。通过在 Llama 和 Qwen 模型（3B 8B）上进行实验，推导了一个基于模型大小、初始性能和训练进度的经验性标度律，该法则可以预测奖励轨迹。

Result: 通过经验性标度律，识别出三个一致的训练阶段（缓慢启动、快速改进和平台期），并发现提前停止训练可以显著降低计算成本，而不会牺牲性能。

Conclusion: 该研究提出了一个预测框架，通过模拟训练动态来优化资源使用，并发现了三个一致的训练阶段：缓慢启动、快速改进和平台期。研究表明，在特定周期数之后继续训练收益甚微，因此提前停止训练可以在不牺牲性能的情况下显著降低计算成本。

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [265] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: 提出了一种基于神经算子的深度学习方法，用于提高海洋流数据的空间分辨率，以满足精细化本地分析的需求。


<details>
  <summary>Details</summary>
Motivation: 为了解决海洋学中高分辨率海流数据对于海岸管理、环境监测和海上安全至关重要，但现有卫星产品（如Copernicus数据）的空间分辨率不足以满足精细化本地分析需求的问题。

Method: 提出了一种基于神经算子的监督深度学习框架，用于求解偏微分方程，并实现了任意分辨率的求解。同时，提出了一种下行模型，并将其应用于Copernicus海洋流数据。

Result: 在真实的Copernicus海洋流数据和合成的Navier-Stokes模拟数据集上评估了所提出的模型，证明了其在提高海洋流数据分辨率方面的有效性。

Conclusion: 该研究引入了一种基于神经算子（neural operators）的监督深度学习框架，用于求解偏微分方程（PDE），并实现任意分辨率的求解。此外，研究还提出了一个结合该框架的下行模型，并将其应用于Copernicus海洋流数据，以提高其空间分辨率。该方法能够模拟代理PDE并以任意分辨率预测解，不受输入分辨率限制。

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [266] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO是一种用于训练大型语言模型的稳定、高效的强化学习算法，它在序列级别进行操作，优于现有算法，并已成功应用于Qwen3模型。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种稳定、高效且性能优越的强化学习算法，用于训练大型语言模型，并解决现有算法在序列级操作上的不足。

Method: GSPO算法基于序列似然定义重要性比率，并执行序列级别的裁剪、奖励和优化，而非像先前算法那样采用令牌级别的采样。

Result: GSPO算法相比GRPO算法在训练效率和性能上表现更优，尤其在MoE模型强化学习训练的稳定性方面有显著提升。

Conclusion: GSPO算法在训练大型语言模型方面实现了优越的训练效率和性能，显著提高了MoE模型的稳定性，并简化了RL基础设施的设计，为最新的Qwen3模型带来了显著的改进。

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [267] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: C-AAE 结合了 AAE 和 ADPCM，通过降维和差分编码来保护用户隐私，同时保持活动识别的准确性，并减小数据量，适用于医疗保健应用。


<details>
  <summary>Details</summary>
Motivation: 可穿戴加速度计和陀螺仪编码的细粒度行为特征可能会被用来重新识别用户，这使得隐私保护对于医疗保健应用至关重要。

Method: 我们引入了一种名为 C-AAE 的压缩匿名自编码器，它将匿名自编码器 (AAE) 与自适应差分脉冲编码调制 (ADPCM) 相结合。AAE 首先将原始传感器窗口投影到保留活动相关特征但抑制身份线索的潜在空间中。然后，ADPCM 对该潜在流进行差分编码，进一步掩盖了剩余的身份信息并减小了比特率。

Result: 实验表明，C-AAE 比单独使用 AAE 可将用户重新识别的 F1 分数降低 10-15 个百分点，同时将活动识别的 F1 分数保持在不受保护的基线 5 个百分点以内。ADPCM 还可将数据量减少约 75%，从而降低传输和存储开销。

Conclusion: C-AAE 在保护隐私和保持效用之间提供了实际的平衡，适用于医疗保健中的连续、基于传感器的活动识别。

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [268] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM 通过创新的 PBAR 和 FIAS 技术，将 LLM 的权重平均压缩至 1.6 位，在保持模型性能的同时大幅降低了计算成本和存储需求。


<details>
  <summary>Details</summary>
Motivation: 部署大型语言模型 (LLM) 因其巨大的参数量和高昂的计算成本而充满挑战。虽然超低比特量化可以显著减少存储和加速推理，但极端压缩（即平均比特宽度小于等于 2）通常会导致严重的性能下降。

Method: Squeeze10-LLM 是一个分阶段的混合精度训练后量化 (PTQ) 框架，通过将 80% 的权重量化到 1 位和 20% 的权重量化到 4 位，实现了平均每权重 1.6 位。该框架包含两个关键创新：后二值化激活鲁棒性 (PBAR) 和全信息激活监督 (FIAS)。PBAR 是一种改进的权重显著性度量，考虑了量化对激活的影响；FIAS 是一种在量化过程中保留完整激活信息以减轻累积误差的策略。

Result: 在 LLaMA 和 LLaMA2 上进行的实验表明，Squeeze10-LLM 在低于 2 位的仅权重量化方面取得了最先进的性能，在六个零样本分类任务上将平均准确率从 43% 提高到 56%，显著优于现有的 PTQ 方法。

Conclusion: Squeeze10-LLM 在 LLM 极低比特量化方面取得了最先进的性能，在六个零样本分类任务上将平均准确率从 43% 提高到 56%，显著优于现有的 PTQ 方法。

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [269] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 在数据稀疏时，利用标签置信度等额外监督信息，通过软标签提升模型性能。关键在于利用类别分布信息而非硬标签置信度，并优化混合系数来调整软标签。


<details>
  <summary>Details</summary>
Motivation: 在训练数据有限的情况下，探索额外监督信息（如标签置信度）对于提升分类模型准确度的价值和作用机制。

Method: 提出一个理论框架，将硬标签和额外监督视为概率分布，通过仿射组合构建软标签，并进行泛化误差分析。

Result: 研究表明，额外监督信息中的类别分布信息比硬标签置信度更重要。混合系数控制调整方向的步长。理论分析了额外监督和混合系数对收敛速度和误差边界的影响。实验证实了基于该理论设计的额外监督能提高分类准确率。

Conclusion: 通过理论分析和实验证明，额外监督信息（如标签置信度）在训练数据有限的情况下，可以通过与硬标签的仿射组合来改进分类模型。研究表明，额外监督的价值在于其提供的类别分布信息，而非硬标签本身的置信度。模型参数（如混合系数）的调整对于优化软标签至关重要，并能影响模型的收敛速度和最终的泛化误差边界。

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [270] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: 本文提出了一种名为PDA-DRL的解决方案，用于优化O-RAN中的网络切片，通过深度强化学习和创新的个性化方法，在满足延迟约束的同时降低了资源利用率。


<details>
  <summary>Details</summary>
Motivation: 在开放RAN（O-RAN）架构中，解决了多个移动虚拟网络运营商（MVNO）争夺物理资源块（PRB）以满足客户概率延迟上限约束并最小化PRB利用率的挑战。

Method: 提出了一种基于LLN的奖励函数，并进行了实际修改。提出了一种名为PDA-DRL（Percentile-based Delay-Aware Deep Reinforcement Learning）的深度强化学习解决方案。引入了一种基于奖励的个性化方法，用于模型权重共享。

Result: PDA-DRL相比于其他基线（包括针对平均延迟约束优化的DRL模型）取得了38%的平均延迟降低。基于奖励的个性化方法在模型权重共享方面优于联邦平均等传统方法以及依赖于流量模式和模型权重距离相似性的策略。

Conclusion: PDA-DRL在满足概率延迟上限约束的同时，显著减少了PRB利用率，并且通过基于奖励的个性化方法，在模型权重共享方面优于传统方法。

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [271] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: 提出一种新的强化学习对抗攻击方法，利用LLM生成对抗性奖励，并通过关键状态识别算法来识别代理的脆弱状态，以诱导其做出次优决策。


<details>
  <summary>Details</summary>
Motivation: 解决对误导性强化学习（RL）系统的对抗性攻击的挑战，而无需修改环境或策略。

Method: 提出了一种奖励迭代优化框架，利用大型语言模型（LLM）生成专门针对目标代理漏洞的对抗性奖励，并设计了一种关键状态识别算法来精确找出目标代理最脆弱的状态。

Result: 实验结果证明了所提方法在各种环境中优于现有方法，能够有效地诱导目标代理做出次优决策。

Conclusion: 该方法在各种环境中均优于现有方法。

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [272] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [273] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: 对用于自主系统的神经形态计算进行了全面概述，重点介绍了算法、硬件、优化策略、事件驱动感知以及未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 自主系统（如机器人、移动代理（例如无人机）和自动驾驶汽车）对智能、自适应和节能的需求日益增长，推动了对神经形态计算的兴趣。神经形态方法通过借鉴生物神经网络系统的灵感，为增强自主平台的感知、决策和响应能力提供了有前景的途径。

Method: 本文调研了神经形态算法、专用硬件和跨层优化策略的最新进展，重点关注它们在现实世界自主场景中的应用。特别关注了事件驱动的动态视觉传感器及其在实现快速、高效感知方面的作用。本文还讨论了通过将脉冲神经网络集成到自主系统架构中来提高能效、鲁棒性、适应性和可靠性的新方法。

Result: 该研究强调了新方法如何通过将脉冲神经网络集成到自主系统架构中来提高能效、鲁棒性、适应性和可靠性。

Conclusion: 本文整合了机器学习、机器人学、神经科学和神经形态工程的观点，全面介绍了该领域的现状，并探讨了实时决策、持续学习以及安全、有弹性自主系统的开发等方面的新兴趋势和开放性挑战。

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [274] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: GraphALP通过LLM过采样和动态加权伪标签技术，解决了类别不平衡且含噪声标签的图节点分类问题，并在实验中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决类别不平衡图节点分类中的一个实际但研究不足的问题，即在处理类别不平衡图时，真实世界的图标签经常包含噪声，而现有方法通常假设标签是干净可靠的。

Method: 提出了一种新颖的图增强框架GraphALP，该框架基于大型语言模型（LLMs）和伪标签技术。具体来说，设计了一种基于LLM的过采样方法来生成合成少数节点，产生标签准确的少数节点以缓解类别不平衡。在类别平衡图的基础上，开发了一种动态加权伪标签方法来获得高置信度伪标签以降低标签噪声比。此外，实现了一种二次LLM引导的过采样机制，以减轻伪标签可能造成的潜在类别分布偏差。

Result: 实验结果表明，GraphALP在具有噪声标签的类别不平衡图上实现了优于最先进方法的性能。

Conclusion: GraphALP在具有噪声标签的类别不平衡图上实现了优于最先进方法的性能。

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [275] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: ChronoSelect 是一种新的深度学习框架，通过分析学习过程中的时间动态来应对带噪声标签的数据，并在实验中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界数据集上训练时，由于存在可被过参数化模型记住的噪声标签，导致泛化性能显著下降的问题。现有方法在静态快照评估方面存在不足，未能利用学习演化的丰富时间动态。

Method: 提出了一种名为 ChronoSelect 的新颖框架，采用创新的四阶段内存架构，将预测历史压缩成紧凑的时间分布。该框架使用具有受控衰减的滑动更新机制，为每个样本维护四个动态内存单元，逐步强调近期模式，同时保留重要的历史知识。这使得能够通过时间轨迹分析和双分支一致性将样本精确地分为三类：干净、边界和噪声。

Result: ChronoSelect 能够通过时间轨迹分析和双分支一致性，精确地将样本分为干净、边界和噪声三类。

Conclusion: ChronoSelect 在合成和真实世界基准测试中展现了最先进的性能。

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [276] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: 为了实现完全自动驾驶，需要对周围环境有很好的理解。这项工作提出了一个新的图神经网络（GNN），通过利用包含交通参与者和矢量化道路网络的异构图，并采用多阶段方法对目标进行分类，来提高模型对未见场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的（SotA）模型在部署到新的/未见过的区域时性能会显著下降，表明模型缺乏泛化能力。

Method: 提出了一种新的图神经网络（GNN），利用包含交通参与者和矢量化道路网络的异构图，并通过多阶段方法对目标（即预测轨迹的终点）进行分类。

Result: 通过跨数据集评估（在Argoverse2上训练，在NuScenes上评估），证明了该模型在提高泛化能力方面的有效性。

Conclusion: 该研究提出了一种新的图神经网络（GNN），它利用包含交通参与者和矢量化道路网络的异构图，并通过多阶段方法对目标（即预测轨迹的终点）进行分类，从而提高了对未见场景的泛化能力。通过跨数据集评估（在Argoverse2上训练，在NuScenes上评估），证明了目标选择过程的有效性。

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [277] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: FedSA-GCL 是一种新的半异步联邦图学习框架，通过 ClusterCast 机制解决现有方法的效率和模型一致性问题，并在实验中取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦图学习（FGL）方法多依赖同步通信，效率低下且不切实际；现有的异步联邦学习（AFL）方法未考虑图数据的独特拓扑属性，可能导致全局模型出现语义漂移和表示不一致。

Method: 提出了一种名为 FedSA-GCL 的半异步联邦框架，利用客户间标签分布差异和图拓扑特征，并通过新颖的 ClusterCast 机制实现高效训练。

Result: 在多个真实世界的图数据集上，使用 Louvain 和 Metis 划分算法进行评估，FedSA-GCL 取得了优于基线方法的性能。

Conclusion: FedSA-GCL 在 Louvain 和 Metis 划分算法上分别平均提高了 2.92% 和 3.4%，表现出强大的鲁棒性和出色的效率，优于 9 种基线方法。

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [278] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: SINDy-LOM通过优化基函数库来改进SINDy方法，提高了模型的长期预测能力和易用性。


<details>
  <summary>Details</summary>
Motivation: 传统的SINDy方法在设计候选函数库时面临挑战，因为对于许多动力系统而言，选择合适的函数库并非易事。

Method: 本研究提出了一种结合稀疏回归技术和新颖库学习策略的SINDy-LOM方法，构建了一个两层优化架构：内层通过稀疏线性组合提取数据驱动模型；外层从递归长期预测精度角度优化基函数，将库设计重新构建为参数化基函数的优化问题。

Result: SINDy-LOM模型具有良好的可解释性和可用性，能够生成简洁的模型，显著降低了用户负担，并提高了模型在递归长期预测方面的可靠性。

Conclusion: SINDy-LOM方法通过优化参数化基函数来构建稀疏模型，能够有效减轻用户在库设计上的负担，并且通过对递归长期预测精度的考量，提高了模型的可靠性，该方法已成功应用于柴油机气路系统等复杂工业系统。

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [279] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: 本文通过大规模实验研究了六种基于 LP 的提升方法，发现它们在浅层树方面表现优于或等于 XGBoost 和 LightGBM，同时提高了集合稀疏度，并且能够稀疏化预训练的集合。


<details>
  <summary>Details</summary>
Motivation: 尽管基于线性规划的完全正确提升方法具有理论吸引力，但它们的实际应用却受到有限的实验研究。

Method: 对六种基于 LP 的提升方法进行了大规模实验研究，包括 NM-Boost 和 QRLP-Boost，并评估了启发式和最优基学习器的使用，同时分析了准确性、集合稀疏度、边距分布、随时性能和超参数敏感性。

Result: 研究表明，完全正确方法在浅层树方面可以超越或匹配 XGBoost 和 LightGBM 等最先进的启发式方法，并且能够产生更稀疏的集合。此外，这些方法可以在不牺牲性能的情况下稀疏化预训练的集合，同时突出了在优化决策树方面的优势和局限性。

Conclusion: 完全正确梯度提升方法可以与 XGBoost 和 LightGBM 等最先进的启发式方法相媲美，尤其是在使用浅层树时，同时能显著提高集合稀疏度。这些方法还可以用于在不牺牲性能的情况下稀疏化预训练的集合。

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [280] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM 利用 Siamese 学习和统计增强技术，通过生成新的追踪变体来丰富数据，解决了深度学习在预测性流程监控中数据量少、变异性低的问题，并在预测任务中取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习在预测性流程监控 (PPM) 中的应用受限于真实事件日志变异性低和规模小的缺点。

Method: SiamSA-PPM 是一个结合了 Siamese 学习和统计增强的自监督学习框架，采用三种新的、基于统计的变换方法，利用流程控制流语义和行为模式生成新的、符合语境的追踪变体，以学习流程前缀的通用表示。

Result: 实验表明，SiamSA-PPM 在预测下一个活动和最终结果方面，与当前最先进 (SOTA) 的方法相比，表现相当或更优。统计增强的性能优于随机变换，并能提高数据变异性。

Conclusion: SiamSA-PPM 是一种有前途的、用于改进流程预测模型训练数据的数据增强方法。

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [281] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: 提出一种基于可微物理学的网格缩减算法，可将网格点数量减少 10 倍，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现代数值模拟的高计算负荷要求在保持合理精度的同时降低离散问题规模的方法。

Method: 提出了一种基于可微物理概念的非结构化网格缩减算法，该算法采用了 k-means 聚类、自动微分和随机最小化算法。

Result: 在所考虑的场景中，网格点数量减少了 10 倍，同时保持了关注点处变量的动态。

Conclusion: 该方法可应用于任意由演化偏微分方程描述的系统的模拟。

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [282] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 针对持续学习恶意软件检测中的安全回归问题，提出了一种回归感知方法，通过修改正向一致性训练来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 恶意软件的快速进化需要机器学习检测器持续适应，但完全重新训练成本高昂。虽然持续学习（CL）是可行的替代方案，但一个被忽视的问题是安全回归——即先前被检测到的恶意软件样本在模型更新后逃脱检测。

Method: 提出了一种回归感知惩罚，并以模型无关的方式将正向一致性训练（PCT）应用于持续学习场景，以保留先前的预测行为。

Result: 该方法有效减少了安全回归，并保持了检测性能。

Conclusion: 该方法在ELSA、Tesseract和AZ-Class数据集上进行了实验，在不同的持续学习场景下有效减少了回归，同时在整个过程中保持了强大的检测性能。

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [283] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: TIDSIT是一种新型电池健康状态（SoH）估计模型，通过引入时间嵌入和注意力机制有效处理不规则时间序列数据，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的方法在处理电池健康状态（SoH）估计时，面临真实世界测量数据不规则（采样间隔不均匀、放电周期长度变化大）的挑战，通常通过提取特征序列来解决，但这会导致信息丢失和准确性下降。

Method: 提出了一种名为时间感知动态序列反向Transformer（TIDSIT）的新型架构。TIDSIT结合了连续时间嵌入以有效表示不规则采样数据，并利用带时间注意力的填充序列来处理可变长度输入，从而避免信息丢失。

Result: TIDSIT在NASA电池退化数据集上取得了显著成果，预测误差降低超过50%，SoH预测误差低于0.58%，性能优于现有模型。

Conclusion: TIDSIT模型在NASA电池退化数据集上的实验结果表明，其性能显著优于现有模型，预测误差降低超过50%，且SoH预测误差保持在0.58%以下。此外，该模型具有通用性，有望在涉及不规则时间序列数据的健康监测任务中得到广泛应用。

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [284] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 在LLM中，通过低秩适配（LoRA）和证据蒸馏，实现高效且准确的不确定性量化，仅需一次前向传播。


<details>
  <summary>Details</summary>
Motivation: 标准LLM中准确的不确定性量化是一个关键挑战，这促使人们采用贝叶斯和基于集成的方法，但这些方法通常需要计算成本高昂的采样，涉及多次前向传播来有效估计预测不确定性。

Method: 提出了一种新颖的方法，通过将需要多次前向传播的、能够感知不确定性的教师模型蒸馏到使用低秩适配（LoRA）进行微调的紧凑型学生模型中，从而在LLM中实现高效且有效的不确定性估计，同时不牺牲性能。比较了两种蒸馏策略：一种使用传统softmax输出，另一种使用狄利克雷分布输出通过证据学习显式建模认知不确定性。

Result: 经验评估表明，学生模型在预测和不确定性量化性能方面可以达到与其教师模型相当或更优的水平，同时仅需一次前向传播。

Conclusion: 该研究首次证明了通过证据蒸馏可以在大型语言模型（LLM）中实现即时且鲁棒的不确定性量化。

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [285] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: 本文介绍了扩散模型在智慧农业和精准农业中的应用，特别是在作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理方面的潜力。扩散模型在数据增强、图像生成和去噪方面表现出色，有望推动农业可持续发展。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口的增长和可耕地资源的日益稀缺，智慧农业和精准农业已成为农业发展的关键方向。人工智能（AI）技术，特别是深度学习模型，已广泛应用于作物监测和病虫害检测等领域。作为一种新兴的生成模型，扩散模型在农业图像处理、数据增强和遥感等任务方面显示出巨大的潜力。

Method: 本文综述了扩散模型在农业领域应用的最新进展，重点关注其在作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理方面的潜力。

Result: 实验结果表明，扩散模型显著提高了数据增强、图像生成和去噪等任务的模型准确性和鲁棒性，尤其是在复杂环境中。

Conclusion: 尽管在计算效率和泛化能力方面存在挑战，但随着技术的进步，扩散模型有望在智慧农业和精准农业中发挥越来越重要的作用，为全球农业的可持续发展提供有力支持。

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [286] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER 是一种结合了贝叶斯模型平均和循环神经网络水文模型的新方法，即使在数据稀缺的情况下也能进行准确、高效且可推广的流量预测，特别适用于未测量流域。


<details>
  <summary>Details</summary>
Motivation: 尽管准确的洪水预测和水资源管理具有关键需求，但许多地区缺乏足够的河流流量观测数据，这限制了降雨-径流分析的能力。虽然存在许多基于物理和机器学习的模型，但在数据稀缺的条件下实现高准确性、可解释性和计算效率仍然是一个重大挑战。

Method: 该研究提出了一种名为 HYPER（利用多模型集成和循环神经网络进行水文预测）的新颖方法，该方法结合了贝叶斯模型平均（BMA）和循环神经网络（RC）。首先，对 43 个“未校准”的流域概念水文模型应用 BMA。然后，通过线性回归训练 RC 模型来纠正 BMA 输出中的错误，这是一个确保高计算效率的非迭代过程。对于未测量的流域，通过将 BMA 和 RC 权重与已测量流域的流域属性相关联来推断所需的权重，从而创建了一个可推广的框架。

Result: 在数据充足的情况下，HYPER（Kling-Gupta Efficiency, KGE 均值为 0.56）的表现与基准 LSTM（KGE 0.55）相当，但仅用了其 5% 的计算时间。在数据稀缺的情况下（23% 的流域已测量），HYPER 保持了稳健的性能（KGE 0.55）和较低的不确定性，而 LSTM 的性能显著下降（KGE -0.04）。

Conclusion: HYPER 提供了一个鲁棒、高效且可推广的流量预测解决方案，尤其适用于未测量的流域，这使其能够广泛应用于各种地区。

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [287] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: This paper revises the bisimulation metric for reinforcement learning by improving its reward gap definition and using adaptive coefficients in updates, addressing limitations in representing scenarios and handling varying importance of rewards and states. The revised metric is theoretically sound and experimentally effective.


<details>
  <summary>Details</summary>
Motivation: The conventional bisimulation metric has two main issues: 1) inability to represent certain distinctive scenarios due to an imprecise definition of the reward gap, and 2) reliance on predefined weights for differences in rewards and subsequent states during recursive updates, overlooking their varying importance across different training stages and task settings.

Method: The paper proposes a revised bisimulation metric with a more precise definition of the reward gap and novel update operators with adaptive coefficients. Theoretical analysis provides convergence guarantees and demonstrates improved representation distinctiveness.

Result: The revised bisimulation metric shows improved representation distinctiveness and has theoretical guarantees of convergence. Experiments on DeepMind Control and Meta-World demonstrate its effectiveness.

Conclusion: The proposed revised bisimulation metric addresses the limitations of the conventional bisimulation metric by introducing a more precise definition of the reward gap and novel update operators with adaptive coefficients. Theoretical guarantees of convergence and improved representation distinctiveness are provided, and experimental results on DeepMind Control and Meta-World demonstrate the effectiveness of the approach.

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [288] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [289] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: C2G-KD 是一种无需真实数据即可通过类条件生成器和 PCA 几何约束进行知识蒸馏的方法，生成的合成样本可以保持类别结构和多样性，并在 MNIST 实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在没有真实训练数据的情况下，利用教师模型生成合成数据进行知识蒸馏，并保持数据的拓扑结构和多样性。

Method: C2G-KD 框架通过训练一个类条件生成器来生成合成样本，该生成器在冻结的教师模型和源自 PCA 的几何约束的指导下进行训练。生成器通过语义和结构损失的组合来学习激活教师的输出来生成样本。通过将生成的样本约束在类特定的 PCA 子空间内（仅从每个类别的两个真实样本估计得出），以保持拓扑一致性和多样性。

Result: 在 MNIST 上的实验表明，即使是最小的类别结构也足以启动有用的合成训练流程。

Conclusion: C2G-KD 是一个无需数据即可进行知识蒸馏的框架，它通过训练一个类条件生成器来生成合成样本。该生成器在冻结的教师模型和源自 PCA 的几何约束的指导下进行训练，并且从不观察真实训练数据。通过将生成的样本约束在从每个类别仅有的两个真实样本估计出的类特定的 PCA 子空间内，可以保持拓扑一致性和多样性。在 MNIST 上的实验表明，即使是最小的类别结构也足以启动有用的合成训练流程。

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [290] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: FMB定律揭示了学习算法和自然选择的统一数学结构，为跨学科算法设计提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 尽管学习算法、优化方法和自然选择看似不同，但它们共享一个共同的数学结构。

Method: 利用Price方程对变化进行符号划分，揭示了普遍的力-度量-偏差（FMB）定律：Δθ = Mf + b + ξ。

Result: FMB定律统一了多种学习和优化算法，并解释了Fisher信息、KL散度等概念的出现。Force（f）通过参数与性能的协方差驱动参数改进，Metric（M）通过曲率的倒数重定尺度，Bias（b）添加动量或改变参考系，Noise（ξ）则用于探索。

Conclusion: 该框架揭示了自然选择、贝叶斯更新、牛顿法、随机梯度下降、随机朗之万动力学、Adam优化等算法的共同结构，为跨学科的算法理解、比较和设计提供了原则性基础。

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [291] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: GPTQ 量化算法在数学上等同于格上的 Babai 最近平面算法，这为其提供了理论基础，并可能有助于未来量化算法的设计。


<details>
  <summary>Details</summary>
Motivation: GPTQ 的内部工作原理被描述为一组合形代数更新，这使得人们难以理解其几何含义或最坏情况的保证。本研究旨在阐明 GPTQ 的理论基础。

Method: 将 GPTQ 的后训练量化方法与 Babai 的最近平面算法进行比较，并证明了它们在数学上的等价性。

Result: GPTQ 的前向后量化过程在数学上等同于格上的 Babai 最近平面算法。这使得 GPTQ 的误差传播步骤具有直观的几何解释，并在无裁剪条件下继承了 Babai 算法的误差上限。

Conclusion: GPTQ 的后训练量化方法在数学上等同于 Babai 的最近平面算法，该算法用于解决经典最近向量问题（CVP）。这种等价性为 GPTQ 提供了牢固的理论基础，并允许将格算法的进展应用于未来数十亿参数模型的量化算法设计。

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [292] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: The paper connects Fisher information matrices and NTK in 2-layer ReLU networks, analyzes NTK's spectral decomposition, and provides an approximation formula for network functions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the relationship between Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU networks with random hidden weights.

Method: The paper analyzes the spectral decomposition of NTK with concrete forms of eigenfunctions and major eigenvalues, and derives an approximation formula for functions represented by 2-layer neural networks.

Result: The paper establishes a connection between Fisher information matrices and NTK as a linear transformation, provides spectral decomposition of NTK, and offers an approximation formula for functions represented by 2-layer neural networks.

Conclusion: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU networks with random hidden weights are related as a linear transformation, and the spectral decomposition of NTK is analyzed.

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [293] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 在缺乏包含人口统计信息的真实数据时，可使用重叠数据集合成数据进行公平性测试，该合成数据能准确反映潜在关系且测试结果与真实数据一致。


<details>
  <summary>Details</summary>
Motivation: 在AI在高风险领域和决策制定中日益普及的背景下，公平性测试至关重要。然而，在行业环境中，法律和隐私问题限制了用于评估群体差异的人口统计数据的收集，内部历史数据集也往往代表性不足。本研究旨在解决在无法获取包含人口统计信息的完整数据集的情况下评估分类器公平性的挑战。

Method: 提出利用单独的、重叠的数据集来构建包含人口统计信息的合成数据，以评估分类器公平性。

Result: 研究验证了合成数据的保真度，并证明了在合成数据上进行的公平性测试与在真实数据上进行的测试所获得的公平性指标是一致的。

Conclusion: 该研究提出了一种在无法获取包含人口统计信息的完整数据集的情况下评估分类器公平性的方法，通过利用单独的、重叠的数据集来构建包含人口统计信息的合成数据，从而准确反映受保护属性与模型特征之间的潜在关系。研究验证了合成数据的保真度，并证明了在合成数据上获得的公平性指标与在真实数据上获得的指标一致。该方法为克服现实世界中公平性测试数据的稀缺性提供了途径，实现了独立的、模型无关的公平性评估，并在真实数据有限的情况下作为可行的替代方案。

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [294] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: 通过使用SE(2)不变的缩放点积注意力机制，提出了一种新的Transformer架构，该架构在处理空间数据和提高性能的同时，内存需求也更低。


<details>
  <summary>Details</summary>
Motivation: 以前的方法需要二次内存来计算所有物体对之间的相对姿态。

Method: 提出了一种SE(2)不变的缩放点积注意力机制，该机制相对于场景中的物体数量只需要线性内存。

Result: 所提出的SE(2)不变的Transformer架构具有与近年来受益于大型语言模型的相同扩展性。

Conclusion: 该方法在实践中是可行的，并且与相当的非不变架构相比，性能有所提高。

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [295] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: 本文提出了一种新的多层次条件扩散模型和评估指标Protein-MMD，用于生成功能性蛋白质。该模型能同时利用序列和结构信息，并有效捕捉蛋白质的多层次结构关系。实验证明了该模型和评估指标的有效性。


<details>
  <summary>Details</summary>
Motivation: 在蛋白质设计领域，尤其是在条件扩散模型方面，可靠的蛋白质生成仍然是一个有待解决的研究问题。考虑到蛋白质的生物学功能由多层次结构决定，因此需要能够整合多层次信息的方法。

Method: 本文提出了一个新颖的多层次条件扩散模型，该模型整合了基于序列和基于结构的信息，用于指导特定功能的蛋白质设计。此外，还提出了一种新的评估指标Protein-MMD，用于评估条件扩散模型生成的蛋白质质量。

Result: 实验结果表明，所提出的生成框架和评估指标在条件蛋白质生成任务上是有效的，并且能够生成高质量的蛋白质。

Conclusion: 本文提出的多层次条件扩散模型能够有效建模不同层级之间的固有层级关系，从而生成具有信息量和区分度的蛋白质表示。同时，提出的Protein-MMD作为新的评估指标，能够同时捕捉真实和生成蛋白质序列在分布和功能上的相似性，并保证条件一致性。

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [296] [Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation](https://arxiv.org/abs/2507.17852)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.MA

TL;DR: "A multi-agent AI system called Tippy automates drug discovery labs using specialized agents, microservices, and OpenAI SDKs, deployed on Kubernetes for scalability and security."


<details>
  <summary>Details</summary>
Motivation: "The paper aims to provide a comprehensive technical analysis of the multi-agent system implementation for drug discovery laboratory automation, building upon previous conceptual work on agentic AI in pharmaceutical research."

Method: "The paper details a distributed microservices architecture for Tippy, comprising five specialized agents (Supervisor, Molecule, Lab, Analysis, and Report). These agents coordinate using OpenAI Agents SDK orchestration and interact with laboratory tools via the Model Context Protocol (MCP). Key technical aspects include agent-specific tool integration, asynchronous communication, Git-based configuration management, Kubernetes deployment with Helm charts, Docker containerization, CI/CD pipelines, vector databases for RAG, and an Envoy reverse proxy."

Result: "The implementation successfully integrates specialized AI agents to coordinate complex laboratory workflows. The system is designed for security, scalability, reliability, and seamless integration with existing laboratory infrastructure through standardized protocols. Production deployment is managed using Kubernetes, Docker, and CI/CD pipelines."

Conclusion: "This work demonstrates the successful implementation and deployment of a multi-agent system (Tippy) for automating drug discovery laboratory workflows, highlighting the effectiveness of specialized AI agents in coordinating complex tasks while ensuring security, scalability, and integration with existing infrastructure."

Abstract: Building on the conceptual framework presented in our previous work on
agentic AI for pharmaceutical research, this paper provides a comprehensive
technical analysis of Tippy's multi-agent system implementation for drug
discovery laboratory automation. We present a distributed microservices
architecture featuring five specialized agents (Supervisor, Molecule, Lab,
Analysis, and Report) that coordinate through OpenAI Agents SDK orchestration
and access laboratory tools via the Model Context Protocol (MCP). The system
architecture encompasses agent-specific tool integration, asynchronous
communication patterns, and comprehensive configuration management through
Git-based tracking. Our production deployment strategy utilizes Kubernetes
container orchestration with Helm charts, Docker containerization, and CI/CD
pipelines for automated testing and deployment. The implementation integrates
vector databases for RAG functionality and employs an Envoy reverse proxy for
secure external access. This work demonstrates how specialized AI agents can
effectively coordinate complex laboratory workflows while maintaining security,
scalability, reliability, and integration with existing laboratory
infrastructure through standardized protocols.

</details>


### [297] [Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation](https://arxiv.org/abs/2507.18224)
*Shiyuan Li,Yixin Liu,Qingsong Wen,Chengqi Zhang,Shirui Pan*

Main category: cs.MA

TL;DR: ARG-Designer是一种用于MAS的新型方法，它能根据任务自动生成定制的代理协作拓扑，实现了更高的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MAS设计方法依赖于模板图修改范式，存在代理数量和交互结构预定义、适应性差等限制。为了解决这些问题，本研究提出了一种新的方法来克服这些局限性。

Method: 提出了一种名为ARG-Designer的新型自回归模型，将MAS设计重构为条件自回归图生成任务，能够从头开始构建协作图。该模型能够根据自然语言任务查询，动态确定所需代理数量、选择角色并建立通信链接。

Result: ARG-Designer在六个不同的基准测试中表现出色，实现了最先进的性能，并且在代币效率和可扩展性方面也显著优于现有方法。

Conclusion: ARG-Designer通过条件自回归图生成任务，为MAS设计提供了新范式，实现了定制化拓扑，并在多个基准测试中取得了最先进的性能，同时提高了效率和可扩展性。

Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged
as a powerful solution for dealing with complex problems across diverse
domains. The effectiveness of MAS is critically dependent on its collaboration
topology, which has become a focal point for automated design research.
However, existing approaches are fundamentally constrained by their reliance on
a template graph modification paradigm with a predefined set of agents and
hard-coded interaction structures, significantly limiting their adaptability to
task-specific requirements. To address these limitations, we reframe MAS design
as a conditional autoregressive graph generation task, where both the system
composition and structure are designed jointly. We propose ARG-Designer, a
novel autoregressive model that operationalizes this paradigm by constructing
the collaboration graph from scratch. Conditioned on a natural language task
query, ARG-Designer sequentially and dynamically determines the required number
of agents, selects their appropriate roles from an extensible pool, and
establishes the optimal communication links between them. This generative
approach creates a customized topology in a flexible and extensible manner,
precisely tailored to the unique demands of different tasks. Extensive
experiments across six diverse benchmarks demonstrate that ARG-Designer not
only achieves state-of-the-art performance but also enjoys significantly
greater token efficiency and enhanced extensibility. The source code of
ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.

</details>


### [298] [Designing Value-Aligned Traffic Agents through Conflict Sensitivity](https://arxiv.org/abs/2507.18284)
*Astrid Rakow,Joe Collenette,Maike Schwammberger,Marija Slavkovik,Gleifer Vs Alves*

Main category: cs.MA

TL;DR: 研究者提出了一种在开发阶段就将价值敏感行为结构化的方法，而不是在运行时解决道德困境，以确保自动驾驶汽车符合法律、社会和道德价值观。


<details>
  <summary>Details</summary>
Motivation: 期望自主交通代理（ATAs）不仅要安全，还要在法律、社会和道德维度上与利益相关者的价值观保持一致。

Method: 采用认知博弈论中的冲突模型来支持开发与利益相关者价值观（法律、社会、道德）一致的自主交通代理（ATAs），特别关注由价值观驱动的竞争目标的价值冲突，并展示冲突分析如何指导价值获取、能力规范、解释和自适应系统优化等关键设计阶段。

Result: 该方法将对价值敏感的行为结构化，并将解决道德困境的重点从运行时转移到开发过程中，通过引入“价值对齐操作设计域”（VODDs）概念来实现。

Conclusion: 该研究提出了一种基于认知博弈论冲突模型的方法，用于开发在法律、社会和道德维度上与利益相关者价值观保持一致的自主交通代理（ATAs）。通过将冲突分析应用于价值获取、能力规范、解释和自适应系统优化等关键设计阶段，并引入“价值对齐操作设计域”（VODDs）概念，该方法旨在将对价值敏感的行为结构化，从而将解决道德困境的重点从运行时转移到开发过程中。

Abstract: Autonomous traffic agents (ATAs) are expected to act in ways tat are not only
safe, but also aligned with stakeholder values across legal, social, and moral
dimensions. In this paper, we adopt an established formal model of conflict
from epistemic game theory to support the development of such agents. We focus
on value conflicts-situations in which agents face competing goals rooted in
value-laden situations and show how conflict analysis can inform key phases of
the design process. This includes value elicitation, capability specification,
explanation, and adaptive system refinement. We elaborate and apply the concept
of Value-Aligned Operational Design Domains (VODDs) to structure autonomy in
accordance with contextual value priorities. Our approach shifts the emphasis
from solving moral dilemmas at runtime to anticipating and structuring
value-sensitive behaviour during development.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [299] [Smoothed Analysis of Online Metric Problems](https://arxiv.org/abs/2507.17834)
*Christian Coester,Jack Umenberger*

Main category: cs.DS

TL;DR: 通过平滑分析，针对 k-server、k-taxi 和追踪大小为 k 的集合问题，提出了优于完全对抗性模型的 polylog(k/σ)-competitive 算法，并证明了其最优性。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过引入平滑分析的视角，改进三个经典在线问题的算法性能，以在部分随机化请求的场景下，提供比完全对抗性模型更优的竞争比。

Method: 研究利用平滑分析，将问题转化为有限度量下的对抗性实例，并在此基础上应用现有算法。具体来说，当度量空间包含在任意范数空间的一个球内，且请求的分布密度函数不超过该球上均匀密度的 1/σ 倍时，该方法有效。

Result: 该研究提出了 polylog(k/σ)-competitive 算法，证明了对于 k-server、k-taxi 和追踪大小为 k 的集合这三个在线问题，在特定条件下（度量空间位于球内，请求分布受限），可以达到比完全对抗性模型（竞争比分别为 2k-1、∞ 和 Θ(k^2)）更好的性能。同时，研究也给出了匹配的上界（sub-polylogarithmic in k/σ）的下界。

Conclusion: 该研究通过平滑分析的方法，为 k-server、k-taxi 和追踪大小为 k 的集合这三个经典在线问题提供了新的解决方案，其竞争比在 polylog(k/σ) 级别，远优于完全对抗环境下的已知结果。

Abstract: We study three classical online problems -- $k$-server, $k$-taxi, and chasing
size $k$ sets -- through a lens of smoothed analysis. Our setting allows
request locations to be adversarial up to small perturbations, interpolating
between worst-case and average-case models. Specifically, we show that if the
metric space is contained in a ball in any normed space and requests are drawn
from distributions whose density functions are upper bounded by $1/\sigma$
times the uniform density over the ball, then all three problems admit
polylog$(k/\sigma)$-competitive algorithms. Our approach is simple: it reduces
smoothed instances to fully adversarial instances on finite metrics and
leverages existing algorithms in a black-box manner. We also provide a lower
bound showing that no algorithm can achieve a competitive ratio
sub-polylogarithmic in $k/\sigma$, matching our upper bounds up to the exponent
of the polylogarithm. In contrast, the best known competitive ratios for these
problems in the fully adversarial setting are $2k-1$, $\infty$ and
$\Theta(k^2)$, respectively.

</details>


### [300] [Better Bounds for Semi-Streaming Single-Source Shortest Paths](https://arxiv.org/abs/2507.17841)
*Sepehr Assadi,Gary Hoppenworth,Janani Sundaresan*

Main category: cs.DS

TL;DR: 本研究在半流模型下，通过提出一个随机算法并证明一个下界，缩小了无向图单源最短路径近似的通道复杂度差距。


<details>
  <summary>Details</summary>
Motivation: 在半流模型中，对无向图进行（单源）最短路径近似是一个长期存在的未解决问题。本研究旨在解决这个问题。

Method: 我们提出了一个简单的随机算法，并证明了一个下界。随机算法的空间复杂度为 $O(\frac{1}{\epsilon} \cdot n \log^3 n)$，通道数为 $O(\frac{1}{\epsilon} \cdot (\frac{\log n}{\log\log n})^2)$。下界证明了任何半流算法需要 $\Omega(\frac{\log n}{\log\log n})$ 个通道来计算常数近似的最短路径。

Result: 我们提出了一个随机算法，能够以高概率计算 $(1+\epsilon)$ 近似的单源最短路径，其空间复杂度为 $O(\frac{1}{\epsilon} \cdot n \log^3 n)$，通道数为 $O(\frac{1}{\epsilon} \cdot (\frac{\log n}{\log\log n})^2)$。我们还证明了任何半流算法需要 $\Omega(\frac{\log n}{\log\log n})$ 个通道来计算常数近似的最短路径。

Conclusion: 本研究在无向图中单源最短路径的半流模型问题上取得了进展。我们提出了一个随机算法，可以在高概率下计算出 $(1+\epsilon)$ 近似的单源最短路径，其空间复杂度为 $O(\frac{1}{\epsilon} \cdot n \log^3 n)$，通道数为 $O(\frac{1}{\epsilon} \cdot (\frac{\log n}{\log\log n})^2)$。我们还证明了任何半流算法要计算常数近似的最短路径（即使是到单个目标顶点的距离），至少需要 $\Omega(\frac{\log n}{\log\log n})$ 个通道。这些结果将通道复杂度的差距从 \text{polylog }n vs \omega(1) 缩小到只有二次方差距。

Abstract: In the semi-streaming model, an algorithm must process any $n$-vertex graph
by making one or few passes over a stream of its edges, use $O(n \cdot
\text{polylog }n)$ words of space, and at the end of the last pass, output a
solution to the problem at hand. Approximating (single-source) shortest paths
on undirected graphs is a longstanding open question in this model. In this
work, we make progress on this question from both upper and lower bound fronts:
  We present a simple randomized algorithm that for any $\epsilon > 0$, with
high probability computes $(1+\epsilon)$-approximate shortest paths from a
given source vertex in \[
  O\left(\frac{1}{\epsilon} \cdot n \log^3 n \right)~\text{space} \quad
\text{and} \quad O\left(\frac{1}{\epsilon} \cdot \left(\frac{\log n}{\log\log
n} \right) ^2\right) ~\text{passes}.
  \] The algorithm can also be derandomized and made to work on dynamic streams
at a cost of some extra $\text{poly}(\log n, 1/\epsilon)$ factors only in the
space. Previously, the best known algorithms for this problem required
$1/\epsilon \cdot \log^{c}(n)$ passes, for an unspecified large constant $c$.
  We prove that any semi-streaming algorithm that with large constant
probability outputs any constant approximation to shortest paths from a given
source vertex (even to a single fixed target vertex and only the distance, not
necessarily the path) requires \[ \Omega\left(\frac{\log n}{\log\log n}\right)
~\text{passes}. \] We emphasize that our lower bound holds for any
constant-factor approximation of shortest paths. Previously, only constant-pass
lower bounds were known and only for small approximation ratios below two.
  Our results collectively reduce the gap in the pass complexity of
approximating single-source shortest paths in the semi-streaming model from
$\text{polylog } n$ vs $\omega(1)$ to only a quadratic gap.

</details>


### [301] [Strong Sparsification for 1-in-3-SAT via Polynomial Freiman-Ruzsa](https://arxiv.org/abs/2507.17878)
*Benjamin Bedert,Tamio-Vesa Nakajima,Karolina Okrasa,Stanislav Živný*

Main category: cs.DS

TL;DR: 提出了一种新的稀疏化技术（强稀疏化），为 1-in-3-SAT 设计了一种算法，并改进了超图着色算法。


<details>
  <summary>Details</summary>
Motivation: 介绍了一种新的稀疏化概念，称为“强稀疏化”，其特点是变量合并而非约束移除。

Method: 算法的正确性依赖于对 $\mathbb{F}_2^d$ 中某些向量集的大小建立次二次界。此结果是使用近期多项式 Freiman-Ruzsa 定理（Gowers、Green、Manners 和 Tao，Ann. Math. 2025）获得的，可能具有独立意义。

Result: 1-in-3-SAT 的强稀疏化算法，以及该算法在近似 3-一致超图的线性排序着色方面的应用，改进了现有算法。

Conclusion: 该研究提出了一种名为“强稀疏化”的新稀疏化概念，其中约束不被移除，但变量可以被合并。主要成果是为 1-in-3-SAT 提供了一种强稀疏化算法。

Abstract: We introduce a new notion of sparsification, called \emph{strong
sparsification}, in which constraints are not removed but variables can be
merged. As our main result, we present a strong sparsification algorithm for
1-in-3-SAT. The correctness of the algorithm relies on establishing a
sub-quadratic bound on the size of certain sets of vectors in $\mathbb{F}_2^d$.
This result, obtained using the recent \emph{Polynomial Freiman-Ruzsa Theorem}
(Gowers, Green, Manners and Tao, Ann. Math. 2025), could be of independent
interest. As an application, we improve the state-of-the-art algorithm for
approximating linearly-ordered colourings of 3-uniform hypergraphs (H{\aa}stad,
Martinsson, Nakajima and{\v{Z}}ivn{\'{y}}, APPROX 2024).

</details>


### [302] [Dual Charging for Half-Integral TSP](https://arxiv.org/abs/2507.17999)
*Nathan Klein,Mehrshad Taziki*

Main category: cs.DS

TL;DR: The max entropy algorithm is a randomized approximation for half-integral TSP and LP solutions. We improve the approximation bounds for both cases using the dual analysis.


<details>
  <summary>Details</summary>
Motivation: We show that the max entropy algorithm is a randomized 1.49776 approximation for half-integral TSP, improving upon the previous known bound of 1.49993 from Karlin et al. This also improves upon the best-known approximation for half-integral TSP due to Gupta et al.

Method: Our improvement results from using the dual, instead of the primal, to analyze the expected cost of the matching. We then extend the analysis to the case when there are an odd number of vertices $n$ at the cost of an additional $O(1/n)$ factor.

Result: The max entropy algorithm is a randomized 1.49776 approximation for half-integral TSP. We also give a 1.4671 approximation for half integral LP solutions with no proper minimum cuts and an even number of vertices.

Conclusion: We show that the max entropy algorithm is a randomized 1.49776 approximation for half-integral TSP, improving upon the previous known bound of 1.49993 from Karlin et al. This also improves upon the best-known approximation for half-integral TSP due to Gupta et al. Our improvement results from using the dual, instead of the primal, to analyze the expected cost of the matching. We also give a 1.4671 approximation for half integral LP solutions with no proper minimum cuts and an even number of vertices, improving upon the bound of Haddadan and Newman of 1.476. We then extend the analysis to the case when there are an odd number of vertices $n$ at the cost of an additional $O(1/n)$ factor.

Abstract: We show that the max entropy algorithm is a randomized 1.49776 approximation
for half-integral TSP, improving upon the previous known bound of 1.49993 from
Karlin et al. This also improves upon the best-known approximation for
half-integral TSP due to Gupta et al. Our improvement results from using the
dual, instead of the primal, to analyze the expected cost of the matching. We
believe this method of analysis could lead to a simpler proof that max entropy
is a better-than-3/2 approximation in the general case.
  We also give a 1.4671 approximation for half integral LP solutions with no
proper minimum cuts and an even number of vertices, improving upon the bound of
Haddadan and Newman of 1.476. We then extend the analysis to the case when
there are an odd number of vertices $n$ at the cost of an additional $O(1/n)$
factor.

</details>


### [303] [On recognizing graphs representing Persistent Perfect Phylogenies](https://arxiv.org/abs/2507.18281)
*Paola Bonizzoni,Gianluca Della Vedova,Mauricio Soto Gomez,Gabriella Trucco*

Main category: cs.DS

TL;DR: 提出了一种判定最大图持久性完美系统存在性的多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决Dollo-1模型（持久性完美系统）的判定问题，并将其与图的识别联系起来，特别是针对最大图。

Method: 提出了一种基于图属性的多项式时间算法，用于判定最大图中的持久性完美系统存在性。

Result: 成功地为最大图中的持久性完美系统存在性问题提供了一个多项式时间算法。

Conclusion: 所提出的算法为最大图中的持久性完美系统存在性判定提供了一个多项式时间解决方案，缩小了完美系统和Dollo-k系统之间的差距。

Abstract: The Persistent Perfect phylogeny, also known as Dollo-1, has been introduced
as a generalization of the well-known perfect phylogenetic model for binary
characters to deal with the potential loss of characters. The problem of
deciding the existence of a Persistent Perfect phylogeny can be reduced to the
one of recognizing a class of bipartite graphs whose nodes are species and
characters. Thus an interesting question is solving directly the problem of
recognizing such graphs. We present a polynomial-time algorithm for deciding
Persistent Perfect phylogeny existence in maximal graphs, where no character's
species set is contained within another character's species set. Our solution,
that relies only on graph properties, narrows the gap between the linear-time
simple algorithm for Perfect Phylogeny and the NP-hardness results for the
Dollo-$k$ phylogeny with $k>1$.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [304] [Low-power switching of memristors exhibiting fractional-order dynamics](https://arxiv.org/abs/2507.18487)
*Nathan Astin,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 忆阻器切换行为的研究表明，通过调整脉冲宽度和幅度，可以最小化焦耳损耗，为节能神经形态计算铺平道路。


<details>
  <summary>Details</summary>
Motivation: 研究开关忆阻器设备的分数阶行为，以实现更节能的神经形态计算。

Method: 提出了一种涉及 Caputo 型导数的分数阶微分方程模型，并研究了焦耳损耗。

Result: 结果表明，最佳开关策略（最小化焦耳损耗）取决于分数阶导数的阶数和运动方程中的功率指数。具体来说，当分数阶导数的阶数超过功率指数的一半时，采用宽脉冲效果更好；否则，通过零电流后跟一个窄脉冲（最大允许幅度）可将焦耳损耗降至最低。

Conclusion: 该研究为下一代节能神经形态计算架构奠定了基础，使其能够更接近地模仿生物对应物。

Abstract: In this conference contribution, we present some initial results on switching
memristive devices exhibiting fractional-order behavior using current pulses.
In our model, it is assumed that the evolution of a state variable follows a
fractional-order differential equation involving a Caputo-type derivative. A
study of Joule losses demonstrates that the best switching strategy minimizing
these losses depends on the fractional derivative's order and the power
exponent in the equation of motion. It is found that when the order of the
fractional derivative exceeds half of the power exponent, the best approach is
to employ a wide pulse. Conversely, when this condition is not met, Joule
losses are minimized by applying a zero current followed by a narrow current
pulse of the highest allowable amplitude. These findings are explored further
in the context of multi-pulse control. Our research lays the foundation for the
advancement of the next generation of energy-efficient neuromorphic computing
architectures that more closely mimic their biological counterparts.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [305] [Safe Reinforcement Learning-based Automatic Generation Control](https://arxiv.org/abs/2507.17868)
*Amr S. Mohamed,Emily Nguyen,Deepa Kundur*

Main category: eess.SY

TL;DR: 在电力系统中安全地使用强化学习进行自动发电控制。


<details>
  <summary>Details</summary>
Motivation: 在电力系统中应用先进的控制和决策算法以提高可靠性、韧性和稳定性的需求日益增长，但使用机器学习技术（尤其是强化学习）的安全性却是一个关键问题，因为它们通常缺乏安全保证。

Method: 本研究提出了一种基于控制障碍函数的框架，以实现安全强化学习和部署。

Result: 该研究开发了安全屏障和强化学习框架，为强化学习在自动发电控制中作为一种安全的选项奠定了基础，为未来详细的验证和应用研究提供了基础。

Conclusion: 该研究提出了一个基于控制障碍函数（CBF）的框架，用于增强强化学习（RL）在电力系统控制中的安全性，特别是自动发电控制（AGC）领域。

Abstract: Amidst the growing demand for implementing advanced control and
decision-making algorithms|to enhance the reliability, resilience, and
stability of power systems|arises a crucial concern regarding the safety of
employing machine learning techniques. While these methods can be applied to
derive more optimal control decisions, they often lack safety assurances. This
paper proposes a framework based on control barrier functions to facilitate
safe learning and deployment of reinforcement learning agents for power system
control applications, specifically in the context of automatic generation
control. We develop the safety barriers and reinforcement learning framework
necessary to establish trust in reinforcement learning as a safe option for
automatic generation control - as foundation for future detailed verification
and application studies.

</details>


### [306] [Trusted Data Fusion, Multi-Agent Autonomy, Autonomous Vehicles](https://arxiv.org/abs/2507.17875)
*R. Spencer Hallyburton,Miroslav Pajic*

Main category: eess.SY

TL;DR: 本研究提出了一种去中心化的信任评估和传感器融合框架，以提高无人机在ISR任务中的安全性和准确性，并通过模拟数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了应对无人机（UAV）在情报、监视和侦察（ISR）任务中，由于其去中心化特性而面临的安全挑战，特别是网络物理攻击。

Method: 本研究提出了一种基于信任的框架，利用隐马尔可夫模型（HMM）来估计智能体及其提供信息的可靠性，实现了去中心化的信任评估和数据融合。

Result: 通过在虚幻引擎模拟器中构建的多智能体空中数据集进行案例研究，证明了所提出的框架在对抗性环境中能够提高ISR性能并检测恶意行为者。

Conclusion: 本研究提出的基于信任的框架能够提高分布式多智能体网络中传感器融合的可靠性和准确性，并且能够检测对抗环境中的恶意行为者。

Abstract: Multi-agent collaboration enhances situational awareness in intelligence,
surveillance, and reconnaissance (ISR) missions. Ad hoc networks of unmanned
aerial vehicles (UAVs) allow for real-time data sharing, but they face security
challenges due to their decentralized nature, making them vulnerable to
cyber-physical attacks. This paper introduces a trust-based framework for
assured sensor fusion in distributed multi-agent networks, utilizing a hidden
Markov model (HMM)-based approach to estimate the trustworthiness of agents and
their provided information in a decentralized fashion. Trust-informed data
fusion prioritizes fusing data from reliable sources, enhancing resilience and
accuracy in contested environments. To evaluate the assured sensor fusion under
attacks on system/mission sensing, we present a novel multi-agent aerial
dataset built from the Unreal Engine simulator. We demonstrate through case
studies improved ISR performance and an ability to detect malicious actors in
adversarial settings.

</details>


### [307] [Rapid Modeling Architecture for Lightweight Simulator to Accelerate and Improve Decision Making for Industrial Systems](https://arxiv.org/abs/2507.17990)
*Takumi Kato,Zhi Li Hu*

Main category: eess.SY

TL;DR: 该研究提出了 RMA 架构，用于构建轻量级工业模拟器，能显著减少建模时间（78.3%），以加速工业系统设计的早期决策。


<details>
  <summary>Details</summary>
Motivation: 工业系统设计早期决策常面临信息有限的问题，导致设计不准确且难以修正。传统模拟器建模时间过长，无法满足快速决策需求。

Method: 提出了 RMA（Rapid Modeling Architecture）来构建轻量级工业模拟器，并构建了原型进行实际工厂布局设计的应用。

Result: 与传统模拟器相比，该研究提出的模拟器将建模时间减少了 78.3%。

Conclusion: 该研究提出了 RMA（Rapid Modeling Architecture）来构建轻量级工业模拟器，以应对工业系统设计早期阶段信息有限的问题。该模拟器能减轻建模负担，同时保留关键细节，从而加快和改进决策过程。

Abstract: Designing industrial systems, such as building, improving, and automating
distribution centers and manufacturing plants, involves critical
decision-making with limited information in the early phases. The lack of
information leads to less accurate designs of the systems, which are often
difficult to resolve later. It is effective to use simulators to model the
designed system and find out the issues early. However, the modeling time
required by conventional simulators is too long to allow for rapid model
creation to meet decision-making demands. In this paper, we propose a Rapid
Modeling Architecture (RMA) for a lightweight industrial simulator that
mitigates the modeling burden while maintaining the essential details in order
to accelerate and improve decision-making. We have prototyped a simulator based
on the RMA and applied it to the actual factory layout design problem. We also
compared the modeling time of our simulator to that of an existing simulator,
and as a result, our simulator achieved a 78.3% reduction in modeling time
compared to conventional simulators.

</details>


### [308] [Quantitative Damping Calculation and Compensation Method for Global Stability Improvement of Inverter-Based Systems](https://arxiv.org/abs/2507.18001)
*Yang Li,Zenghui Zheng,Xiangyang Wu,Jiayong Li,Wei Wang,Qiang Zeng,Zhikang Shuai*

Main category: eess.SY

TL;DR: 本研究提出了一种量化阻尼计算和补偿方法，以提高逆变器系统的全局稳定性，并通过仿真和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决由小信号稳定性问题引起的宽带振荡对多逆变器系统安全运行构成的威胁，需要明确保证系统全局稳定性所需的精确阻尼补偿量。

Method: 1.提出量化阻尼计算算法，用于计算和确定阻尼补偿所需的量及位置。
2.提出一种包含输出电流前馈控制策略的AD，使AD接近纯阻性，以提高阻尼效率。

Result: 所提出的方法能够有效提高逆变器系统的全局稳定性，并通过三逆变器测试系统的仿真和实验得到验证。

Conclusion: 本研究提出的基于系统节点导纳模型的量化阻尼计算算法，连同输出电流前馈控制策略的特定AD，能够有效提高逆变器系统阻尼效率，从而增强系统全局稳定性。仿真和实验结果验证了该方法的有效性。

Abstract: Small-signal stability issues-induced broadband oscillations pose significant
threats to the secure operation of multi-inverter systems, attracting extensive
research attention. Researches revealed that system instability is led by the
lacking of positive damping, yet it has not been clearly specified how much the
exact amount of damping compensation required to sufficiently ensure system
global stability. This paper presents a feasible solution for quantitative
damping calculation and compensation to enhance the global stability of
inverter-based systems. First, based on the system nodal admittance model, a
quantitative damping calculation algorithm is presented, which can suggest the
required damping compensation as well as compensation location for sufficient
stability improvement. Then, we propose a specific AD with output current
feedforward control strategy, which make the AD be quasi-pure resistive and can
effectively enhance system damping efficiency. Finally, a testing system with
three inverters is used as case study, showing that the proposed method
provides a promising solution to efficiently enhance the global stability
improvement of inverter-based systems. Simulations and experiments validate the
proposed method.

</details>


### [309] [Carbon Emission Flow Tracing: Fast Algorithm and California Grid Study](https://arxiv.org/abs/2507.18077)
*Yuqing Shen,Yuanyuan Shi,Daniel Kirschen,Yize Chen*

Main category: eess.SY

TL;DR: 本研究提出了一种计算高效的算法，用于量化电力系统中节点碳排放率，并揭示了加州电网的排放模式。


<details>
  <summary>Details</summary>
Motivation: 解决在电力系统脱碳背景下，如何量化单个发电机排放如何通过电网传输以及如何影响特定位置电力用户的空间和时间影响不明确的问题。

Method: 利用图论中的拓扑排序和有向环移除技术，对发电调度和最优潮流解形成的定向图进行处理，以精确量化节点平均和边际碳排放率。

Result: 开发了一种能够精确量化节点碳排放率的算法，并成功应用于加州电网的模拟，揭示了详细的、面向每个加州县的、符合时间和空间的排放分析。

Conclusion: 该研究提出了一种新颖且计算效率高的方法，用于精确量化节点平均和边际碳排放率，适用于交流和直流最优潮流问题。该方法利用基于图的拓扑排序和有向环移除技术，并已在加州电网上进行了模拟验证，揭示了发电量在空间上的排放模式。

Abstract: Power systems decarbonization are at the focal point of the clean energy
transition. While system operators and utility companies increasingly publicize
system-level carbon emission information, it remains unclear how emissions from
individual generators are transported through the grid and how they impact
electricity users at specific locations. This paper presents a novel and
computationally efficient approach for exact quantification of nodal average
and marginal carbon emission rates, applicable to both AC and DC optimal power
flow problems. The approach leverages graph-based topological sorting and
directed cycle removal techniques, applied to directed graphs formed by
generation dispatch and optimal power flow solutions. Our proposed algorithm
efficiently identifies each generator's contribution to each node, capturing
how emissions are spatially distributed under varying system conditions. To
validate its effectiveness and reveal locational and temporal emission patterns
in the real world, we simulate the 8,870-bus realistic California grid using
actual CAISO data and the CATS model. Based on year long hourly data on nodal
loads and renewable generation, obtained or estimated from CAISO public data,
our method accurately estimates power flow conditions, generation mixes, and
systemwide emissions, and delivers fine grained spatiotemporal emission
analysis for every California county. Both our algorithm and the California
study are open-sourced, providing a foundation for future research on grid
emissions, planning, operations, and energy policy.

</details>


### [310] [Towards Microgrid Resilience Enhancement via Mobile Power Sources and Repair Crews: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2507.18095)
*Yi Wang,Dawei Qiu,Fei Teng,Goran Strbac*

Main category: eess.SY

TL;DR: 该论文提出了一种新颖的分层多智能体强化学习方法，用于在通信基础设施可能受损的极端情况下，去中心化地协调移动电源和维修人员，以提高微电网的韧性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究以中心化的方式解决移动电源（MPS）和维修人员（RCs）的协同调度问题，但这种方式假设通信网络在事件发生后仍能完全运行。然而，极端事件可能会损坏通信基础设施，使得中心化决策变得不切实际。本研究旨在解决这一问题。

Method: 提出了一种分层多智能体强化学习方法，采用两级框架。高级动作用于在电力和交通网络之间切换决策，低级动作通过混合策略构建，用于分别计算电力和交通网络的连续调度和离散路由决策。该方法还包含一个封装系统动态的嵌入式函数，以增强学习稳定性和可扩展性。

Result: 该研究提出了一种去中心化的框架来解决移动电源（MPS）和维修人员（RCs）的韧性驱动调度问题，并通过分层多智能体强化学习方法来解决。

Conclusion: 所提出的去中心化方法在 IEEE 33 节点和 69 节点电网的案例研究中被证明是有效的，能够成功恢复负载。

Abstract: Mobile power sources (MPSs) have been gradually deployed in microgrids as
critical resources to coordinate with repair crews (RCs) towards resilience
enhancement owing to their flexibility and mobility in handling the complex
coupled power-transport systems. However, previous work solves the coordinated
dispatch problem of MPSs and RCs in a centralized manner with the assumption
that the communication network is still fully functioning after the event.
However, there is growing evidence that certain extreme events will damage or
degrade communication infrastructure, which makes centralized decision making
impractical. To fill this gap, this paper formulates the resilience-driven
dispatch problem of MPSs and RCs in a decentralized framework. To solve this
problem, a hierarchical multi-agent reinforcement learning method featuring a
two-level framework is proposed, where the high-level action is used to switch
decision-making between power and transport networks, and the low-level action
constructed via a hybrid policy is used to compute continuous scheduling and
discrete routing decisions in power and transport networks, respectively. The
proposed method also uses an embedded function encapsulating system dynamics to
enhance learning stability and scalability. Case studies based on IEEE 33-bus
and 69-bus power networks are conducted to validate the effectiveness of the
proposed method in load restoration.

</details>


### [311] [Regional Frequency-Constrained Planning for the Optimal Sizing of Power Systems via Enhanced Input Convex Neural Networks](https://arxiv.org/abs/2507.18102)
*Yi Wang,Goran Strbac*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large renewable penetration has been witnessed in power systems, resulting in
reduced levels of system inertia and increasing requirements for frequency
response services. There have been plenty of studies developing
frequency-constrained models for power system security. However, most existing
literature only considers uniform frequency security, while neglecting
frequency spatial differences in different regions. To fill this gap, this
paper proposes a novel planning model for the optimal sizing problem of power
systems, capturing regional frequency security and inter-area frequency
oscillations. Specifically, regional frequency constraints are first extracted
via an enhanced input convex neural network (ICNN) and then embedded into the
original optimisation for frequency security, where a principled weight
initialisation strategy is adopted to deal with the gradient vanishing issues
of non-negative weights in traditional ICNNs and enhance its fitting ability.
An adaptive genetic algorithm with sparsity calculation and local search is
developed to separate the planning model into two stages and effectively solve
it iteratively. Case studies have been conducted on three different power
systems to verify the effectiveness of the proposed frequency-constrained
planning model in ensuring regional system security and obtaining realistic
investment decisions.

</details>


### [312] [Two-Stage TSO-DSO Services Provision Framework for Electric Vehicle Coordination](https://arxiv.org/abs/2507.18110)
*Yi Wang,Dawei Qiu,Fei Teng,Goran Strbac*

Main category: eess.SY

TL;DR: 该研究提出了一种新颖的两阶段框架，利用大量电动汽车（EV）通过去中心化运行和通信高效强化学习（RL）来同时提供频率服务和电压支持，以应对可再生能源并网带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 为了解决高可再生能源渗透率导致电网惯量降低和频率响应需求增加的问题，并协调输电系统运营商（TSO）的频率控制和配电系统运营商（DSO）的电压安全，特别是当电动汽车（EV）同时参与两种服务时可能带来的电压安全问题。

Method: 该研究提出了一种两阶段的服务提供框架，首先进行日前调度以参与输电系统运营商（TSO）的频率备用，然后在实时调度中通过去中心化运行模式和通信高效强化学习（RL）算法来支持配电系统运营商（DSO）的电压安全。

Result: 通过在6节点输电网络和33节点/69节点配电网络上进行的案例研究，验证了该方法在支持EV参与频率服务和电压支持方面的有效性和可扩展性。

Conclusion: 该论文提出了一种两阶段的服务提供框架，用于大规模多电动汽车（EV）参与电网频率服务和电压支持，以解决高可再生能源渗透率带来的系统惯量降低和频率响应需求增加的问题。

Abstract: High renewable penetration has been witnessed in power systems, resulting in
reduced system inertia and increasing requirements for frequency response
services. Electric vehicles (EVs), owing to their vehicle-to-grid (V2G)
capabilities, can provide cost-effective frequency services for transmission
system operators (TSOs). However, EVs that are inherently connected to
distribution networks may pose voltage security issues for distribution system
operators (DSOs) when supporting TSO frequency. To coordinate both TSO
frequency and DSO voltage, this paper proposes a two-stage service provision
framework for multi-EVs. At stage one, EVs participate in day-ahead TSO-DSO
interactions for frequency reserve schedules; at stage two, EVs make real-time
dispatching behaviors in distribution networks for reserve delivery while
supporting DSO voltage. Considering the potentially large EV number and
environment complexity, a decentralized operation paradigm is introduced for
real-time EV dispatches at stage two, while a communication-efficient
reinforcement learning (RL) algorithm is proposed to reduce the communication
overhead during large-scale multi-agent RL training without compromising policy
performance. Case studies are carried out on a 6-bus transmission and 33-bus
distribution network as well as a 69-bus distribution network to evaluate the
effectiveness and scalability of the proposed method in enabling EVs for
frequency service and voltage support.

</details>


### [313] [Data-Driven Model Order Reduction for Continuous- and Discrete-Time Nonlinear Systems](https://arxiv.org/abs/2507.18131)
*Behrad Samari,Henrik Sandberg,Karl H. Johansson,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 针对具有未知模型的非线性系统，提出数据驱动框架构建降阶模型（ROM），利用数据和仿真函数（SF）建立模型间相似性，通过半正定规划保证模型和SF的构建，并成功用于控制器合成以满足高层逻辑属性。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，构建有效的降阶模型（ROM）面临严峻挑战，特别是在处理具有高度非线性项的动力系统，以及在实际系统模型未知的情况下。本研究旨在解决这些挑战，提出一种数据驱动的方法来构建未知模型的非线性动力系统的ROM。

Method: 本研究提出了一种数据驱动的框架，利用两个输入-状态轨迹数据集构建系统的基于数据的闭环表示。通过仿真函数（SF）的概念，建立了原始系统输出轨迹与其数据驱动ROM输出轨迹之间的相似性关系。为实现此目的，提出了数据依赖的半正定规划，作为同时构建ROM和SF的充分条件，并提供正确性保证。控制器设计首先在数据驱动的ROM上进行，然后通过接口函数将结果转换回原始系统。

Result: 研究结果表明，所获得的数据驱动ROM可用于合成控制器，以确保未知系统满足高层逻辑属性。通过四个包含未知非线性动力学的基准案例评估了数据驱动方法的有效性。

Conclusion: 该研究提出了一种数据驱动的框架，用于为具有未知数学模型的连续和离散时间非线性动力系统构建降阶模型（ROM）。该框架利用两个输入-状态轨迹数据集来构建系统的基于数据的闭环表示，并使用仿真函数（SF）建立原始系统输出轨迹与其数据驱动ROM输出轨迹之间的相似性关系，从而为它们的接近度提供形式化表征。研究提出了数据依赖的半正定规划作为同时构建ROM和SF的充分条件，并提供了正确性保证。最终，该框架能够合成控制器，确保满足高层逻辑属性，并已通过四个包含未知非线性动力学的基准案例进行了评估。

Abstract: Model order reduction simplifies high-dimensional dynamical systems by
deriving lower-dimensional models that preserve essential system
characteristics. These techniques are crucial to controller design for complex
systems while significantly reducing computational costs. Nevertheless,
constructing effective reduced-order models (ROMs) poses considerable
challenges, particularly for dynamical systems characterized by highly
nonlinear terms. These challenges are further exacerbated when the actual
system model is unavailable, a scenario frequently encountered in real-world
applications. In this work, we propose a data-driven framework for the
construction of ROMs for both continuous- and discrete-time nonlinear dynamical
systems with unknown mathematical models. By leveraging two sets of data
collected from the system, referred to as two input-state trajectories, we
first construct a data-based closed-loop representation of the system. We then
establish a similarity relation between the output trajectories of the original
system and those of its data-driven ROM employing the notion of simulation
functions (SFs), thereby enabling a formal characterization of their closeness.
To achieve this, we propose data-dependent semidefinite programs as sufficient
conditions to simultaneously construct both ROMs and SFs, while offering
correctness guarantees. We demonstrate that the obtained data-driven ROMs can
be employed for synthesizing controllers that ensure the unknown system
satisfies high-level logic properties. This is accomplished by first designing
controllers for the data-driven ROMs and then translating the results back to
the original system through an interface function. We evaluate the efficacy of
our data-driven findings through four benchmark case studies involving unknown
dynamics with highly nonlinear terms.

</details>


### [314] [Data-Driven Incremental GAS Certificate of Nonlinear Homogeneous Networks: A Formal Modular Approach](https://arxiv.org/abs/2507.18141)
*Mahdieh Zaker,David Angeli,Abolfazl Lavaei*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的组合方法，用于验证具有未知动力学的互连同质网络（度为一）的增量全局渐近稳定性（delta-GAS）。该方法通过求解子系统的delta-ISS Lyapunov函数，并利用小增益组合条件，实现了样本复杂性的线性增长，克服了现有方法的局限性，并在一个大型网络中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 为了验证具有未知动力学的互连同质网络（度为一）的增量全局渐近稳定性（delta-GAS），并克服现有方法在样本复杂性方面随网络规模指数增长而导致的实际应用困难。

Method: 本文提出了一种数据驱动的组合方法，首先将增量全局渐近稳定性（delta-GAS）的Lyapunov条件重构为鲁棒优化问题（ROP），然后通过场景优化程序（SOP）解决，以获取未知子系统的delta-ISS Lyapunov函数。最后，利用小增益组合条件，基于子系统的数据驱动delta-ISS Lyapunov函数，构建互连网络的增量Lyapunov函数。

Result: 所提出的数据驱动组合方法在样本复杂性方面与子系统粒度相匹配，随着子系统数量的增加，所需数据呈线性增长。与现有单片方法相比，该方法在样本复杂性方面具有显著优势，并成功应用于一个包含10000个子系统的未知非线性同质网络，验证了该网络的delta-GAS。

Conclusion: 该方法通过利用子系统增量输入状态稳定性（delta-ISS）的Lyapunov函数，并结合小增益组合条件，成功地构建了未知互连网络的增量Lyapunov函数，从而验证了所提出的数据驱动组合方法在样本复杂性方面的优势，以及其在大型网络应用中的有效性。

Abstract: This work focuses on a compositional data-driven approach to verify
incremental global asymptotic stability (delta-GAS) over interconnected
homogeneous networks of degree one with unknown mathematical dynamics. Our
proposed approach leverages the concept of incremental input-to-state stability
(delta-ISS) of subsystems, characterized by delta-ISS Lyapunov functions. To
implement our data-driven scheme, we initially reframe the delta-ISS Lyapunov
conditions as a robust optimization program (ROP). However, due to the presence
of unknown subsystem dynamics in the ROP constraints, we develop a scenario
optimization program (SOP) by gathering data from trajectories of each unknown
subsystem. We solve the SOP and construct a delta-ISS Lyapunov function for
each subsystem with unknown dynamics. We then leverage a small-gain
compositional condition to facilitate the construction of an incremental
Lyapunov function for an unknown interconnected network with unknown dynamics
based on its data-driven delta-ISS Lyapunov functions of individual subsystems,
while providing correctness guarantees. We demonstrate that our data-driven
compositional approach aligns sample complexity with subsystem granularity,
resulting in a linear increase in required data as the number of subsystems
rises. In contrast, the existing monolithic approach in the literature exhibits
exponential growth in sample complexity with increasing number of subsystems,
rendering it impractical for real-world applications. To validate the
effectiveness of our compositional data-driven approach, we apply it to an
unknown nonlinear homogeneous network of degree one, comprising 10000
subsystems. By gathering data from each unknown subsystem, we demonstrate that
the interconnected network is delta-GAS with a correctness guarantee.

</details>


### [315] [Unit Commitment Framework for Nuclear Reactors with Reactivity Decline](https://arxiv.org/abs/2507.18150)
*Shiny Choudhury,Michael Davidson,George Tynan*

Main category: eess.SY

TL;DR: 核电厂的运行灵活性可以通过考虑燃料循环和氙毒化等物理约束来提高，这对于能源系统的准确调度和核能整合至关重要。


<details>
  <summary>Details</summary>
Motivation: 传统核反应堆模型常被视为固定运行模式，忽视了其在实际运行中，尤其是在不同燃料循环阶段和氙毒化效应下的运行灵活性。这导致对核电厂调度和在能源系统中整合核能的能力的评估存在偏差。

Method: 提出了一种结合了物理信息和元启发式算法的建模方法，将燃料循环动力学嵌入机组承诺（UC）框架。该框架能够追踪反应性余量，动态激活与氙毒化相关的约束，并根据核芯状况内生地实施重新燃料中断。

Result: 该模型应用于代表性的反应堆机队，在从基本负荷到部分负荷的不同运行模式下，揭示了灵活运行可以减缓反应性衰减并延长燃料循环。

Conclusion: 通过将核电厂的燃料循环动力学嵌入机组承诺框架中，并考虑了氙毒化等物理约束，可以实现考虑反应性演变的核电调度。这种方法能够更准确地反映核电厂的实际运行能力，并揭示了灵活运行有助于减缓反应性衰减、延长燃料循环。因此，将燃料循环意识纳入核电调度对于准确安排核电机组运行至关重要，也为在能源系统中整合核能提供了可行途径。

Abstract: Nuclear reactors are often modeled as inflexible, baseload generators with
fixed downtimes and restrictive ramping limits. In practice, however, a
reactor's operational flexibility is closely tied to it's fuel cycle stage and
the associated reactivity margin. A key physical constraint to power
maneuverability is xenon poisoning, caused by an increase in neutron absorbing
xenon concentration following a power ramp down. This can delay or even prevent
subsequent power ramp up due to suppressed core reactivity. Additionally, if a
reactor is shutdown during periods of low reactivity, restart times can vary
significantly due to these xenon transients, leading to longer downtimes. This
work introduces a physics informed, metaheuristic modeling approach that embeds
fuel cycle dynamics directly with a unit commitment (UC) framework. The
framework tracks reactivity margin, dynamically activates xenon related
constraints, and endogenously implements refueling outages based on the core
conditions. By capturing intra-cycle reactivity evolution and the conditional
onset of xenon poisoning, the formulation allows for operation dependent
nuclear dispatch that reflects both regulatory limits and physical behavior.
When applied to a representative reactor fleet operating in distinct modes of
operation -- ranging from baseload to part load -- the framework reveals that
flexible operation can slow reactivity degradation and extend fuel cycles. The
results show that fuel cycle aware flexibility modeling is critical for
accurate scheduling of nuclear reactors and offers a tractable pathway to
integrate nuclear power in energy system models.

</details>


### [316] [Stability Constrained Voltage Control in Distribution Grids with Arbitrary Communication Infrastructure](https://arxiv.org/abs/2507.18158)
*Zhenyi Yuan,Jie Feng,Yuanyuan Shi,Jorge Cortés*

Main category: eess.SY

TL;DR: 本文提出了一种新的学习型强力控制器设计框架，该框架利用通信基础设施来改善电压调节和闭环稳定性，并使用ICNN和监督学习进行训练，仿真结果有效。


<details>
  <summary>Details</summary>
Motivation: 旨在解决学习型强力控制器在配电网电压调节中的应用，并确保闭环系统的稳定性。与现有方法将可证明稳定的控制器限制为分散式不同，本文提出了一种统一的设计框架，该框架能够利用物理电网之上的任意通信基础设施。

Method: 提出了一种统一的设计框架，用于设计基于学习的电压调节控制器，并考虑了闭环系统的稳定性。该框架允许控制器利用任意通信基础设施，并能处理本地总线以外的信息。研究人员提出了一种设计流程，用于构建输入凸神经网络（ICNN）控制器，该控制器在设计时就满足稳定性约束，并通过监督学习进行训练。

Result: 仿真结果表明，该框架在加州大学圣地亚哥分校（UCSD）微电网测试台上能够有效地进行电压调节，并突出了通信在提高控制性能方面的作用。

Conclusion: 所提出的框架能够集成任意通信基础设施，使控制器能够利用本地总线以外的信息，从而放宽了对控制器设计的保守限制。研究人员提供了一个设计流程，用于构建基于输入凸神经网络（ICNN）的控制器，该控制器在任意通信场景下都具有内置的稳定性保证，并通过监督学习进行训练。

Abstract: We consider the problem of designing learning-based reactive power
controllers that perform voltage regulation in distribution grids while
ensuring closed-loop system stability. In contrast to existing methods, where
the provably stable controllers are restricted to be decentralized, we propose
a unified design framework that enables the controllers to take advantage of an
arbitrary communication infrastructure on top of the physical power network.
This allows the controllers to incorporate information beyond their local bus,
covering existing methods as a special case and leading to less conservative
constraints on the controller design. We then provide a design procedure to
construct input convex neural network (ICNN) based controllers that satisfy the
identified stability constraints by design under arbitrary communication
scenarios, and train these controllers using supervised learning. Simulation
results on the the University of California, San Diego (UCSD) microgrid testbed
illustrate the effectiveness of the framework and highlight the role of
communication in improving control performance.

</details>


### [317] [Optimal Integration Of Heat-Pump And Solar Thermal Energy In The Pre-heating Loop Of Wood And Gas Boiler Based District Heating System](https://arxiv.org/abs/2507.18204)
*Hamza Mettali,Rousset François,Eric Bideaux,Clausse Marc*

Main category: eess.SY

TL;DR: 通过MILP模型优化区域供热的太阳能整合，考虑了成本、环境影响和温度依赖性。


<details>
  <summary>Details</summary>
Motivation: 为了在区域能源网络中脱碳供热，特别是利用太阳能热能，需要优化系统设计以应对温度变化和权衡技术经济与环境因素。

Method: 提出了一种混合整数线性规划（MILP）模型，并结合了温度离散化技术，以优化包含太阳能热能（带或不带热泵）的区域能源网络的设计。

Result: 研究表明，MILP模型可以提高收敛性，减少MIP间隙，并有效利用太阳能。在不同碳税和排放情景下，太阳能整合量可达11,932平方米，但可能增加天然气依赖和储热损失。生物质锅炉可以满足45%的热需求，降低单位热成本，但限制了可再生能源的普及。

Conclusion: 虽然高碳税可以促进太阳能的整合，但它也可能导致储存效率低下。生物质可以提高成本效益和系统稳定性，但会限制可再生能源的渗透。

Abstract: The integration of renewable sources is essential for decarbonizing heat
production in district energy networks. Beyond biomass-based solutions, solar
thermal energy, with or without heat pumps, presents a significant opportunity.
However, system performance is highly dependent on outdoor and setpoint
temperatures. This study aims to optimize system design using a multi-criteria
approach that considers techno-economic and environmental (CO2) factors. A
Mixed-Integer Linear Programming (MILP) model is developed, incorporating
temperature discretization for problem linearization and capturing key dynamic
characteristics of heat generators. The model improves convergence, reducing a
19% MIP gap in 26 hours to 10% in 12 hours by dissipating 6% excess solar heat.
A multi-scenario analysis under two carbon taxation levels and different CO2
emission cases revealed solar integration up to 11,932 m${}^2$ but increased
gas reliance (50%) and TES losses (49%). Wood boiler inclusion reduced solar
dependency, covering 45% of heat, lowered LCOH, but limited renewable
penetration. Higher carbon taxes boosted solar adoption but faced storage
inefficiencies, while biomass enhanced cost efficiency and system stability.

</details>


### [318] [Maneuvering-based Dynamic Thrust Allocation for Fully-Actuated Vessels](https://arxiv.org/abs/2507.18309)
*Emir Cem Gezer,Roger Skjetne*

Main category: eess.SY

TL;DR: 本文提出了一种新的推力分配方法，使用控制李雅普诺夫函数和控制障碍函数，以实现简单、有效以及平滑和动态的推力参考信号。


<details>
  <summary>Details</summary>
Motivation: 为了在船舶操纵中实现简单、有效以及平滑和动态的推力参考信号。

Method: 该方法使用控制李雅普诺夫函数来创建推力滤波器，并使用控制障碍函数来确保推力饱和限制得到遵守。

Result: 该方法成功地实现了推力分配，同时考虑了动态跟踪和饱和限制。

Conclusion: 本文提出了一种新的方法来解决海上船舶的推力分配问题，该方法利用了船舶操纵问题，并为全驱动船舶设计了非线性参考滤波器。

Abstract: This paper introduces a new approach to solving the thrust allocation problem
using the maneuvering problem in the maritime domain for fully actuated
vessels. The method uses a control Lyapunov function to create a nonlinear
reference filter for the thruster forces. The filter ensures dynamic tracking
of the optimal thrust allocation solution with rate limitation in the output
thruster references. It further uses control barrier functions to ensure that
the thruster force saturation limits are respected. The approach aims for
simplicity and effectiveness, as well as smooth and dynamic thruster reference
signals, in the implementation of thrust allocation for marine vessels.

</details>


### [319] [Toward Sustainable Vertical Farming: Impacts of Environmental Factors and Energy Mix on Performance and Costs](https://arxiv.org/abs/2507.18419)
*Francesco Ceccanti,Aldo Bischi,Umberto Desideri,Andrea Baccioli*

Main category: eess.SY

TL;DR: 垂直农业的效率和成本效益高度依赖于PPFD、CO2和温度等因素，而与外部气候和隔热关系不大。要实现可持续性，需要使用近乎全脱碳的能源。


<details>
  <summary>Details</summary>
Motivation: 为了提高能源效率和降低成本，需要为垂直农业设计和操作指南，因为其在稳定、高质量、无虫蔬菜生产以及与能源系统和城市发展的协同作用方面具有潜力。

Method: 本研究通过结合三个不同气候区（挪威、中国和迪拜）的温度、光合光子通量密度（PPFD）和二氧化碳浓度三个不同水平，并结合两种绝缘厚度，评估了162种不同的种植场景，以分析垂直农业系统的生产性能和能源消耗。

Result: PPFD是影响作物生长和能源消耗的主要因素，分别占0.85和0.73的相关性。然而，绝缘层和外部气候对作物生产力的影响不大。研究确定了最具成本效益的设置是24°C、250 PPFD、1400 ppm CO2，并带有隔热层，这在所有气候下都是一致的。

Conclusion: 只有接近全脱碳的能源系统才能支持垂直农业，而不会与进口生菜相比增加碳排放量。

Abstract: The increasing interest in vertical farming arises from its ability to ensure
consistent, high-quality, and pest-free vegetable production while supporting
synergies with energy systems and urban development. Accordingly, standardized
design and operation guidelines are essential to improve energy efficiency and
lower costs. This study analyzes the production performance and energy
consumption of a vertical farming system, assessing its efficiency,
sustainability, and economic viability. A total of 162 scenarios were evaluated
by combining three levels of temperature, photosynthetic photon flux density
(PPFD), and CO2 concentration across three distinct climatic zones, namely
Norway, China, and Dubai, which also differ from a socio-environmental
viewpoint. Two insulation thicknesses were also tested in each scenario.
Results indicate that due to the heating, ventilation, and air conditioning and
dehumidification (HVACD) system, neither the insulation layer nor the external
climate significantly influences crop productivity. PPFD proved to be the
dominant factor in crop growth (correlation: 0.85), followed by CO2 (0.36) and
indoor temperature (0.22). PPFD also emerged as the primary driver of overall
energy consumption (correlation: 0.73), as it affects both lighting and HVACD
loads. Notably, the lowest specific energy consumption (SEC) coincided with the
lowest crop productivity (55 kg/m2). The levelized cost of lettuce (LCoL),
balancing productivity and energy use, identified the most cost-effective setup
as 24C, 250 PPFD, 1400 ppm CO2, with insulation, consistent across all
climates. Ultimately, only nearly decarbonized energy systems can support
vertical farming without increasing CO2 emissions compared to imported lettuce.

</details>


### [320] [A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks](https://arxiv.org/abs/2507.18492)
*Mirhan Ürkmez,Carsten Kallesøe,Jan Dimon Bendtsen,Eric C. Kerrigan,John Leth*

Main category: eess.SY

TL;DR: 通过鲁棒模型预测控制（RMPC）优化水泵调度，以降低成本并应对不确定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决水务网络中由水泵引起的电费高昂问题，并应对模型不确定性和需水量预测误差带来的水泵调度挑战。

Method: 提出一种鲁棒模型预测控制（RMPC）方法，采用线性模型和有界扰动，通过稀疏化优化问题将计算复杂度从$\\\mathcal{O}(N^6)$降至$\\\mathcal{O}(N^3)$。

Result: 在模拟丹麦兰德斯镇水务管网的模型上评估，该方法在满足约束方面优于名义和约束收紧MPC方法，并提供了可比的经济效益。

Conclusion: 所提出之鲁棒模型预测控制（RMPC）方法在满足约束方面优于名义和约束收紧模型预测控制（MPC）方法，并且在经济效益方面相当。

Abstract: Water utilities aim to reduce the high electrical costs of Water Distribution
Networks (WDNs), primarily driven by pumping. However, pump scheduling is
challenging due to model uncertainties and water demand forecast errors. This
paper presents a Robust Model Predictive Control (RMPC) method for optimal and
reliable pump scheduling, extending a previous efficient robust control method
tailored to our model. A linear model with bounded additive disturbances is
used to represent tank water level evolution, with uncertainty bounds derived
from WDN simulation and demand data. At each time step, a pump scheduling
policy, affine in past disturbances, is optimized to satisfy system constraints
over a prediction horizon. The resulting policies are then applied in a
receding horizon fashion. The optimization problem is formulated to require
$\mathcal{O}(N^6)$ computations per iteration with an interior-point method,
which is reduced to $\mathcal{O}(N^3)$ by reformulating it into a sparse form.
When evaluated on a model representing the water distribution network of
Randers, a medium-sized town in Denmark, the method surpasses nominal and
constraint-tightening model predictive control (MPC) approaches in terms of
meeting constraints and provides comparable economic outcomes.

</details>


### [321] [Global Observer Design for a Class of Linear Observed Systems on Groups](https://arxiv.org/abs/2507.18493)
*Changwu Liu,Yuan Shen*

Main category: eess.SY

TL;DR: 提出了一种用于一类线性观测系统的统一观测器框架，通过系统嵌入和优化重构实现状态估计，并讨论了稳定性条件及其在导航问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决一类线性观测系统（其在群上的表示与实际状态估计问题中的几何结构相关）的观测器设计问题，提出了一种统一的观测器框架。

Method: 通过将双线性系统限制在其正态子群上，实现系统到线性时变系统的嵌入，并为嵌入系统设计类卡尔曼观测器，最后通过优化重构值空间状态。

Result: 在秩条件满足且找到重构优化的全局最优值时，实现了全局指数稳定性（GES）；当联合估计输入偏差时，可保证半全局稳定性。并通过对双帧系统的应用和两个算例进行了说明。

Conclusion: 该方法通过将原始系统嵌入线性时变系统，并为嵌入系统设计类卡尔曼观测器，然后通过优化重构值空间状态，实现了对一类线性观测系统的统一观测器框架。在秩条件满足的情况下，若能找到重构优化的全局最优值，即可实现全局指数稳定性（GES）。当联合估计输入偏差时，可保证半全局稳定性。该理论应用于双帧系统的GES观测器设计，可用于建模导航问题。

Abstract: Linear observed systems on groups encode the geometry of a variety of
practical state estimation problems. In this paper, we propose a unified
observer framework for a class of linear observed systems by restricting a
bi-invariant system on a Lie group to its normal subgroup. This structural
property powerfully enables a system immersion of the original system into a
linear time-varying system. Leveraging the immersion, an observer is
constructed by first designing a Kalman-like observer for the immersed system
and then reconstructing the group-valued state via optimization. Under a rank
condition, global exponential stability (GES) is achieved provided one global
optimum of the reconstruction optimization is found, reflecting the topological
difficulties inherent to the non-Euclidean state space. Semi-global stability
is guaranteed when input biases are jointly estimated. The theory is applied to
the GES observer design for two-frame systems, capable of modeling a family of
navigation problems. Two non-trivial examples are provided to illustrate
implementation details.

</details>


### [322] [Design and optimization of a novel leaf-shape antenna for RF energy transfer](https://arxiv.org/abs/2507.18630)
*Junbin Zhong,Mingtong Chen,Zhengbao Yang*

Main category: eess.SY

TL;DR: 本文提出了一种受叶子启发的新型天线，用于射频能量收集。天线在915兆赫频率下优化，S11参数接近-20分贝，在200厘米的距离内为设备供电。生物启发式设计提高了射频能量传输效率。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发一种受自然叶子启发的、用于射频能量收集的新型叶状天线，并通过阻抗匹配优化其在915兆赫频率下的性能。

Method: 该研究利用AutoCAD和HFSS软件对天线进行建模，并制造了印刷电路板（PCB）原型，以优化天线的性能。

Result: 天线在915兆赫频率下的S11参数接近-20分贝，表明其能够有效地收集射频能量，并能在最远200厘米的距离为设备供电。

Conclusion: 该生物启发式设计的提出的天线提高了射频能量的传输效率。

Abstract: In this research, the design and optimization of a novel leaf-shaped antenna
inspired by natural leaf structures for radio frequency energy transfer is
presented. The objectives of this study are to develop a bio-inspired antenna,
optimize its performance through impedance matching for the 915 MHz frequency
band, and evaluate its efficiency in capturing RF energy. The design process
involves selecting an appropriate leaf shape, modeling the antenna using
AutoCAD and HFSS software, and fabricating a printed circuit board (PCB)
prototype. Simulations and physical tests are conducted to optimize the
antennas performance, achieving an S11 parameter of nearly -20 dB at 915 MHz,
indicating effective energy capture. Experimental results demonstrate the
antennas ability to power a device at distances up to 200 cm, with charging
times reflecting its efficiency. The study concludes that the bio-inspired
design of the proposed antenna improves RF energy transfer. Future work should
focus on testing the antennas penetration through concrete and developing a
feedback system for autonomous alignment.

</details>
