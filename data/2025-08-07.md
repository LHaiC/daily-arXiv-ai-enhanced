<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 133]
- [cs.CL](#cs.CL) [Total: 67]
- [cs.RO](#cs.RO) [Total: 16]
- [quant-ph](#quant-ph) [Total: 40]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.NE](#cs.NE) [Total: 4]
- [physics.app-ph](#physics.app-ph) [Total: 4]
- [eess.SP](#eess.SP) [Total: 19]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 11]
- [cs.DS](#cs.DS) [Total: 5]
- [cs.GR](#cs.GR) [Total: 4]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LG](#cs.LG) [Total: 78]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 15]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.SI](#cs.SI) [Total: 8]
- [cs.AR](#cs.AR) [Total: 6]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.MA](#cs.MA) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 该研究提出了一种利用大型语言模型（LLM）自动化VR培训内容生成的方法，通过提取文本信息并将其转化为动画和视觉提示，以提高培训效果并降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 尽管虚拟现实（VR）在劳动力培训方面具有优势，但开发VR培训应用面临创建准确且引人入胜的教学内容的挑战，这需要耗费大量时间、专业知识和资源。

Method: 提出一种利用大型语言模型（LLM）自动从文本生成虚拟指令的新方法。该系统包括一个LLM模块，用于从文本中提取任务相关信息，以及一个智能模块，用于将这些信息转化为VR环境中的动画演示和视觉提示。智能模块接收LLM模块的输入并解释提取的信息，然后指令生成器利用数据库中的相关数据创建培训内容，通过改变虚拟对象的颜色和创建动画来演示任务。

Result: 该方法能够生成虚拟指令，通过改变虚拟对象的颜色和创建动画来演示任务，从而提高培训效果并降低开发成本，使VR培训更具可扩展性和适应性。

Conclusion: 该方法通过利用大型语言模型（LLM）自动从文本生成虚拟指令，增强了培训效果，降低了开发成本，使得基于VR的培训更具可扩展性并能适应不断变化的工业需求。

Abstract: Virtual Reality (VR) has emerged as a powerful tool for workforce training,
offering immersive, interactive, and risk-free environments that enhance skill
acquisition, decision-making, and confidence. Despite its advantages,
developing VR applications for training remains a significant challenge due to
the time, expertise, and resources required to create accurate and engaging
instructional content. To address these limitations, this paper proposes a
novel approach that leverages Large Language Models (LLMs) to automate the
generation of virtual instructions from textual input. The system comprises two
core components: an LLM module that extracts task-relevant information from the
text, and an intelligent module that transforms this information into animated
demonstrations and visual cues within a VR environment. The intelligent module
receives input from the LLM module and interprets the extracted information.
Based on this, an instruction generator creates training content using relevant
data from a database. The instruction generator generates the instruction by
changing the color of virtual objects and creating animations to illustrate
tasks. This approach enhances training effectiveness and reduces development
overhead, making VR-based training more scalable and adaptable to evolving
industrial needs.

</details>


### [2] [Outlier Detection Algorithm for Circle Fitting](https://arxiv.org/abs/2508.03720)
*Ahmet Gökhan Poyraz*

Main category: cs.CV

TL;DR: 提出了一种名为PCOD的极坐标异常值检测算法，用于提高圆拟合精度，特别是在工业零件测量中。实验证明该算法效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决在圆拟合应用中，点集包含噪声时算法效果不佳的问题，并提高圆拟合在工业环境中的精度和效率。

Method: 将点集转换为极坐标，计算局部和全局标准差，通过比较局部均值和全局标准差来识别异常值。

Result: PCOD算法在精度方面优于其他九种圆拟合算法和五种异常值检测方法，证明了其在工业环境中的应用潜力。

Conclusion: 提出了一种名为PCOD的在极坐标下基于异常值检测的算法，并成功应用于工业垫圈零件的高精度直径测量中，实验结果表明该算法在精度方面优于其他方法，具有工业应用潜力。

Abstract: Circle fitting methods are extensively utilized in various industries,
particularly in quality control processes and design applications. The
effectiveness of these algorithms can be significantly compromised when the
point sets to be predicted are noisy. To mitigate this issue, outlier detection
and removal algorithms are often applied before the circle fitting procedure.
This study introduces the Polar Coordinate-Based Outlier Detection (PCOD)
algorithm, which can be effectively employed in circle fitting applications. In
the proposed approach, the point set is first transformed into polar
coordinates, followed by the calculation of both local and global standard
deviations. Outliers are then identified by comparing local mean values with
the global standard deviation. The practicality and efficiency of the proposed
method are demonstrated by focusing on the high-precision diameter measurement
of industrial washer parts. Images from a machine vision system are processed
through preprocessing steps, including sub-pixel edge detection. The resulting
sub-pixel edge points are then cleaned using the proposed outlier detection and
removal algorithm, after which circle fitting is performed. A comparison is
made using ten different circle fitting algorithms and five distinct outlier
detection methods. The results indicate that the proposed method outperforms
the other approaches, delivering the best performance in terms of accuracy
within the dataset, thereby demonstrating its potential for enhancing circle
fitting applications in industrial environments.

</details>


### [3] [Enhancing Diameter Measurement Accuracy in Machine Vision Applications](https://arxiv.org/abs/2508.03721)
*Ahmet Gokhan Poyraz,Ahmet Emir Dirik,Hakan Gurkan,Mehmet Kacmaz*

Main category: cs.CV

TL;DR: 该研究提出两种新方法（基于转换因子和基于像素）来提高相机测量系统的精度，通过使用参考部件将误差从13-114微米降低到1-2微米。


<details>
  <summary>Details</summary>
Motivation: 即使使用了像远心镜头这样的专用设备，相机测量系统中的机械和软件因素仍可能导致测量误差，尤其是在使用相同设置测量不同直径部件的应用中。

Method: 提出两种新方法以提高测量精度：一种是基于转换因子的方法，从已知参考件估计转换因子来计算未知部件的直径（mm）；另一种是基于像素的方法，直接使用参考件的基于像素的直径信息来估计直径（mm）。

Result: 实验证明，该方法能将原始测量误差（13-114微米）降低到1-2微米，并能高精度测量相机视野内的所有部件。

Conclusion: 通过使用几个已知的参考部件，所提出的方法能够对相机视野内的所有部件进行高精度测量。该方法通过显著降低错误率和提高测量可靠性，进一步丰富了现有的直径测量文献。

Abstract: In camera measurement systems, specialized equipment such as telecentric
lenses is often employed to measure parts with narrow tolerances. However,
despite the use of such equipment, measurement errors can occur due to
mechanical and software-related factors within the system. These errors are
particularly evident in applications where parts of different diameters are
measured using the same setup. This study proposes two innovative approaches to
enhance measurement accuracy using multiple known reference parts: a conversion
factor-based method and a pixel-based method. In the first approach, the
conversion factor is estimated from known references to calculate the diameter
(mm) of the unknown part. In the second approach, the diameter (mm) is directly
estimated using pixel-based diameter information from the references. The
experimental setup includes an industrial-grade camera and telecentric lenses.
Tests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show
that measurement errors, which originally ranged from 13-114 micrometers, were
reduced to 1-2 micrometers using the proposed methods. By utilizing only a few
known reference parts, the proposed approach enables high-accuracy measurement
of all parts within the camera's field of view. Additionally, this method
enhances the existing diameter measurement literature by significantly reducing
error rates and improving measurement reliability.

</details>


### [4] [A machine learning approach for image classification in synthetic aperture RADAR](https://arxiv.org/abs/2508.04234)
*Romina Gaburro,Patrick Healy,Shraddha Naidu,Clifford Nolan*

Main category: cs.CV

TL;DR: CNNs achieve high accuracy in classifying objects and ice types using SAR data, with the study also examining the impact of antenna height.


<details>
  <summary>Details</summary>
Motivation: The paper aims to identify and classify ground objects using SAR data with CNNs, and to investigate the effect of SAR data acquisition at different antenna heights on classification success.

Method: The study uses Convolutional Neural Networks (CNNs) and a single scattering approximation to classify object shapes using simulated and reconstructed SAR data, and to identify ice types in real SAR imagery from Sentinel-1.

Result: High classification accuracy (>=75%) was achieved in both experiments (object shape classification and ice type identification), demonstrating the effectiveness of CNNs.

Conclusion: CNNs are effective for both geometric and environmental classification tasks using SAR data, and the study explores the impact of data acquisition height on classification success.

Abstract: We consider the problem in Synthetic Aperture RADAR (SAR) of identifying and
classifying objects located on the ground by means of Convolutional Neural
Networks (CNNs). Specifically, we adopt a single scattering approximation to
classify the shape of the object using both simulated SAR data and
reconstructed images from this data, and we compare the success of these
approaches. We then identify ice types in real SAR imagery from the satellite
Sentinel-1. In both experiments we achieve a promising high classification
accuracy ($\geq$75\%). Our results demonstrate the effectiveness of CNNs in
using SAR data for both geometric and environmental classification tasks. Our
investigation also explores the effect of SAR data acquisition at different
antenna heights on our ability to classify objects successfully.

</details>


### [5] [Learning Using Privileged Information for Litter Detection](https://arxiv.org/abs/2508.04124)
*Matthias Bartolo,Konstantinos Makantasis,Dylan Seychell*

Main category: cs.CV

TL;DR: 提出一种结合特权信息和深度学习对象检测的新方法，用于改进垃圾检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 为了应对全球日益增长的垃圾污染问题，开发能够有效检测垃圾的自动化工具。

Method: 将特权信息与深度学习对象检测相结合，并通过将边界框信息编码为二值掩码来改进检测。

Result: 在所有评估的模型上均观察到持续的性能提升，不仅提高了训练集内的检测准确性，而且泛化到其他垃圾检测场景的能力也很好，同时没有增加模型复杂性。

Conclusion: 该方法为垃圾检测提供了一个实用的解决方案，平衡了准确性和效率，并在 SODA、BDW 和 UAVVaste 数据集上均取得了性能提升，同时保持了模型的计算效率和可扩展性。

Abstract: As litter pollution continues to rise globally, developing automated tools
capable of detecting litter effectively remains a significant challenge. This
study presents a novel approach that combines, for the first time, privileged
information with deep learning object detection to improve litter detection
while maintaining model efficiency. We evaluate our method across five widely
used object detection models, addressing challenges such as detecting small
litter and objects partially obscured by grass or stones. In addition to this,
a key contribution of our work can also be attributed to formulating a means of
encoding bounding box information as a binary mask, which can be fed to the
detection model to refine detection guidance. Through experiments on both
within-dataset evaluation on the renowned SODA dataset and cross-dataset
evaluation on the BDW and UAVVaste litter detection datasets, we demonstrate
consistent performance improvements across all models. Our approach not only
bolsters detection accuracy within the training sets but also generalises well
to other litter detection contexts. Crucially, these improvements are achieved
without increasing model complexity or adding extra layers, ensuring
computational efficiency and scalability. Our results suggest that this
methodology offers a practical solution for litter detection, balancing
accuracy and efficiency in real-world applications.

</details>


### [6] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: This paper enhances multimodal emotion recognition by using Gemini to add MLLM reasoning as priors and employing a new loss function (Balanced Dual-Contrastive Learning) to fix class imbalance, achieving better results on the MER2024 benchmark.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve multimodal emotion recognition by leveraging trustworthy prior reasoning knowledge from Multimodal Large Language Models (MLLMs) and to address the challenge of class imbalance inherent in such tasks.

Method: The study integrates prior reasoning knowledge from MLLMs into multimodal emotion recognition. Gemini is used to generate fine-grained, modality-separable reasoning traces, which are then injected as priors during the fusion stage to enhance cross-modal interactions. To handle class imbalance, a Balanced Dual-Contrastive Learning loss formulation is introduced to balance inter-class and intra-class distributions.

Result: The prior-enhanced framework yielded substantial performance gains on the MER2024 benchmark, demonstrating the effectiveness of combining MLLM-derived reasoning with lightweight fusion networks for robust and scalable emotion recognition.

Conclusion: The study demonstrates that integrating trustworthy prior reasoning knowledge from MLLMs into multimodal emotion recognition, using Gemini for generating reasoning traces and employing Balanced Dual-Contrastive Learning to address class imbalance, leads to substantial performance gains on the MER2024 benchmark. This approach effectively combines the reliability of MLLM reasoning with the domain adaptability of lightweight fusion networks for robust and scalable emotion recognition.

Abstract: This study investigates the integration of trustworthy prior reasoning
knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to
generate fine-grained, modality-separable reasoning traces, which are injected
as priors during the fusion stage to enrich cross-modal interactions. To
mitigate the pronounced class-imbalance in multimodal emotion recognition, we
introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly
balances inter-class and intra-class distributions. Applied to the MER2024
benchmark, our prior-enhanced framework yields substantial performance gains,
demonstrating that the reliability of MLLM-derived reasoning can be
synergistically combined with the domain adaptability of lightweight fusion
networks for robust, scalable emotion recognition.

</details>


### [7] [From Waveforms to Pixels: A Survey on Audio-Visual Segmentation](https://arxiv.org/abs/2508.03724)
*Jia Li,Yapeng Tian*

Main category: cs.CV

TL;DR: 该调查全面概述了音频-视觉分割（AVS）领域，涵盖了其定义、数据集、评估指标和方法演变。文章分析了各种编码、融合和解码策略，以及不同的训练范式，并对现有方法进行了比较。最后，指出了AVS面临的挑战，并提出了未来的研究方向，例如改进时间推理、多模态融合和利用基础模型。


<details>
  <summary>Details</summary>
Motivation: AVS（音频-视觉分割）旨在通过同时利用视觉和音频模态来识别和分割视频中发声的物体，已成为多模态感知中的一个重要研究领域，能够实现细粒度的物体级理解。

Method: 该调查全面概述了AVS的定义、数据集、评估指标和方法演变，分析了单模态和多模态编码、视听融合策略以及解码器设计。文章还探讨了从全监督到无监督的各种训练范式，并对标准基准进行了广泛的AVS方法比较。

Result: 文章对AVS方法在标准基准上的表现进行了广泛比较，强调了不同的架构选择、融合策略和训练范式对性能的影响。

Conclusion: AVS是一个重要的多模态感知研究领域，在视频中识别和分割发声物体，能够实现细粒度的物体级别理解。该调查全面概述了AVS的定义、数据集、评估指标和方法演变，分析了单模态和多模态编码、视听融合策略以及解码器设计。文章还探讨了从全监督到无监督的各种训练范式，并对标准基准进行了广泛的AVS方法比较。最后，文章指出了AVS面临的挑战，如时间建模不足、对视觉的模态偏见、复杂环境鲁棒性差和计算需求高等，并提出了改进时间推理、多模态融合、利用基础模型进行泛化和少样本学习、通过自监督和弱监督学习减少对标签的依赖以及结合更高级别的推理以实现更智能的AVS系统等未来方向。

Abstract: Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing
objects in videos by leveraging both visual and audio modalities. It has
emerged as a significant research area in multimodal perception, enabling
fine-grained object-level understanding. In this survey, we present a
comprehensive overview of the AVS field, covering its problem formulation,
benchmark datasets, evaluation metrics, and the progression of methodologies.
We analyze a wide range of approaches, including architectures for unimodal and
multimodal encoding, key strategies for audio-visual fusion, and various
decoder designs. Furthermore, we examine major training paradigms, from fully
supervised learning to weakly supervised and training-free methods. Notably, we
provide an extensive comparison of AVS methods across standard benchmarks,
highlighting the impact of different architectural choices, fusion strategies,
and training paradigms on performance. Finally, we outline the current
challenges, such as limited temporal modeling, modality bias toward vision,
lack of robustness in complex environments, and high computational demands, and
propose promising future directions, including improving temporal reasoning and
multimodal fusion, leveraging foundation models for better generalization and
few-shot learning, reducing reliance on labeled data through selfand weakly
supervised learning, and incorporating higher-level reasoning for more
intelligent AVS systems.

</details>


### [8] [A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding](https://arxiv.org/abs/2508.03725)
*Yida Wang,Taiting Lu,Runze Liu,Lanqing Yang,Yifan Yang,Zhe Chen,Yuehai Wang,Yixin Liu,Kaiyuan Lin,Xiaomeng Chen,Dian Ding,Yijie Li,Yi-Chao Chen,Yincheng Jin,Mahanth Gowda*

Main category: cs.CV

TL;DR: 提出LLM4-IC8K框架，利用LLMs解决IC封装几何标注问题，通过两阶段训练和ICGeo8K数据集，显著提升了几何感知和标注的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的IC封装几何标注方法在处理非结构化的封装图和抽象的图注释时面临巨大挑战，并且缺乏直接从IC机械图进行自动化封装几何标注的方法。现有的LMMs在几何感知方面存在严重不足，无法有效解决封装几何标注问题。

Method: 提出了一种名为LLM4-IC8K的新框架，该框架将IC机械图视为图像，并利用LLMs进行结构化几何解释。该框架通过两个阶段进行训练：首先在合成生成的IC封装图上训练LMMs以学习基础几何推理，然后在真实世界的数据集上进行微调以提高鲁棒性和准确性。具体而言，LLM4-IC8K解决了识别引脚数量、计算每个引脚的中心坐标以及估计单个引脚尺寸这三个子任务。

Result: LLM4-IC8K框架在IC封装几何标注任务上取得了优于现有LMMs的性能，并在ICGeo8K数据集上进行了广泛的实验验证。ICGeo8K数据集包含8,608个标注样本，包括4138个手工制作的IC封装样本和4470个合成样本。

Conclusion: LLM4-IC8K框架在IC封装几何标注任务上显著优于现有的LMMs，能够准确地识别引脚数量、计算引脚中心坐标并估计引脚尺寸。

Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated
circuits (IC) is essential in defining the physical interface between
components and the PCB layout, requiring exceptional visual perception
proficiency. However, due to the unstructured footprint drawing and abstract
diagram annotations, automated parsing and accurate footprint geometry modeling
remain highly challenging. Despite its importance, no methods currently exist
for automated package geometry labeling directly from IC mechanical drawings.
In this paper, we first investigate the visual perception performance of Large
Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our
findings reveal that current LMMs severely suffer from inaccurate geometric
perception, which hinders their performance in solving the footprint geometry
labeling problem. To address these limitations, we propose LLM4-IC8K, a novel
framework that treats IC mechanical drawings as images and leverages LLMs for
structured geometric interpretation. To mimic the step-by-step reasoning
approach used by human engineers, LLM4-IC8K addresses three sub-tasks:
perceiving the number of pins, computing the center coordinates of each pin,
and estimating the dimensions of individual pins. We present a two-stage
framework that first trains LMMs on synthetically generated IC footprint
diagrams to learn fundamental geometric reasoning and then fine-tunes them on
real-world datasheet drawings to enhance robustness and accuracy in practical
scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with
8,608 labeled samples, including 4138 hand-crafted IC footprint samples and
4470 synthetically generated samples. Extensive experiments demonstrate that
our model outperforms state-of-the-art LMMs on the proposed benchmark.

</details>


### [9] [TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization](https://arxiv.org/abs/2508.03727)
*Tai Hyoung Rhee,Dong-guw Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出了一种创新的扩散模型去噪框架，用于处理红外热图像中的噪声，通过结合潜在空间和离散小波变换进行优化，并在实验中取得了优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决红外热图像（TIR）中存在的严重非均匀固定模式噪声问题，这种噪声会影响物体检测、定位和建图等感知任务。

Method: 提出了一种基于扩散的红外热图像去噪框架，利用潜在空间表示和高频细节，并通过结合潜在空间损失和离散小波变换（DWT）/双树复小波变换（DTCWT）损失的新型损失函数来微调预训练的稳定扩散模型。此外，还实现了一个级联细化阶段以增强精细细节。

Result: 实验证明，该方法在去噪任务上取得了优于现有技术（SOTA）的性能，并且在各种具有挑战性的真实世界红外热图像数据集上表现出鲁棒的零样本泛化能力。

Conclusion: 该方法在基准数据集上表现优于最先进的去噪方法，并能有效地泛化到各种具有挑战性的真实世界红外热图像数据集，证明了其在实际机器人应用中的有效性。

Abstract: Thermal infrared imaging exhibits considerable potentials for robotic
perception tasks, especially in environments with poor visibility or
challenging lighting conditions. However, TIR images typically suffer from
heavy non-uniform fixed-pattern noise, complicating tasks such as object
detection, localization, and mapping. To address this, we propose a
diffusion-based TIR image denoising framework leveraging latent-space
representations and wavelet-domain optimization. Utilizing a pretrained stable
diffusion model, our method fine-tunes the model via a novel loss function
combining latent-space and discrete wavelet transform (DWT) / dual-tree complex
wavelet transform (DTCWT) losses. Additionally, we implement a cascaded
refinement stage to enhance fine details, ensuring high-fidelity denoising
results. Experiments on benchmark datasets demonstrate superior performance of
our approach compared to state-of-the-art denoising methods. Furthermore, our
method exhibits robust zero-shot generalization to diverse and challenging
real-world TIR datasets, underscoring its effectiveness for practical robotic
deployment.

</details>


### [10] [What is Beneath Misogyny: Misogynous Memes Classification and Explanation](https://arxiv.org/abs/2508.03732)
*Kushal Kanwar,Dushyant Singh Chauhan,Gopendra Vikram Singh,Asif Ekbal*

Main category: cs.CV

TL;DR: 提出了一种名为MM-Misogyny的新型多模态方法，用于检测、分类和解释模因中的厌女内容。该模型通过单独处理文本和图像模态，然后利用交叉注意力机制将它们融合，从而实现对厌女内容的细粒度理解。该方法在WBMS数据集上的评估结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于其多模态性质（图像和文本）以及在不同社会背景下细微的表现形式，检测和理解模因为何具有厌女倾向是一项研究挑战。

Method: 提出了一种新颖的多模态方法MM-Misogyny，该方法分别处理文本和图像模态，并通过交叉注意力机制将它们统一到多模态上下文中。然后，通过分类器和大型语言模型(LLM)轻松处理生成的多模态上下文，以进行标记、分类和解释。

Result: 在Newly curated dataset (WBMS)上对所提出的模型进行了评估，该数据集是通过从网络空间收集厌女模因并将其分为四个类别（即厨房、领导力、工作和购物）而创建的。

Conclusion: 该模型不仅能检测和分类厌女症，还能深入了解厌女症在生活领域中的运作方式。结果表明，与现有方法相比，该方法具有优越性。

Abstract: Memes are popular in the modern world and are distributed primarily for
entertainment. However, harmful ideologies such as misogyny can be propagated
through innocent-looking memes. The detection and understanding of why a meme
is misogynous is a research challenge due to its multimodal nature (image and
text) and its nuanced manifestations across different societal contexts. We
introduce a novel multimodal approach, \textit{namely},
\textit{\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic
content in memes. \textit{\textbf{MM-Misogyny}} processes text and image
modalities separately and unifies them into a multimodal context through a
cross-attention mechanism. The resulting multimodal context is then easily
processed for labeling, categorization, and explanation via a classifier and
Large Language Model (LLM). The evaluation of the proposed model is performed
on a newly curated dataset (\textit{\textbf{W}hat's \textbf{B}eneath
\textbf{M}isogynous \textbf{S}tereotyping (WBMS)}) created by collecting
misogynous memes from cyberspace and categorizing them into four categories,
\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only
detects and classifies misogyny, but also provides a granular understanding of
how misogyny operates in domains of life. The results demonstrate the
superiority of our approach compared to existing methods. The code and dataset
are available at
\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.

</details>


### [11] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 提出了一种无需训练的、有效的、一致的主题生成方法，该方法通过掩码跨图像注意力共享和区域特征协调来解决文本到图像生成中的主体一致性问题，从而在不影响模型原有能力的情况下，提高主体在图像序列中的一致性。


<details>
  <summary>Details</summary>
Motivation: 在生成连贯的、讲述视觉故事的图像序列时，在所有故事场景中保持主体一致性是一个关键挑战。现有的方法通常依赖于对模型进行微调或重新训练，这不仅计算成本高、耗时，而且常常会干扰模型预先已有的能力。

Method: 采用无训练方法，通过引入掩码跨图像注意力共享动态对齐一批图像中的主题特征，并使用区域特征协调来优化视觉上相似的细节，以提高主题一致性。

Result: 实验结果表明，该方法在各种场景下成功地生成了视觉上一致的主体，同时保持了扩散模型的创意能力。

Conclusion: 该方法成功地在各种场景中生成了视觉上一致的主体，同时保持了扩散模型的创意能力。

Abstract: Generating a coherent sequence of images that tells a visual story, using
text-to-image diffusion models, often faces the critical challenge of
maintaining subject consistency across all story scenes. Existing approaches,
which typically rely on fine-tuning or retraining models, are computationally
expensive, time-consuming, and often interfere with the model's pre-existing
capabilities. In this paper, we follow a training-free approach and propose an
efficient consistent-subject-generation method. This approach works seamlessly
with pre-trained diffusion models by introducing masked cross-image attention
sharing to dynamically align subject features across a batch of images, and
Regional Feature Harmonization to refine visually similar details for improved
subject consistency. Experimental results demonstrate that our approach
successfully generates visually consistent subjects across a variety of
scenarios while maintaining the creative abilities of the diffusion model.

</details>


### [12] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 通过结合DINOv2和射频数据，提出了一种新的深度学习方法，显著提高了建筑物绘图的精度，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统智能城市绘图技术（如卫星图像、激光雷达扫描和手动注释）在成本、可及性和准确性方面存在局限性。开源绘图平台虽然被广泛用于人工智能应用，但其中存在的</strong>人</strong>为错误和现实环境的不断变化会导致偏差，从而影响基于这些数据训练的神经网络性能。因此，需要一种新的方法来提高绘图的准确性。

Method: 本研究提出了一种基于深度学习的方法，该方法集成了DINOv2架构，并通过结合开源地图和从无线用户设备及基站收集的射频（RF）数据来改进建筑物绘图。该方法利用视觉Transformer架构，在一个统一的框架内联合处理RF和地图模态，以捕捉空间依赖性和结构先验，从而提高绘图精度。

Result: 在评估中，本研究采用了一个由华为公司联合制作的合成数据集。研究人员开发并训练了一个仅利用聚合路径损耗信息来解决绘图问题的模型。根据三个性能指标（Jaccard指数（IoU）、Hausdorff距离和Chamfer距离）对结果进行了衡量。研究设计的方法取得了65.3%的宏观IoU，显著超过了错误地图基线（40.1%）、文献中的仅RF方法（37.3%）以及研究设计的非AI融合基线（42.2%）。

Conclusion: 本研究提出了一种结合DINOv2和射频（RF）数据来改进建筑物绘图的深度学习方法，并在合成数据集上进行了评估。实验结果表明，该方法在宏观IoU方面达到了65.3%，显著优于现有基线方法。

Abstract: Environment mapping is an important computing task for a wide range of smart
city applications, including autonomous navigation, wireless network operations
and extended reality environments. Conventional smart city mapping techniques,
such as satellite imagery, LiDAR scans, and manual annotations, often suffer
from limitations related to cost, accessibility and accuracy. Open-source
mapping platforms have been widely utilized in artificial intelligence
applications for environment mapping, serving as a source of ground truth.
However, human errors and the evolving nature of real-world environments
introduce biases that can negatively impact the performance of neural networks
trained on such data. In this paper, we present a deep learning-based approach
that integrates the DINOv2 architecture to improve building mapping by
combining maps from open-source platforms with radio frequency (RF) data
collected from multiple wireless user equipments and base stations. Our
approach leverages a vision transformer-based architecture to jointly process
both RF and map modalities within a unified framework, effectively capturing
spatial dependencies and structural priors for enhanced mapping accuracy. For
the evaluation purposes, we employ a synthetic dataset co-produced by Huawei.
We develop and train a model that leverages only aggregated path loss
information to tackle the mapping problem. We measure the results according to
three performance metrics which capture different qualities: (i) The Jaccard
index, also known as intersection over union (IoU), (ii) the Hausdorff
distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of
65.3%, significantly surpassing (i) the erroneous maps baseline, which yields
40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and
(iii) a non-AI fusion baseline that we designed which yields 42.2%.

</details>


### [13] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: VQ-DeepISC, a digital semantic communication system, uses Swin Transformer and VQ for feature extraction and discrete transmission, with attention-based channel adaptation. It employs KLD regularization and EMA to prevent codebook collapse. Achieves better reconstruction fidelity than existing methods.


<details>
  <summary>Details</summary>
Motivation: The fundamental difficulty in digitizing semantic features lies in preserving continuity and context from analog representations during compression into discrete symbols while maintaining robustness to channel degradation. This paper aims to overcome this challenge by proposing a novel system.

Method: The paper proposes a vector quantized (VQ)-enabled digital semantic communication system (VQ-DeepISC) that utilizes a Swin Transformer backbone for hierarchical semantic feature extraction, VQ modules for projecting features into discrete latent spaces, and an attention mechanism-driven channel adaptation module for optimizing index transmission. To address codebook collapse, distributional regularization via minimizing Kullback-Leibler divergence (KLD) and exponential moving average (EMA) are employed. Digital communication is implemented using QPSK modulation and OFDM adhering to the IEEE 802.11a standard.

Result: Experimental results show that the proposed VQ-DeepISC system achieves superior reconstruction fidelity over benchmark methods.

Conclusion: The proposed VQ-DeepISC system demonstrates superior reconstruction fidelity compared to benchmark methods, showcasing the effectiveness of vector quantization and channel adaptation in digital semantic communication.

Abstract: Discretization of semantic features enables interoperability between semantic
and digital communication systems, showing significant potential for practical
applications. The fundamental difficulty in digitizing semantic features stems
from the need to preserve continuity and context in inherently analog
representations during their compression into discrete symbols while ensuring
robustness to channel degradation. In this paper, we propose a vector quantized
(VQ)-enabled digital semantic communication system with channel adaptive image
transmission, named VQ-DeepISC. Guided by deep joint source-channel coding
(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic
feature extraction, followed by VQ modules projecting features into discrete
latent spaces. Consequently, it enables efficient index-based transmission
instead of raw feature transmission. To further optimize this process, we
develop an attention mechanism-driven channel adaptation module to dynamically
optimize index transmission. Secondly, to counteract codebook collapse during
training process, we impose a distributional regularization by minimizing the
Kullback-Leibler divergence (KLD) between codeword usage frequencies and a
uniform prior. Meanwhile, exponential moving average (EMA) is employed to
stabilize training and ensure balanced feature coverage during codebook
updates. Finally, digital communication is implemented using quadrature phase
shift keying (QPSK) modulation alongside orthogonal frequency division
multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental
results demonstrate superior reconstruction fidelity of the proposed system
over benchmark methods.

</details>


### [14] [Tobler's First Law in GeoAI: A Spatially Explicit Deep Learning Model for Terrain Feature Detection Under Weak Supervision](https://arxiv.org/abs/2508.03745)
*Wenwen Li,Chia-Yu Hsu,Maosheng Hu*

Main category: cs.CV

TL;DR: 本研究提出了一种新的弱监督深度学习方法，利用地理空间显式模型和注意力机制来检测物体，解决了GeoAI中的数据和空间问题，并成功应用于火星撞击坑的检测。


<details>
  <summary>Details</summary>
Motivation: 解决GeoAI领域中AI模型设计缺乏训练数据和忽视空间原则与空间效应的核心挑战，旨在促进AI与地理空间研究的深度融合。

Method: 提出了一种基于 Tobler 地理第一定律的空间显式模型，用于仅使用弱标签进行物体检测。该方法结合了注意力图和多阶段训练策略来提高检测性能。

Result: 开发出的弱监督深度学习模型能够有效检测地表物体（包括自然和人造特征），并成功应用于火星撞击坑的检测，证明了其泛化能力和在减少手动工作量方面的潜力。

Conclusion: 该研究通过开发一种基于地理空间显式模型和多阶段训练策略的弱监督深度学习模型，解决了地理空间人工智能（GeoAI）领域中训练数据不足和空间效应被忽视的挑战，并成功应用于火星撞击坑的检测，展示了其在识别地表特征方面的通用性，推动了GeoAI的理论和方法发展。

Abstract: Recent interest in geospatial artificial intelligence (GeoAI) has fostered a
wide range of applications using artificial intelligence (AI), especially deep
learning, for geospatial problem solving. However, major challenges such as a
lack of training data and the neglect of spatial principles and spatial effects
in AI model design remain, significantly hindering the in-depth integration of
AI with geospatial research. This paper reports our work in developing a deep
learning model that enables object detection, particularly of natural features,
in a weakly supervised manner. Our work makes three contributions: First, we
present a method of object detection using only weak labels. This is achieved
by developing a spatially explicit model based on Tobler's first law of
geography. Second, we incorporate attention maps into the object detection
pipeline and develop a multistage training strategy to improve performance.
Third, we apply this model to detect impact craters on Mars, a task that
previously required extensive manual effort. The model generalizes to both
natural and human-made features on the surfaces of Earth and other planets.
This research advances the theoretical and methodological foundations of GeoAI.

</details>


### [15] [Closed-Circuit Television Data as an Emergent Data Source for Urban Rail Platform Crowding Estimation](https://arxiv.org/abs/2508.03749)
*Riccardo Fiorista,Awad Abdelhalim,Anson F. Stewart,Gabriel L. Pincus,Ian Thistle,Jinhua Zhao*

Main category: cs.CV

TL;DR: 本研究探索了使用CCTV监控录像来准确估算城市轨道交通站台客流量的潜力。研究人员测试了三种先进的计算机视觉技术（对象检测、人群分类和语义分割），并提出了一种新的基于线性优化的方法来提高计数准确性。结果表明，仅使用CCTV数据就可以提供准确的实时客流量信息，有助于改善交通运营和乘客体验。


<details>
  <summary>Details</summary>
Motivation: 准确估计城市轨道交通站台客流量可以提高公交机构做出明智运营决策的能力，从而提高安全性、运营效率和客户体验，尤其是在拥挤的情况下。然而，感知实时客流量仍然具有挑战性，并且通常依赖于自动票务收集数据或工作人员观察等间接指标。

Method: 研究比较了三种用于从平台CCTV图像中提取与人群相关特征的先进计算机视觉方法：(a)使用YOLOv11、RT-DETRv2和APGCC进行对象检测和计数；(b)通过定制训练的视觉Transformer（Crowd-ViT）进行人群级别分类；(c)使用DeepLabV3进行语义分割。此外，研究还提出了一种新颖、高效的基于线性优化的方法，用于从生成的分割图中提取计数，同时考虑了图像对象的深度，从而考虑了沿平台的乘客分散情况。

Result: 研究结果表明，计算机视觉方法可以为人群估计提供实质性价值。在与华盛顿都会区交通局（WMATA）合作创建的、包含600多小时视频材料的隐私保护数据集上进行了测试。

Conclusion: CCTV图像数据，独立于公交机构可用的其他数据源，能够实现更精确的实时拥挤度估计，并最终实现及时的运营响应以缓解平台拥挤。

Abstract: Accurately estimating urban rail platform occupancy can enhance transit
agencies' ability to make informed operational decisions, thereby improving
safety, operational efficiency, and customer experience, particularly in the
context of crowding. However, sensing real-time crowding remains challenging
and often depends on indirect proxies such as automatic fare collection data or
staff observations. Recently, Closed-Circuit Television (CCTV) footage has
emerged as a promising data source with the potential to yield accurate,
real-time occupancy estimates. The presented study investigates this potential
by comparing three state-of-the-art computer vision approaches for extracting
crowd-related features from platform CCTV imagery: (a) object detection and
counting using YOLOv11, RT-DETRv2, and APGCC; (b) crowd-level classification
via a custom-trained Vision Transformer, Crowd-ViT; and (c) semantic
segmentation using DeepLabV3. Additionally, we present a novel, highly
efficient linear-optimization-based approach to extract counts from the
generated segmentation maps while accounting for image object depth and, thus,
for passenger dispersion along a platform. Tested on a privacy-preserving
dataset created in collaboration with the Washington Metropolitan Area Transit
Authority (WMATA) that encompasses more than 600 hours of video material, our
results demonstrate that computer vision approaches can provide substantive
value for crowd estimation. This work demonstrates that CCTV image data,
independent of other data sources available to a transit agency, can enable
more precise real-time crowding estimation and, eventually, timely operational
responses for platform crowding mitigation.

</details>


### [16] [Modular Transformer Architecture for Precision Agriculture Imaging](https://arxiv.org/abs/2508.03751)
*Brian Gopalan,Nathalia Nascimento,Vishal Monga*

Main category: cs.CV

TL;DR: 该论文提出了一种创新的质量感知深度学习框架，通过智能处理不同质量的无人机图像（如模糊和噪声），在杂草分割任务上取得了比传统方法更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 本篇论文解决了精确农业中从无人机视频中进行高效、准确的杂草分割的关键需求。

Method: 提出了一种质量感知模块化深度学习框架，通过分析诸如模糊和噪声等质量条件，并将输入路由到针对每种退化类型进行了优化的专用预处理和Transformer模型来解决常见的图像退化问题。该系统首先使用平均绝对偏差和拉普拉斯算子分析无人机图像中的噪声和模糊。然后，数据被动态地路由到一个三个视觉Transformer模型中的一个：一个用于清晰图像的基线模型，一个用于降噪的带有Fisher Vector编码的修改Transformer模型，或另一个用于模糊校正的带有解卷的Lucy-Robinson解码器的模型。

Result: 该系统通过动态路由策略，在分割质量和计算效率方面均优于现有的基于CNN的方法。

Conclusion: 该系统在分割质量和计算效率方面均优于现有的基于CNN的方法，展示了深度学习在农业领域应用的重大进展。

Abstract: This paper addresses the critical need for efficient and accurate weed
segmentation from drone video in precision agriculture. A quality-aware modular
deep-learning framework is proposed that addresses common image degradation by
analyzing quality conditions-such as blur and noise-and routing inputs through
specialized pre-processing and transformer models optimized for each
degradation type. The system first analyzes drone images for noise and blur
using Mean Absolute Deviation and the Laplacian. Data is then dynamically
routed to one of three vision transformer models: a baseline for clean images,
a modified transformer with Fisher Vector encoding for noise reduction, or
another with an unrolled Lucy-Robinson decoder to correct blur. This novel
routing strategy allows the system to outperform existing CNN-based methods in
both segmentation quality and computational efficiency, demonstrating a
significant advancement in deep-learning applications for agriculture.

</details>


### [17] [Generating Synthetic Invoices via Layout-Preserving Content Replacement](https://arxiv.org/abs/2508.03754)
*Bevin V,Ananthakrishnan P V,Ragesh KR,Sanjay M,Vineeth S,Bibin Wilson*

Main category: cs.CV

TL;DR: 为了解决私有数据和高昂标注成本的问题，我们提出了一种新颖的流程，可以生成高保真的合成发票文档及其对应的结构化数据。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在自动化发票处理中的性能在很大程度上取决于大规模、多样化的数据集。然而，此类数据集的获取受到隐私法规和手动标注成本的限制。

Method: 首先利用光学字符识别（OCR）从源发票中提取文本内容和精确的空间布局。然后，使用大型语言模型（LLM）生成的上下文相关的、逼真的合成内容替换选定的数据字段。最后，采用图像修复技术擦除图像中的原始文本，并将其替换为新的合成文本，同时保持精确的布局和字体特征。

Result: 生成高保真度的合成发票文档及其对应的结构化数据。

Conclusion: 该方法为扩充小规模、私有数据集提供了一种可扩展且自动化的解决方案，从而能够创建大规模、多样化的语料库，用于训练更鲁棒、更准确的文档智能模型。

Abstract: The performance of machine learning models for automated invoice processing
is critically dependent on large-scale, diverse datasets. However, the
acquisition of such datasets is often constrained by privacy regulations and
the high cost of manual annotation. To address this, we present a novel
pipeline for generating high-fidelity, synthetic invoice documents and their
corresponding structured data. Our method first utilizes Optical Character
Recognition (OCR) to extract the text content and precise spatial layout from a
source invoice. Select data fields are then replaced with contextually
realistic, synthetic content generated by a large language model (LLM).
Finally, we employ an inpainting technique to erase the original text from the
image and render the new, synthetic text in its place, preserving the exact
layout and font characteristics. This process yields a pair of outputs: a
visually realistic new invoice image and a perfectly aligned structured data
file (JSON) reflecting the synthetic content. Our approach provides a scalable
and automated solution to amplify small, private datasets, enabling the
creation of large, varied corpora for training more robust and accurate
document intelligence models.

</details>


### [18] [Refine-IQA: Multi-Stage Reinforcement Finetuning for Perceptual Image Quality Assessment](https://arxiv.org/abs/2508.03763)
*Ziheng Jia,Jiaying Qian,Zicheng Zhang,Zijian Chen,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出Refine-IQA框架，通过多阶段强化微调（RFT）和新数据集/奖励函数，提升了图像质量评估（IQA）的感知和评分能力，并增强了模型的“思考”过程。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RFT的IQA方法在低层视觉域（如图像质量评估IQA）的应用中，通常只使用基于规则的输出来进行奖励监督，而没有对“思考”过程进行奖励监督，这可能限制了模型的性能上限。此外，这些方法通常直接在下游IQA任务上进行微调，而没有明确增强模型原生的低层视觉质量感知能力。

Method: 提出了一种名为Refine-IQA的多阶段强化微调（RFT）框架，包括两个阶段：阶段一构建了Refine-Perception-20K数据集并设计了多任务奖励函数来增强模型的视觉质量感知；阶段二针对质量评分任务，引入了涉及概率差异奖励的策略来监督“思考”过程。

Result: Refine-IQA框架及其模型在感知和评分任务上均表现出色，并且其“思考”（质量解释）能力在相应的质量解释基准上也取得了优异的结果。

Conclusion: Refine-IQA框架在感知和评分任务上都取得了出色的性能，并且其“思考”（质量解释）能力在相应的质量解释基准上也取得了优异的结果。

Abstract: Reinforcement fine-tuning (RFT) is a proliferating paradigm for LMM training.
Analogous to high-level reasoning tasks, RFT is similarly applicable to
low-level vision domains, including image quality assessment (IQA). Existing
RFT-based IQA methods typically use rule-based output rewards to verify the
model's rollouts but provide no reward supervision for the "think" process,
leaving its correctness and efficacy uncontrolled. Furthermore, these methods
typically fine-tune directly on downstream IQA tasks without explicitly
enhancing the model's native low-level visual quality perception, which may
constrain its performance upper bound. In response to these gaps, we propose
the multi-stage RFT IQA framework (Refine-IQA). In Stage-1, we build the
Refine-Perception-20K dataset (with 12 main distortions, 20,907
locally-distorted images, and over 55K RFT samples) and design multi-task
reward functions to strengthen the model's visual quality perception. In
Stage-2, targeting the quality scoring task, we introduce a probability
difference reward involved strategy for "think" process supervision. The
resulting Refine-IQA Series Models achieve outstanding performance on both
perception and scoring tasks-and, notably, our paradigm activates a robust
"think" (quality interpreting) capability that also attains exceptional results
on the corresponding quality interpreting benchmark.

</details>


### [19] [4D-PreNet: A Unified Preprocessing Framework for 4D-STEM Data Analysis](https://arxiv.org/abs/2508.03775)
*Mingyu Liu,Zian Mao,Zhu Liu,Haoran Zhang,Jintao Guo,Xiaoya He,Xi Huang,Shufen Chu,Chun Cheng,Jun Ding,Yujun Xie*

Main category: cs.CV

TL;DR: 4D-STEM数据预处理的瓶颈通过名为4D-PreNet的深度学习流程得到解决，该流程能同时进行去噪、中心校正和椭圆失真校准，提高了4D-STEM实时分析的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 高通量4D-STEM数据采集面临数据预处理的瓶颈，包括普遍存在的噪声、束中心漂移和椭圆畸变，这些都会系统性地影响定量测量。现有的校正算法通常是材料特定的，并且不能提供稳健、可泛化的解决方案。

Method: 提出了一种名为4D-PreNet的端到端深度学习流程，该流程集成了注意力增强的U-Net和ResNet架构，能够同时执行去噪、中心校正和椭圆失真校准。该网络使用包含多种噪声水平、漂移幅度和失真类型的模拟数据集进行训练，以实现对实验数据的有效泛化。

Result: 定量评估表明，该流程在去噪任务中可将均方误差最多降低50%，在中心检测任务中实现了亚像素级别的中心定位，平均误差低于0.04像素。与传统算法相比，在噪声抑制和衍射图样恢复方面均有所改进。

Conclusion: 该研究提出了4D-PreNet，一个端到端的深度学习流程，用于同时进行去噪、中心校正和椭圆失真校准，从而解决了4D-STEM数据预处理的关键瓶颈。通过在大量模拟数据集上进行训练，该网络能够有效地泛化到实验数据，在去噪方面将均方误差降低高达50%，在中心检测任务中实现了亚像素级别的定位（平均误差低于0.04像素）。与传统算法相比，4D-PreNet在噪声抑制和衍射图样恢复方面均有显著改善，为高通量、可靠的4D-STEM实时分析和自动化表征提供了支持。

Abstract: Automated experimentation with real time data analysis in scanning
transmission electron microscopy (STEM) often require end-to-end framework. The
four-dimensional scanning transmission electron microscopy (4D-STEM) with
high-throughput data acquisition has been constrained by the critical
bottleneck results from data preprocessing. Pervasive noise, beam center drift,
and elliptical distortions during high-throughput acquisition inevitably
corrupt diffraction patterns, systematically biasing quantitative measurements.
Yet, conventional correction algorithms are often material-specific and fail to
provide a robust, generalizable solution. In this work, we present 4D-PreNet,
an end-to-end deep-learning pipeline that integrates attention-enhanced U-Net
and ResNet architectures to simultaneously perform denoising, center
correction, and elliptical distortion calibration. The network is trained on
large, simulated datasets encompassing a wide range of noise levels, drift
magnitudes, and distortion types, enabling it to generalize effectively to
experimental data acquired under varying conditions. Quantitative evaluations
demonstrate that our pipeline reduces mean squared error by up to 50% during
denoising and achieves sub-pixel center localization in the center detection
task, with average errors below 0.04 pixels. The outputs are bench-marked
against traditional algorithms, highlighting improvements in both noise
suppression and restoration of diffraction patterns, thereby facilitating
high-throughput, reliable 4D-STEM real-time analysis for automated
characterization.

</details>


### [20] [RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization](https://arxiv.org/abs/2508.04335)
*Yanyan Li,Ze Yang,Keisuke Tateno,Federico Tombari Liang Zhao,Gim Hee Lee*

Main category: cs.CV

TL;DR: RiemanLine是一种新的3D直线表示方法，可以在黎曼流形上处理独立直线和平行线组，减少了参数空间，提高了相机定位和结构映射的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D直线表示方法主要处理独立的直线，忽略了在人造环境中普遍存在的平行线等结构规律性。这导致在相机定位和结构映射等应用中存在局限性。

Method: RiemanLine是一种新的3D直线最小参数化方法，它在黎曼流形上进行公式化，将每条直线解耦为全局（共享消失方向）和局部（尺度法向量）两个部分。平行线被自然地嵌入，减少了参数空间。该方法集成到因子图框架中，通过基于流的束调整进行全局方向对齐和局部重投影优化。

Result: RiemanLine在ICL-NUIM、TartanAir和合成数据集上进行了广泛的实验。结果表明，与现有方法相比，该方法在姿态估计和直线重建方面取得了显著更高的准确性，同时减小了参数维度并提高了收敛稳定性。

Conclusion: RiemanLine是一种统一的最小表示，用于3D直线，它在黎曼流形上进行公式化，能够同时处理单独的直线和平行线组。通过将每个直线地标解耦为全局和局部组件——在单位球体$	ext{S}^2$上优化的共享消失方向，以及在正交子空间上约束的尺度法向量——可以紧凑地编码结构规律性。对于n条平行线，该表示将参数空间从4n（例如，在正交范数下）减少到2n+2，从而在没有显式约束的情况下自然地嵌入了平行性。该方法已集成到因子图框架中，实现了基于流形的整体束调整，可进行全局方向对齐和局部重投影优化。在ICL-NUIM、TartanAir和合成基准的广泛实验表明，与现有方法相比，RiemanLine在姿态估计和直线重建方面具有更高的准确性，同时降低了参数维度并提高了收敛稳定性。

Abstract: Minimal parametrization of 3D lines plays a critical role in camera
localization and structural mapping. Existing representations in robotics and
computer vision predominantly handle independent lines, overlooking structural
regularities such as sets of parallel lines that are pervasive in man-made
environments. This paper introduces \textbf{RiemanLine}, a unified minimal
representation for 3D lines formulated on Riemannian manifolds that jointly
accommodates both individual lines and parallel-line groups. Our key idea is to
decouple each line landmark into global and local components: a shared
vanishing direction optimized on the unit sphere $\mathcal{S}^2$, and scaled
normal vectors constrained on orthogonal subspaces, enabling compact encoding
of structural regularities. For $n$ parallel lines, the proposed representation
reduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally
embedding parallelism without explicit constraints. We further integrate this
parameterization into a factor graph framework, allowing global direction
alignment and local reprojection optimization within a unified manifold-based
bundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic
benchmarks demonstrate that our method achieves significantly more accurate
pose estimation and line reconstruction, while reducing parameter
dimensionality and improving convergence stability.

</details>


### [21] [HPSv3: Towards Wide-Spectrum Human Preference Score](https://arxiv.org/abs/2508.03789)
*Yuhang Ma,Xiaoshi Wu,Keqiang Sun,Hongsheng Li*

Main category: cs.CV

TL;DR: HPSv3是一个新的评估指标，CoHP是一个用于提升图像质量的方法。


<details>
  <summary>Details</summary>
Motivation: 现有以人为中心评估文本到图像生成模型的指标存在数据覆盖范围有限、特征提取不佳和损失函数效率低下等问题。

Method: 发布了一个包含108万文本-图像对和117万标注配对比较的HPDv3数据集；引入了一个基于VLM的偏好模型，使用不确定性感知排序损失进行细粒度排序；提出了一种迭代式图像增强方法CoHP，利用HPSv3选择最佳图像。

Result: 实验证明HPSv3可作为广泛的图像评估的鲁棒指标，CoHP能有效提升图像生成质量。

Conclusion: HPSv3 是一个鲁棒的指标，可用于广泛的图像评估；CoHP 是一个有效且符合人类偏好的图像生成质量提升方法。

Abstract: Evaluating text-to-image generation models requires alignment with human
perception, yet existing human-centric metrics are constrained by limited data
coverage, suboptimal feature extraction, and inefficient loss functions. To
address these challenges, we introduce Human Preference Score v3 (HPSv3). (1)
We release HPDv3, the first wide-spectrum human preference dataset integrating
1.08M text-image pairs and 1.17M annotated pairwise comparisons from
state-of-the-art generative models and low to high-quality real-world images.
(2) We introduce a VLM-based preference model trained using an
uncertainty-aware ranking loss for fine-grained ranking. Besides, we propose
Chain-of-Human-Preference (CoHP), an iterative image refinement method that
enhances quality without extra data, using HPSv3 to select the best image at
each step. Extensive experiments demonstrate that HPSv3 serves as a robust
metric for wide-spectrum image evaluation, and CoHP offers an efficient and
human-aligned approach to improve image generation quality. The code and
dataset are available at the HPSv3 Homepage.

</details>


### [22] [Deep learning framework for crater detection and identification on the Moon and Mars](https://arxiv.org/abs/2508.03920)
*Yihan Ma,Zeyang Yu,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本研究利用CNN、YOLO和ResNet等深度学习模型，开发了一个两阶段的框架，用于自动检测和识别火星与月球表面的撞击坑。研究结果表明，YOLO在整体检测上表现均衡，ResNet-50则擅长精确识别大型撞击坑。


<details>
  <summary>Details</summary>
Motivation: 撞击坑是行星地貌的重要特征，其空间分布和形态特征蕴含着关于行星表面成分、地质历史和撞击过程的关键信息。随着深度学习模型的快速发展，自动检测撞击坑引起了广泛兴趣。

Method: 本研究采用包含卷积神经网络（CNN）、YOLO和ResNet等改进模型，通过一个两阶段的框架进行撞击坑的检测和识别。第一阶段使用经典的CNN、ResNet-50和YOLO进行撞击坑识别；第二阶段则利用YOLO进行撞击坑的本地化检测。最后，结合遥感数据，为火星和月球的选定区域提供撞击坑的检测和识别总结报告。

Result: 本研究在火星和月球的选定区域应用了基于深度学习的撞击坑检测框架，并根据遥感数据进行了识别。实验结果显示，YOLO模型在撞击坑检测方面展现了最均衡的性能，而ResNet-50在精确识别大型撞击坑方面表现突出。

Conclusion: YOLO在行星表面撞击坑检测方面表现出最均衡的性能，而ResNet-50在识别大型撞击坑方面具有高精度。

Abstract: Impact craters are among the most prominent geomorphological features on
planetary surfaces and are of substantial significance in planetary science
research. Their spatial distribution and morphological characteristics provide
critical information on planetary surface composition, geological history, and
impact processes. In recent years, the rapid advancement of deep learning
models has fostered significant interest in automated crater detection. In this
paper, we apply advancements in deep learning models for impact crater
detection and identification. We use novel models, including Convolutional
Neural Networks (CNNs) and variants such as YOLO and ResNet. We present a
framework that features a two-stage approach where the first stage features
crater identification using simple classic CNN, ResNet-50 and YOLO. In the
second stage, our framework employs YOLO-based detection for crater
localisation. Therefore, we detect and identify different types of craters and
present a summary report with remote sensing data for a selected region. We
consider selected regions for craters and identification from Mars and the Moon
based on remote sensing data. Our results indicate that YOLO demonstrates the
most balanced crater detection performance, while ResNet-50 excels in
identifying large craters with high precision.

</details>


### [23] [OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment](https://arxiv.org/abs/2508.04611)
*Tongfan Guan,Jiaxin Guo,Chen Wang,Yun-Hui Liu*

Main category: cs.CV

TL;DR: OmniDepth是一个统一框架，通过交叉注意力机制融合单目和双目深度估计，提高了精度，解决了反射和透明表面的问题，并在基准测试中取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计虽然能捕捉丰富的上下文先验知识，但缺乏几何精度；而双目方法虽然利用了对极几何，但在处理反射或无纹理表面等歧义性时存在困难。这两种范式在实践中仍然是分离的，尽管存在事后协同作用。本研究旨在弥合这一差距。

Method: 提出了一种名为OmniDepth的统一框架，通过迭代的双向对齐其潜在表示来融合单目和双目深度估计。其核心在于一种新颖的交叉注意力对齐机制，在双目推理过程中动态地同步单目上下文线索和双目假设表示。

Result: OmniDepth在Middlebury和ETH3D数据集上将零样本泛化误差降低了40%以上，并成功解决了透明和反射表面上的长期存在的失败案例。

Conclusion: OmniDepth通过融合单目和双目深度估计的优势，实现了鲁棒的3D感知，克服了单一模态的局限性，并在各种数据集和具有挑战性的场景中取得了最先进的成果。

Abstract: Monocular and stereo depth estimation offer complementary strengths:
monocular methods capture rich contextual priors but lack geometric precision,
while stereo approaches leverage epipolar geometry yet struggle with
ambiguities such as reflective or textureless surfaces. Despite post-hoc
synergies, these paradigms remain largely disjoint in practice. We introduce
OmniDepth, a unified framework that bridges both through iterative
bidirectional alignment of their latent representations. At its core, a novel
cross-attentive alignment mechanism dynamically synchronizes monocular
contextual cues with stereo hypothesis representations during stereo reasoning.
This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by
injecting monocular structure priors while refining monocular depth with stereo
geometry within a single network. Extensive experiments demonstrate
state-of-the-art results: \textbf{OmniDepth reduces zero-shot generalization
error by $\!>\!40\%$ on Middlebury and ETH3D}, while addressing longstanding
failures on transparent and reflective surfaces. By harmonizing multi-view
geometry with monocular context, OmniDepth enables robust 3D perception that
transcends modality-specific limitations. Codes available at
https://github.com/aeolusguan/OmniDepth.

</details>


### [24] [Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model](https://arxiv.org/abs/2508.03925)
*Shen Zhu,Yinzhu Jin,Ifrah Zawar,P. Thomas Fletcher*

Main category: cs.CV

TL;DR: 提出了一种新的扩散模型，用于生成具有对应关系的基于点的形状表示，并在海马体形状生成任务中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法忽略了点对应关系，而传统的统计形状模型考虑了这一点。本研究旨在构建一种能够生成保留训练数据中存在的点对应关系的逼真点状形状表示的扩散模型。

Method: 提出了一种旨在生成具有对应关系的基于点的形状表示的扩散模型。

Result: 所提出的模型能够生成逼真的点状海马体形状表示，并保留了点对应关系。

Conclusion: 该模型有效生成了具有高度真实性的点状海马体形状表示，并且优于现有方法。此外，通过下游任务，例如条件生成健康和AD受试者以及通过反事实生成预测疾病进展的形态学变化，展示了该生成器的应用。

Abstract: We propose a diffusion model designed to generate point-based shape
representations with correspondences. Traditional statistical shape models have
considered point correspondences extensively, but current deep learning methods
do not take them into account, focusing on unordered point clouds instead.
Current deep generative models for point clouds do not address generating
shapes with point correspondences between generated shapes. This work aims to
formulate a diffusion model that is capable of generating realistic point-based
shape representations, which preserve point correspondences that are present in
the training data. Using shape representation data with correspondences derived
from Open Access Series of Imaging Studies 3 (OASIS-3), we demonstrate that our
correspondence-preserving model effectively generates point-based hippocampal
shape representations that are highly realistic compared to existing methods.
We further demonstrate the applications of our generative model by downstream
tasks, such as conditional generation of healthy and AD subjects and predicting
morphological changes of disease progression by counterfactual generation.

</details>


### [25] [Radar-Based NLoS Pedestrian Localization for Darting-Out Scenarios Near Parked Vehicles with Camera-Assisted Point Cloud Interpretation](https://arxiv.org/abs/2508.04033)
*Hee-Yeun Kim,Byeonggyu Park,Byonghyok Choi,Hansang Cho,Byungkwan Kim,Soomok Lee,Mingu Jeon,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.CV

TL;DR: 通过结合摄像头和雷达数据，提出了一种新的NLoS行人本地化方法，以解决城市道路停车造成的盲区问题，并提高了行人检测的早期性和道路安全。


<details>
  <summary>Details</summary>
Motivation: 城市环境中路侧停车造成的非视线（NLoS）盲区对道路安全构成重大威胁，特别是行人突然出现的情况。现有方法依赖预定义空间信息或假设简单的墙体反射，泛化性和实用性受限。停车车辆作为动态障碍物，会使预定义空间信息失效，导致传感器解释错误。

Method: 本文提出了一种结合单目摄像头图像和二维雷达点云（PCD）数据的NLoS行人本地化框架。首先，通过图像分割检测停车车辆，并估计深度以获取近似的空间特征；然后，利用二维雷达PCD数据对该信息进行精炼，实现精确的空间推断。

Result: 实验结果表明，所提出的方法在真实城市道路环境中能够增强早期行人检测能力，提高道路安全。

Conclusion: 本文提出的非视线（NLoS）行人本地化框架通过融合单目摄像头图像和二维雷达点云（PCD）数据，有效解决了城市环境中路侧停车导致的NLoS盲区问题，提高了行人检测的早期性和道路安全性。

Abstract: The presence of Non-Line-of-Sight (NLoS) blind spots resulting from roadside
parking in urban environments poses a significant challenge to road safety,
particularly due to the sudden emergence of pedestrians. mmWave technology
leverages diffraction and reflection to observe NLoS regions, and recent
studies have demonstrated its potential for detecting obscured objects.
However, existing approaches predominantly rely on predefined spatial
information or assume simple wall reflections, thereby limiting their
generalizability and practical applicability. A particular challenge arises in
scenarios where pedestrians suddenly appear from between parked vehicles, as
these parked vehicles act as temporary spatial obstructions. Furthermore, since
parked vehicles are dynamic and may relocate over time, spatial information
obtained from satellite maps or other predefined sources may not accurately
reflect real-time road conditions, leading to erroneous sensor interpretations.
To address this limitation, we propose an NLoS pedestrian localization
framework that integrates monocular camera image with 2D radar point cloud
(PCD) data. The proposed method initially detects parked vehicles through image
segmentation, estimates depth to infer approximate spatial characteristics, and
subsequently refines this information using 2D radar PCD to achieve precise
spatial inference. Experimental evaluations conducted in real-world urban road
environments demonstrate that the proposed approach enhances early pedestrian
detection and contributes to improved road safety. Supplementary materials are
available at https://hiyeun.github.io/NLoS/.

</details>


### [26] [Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation](https://arxiv.org/abs/2508.03953)
*Xiangcen Wu,Shaheer U. Saeed,Yipei Wang,Ester Bonmati Coll,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出一个推荐系统，通过推荐最佳成像模态和图像部分来辅助基于机器学习的分割模型，以最大化前列腺癌分割性能。该系统通过训练策略网络来辅助肿瘤定位，并能独立开发最优策略，为放射科医生提供交互式辅助。


<details>
  <summary>Details</summary>
Motivation: 为了解决放射科医生在阅读医学图像时混合使用不同模态和局部区域信息，以及独立或并发地使用不同位置信息的问题，提出一个推荐系统来辅助机器学习分割模型。

Method: 提出一个推荐系统，训练一个策略网络来辅助肿瘤定位，推荐最佳成像模态和感兴趣的图像部分。该系统使用预训练的分割网络来模拟放射科医生的检查，并将局部分割区域作为输入进行迭代，直到所有癌症都被最佳定位。

Result: 通过使用1325个标记的多参数MRI图像的数据集进行验证，证明了该方法在提高注释效率和分割准确性方面的潜力，尤其是在存在具有挑战性的病理时。实验结果表明，该方法能够超越标准的分割网络，并且训练出的智能体独立开发了其自身的最优策略。

Conclusion: 该方法在提高分割准确性和效率方面显示出潜力，尤其是在处理具有挑战性的病理时，并且其性能优于标准的分割网络。训练好的模型能够独立开发最优策略，这可能与现有的放射科医生指南（如PI-RADS）不一致，并为人类放射科医生提供交互式辅助应用。

Abstract: Radiologists often mix medical image reading strategies, including inspection
of individual modalities and local image regions, using information at
different locations from different images independently as well as
concurrently. In this paper, we propose a recommend system to assist machine
learning-based segmentation models, by suggesting appropriate image portions
along with the best modality, such that prostate cancer segmentation
performance can be maximised. Our approach trains a policy network that assists
tumor localisation, by recommending both the optimal imaging modality and the
specific sections of interest for review. During training, a pre-trained
segmentation network mimics radiologist inspection on individual or variable
combinations of these imaging modalities and their sections - selected by the
policy network. Taking the locally segmented regions as an input for the next
step, this dynamic decision making process iterates until all cancers are best
localised. We validate our method using a data set of 1325 labelled
multiparametric MRI images from prostate cancer patients, demonstrating its
potential to improve annotation efficiency and segmentation accuracy,
especially when challenging pathology is present. Experimental results show
that our approach can surpass standard segmentation networks. Perhaps more
interestingly, our trained agent independently developed its own optimal
strategy, which may or may not be consistent with current radiologist
guidelines such as PI-RADS. This observation also suggests a promising
interactive application, in which the proposed policy networks assist human
radiologists.

</details>


### [27] [A Foundation Model for DAS Signal Recognition and Visual Prompt Tuning of the Pre-trained Model for Downstream Tasks](https://arxiv.org/abs/2508.04316)
*Kun Gui,Hongliang Ren,Shang Shi,Jin Lu,Changqiu Yu,Quanjun Cao,Guomin Gu,Qi Xuan*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Distributed Acoustic Sensing (DAS) technology finds growing applications
across various domains. However, data distribution disparities due to
heterogeneous sensing environments pose challenges for data-driven artificial
intelligence (AI) models, limiting cross-domain generalization and facing a
shortage of labeled training data. To address these issues, this study proposes
a foundational model for DAS signal recognition based on a Masked Autoencoder,
named MAEPD. The MAEPD model is pretrained on a dataset of 635,860 samples,
encompassing DAS gait spatiotemporal signals, 2D GASF images for perimeter
security, 2D time-frequency images for pipeline leakage, and open-dataset
signals including whale vocalizations and seismic activities, using a
self-supervised mask reconstruction task to capture deep semantic features of
DAS signals. Visual Prompt Tuning (VPT) is employed for downstream recognition
tasks. This method freezes the pretrained backbone parameters and fine-tunes
only a small set of learnable visual prompt vectors inserted into the
Transformer encoder layers. Experiments on the NVIDIA GeForce RTX 4080 Super
platform validate MAEPD using indoor gait recognition as a downstream task. The
VPT-Deep approach achieves a classification accuracy of 96.94% with just 0.322%
of parameters fine-tuned, surpassing the traditional Full Fine Tuning (FFT)
method by 0.61% and reducing training time by 45%. The model also exhibits
robust performance in pipeline leakage detection, confirming the generality,
efficiency, and scalability of MAEPD as a foundational model. This approach
offers a novel paradigm for addressing the limited generalization of signal
recognition models in the DAS domain.

</details>


### [28] [Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm](https://arxiv.org/abs/2508.03955)
*Lin Zhang,Zefan Cai,Yufan Zhou,Shentong Mo,Jinhong Lin,Cheng-En Wu,Yibing Wei,Yijing Zhang,Ruiyi Zhang,Wen Xiao,Tong Sun,Junjie Hu,Pedro Morgado*

Main category: cs.CV

TL;DR: 提出了一种高效的两阶段训练范式，用于音频同步视觉动画，通过利用大规模嘈杂视频进行预训练和少量高质量数据进行微调，显著减少了手动整理工作，并能泛化到开放类别。


<details>
  <summary>Details</summary>
Motivation: 现有的音频同步视觉动画方法严重依赖昂贵的手动整理，难以扩展到开放世界中多样化的音频-视频类别。

Method: 提出了一种高效的两阶段训练范式，第一阶段自动整理大规模视频进行预训练，第二阶段在少量手动整理的高质量样本上进行微调。通过多特征条件和窗口注意力增强同步性，并利用预训练的文本到视频生成器和音频编码器，仅引入 1.9% 的额外可训练参数。

Result: 与现有方法相比，该方法将手动整理的需求减少了 10 倍以上，同时在 AVSync48 基准上表现出对许多开放类别的泛化能力。

Conclusion: 该方法通过高效的两阶段训练范式，利用丰富但嘈杂的视频进行扩展，大大减少了对手动整理的需求，并能推广到许多开放类别。

Abstract: Recent advances in audio-synchronized visual animation enable control of
video content using audios from specific classes. However, existing methods
rely heavily on expensive manual curation of high-quality, class-specific
training videos, posing challenges to scaling up to diverse audio-video classes
in the open world. In this work, we propose an efficient two-stage training
paradigm to scale up audio-synchronized visual animation using abundant but
noisy videos. In stage one, we automatically curate large-scale videos for
pretraining, allowing the model to learn diverse but imperfect audio-video
alignments. In stage two, we finetune the model on manually curated
high-quality examples, but only at a small scale, significantly reducing the
required human effort. We further enhance synchronization by allowing each
frame to access rich audio context via multi-feature conditioning and window
attention. To efficiently train the model, we leverage pretrained text-to-video
generator and audio encoders, introducing only 1.9\% additional trainable
parameters to learn audio-conditioning capability without compromising the
generator's prior knowledge. For evaluation, we introduce AVSync48, a benchmark
with videos from 48 classes, which is 3$\times$ more diverse than previous
benchmarks. Extensive experiments show that our method significantly reduces
reliance on manual curation by over 10$\times$, while generalizing to many open
classes.

</details>


### [29] [RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification](https://arxiv.org/abs/2508.03967)
*Mamadou Keita,Wassim Hamidouche,Hessen Bougueffa Eutamene,Abdelmalik Taleb-Ahmed,Abdenour Hadid*

Main category: cs.CV

TL;DR: RAVID是首个利用视觉检索增强生成（RAG）的AI生成图像检测框架，通过检索相关图像和融合技术，在检测准确性和鲁棒性方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法在泛化性和鲁棒性方面存在不足，并且主要依赖低级伪影和特定模型特征，而视觉RAG方法在视觉知识探索方面尚不成熟。RAVID旨在通过动态检索相关图像来解决这些问题，以提高检测的泛化性和鲁棒性。

Method: RAVID框架利用视觉检索增强生成（RAG）技术，通过微调CLIP图像编码器（RAVID CLIP）并结合类别提示来增强表示学习，然后利用视觉语言模型（VLM）融合检索到的相关图像和查询图像来丰富输入，从而提高检测精度。

Result: RAVID在UniversalFakeDetect基准测试中实现了93.85%的平均准确率，优于现有方法。在图像退化（如高斯模糊和JPEG压缩）条件下，RAVID的平均准确率为80.27%，而最先进的C2P-CLIP模型为63.44%，证明了其在不同条件下的鲁棒性。

Conclusion: RAVID在检测AI生成图像方面取得了最先进的性能，在UniversalFakeDetect基准测试中平均准确率为93.85%，并且在图像退化下表现出更强的鲁棒性，平均准确率为80.27%，优于现有方法。

Abstract: In this paper, we introduce RAVID, the first framework for AI-generated image
detection that leverages visual retrieval-augmented generation (RAG). While RAG
methods have shown promise in mitigating factual inaccuracies in foundation
models, they have primarily focused on text, leaving visual knowledge
underexplored. Meanwhile, existing detection methods, which struggle with
generalization and robustness, often rely on low-level artifacts and
model-specific features, limiting their adaptability. To address this, RAVID
dynamically retrieves relevant images to enhance detection. Our approach
utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with
category-related prompts to improve representation learning. We further
integrate a vision-language model (VLM) to fuse retrieved images with the
query, enriching the input and improving accuracy. Given a query image, RAVID
generates an embedding using RAVID CLIP, retrieves the most relevant images
from a database, and combines these with the query image to form an enriched
input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the
UniversalFakeDetect benchmark, which covers 19 generative models, show that
RAVID achieves state-of-the-art performance with an average accuracy of 93.85%.
RAVID also outperforms traditional methods in terms of robustness, maintaining
high accuracy even under image degradations such as Gaussian blur and JPEG
compression. Specifically, RAVID achieves an average accuracy of 80.27% under
degradation conditions, compared to 63.44% for the state-of-the-art model
C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG
compression scenarios. The code will be publicly available upon acceptance.

</details>


### [30] [Investigating the Impact of Large-Scale Pre-training on Nutritional Content Estimation from 2D Images](https://arxiv.org/abs/2508.03996)
*Michele Andrade,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Eduardo Luz*

Main category: cs.CV

TL;DR: 本研究评估了预训练数据集对2D食物营养估算模型性能的影响。实验表明，在专有数据集JFT-300M上预训练的模型效果最佳，而使用公共数据集（如ImageNet和COYO）预训练的模型效果较差，并且在COYO上预训练的模型甚至不如在ImageNet上预训练的模型。


<details>
  <summary>Details</summary>
Motivation: 旨在探究大规模预训练数据集对基于2D图像的食物营养估算模型性能的影响，并解决现有方法依赖专有数据集进行预训练而导致的复现性问题。

Method: 通过在Nutrition5k数据集上对Vision Transformer (ViT)模型进行微调和评估，并与在ImageNet和COYO等公共数据集以及专有的JFT-300M数据集上预训练的基线CNN模型（InceptionV2和ResNet-50）进行比较，来研究大规模预训练数据集对仅使用2D图像进行营养估算任务的深度学习模型性能的影响。

Result: 在Nutrition5k数据集上的实验结果显示，使用JFT-300M数据集预训练的模型在营养估算任务上显著优于使用公共数据集（ImageNet和COYO）预训练的模型。出乎意料的是，在COYO数据集上预训练的模型表现不如在ImageNet上预训练的模型，这与研究假设相悖。

Conclusion: 研究结果表明，在用于2D营养估算的食物图像上进行迁移学习时，预训练数据集的规模、领域相关性和数据质量至关重要，而JFT-300M数据集在这些方面表现出显著优势。

Abstract: Estimating the nutritional content of food from images is a critical task
with significant implications for health and dietary monitoring. This is
challenging, especially when relying solely on 2D images, due to the
variability in food presentation, lighting, and the inherent difficulty in
inferring volume and mass without depth information. Furthermore,
reproducibility in this domain is hampered by the reliance of state-of-the-art
methods on proprietary datasets for large-scale pre-training. In this paper, we
investigate the impact of large-scale pre-training datasets on the performance
of deep learning models for nutritional estimation using only 2D images. We
fine-tune and evaluate Vision Transformer (ViT) models pre-trained on two large
public datasets, ImageNet and COYO, comparing their performance against
baseline CNN models (InceptionV2 and ResNet-50) and a state-of-the-art method
pre-trained on the proprietary JFT-300M dataset. We conduct extensive
experiments on the Nutrition5k dataset, a large-scale collection of real-world
food plates with high-precision nutritional annotations. Our evaluation using
Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAE%) reveals
that models pre-trained on JFT-300M significantly outperform those pre-trained
on public datasets. Unexpectedly, the model pre-trained on the massive COYO
dataset performs worse than the model pre-trained on ImageNet for this specific
regression task, refuting our initial hypothesis. Our analysis provides
quantitative evidence highlighting the critical role of pre-training dataset
characteristics, including scale, domain relevance, and curation quality, for
effective transfer learning in 2D nutritional estimation.

</details>


### [31] [JanusNet: Hierarchical Slice-Block Shuffle and Displacement for Semi-Supervised 3D Multi-Organ Segmentation](https://arxiv.org/abs/2508.03997)
*Zheng Zhang,Tianzhuzi Tan,Guanchun Yin,Bo Zhang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: JanusNet 是一种 3D 医学数据增强框架，通过“切片-块随机播放”和“置信度引导置换”来保持解剖连续性并关注难分割区域，从而提高分割精度，尤其是在标记数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 为了更好地遵循和利用人体解剖信息，解决现有随机混合体积块的方法会破坏 3D 医学图像沿正交轴的固有解剖连续性，导致严重的结构不一致和在小尺寸器官等挑战性区域的训练不足问题。

Method: JanusNet 提出了一种数据增强框架，通过“切片-块随机播放”步骤沿随机轴对卷内的同索引切片块进行对齐播放，同时保留垂直于扰动轴的平面上的解剖上下文。此外，“置信度引导置换”步骤利用预测可靠性替换每个切片内的块，以增强来自困难区域的信号。

Result: JanusNet 框架在 Synapse 和 AMOS 数据集上显著超越了现有技术。

Conclusion: JanusNet 在 Synapse 和 AMOS 数据集上的广泛实验表明，其显著优于最先进的方法，例如，在仅有 20% 标记数据的 Synapse 数据集上实现了 4% 的 DSC 增益。

Abstract: Limited by the scarcity of training samples and annotations, weakly
supervised medical image segmentation often employs data augmentation to
increase data diversity, while randomly mixing volumetric blocks has
demonstrated strong performance. However, this approach disrupts the inherent
anatomical continuity of 3D medical images along orthogonal axes, leading to
severe structural inconsistencies and insufficient training in challenging
regions, such as small-sized organs, etc. To better comply with and utilize
human anatomical information, we propose JanusNet}, a data augmentation
framework for 3D medical data that globally models anatomical continuity while
locally focusing on hard-to-segment regions. Specifically, our Slice-Block
Shuffle step performs aligned shuffling of same-index slice blocks across
volumes along a random axis, while preserving the anatomical context on planes
perpendicular to the perturbation axis. Concurrently, the Confidence-Guided
Displacement step uses prediction reliability to replace blocks within each
slice, amplifying signals from difficult areas. This dual-stage, axis-aligned
framework is plug-and-play, requiring minimal code changes for most
teacher-student schemes. Extensive experiments on the Synapse and AMOS datasets
demonstrate that JanusNet significantly surpasses state-of-the-art methods,
achieving, for instance, a 4% DSC gain on the Synapse dataset with only 20%
labeled data.

</details>


### [32] [CAD-Judge: Toward Efficient Morphological Grading and Verification for Text-to-CAD Generation](https://arxiv.org/abs/2508.04002)
*Zheyuan Zhou,Jiayi Han,Liang Du,Naiyu Fang,Lemiao Qiu,Shuyou Zhang*

Main category: cs.CV

TL;DR: CAD-Judge is a new system that uses compilers to judge and review CAD models generated from text, making the process faster and more accurate than traditional methods using VLMs. It improves Text-to-CAD systems by optimizing rewards and verifying models, leading to state-of-the-art results with better efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Text-to-CAD systems face challenges such as slow rendering of CAD models and the high cost and potential reward hacking associated with deploying Vision-Language Models (VLMs) for review. The proposed system aims to address these issues by providing an efficient and effective reward system for grading and validation.

Method: The paper proposes CAD-Judge, a novel, verifiable reward system that uses a Compiler-as-a-Judge Module (CJM) for efficient and effective CAD preference grading and grammatical validation. It optimizes model alignment by maximizing generative utility through prospect theory. Additionally, it introduces an agentic CAD generation approach and a Compiler-as-a-Review Module (CRM) to efficiently verify and refine generated CAD models.

Result: Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance and superior efficiency in Text-to-CAD generation.

Conclusion: The proposed method achieves state-of-the-art performance in Text-to-CAD generation while maintaining superior efficiency, validated through extensive experiments on challenging CAD datasets.

Abstract: Computer-Aided Design (CAD) models are widely used across industrial design,
simulation, and manufacturing processes. Text-to-CAD systems aim to generate
editable, general-purpose CAD models from textual descriptions, significantly
reducing the complexity and entry barrier associated with traditional CAD
workflows. However, rendering CAD models can be slow, and deploying VLMs to
review CAD models can be expensive and may introduce reward hacking that
degrades the systems. To address these challenges, we propose CAD-Judge, a
novel, verifiable reward system for efficient and effective CAD preference
grading and grammatical validation. We adopt the Compiler-as-a-Judge Module
(CJM) as a fast, direct reward signal, optimizing model alignment by maximizing
generative utility through prospect theory. To further improve the robustness
of Text-to-CAD in the testing phase, we introduce a simple yet effective
agentic CAD generation approach and adopt the Compiler-as-a-Review Module
(CRM), which efficiently verifies the generated CAD models, enabling the system
to refine them accordingly. Extensive experiments on challenging CAD datasets
demonstrate that our method achieves state-of-the-art performance while
maintaining superior efficiency.

</details>


### [33] [$\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016)
*Weilun Feng,Haotong Qin,Chuanguang Yang,Xiangqi Li,Han Yang,Yuqi Li,Zhulin An,Libo Huang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: $	ext{S}^2$Q-VDiT 是一种创新的量化框架，通过智能数据选择和注意力引导的蒸馏技术，有效解决了视频扩散模型中的量化挑战，实现了显著的模型压缩和推理加速，且性能无损。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型（如扩散 Transformer）参数量巨大，导致计算成本高。量化是解决此问题的有效方法。然而，视频扩散模型（V-DMs）中空间和时间信息的联合建模会产生过长的 token 序列，带来校准方差和学习难题。

Method: $	ext{S}^2$Q-VDiT 是一种用于视频扩散模型的后训练量化框架，结合了显著性数据和稀疏 token 蒸馏。它通过“Hessian 感知显著性数据选择”来构建高质量的校准数据集，并利用“注意力引导的稀疏 token 蒸馏”来解决学习挑战。

Result: 在 W4A6 量化下，$	ext{S}^2$Q-VDiT 实现了无损性能，模型压缩率达到 3.9 倍，推理速度提升 1.3 倍。

Conclusion: $	ext{S}^2$Q-VDiT 框架在 W4A6 量化下实现了无损性能，同时实现了 3.9 倍的模型压缩和 1.3 倍的推理加速。

Abstract: Diffusion transformers have emerged as the mainstream paradigm for video
generation models. However, the use of up to billions of parameters incurs
significant computational costs. Quantization offers a promising solution by
reducing memory usage and accelerating inference. Nonetheless, we observe that
the joint modeling of spatial and temporal information in video diffusion
models (V-DMs) leads to extremely long token sequences, which introduces high
calibration variance and learning challenges. To address these issues, we
propose \textbf{$\text{S}^2$Q-VDiT}, a post-training quantization framework for
V-DMs that leverages \textbf{S}alient data and \textbf{S}parse token
distillation. During the calibration phase, we identify that quantization
performance is highly sensitive to the choice of calibration data. To mitigate
this, we introduce \textit{Hessian-aware Salient Data Selection}, which
constructs high-quality calibration datasets by considering both diffusion and
quantization characteristics unique to V-DMs. To tackle the learning
challenges, we further analyze the sparse attention patterns inherent in V-DMs.
Based on this observation, we propose \textit{Attention-guided Sparse Token
Distillation}, which exploits token-wise attention distributions to emphasize
tokens that are more influential to the model's output. Under W4A6
quantization, $\text{S}^2$Q-VDiT achieves lossless performance while delivering
$3.9\times$ model compression and $1.3\times$ inference acceleration. Code will
be available at
\href{https://github.com/wlfeng0509/s2q-vdit}{https://github.com/wlfeng0509/s2q-vdit}.

</details>


### [34] [Can Large Multimodal Models Actively Recognize Faulty Inputs? A Systematic Evaluation Framework of Their Input Scrutiny Ability](https://arxiv.org/abs/2508.04017)
*Haiqi Yang,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.CV

TL;DR: 该研究评估了 LMM 主动检测和审查有缺陷输入的能力，发现大多数模型在此方面存在不足，尤其是在处理表面语言错误和某些条件缺陷时。Gemini 2.5 pro 和 Claude Sonnet 4 在多模态信息平衡方面表现较好，而 aya-vision-8b 则过度依赖文本。研究强调了增强 LMM 输入验证能力的必要性。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMM）在处理复杂的多模态任务方面展现出了强大的能力，但最近的研究表明，它们倾向于被动接受有缺陷的输入，导致在无效提示上进行无效推理。然而，LMM 是否能主动检测和审查有缺陷的输入这一关键问题仍未得到探索。

Method: 该研究引入了输入审查能力评估框架（ISEval），该框架包括七类有缺陷的前提和三个评估指标。

Result: 对十个先进的 LMM 的广泛评估已确定了关键发现。大多数模型在没有指导的情况下难以主动检测有缺陷的文本前提，这表明它们在很大程度上依赖明确的提示来识别前提错误。错误类型会影响性能：模型在识别逻辑谬误方面表现出色，但在处理表面语言错误和某些条件缺陷方面存在不足。多模态信任度各不相同——Gemini 2.5 pro 和 Claude Sonnet 4 在视觉和文本信息之间取得了平衡，而 aya-vision-8b 在冲突中过度依赖文本。

Conclusion: 大多数模型在没有指导的情况下难以主动检测有缺陷的文本前提，这反映了它们在很大程度上依赖明确的提示来识别前提错误。错误类型会影响性能：模型在识别逻辑谬误方面表现出色，但在处理表面语言错误和某些条件缺陷方面存在不足。多模态信任度各不相同——Gemini 2.5 pro 和 Claude Sonnet 4 在视觉和文本信息之间取得了平衡，而 aya-vision-8b 在冲突中过度依赖文本。这些发现强调了增强 LMM 主动验证输入有效性的紧迫需求，并为缓解该问题提供了新的见解。

Abstract: Large Multimodal Models (LMMs) have witnessed remarkable growth, showcasing
formidable capabilities in handling intricate multimodal tasks with exceptional
performance. Recent research has underscored the inclination of large language
models to passively accept defective inputs, often resulting in futile
reasoning on invalid prompts. However, the same critical question of whether
LMMs can actively detect and scrutinize erroneous inputs still remains
unexplored. To address this gap, we introduce the Input Scrutiny Ability
Evaluation Framework (ISEval), which encompasses seven categories of flawed
premises and three evaluation metrics. Our extensive evaluation of ten advanced
LMMs has identified key findings. Most models struggle to actively detect
flawed textual premises without guidance, which reflects a strong reliance on
explicit prompts for premise error identification. Error type affects
performance: models excel at identifying logical fallacies but struggle with
surface-level linguistic errors and certain conditional flaws. Modality trust
varies-Gemini 2.5 pro and Claude Sonnet 4 balance visual and textual info,
while aya-vision-8b over-rely on text in conflicts. These insights underscore
the urgent need to enhance LMMs' proactive verification of input validity and
shed novel insights into mitigating the problem. The code is available at
https://github.com/MLGroupJLU/LMM_ISEval.

</details>


### [35] [Prototype-Driven Structure Synergy Network for Remote Sensing Images Segmentation](https://arxiv.org/abs/2508.04022)
*Junyi Wang,Jinjiang Li,Guodong Fan,Yakun Ju,Xiang Fang,Alex C. Kot*

Main category: cs.CV

TL;DR: PDSSNet addresses challenges in remote sensing image segmentation by integrating semantic prototypes and spatial structure. It uses APEM for unbiased prototypes, SSCM for semantic-structure refinement, and CSAM for feature discrimination, achieving superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and even emerging class-guided approaches struggle with high intra-class variance and high inter-class similarity in remote sensing image semantic segmentation, leading to incomplete segmentation results due to coarse class prototypes and neglect of target structural information.

Method: PDSSNet, which includes an Adaptive Prototype Extraction Module (APEM) to extract unbiased class prototypes, a Semantic-Structure Coordination Module (SSCM) that uses hierarchical semantics-first, structure-second principle to refine semantic representation, and a Channel Similarity Adjustment Module (CSAM) to focus on discriminative features between classes.

Result: Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art methods.

Conclusion: PDSSNet outperforms state-of-the-art methods in remote sensing image semantic segmentation.

Abstract: In the semantic segmentation of remote sensing images, acquiring complete
ground objects is critical for achieving precise analysis. However, this task
is severely hindered by two major challenges: high intra-class variance and
high inter-class similarity. Traditional methods often yield incomplete
segmentation results due to their inability to effectively unify class
representations and distinguish between similar features. Even emerging
class-guided approaches are limited by coarse class prototype representations
and a neglect of target structural information.
  Therefore, this paper proposes a Prototype-Driven Structure Synergy Network
(PDSSNet). The design of this network is based on a core concept, a complete
ground object is jointly defined by its invariant class semantics and its
variant spatial structure. To implement this, we have designed three key
modules. First, the Adaptive Prototype Extraction Module (APEM) ensures
semantic accuracy from the source by encoding the ground truth to extract
unbiased class prototypes. Subsequently, the designed Semantic-Structure
Coordination Module (SSCM) follows a hierarchical semantics-first,
structure-second principle. This involves first establishing a global semantic
cognition, then leveraging structural information to constrain and refine the
semantic representation, thereby ensuring the integrity of class information.
Finally, the Channel Similarity Adjustment Module (CSAM) employs a dynamic
step-size adjustment mechanism to focus on discriminative features between
classes.
  Extensive experiments demonstrate that PDSSNet outperforms state-of-the-art
methods. The source code is available at
https://github.com/wangjunyi-1/PDSSNet.

</details>


### [36] [Dual Prompt Learning for Adapting Vision-Language Models to Downstream Image-Text Retrieval](https://arxiv.org/abs/2508.04028)
*Yifan Wang,Tao Wang,Chenwei Tang,Caiyang Yu,Zhengqing Zang,Mengmi Zhang,Shudong Huang,Jiancheng Lv*

Main category: cs.CV

TL;DR: 针对图像-文本检索（ITR）任务中的细粒度属性和相似子类别区分难题，提出了一种名为DCAR的双提示学习框架。该框架通过联合优化类别和属性特征，并动态调整提示向量，在新建的FDRD数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 提示学习在适应预训练视觉-语言模型（VLMs）到下游任务（如图像分类）方面取得了显著成功，但将其应用于下游图像-文本检索（ITR）任务更具挑战性，其难点在于区分下游数据的细粒度属性和相似子类别。

Method: 提出了一种名为DCAR（Dual prompt Learning with Joint Category-Attribute Reweighting）的新型双提示学习框架，通过动态调整来自语义和视觉维度的提示向量来改进CLIP在图像-文本检索任务上的表现。DCAR联合优化属性和类别特征以增强细粒度表示学习，具体地：1.在属性层面，基于文本-图像互信息相关性动态更新属性描述的权重；2.在类别层面，引入多视角负样本并进行类别匹配加权以学习子类别区分。

Result: DCAR框架能够动态调整提示向量，在图像-文本检索任务上取得了显著的性能提升，并且在FDRD数据集上进行了验证。

Conclusion: DCAR在FDRD数据集上实现了最先进的性能，优于现有的基线方法。

Abstract: Recently, prompt learning has demonstrated remarkable success in adapting
pre-trained Vision-Language Models (VLMs) to various downstream tasks such as
image classification. However, its application to the downstream Image-Text
Retrieval (ITR) task is more challenging. We find that the challenge lies in
discriminating both fine-grained attributes and similar subcategories of the
downstream data. To address this challenge, we propose Dual prompt Learning
with Joint Category-Attribute Reweighting (DCAR), a novel dual-prompt learning
framework to achieve precise image-text matching. The framework dynamically
adjusts prompt vectors from both semantic and visual dimensions to improve the
performance of CLIP on the downstream ITR task. Based on the prompt paradigm,
DCAR jointly optimizes attribute and class features to enhance fine-grained
representation learning. Specifically, (1) at the attribute level, it
dynamically updates the weights of attribute descriptions based on text-image
mutual information correlation; (2) at the category level, it introduces
negative samples from multiple perspectives with category-matching weighting to
learn subcategory distinctions. To validate our method, we construct the
Fine-class Described Retrieval Dataset (FDRD), which serves as a challenging
benchmark for ITR in downstream data domains. It covers over 1,500 downstream
fine categories and 230,000 image-caption pairs with detailed attribute
annotations. Extensive experiments on FDRD demonstrate that DCAR achieves
state-of-the-art performance over existing baselines.

</details>


### [37] [CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion](https://arxiv.org/abs/2508.04036)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Syahid Al Irfan,Hindriyanto Dwi Purnomo,Radius Tanone*

Main category: cs.CV

TL;DR: CORE-ReID V2通过CycleGAN进行数据合成和ECAB/SECAB集成融合机制，在无监督域自适应的行人/车辆重识别任务中取得了最先进的性能，并支持轻量级骨干网络。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决无监督域自适应（UDA）在行人重识别和车辆重识别中的挑战，并将其应用扩展到对象重识别领域，以提高跨域识别的性能和效率。

Method: 该研究提出了CORE-ReID V2框架，该框架在前作CORE-ReID的基础上进行了扩展。在预训练阶段，利用CycleGAN合成多样化数据，以缩小不同域之间的图像特征差异。在微调阶段，采用包含高效通道注意力块（ECAB）和简化高效通道注意力块（SECAB）的先进集成融合机制，以增强局部和全局特征表示，并减少目标样本伪标签的模糊性。

Result: 在广泛使用的UDA行人重识别和车辆重识别数据集上的实验结果表明，CORE-ReID V2框架的性能优于现有最先进的方法，在mAP和Rank-k准确率（Top-1, Top-5, Top-10）方面均 đạt 到顶级水平。该框架支持ResNet18和ResNet34等轻量级骨干网络，确保了可扩展性和效率。

Conclusion: 该研究提出的CORE-ReID V2框架在无监督域自适应（UDA）的行人重识别和车辆重识别任务中取得了显著成果，并且在对象重识别领域也具有广泛适用性。通过实验证明，该框架在常用UDA数据集上超越了现有最先进的方法，在mAP和Rank-k准确率（Top-1, Top-5, Top-10）方面表现优异。此外，该框架支持轻量级骨干网络（如ResNet18和ResNet34），保证了可扩展性和效率。

Abstract: This study presents CORE-ReID V2, an enhanced framework building upon
CORE-ReID. The new framework extends its predecessor by addressing Unsupervised
Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with
further applicability to Object ReID. During pre-training, CycleGAN is employed
to synthesize diverse data, bridging image characteristic gaps across different
domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting
of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient
Channel Attention Block (SECAB), enhances both local and global feature
representations while reducing ambiguity in pseudo-labels for target samples.
Experimental results on widely used UDA Person ReID and Vehicle ReID datasets
demonstrate that the proposed framework outperforms state-of-the-art methods,
achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy
(Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones
such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our
work not only pushes the boundaries of UDA-based Object ReID but also provides
a solid foundation for further research and advancements in this domain. Our
codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID-V2.

</details>


### [38] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D是一个新的统一的、端到端的框架，用于精确的单木和语义分割。它通过新的组件实现了最先进的性能，并能很好地泛化到未见过的测试集。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在处理自然森林环境的复杂性和变异性时，常常面临挑战，而森林激光雷达3D点云的分割（包括单木和语义分割）对于推进森林管理和生态研究至关重要。

Method: ForestFormer3D是一个统一的、端到端的框架，它结合了ISA引导的查询点选择、推理过程中的基于分数的块合并策略以及一对多关联机制来进行有效训练。

Result: ForestFormer3D在FOR-instanceV2数据集的单木分割任务上取得了最先进的性能，并且在Wytham woods和LAUTx等未见过的测试集上表现出良好的泛化能力。

Conclusion: ForestFormer3D在FOR-instanceV2数据集的单木分割任务上取得了最先进的性能，并且在未见过的测试集（Wytham woods and LAUTx）上表现出良好的泛化能力，证明了其在不同森林条件和传感器模式下的鲁棒性。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly
available at https://bxiang233.github.io/FF3D/.

</details>


### [39] [SPJFNet: Self-Mining Prior-Guided Joint Frequency Enhancement for Ultra-Efficient Dark Image Restoration](https://arxiv.org/abs/2508.04041)
*Tongshun Zhang,Pingling Liu,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: SPJFNet通过自挖掘指导模块（SMGM）和双频引导框架（DFGF）解决了暗图像恢复的效率问题，实现了高性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 当前暗图像恢复方法存在效率瓶颈，包括外部先验的计算负担和误差校正成本、复杂多阶段增强管道中的冗余操作以及全局计算需求过高等问题。

Method: 提出了一种高效的自挖掘先验引导联合频率增强网络（SPJFNet），包括自挖掘指导模块（SMGM）和双频引导框架（DFGF）。SMGM直接从网络生成轻量级内生引导，消除了对外部先验的依赖。DFGF通过小波分解和傅里叶增强将多层操作链压缩为单一高效操作，并部署专门的高/低频分支以降低计算复杂性。

Result: SPJFNet在多个基准测试中取得了优于现有方法（state-of-the-art）的性能，并且显著提高了效率，大幅降低了模型复杂度和计算开销。

Conclusion: SPJFNet在多个基准测试中超越了最先进的性能，并实现了显著的效率提升，大大降低了模型复杂性和计算开销。

Abstract: Current dark image restoration methods suffer from severe efficiency
bottlenecks, primarily stemming from: (1) computational burden and error
correction costs associated with reliance on external priors (manual or
cross-modal); (2) redundant operations in complex multi-stage enhancement
pipelines; and (3) indiscriminate processing across frequency components in
frequency-domain methods, leading to excessive global computational demands. To
address these challenges, we propose an Efficient Self-Mining Prior-Guided
Joint Frequency Enhancement Network (SPJFNet). Specifically, we first introduce
a Self-Mining Guidance Module (SMGM) that generates lightweight endogenous
guidance directly from the network, eliminating dependence on external priors
and thereby bypassing error correction overhead while improving inference
speed. Second, through meticulous analysis of different frequency domain
characteristics, we reconstruct and compress multi-level operation chains into
a single efficient operation via lossless wavelet decomposition and joint
Fourier-based advantageous frequency enhancement, significantly reducing
parameters. Building upon this foundation, we propose a Dual-Frequency Guidance
Framework (DFGF) that strategically deploys specialized high/low frequency
branches (wavelet-domain high-frequency enhancement and Fourier-domain
low-frequency restoration), decoupling frequency processing to substantially
reduce computational complexity. Rigorous evaluation across multiple benchmarks
demonstrates that SPJFNet not only surpasses state-of-the-art performance but
also achieves significant efficiency improvements, substantially reducing model
complexity and computational overhead. Code is available at
https://github.com/bywlzts/SPJFNet.

</details>


### [40] [VisualTrans: A Benchmark for Real-World Visual Transformation Reasoning](https://arxiv.org/abs/2508.04043)
*Yuheng Ji,Yipu Wang,Yuyang Liu,Xiaoshuai Hao,Yue Liu,Yuting Zhao,Huaihai Lyu,Xiaolong Zheng*

Main category: cs.CV

TL;DR: VisualTrans是第一个针对真实世界人机交互场景的视觉转换推理（VTR）基准，包含12个任务和对空间、程序、量化推理的评估。现有模型在动态和多步骤推理方面表现不佳，尤其是在中间状态识别和序列规划方面，这表明在时序建模和因果推理方面存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉转换推理（VTR）基准存在模拟到现实的差距、任务复杂度有限以及推理覆盖不完整等问题，限制了其在真实世界场景中的实际应用。为了解决这些局限性，我们需要一个更全面的基准来推动VTR技术的发展。

Method: 我们引入了一个名为VisualTrans的基准，旨在弥补现有基准在真实世界场景中的不足。该基准包含12个语义多样的操作任务，并系统地评估了空间、程序和量化三个关键的推理维度。我们构建了一个可扩展的数据创建流程，利用第一人称操作视频，结合任务选择、图像对提取、利用大型多模态模型进行自动元数据注释以及结构化问题生成。通过人工验证确保了最终基准的高质量和可解释性。

Result: 在对各种最先进的视觉语言模型进行的评估中，模型在静态空间任务方面表现强劲。然而，在动态、多步骤推理场景中，特别是在中间状态识别和变换序列规划等领域，模型表现出明显的不足。

Conclusion: 现有模型在处理动态、多步骤推理方面存在明显不足，特别是在中间状态识别和变换序列规划方面，这揭示了模型在时序建模和因果推理方面的根本性弱点，为未来研发更强大、更具泛化能力的视觉转换推理系统提供了明确的研究方向。

Abstract: Visual transformation reasoning (VTR) is a vital cognitive capability that
empowers intelligent agents to understand dynamic scenes, model causal
relationships, and predict future states, and thereby guiding actions and
laying the foundation for advanced intelligent systems. However, existing
benchmarks suffer from a sim-to-real gap, limited task complexity, and
incomplete reasoning coverage, limiting their practical use in real-world
scenarios. To address these limitations, we introduce VisualTrans, the first
comprehensive benchmark specifically designed for VTR in real-world
human-object interaction scenarios. VisualTrans encompasses 12 semantically
diverse manipulation tasks and systematically evaluates three essential
reasoning dimensions - spatial, procedural, and quantitative - through 6
well-defined subtask types. The benchmark features 472 high-quality
question-answer pairs in various formats, including multiple-choice, open-ended
counting, and target enumeration. We introduce a scalable data construction
pipeline built upon first-person manipulation videos, which integrates task
selection, image pair extraction, automated metadata annotation with large
multimodal models, and structured question generation. Human verification
ensures the final benchmark is both high-quality and interpretable. Evaluations
of various state-of-the-art vision-language models show strong performance in
static spatial tasks. However, they reveal notable shortcomings in dynamic,
multi-step reasoning scenarios, particularly in areas like intermediate state
recognition and transformation sequence planning. These findings highlight
fundamental weaknesses in temporal modeling and causal reasoning, providing
clear directions for future research aimed at developing more capable and
generalizable VTR systems. The dataset and code are available at
https://github.com/WangYipu2002/VisualTrans.

</details>


### [41] [Iterative pseudo-labeling based adaptive copy-paste supervision for semi-supervised tumor segmentation](https://arxiv.org/abs/2508.04044)
*Qiangguo Jin,Hui Cui,Junbo Wang,Changming Sun,Yimiao He,Ping Xuan,Linlin Wang,Cong Cong,Leyi Wei,Ran Su*

Main category: cs.CV

TL;DR: 本研究提出了一种名为IPA-CP的半监督学习方法，用于解决医学图像中肿瘤分割的挑战，特别是对于数量多或体积小的肿瘤。该方法通过不确定性自适应增强和迭代伪标签策略来提高性能，并在实验中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法在处理包含大量肿瘤或小体积肿瘤的挑战性场景方面存在不足，并且在标记和未标记数据方面，数据增强策略的能力尚未得到充分研究。

Method: IPA-CP框架结合了基于双向不确定性的自适应增强机制和迭代伪标签转换策略，以生成更鲁棒和信息丰富的伪标签，并将不确定性注入到自适应增强中。

Result: IPA-CP框架在内部和公开数据集上的大量实验结果表明，该方法在医学图像分割方面优于最先进的半监督学习方法。

Conclusion: 该研究提出的IPA-CP框架在肿瘤分割任务中优于最先进的半监督学习方法，并且消融实验证明了其技术贡献的有效性。

Abstract: Semi-supervised learning (SSL) has attracted considerable attention in
medical image processing. The latest SSL methods use a combination of
consistency regularization and pseudo-labeling to achieve remarkable success.
However, most existing SSL studies focus on segmenting large organs, neglecting
the challenging scenarios where there are numerous tumors or tumors of small
volume. Furthermore, the extensive capabilities of data augmentation
strategies, particularly in the context of both labeled and unlabeled data,
have yet to be thoroughly investigated. To tackle these challenges, we
introduce a straightforward yet effective approach, termed iterative
pseudo-labeling based adaptive copy-paste supervision (IPA-CP), for tumor
segmentation in CT scans. IPA-CP incorporates a two-way uncertainty based
adaptive augmentation mechanism, aiming to inject tumor uncertainties present
in the mean teacher architecture into adaptive augmentation. Additionally,
IPA-CP employs an iterative pseudo-label transition strategy to generate more
robust and informative pseudo labels for the unlabeled samples. Extensive
experiments on both in-house and public datasets show that our framework
outperforms state-of-the-art SSL methods in medical image segmentation.
Ablation study results demonstrate the effectiveness of our technical
contributions.

</details>


### [42] [Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation](https://arxiv.org/abs/2508.04049)
*Jiayi He,Xu Wang,Shengeng Tang,Yaxiong Wang,Lechao Cheng,Dan Guo*

Main category: cs.CV

TL;DR: "A new framework decouples sign language motion from signer identity, using a signer-independent motion lexicon and a synthesis stage for photorealistic videos of any signer, overcoming data requirements and generalization issues."


<details>
  <summary>Details</summary>
Motivation: "Sign language video generation requires producing natural signing motions with realistic appearances under precise semantic control, yet faces two critical challenges: excessive signer-specific data requirements and poor generalization."

Method: "We propose a new paradigm for sign language video generation that decouples motion semantics from signer identity through a two-phase synthesis framework. First, we construct a signer-independent multimodal motion lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D mesh sequences, requiring only one recording per sign. This compact representation enables our second key innovation: a discrete-to-continuous motion synthesis stage that transforms retrieved gloss sequences into temporally coherent motion trajectories, followed by identity-aware neural rendering to produce photorealistic videos of arbitrary signers."

Result: "Extensive experiments demonstrate that disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization."

Conclusion: "Disentangling motion from identity is not just viable but advantageous - enabling both high-quality synthesis and unprecedented flexibility in signer personalization."

Abstract: Sign language video generation requires producing natural signing motions
with realistic appearances under precise semantic control, yet faces two
critical challenges: excessive signer-specific data requirements and poor
generalization. We propose a new paradigm for sign language video generation
that decouples motion semantics from signer identity through a two-phase
synthesis framework. First, we construct a signer-independent multimodal motion
lexicon, where each gloss is stored as identity-agnostic pose, gesture, and 3D
mesh sequences, requiring only one recording per sign. This compact
representation enables our second key innovation: a discrete-to-continuous
motion synthesis stage that transforms retrieved gloss sequences into
temporally coherent motion trajectories, followed by identity-aware neural
rendering to produce photorealistic videos of arbitrary signers. Unlike prior
work constrained by signer-specific datasets, our method treats motion as a
first-class citizen: the learned latent pose dynamics serve as a portable
"choreography layer" that can be visually realized through different human
appearances. Extensive experiments demonstrate that disentangling motion from
identity is not just viable but advantageous - enabling both high-quality
synthesis and unprecedented flexibility in signer personalization.

</details>


### [43] [DOMR: Establishing Cross-View Segmentation via Dense Object Matching](https://arxiv.org/abs/2508.04050)
*Jitong Liao,Yulu Gao,Shaofei Huang,Jialin Gao,Jie Lei,Ronghua Liang,Si Liu*

Main category: cs.CV

TL;DR: 提出了一种名为DOMR的框架，用于解决跨视图物体对应问题。该框架通过联合建模物体间的关系，实现了密集物体匹配和掩码细化，并在Ego-Exo4D基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 跨视图物体对应是视觉理解中的一个关键但具有挑战性的任务，旨在匹配自我想象（第一人称）和异想（第三人称）视图中的物体。

Method: 提出了一种名为“密集物体匹配和细化”（DOMR）的框架，该框架以“密集物体匹配”（DOM）模块为中心，该模块能够联合对多个物体进行建模。DOM集成了建议生成模块和密集匹配模块，该模块联合编码视觉、空间和语义线索，明确构建物体间的关系以实现物体间的密集匹配。此外，将DOM与掩码细化头相结合，以提高预测掩码的完整性和准确性。

Result: 在Ego-Exo4D基准上，该方法在Ego→Exo上的平均IoU为49.7%，在Exo→Ego上的平均IoU为55.2%。

Conclusion: 该方法在Ego-Exo4D基准上取得了最先进的性能，Ego→Exo的平均IoU为49.7%，Exo→Ego的平均IoU为55.2%，分别比先前的方法提高了5.8%和4.3%，验证了该集成方法在跨视图理解方面的有效性。

Abstract: Cross-view object correspondence involves matching objects between egocentric
(first-person) and exocentric (third-person) views. It is a critical yet
challenging task for visual understanding. In this work, we propose the Dense
Object Matching and Refinement (DOMR) framework to establish dense object
correspondences across views. The framework centers around the Dense Object
Matcher (DOM) module, which jointly models multiple objects. Unlike methods
that directly match individual object masks to image features, DOM leverages
both positional and semantic relationships among objects to find
correspondences. DOM integrates a proposal generation module with a dense
matching module that jointly encodes visual, spatial, and semantic cues,
explicitly constructing inter-object relationships to achieve dense matching
among objects. Furthermore, we combine DOM with a mask refinement head designed
to improve the completeness and accuracy of the predicted masks, forming the
complete DOMR framework. Extensive evaluations on the Ego-Exo4D benchmark
demonstrate that our approach achieves state-of-the-art performance with a mean
IoU of 49.7% on Ego$\to$Exo and 55.2% on Exo$\to$Ego. These results outperform
those of previous methods by 5.8% and 4.3%, respectively, validating the
effectiveness of our integrated approach for cross-view understanding.

</details>


### [44] [Towards Globally Predictable k-Space Interpolation: A White-box Transformer Approach](https://arxiv.org/abs/2508.04051)
*Chen Luo,Qiyu Jin,Taofeng Xie,Xuemei Wang,Huayu Wang,Congcong Liu,Liming Tang,Guoqing Chen,Zhuo-Xu Cui,Dong Liang*

Main category: cs.CV

TL;DR: A new white-box Transformer (GPI-WT) for k-space interpolation in MRI leverages global dependencies and offers better interpretability than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods exploit local predictability but overlook global dependencies in k-space. Transformers can capture long-range dependencies, but their lack of interpretability raises concerns. GPI-WT aims to leverage Transformers for k-space interpolation while addressing the interpretability issue.

Method: GPI-WT is constructed by unfolding the subgradient-based optimization algorithm of the SLR model into a cascaded network, naturally inducing a learnable attention mechanism from the global annihilation filters in the SLR model.

Result: Experimental results demonstrate that GPI-WT significantly outperforms state-of-the-art approaches in k-space interpolation accuracy and provides superior interpretability.

Conclusion: GPI-WT, a white-box Transformer framework based on Globally Predictable Interpolation (GPI), significantly outperforms state-of-the-art approaches in k-space interpolation accuracy and provides superior interpretability.

Abstract: Interpolating missing data in k-space is essential for accelerating imaging.
However, existing methods, including convolutional neural network-based deep
learning, primarily exploit local predictability while overlooking the inherent
global dependencies in k-space. Recently, Transformers have demonstrated
remarkable success in natural language processing and image analysis due to
their ability to capture long-range dependencies. This inspires the use of
Transformers for k-space interpolation to better exploit its global structure.
However, their lack of interpretability raises concerns regarding the
reliability of interpolated data. To address this limitation, we propose
GPI-WT, a white-box Transformer framework based on Globally Predictable
Interpolation (GPI) for k-space. Specifically, we formulate GPI from the
perspective of annihilation as a novel k-space structured low-rank (SLR) model.
The global annihilation filters in the SLR model are treated as learnable
parameters, and the subgradients of the SLR model naturally induce a learnable
attention mechanism. By unfolding the subgradient-based optimization algorithm
of SLR into a cascaded network, we construct the first white-box Transformer
specifically designed for accelerated MRI. Experimental results demonstrate
that the proposed method significantly outperforms state-of-the-art approaches
in k-space interpolation accuracy while providing superior interpretability.

</details>


### [45] [Uni-DocDiff: A Unified Document Restoration Model Based on Diffusion](https://arxiv.org/abs/2508.04055)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Binbin Li,Xiaojun Bi,Yu Zhou*

Main category: cs.CV

TL;DR: Uni-DocDiff是一个基于扩散模型的统一文档修复方法，通过可学习的任务提示、先验池和先验融合模块解决了现有方法的局限性，实现了良好的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有文档修复方法通常将各项任务独立处理，导致系统复杂且难以扩展。近期研究试图统一多任务，但受限于手工提示、繁琐预处理和未能充分利用共享架构中的跨任务协同作用。

Method: 提出了一种基于扩散模型的统一且高度可扩展的文档修复模型Uni-DocDiff。该模型开发了可学习的任务提示设计，并设计了结合局部高频和全局低频特征的先验池（Prior Pool）以及能够自适应选择相关先验信息的先验融合模块（Prior Fusion Module, PFM）。

Result: 通过大量实验证明，Uni-DocDiff模型在各种文档修复任务上取得了与特定任务专家模型相当或更优的性能，并且能够轻松扩展以适应新任务。

Conclusion: Uni-DocDiff模型在各种文档修复任务上取得了与特定任务专家模型相当甚至更优的性能，并具备良好的任务可扩展性，能够轻松适应新任务。

Abstract: Removing various degradations from damaged documents greatly benefits
digitization, downstream document analysis, and readability. Previous methods
often treat each restoration task independently with dedicated models, leading
to a cumbersome and highly complex document processing system. Although recent
studies attempt to unify multiple tasks, they often suffer from limited
scalability due to handcrafted prompts and heavy preprocessing, and fail to
fully exploit inter-task synergy within a shared architecture. To address the
aforementioned challenges, we propose Uni-DocDiff, a Unified and highly
scalable Document restoration model based on Diffusion. Uni-DocDiff develops a
learnable task prompt design, ensuring exceptional scalability across diverse
tasks. To further enhance its multi-task capabilities and address potential
task interference, we devise a novel \textbf{Prior \textbf{P}ool}, a simple yet
comprehensive mechanism that combines both local high-frequency features and
global low-frequency features. Additionally, we design the \textbf{Prior
\textbf{F}usion \textbf{M}odule (PFM)}, which enables the model to adaptively
select the most relevant prior information for each specific task. Extensive
experiments show that the versatile Uni-DocDiff achieves performance comparable
or even superior performance compared with task-specific expert models, and
simultaneously holds the task scalability for seamless adaptation to new tasks.

</details>


### [46] [TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation](https://arxiv.org/abs/2508.04058)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: TCSAFormer透過壓縮注意力（CA）模組和雙分支前饋網路（DBFFN）模組，解決了現有 Transformer 模型在醫學影像分割中計算複雜度和特徵捕捉能力的問題，並在多個數據集上取得了優於 SOTA 方法的性能，同時降低了計算開銷。


<details>
  <summary>Details</summary>
Motivation: 為了克服 Transformer 在醫學影像分割中計算複雜度高（與輸入序列成二次方關係）以及前饋網路（FFN）模組依賴全連接層、限制了捕捉局部上下文資訊和多尺度特徵的能力這兩個主要限制，作者提出了一種高效的醫學影像分割網路 TCSAFormer。

Method: TCSAFormer採用了兩種關鍵技術：1. 壓縮注意力（CA）模組，結合了權杖壓縮和像素級稀疏注意力，動態地關注每個查詢最相關的鍵值對，透過修剪全局不相關權杖和合併冗餘權杖，顯著降低了計算複雜度，同時增強了模型捕捉權杖之間關係的能力。2. 雙分支前饋網路（DBFFN）模組，取代了標準FFN，用於捕捉局部上下文特徵和多尺度資訊，從而增強了模型的特徵表示能力。

Result: 實驗結果表明，TCSAFormer在分割性能上優於現有的最先進（SOTA）方法，同時保持較低的計算開銷，達到了效率和準確性之間的最佳權衡。

Conclusion: TCSAFormer在ISIC-2018、CVC-ClinicDB和Synapse三個公開的醫學影像分割資料集上進行了廣泛的實驗評估，實驗結果證明TCSAFormer在分割性能上優於現有的最先進（SOTA）方法，同時保持較低的計算開銷，達到了效率和準確性之間的最佳權衡。

Abstract: In recent years, transformer-based methods have achieved remarkable progress
in medical image segmentation due to their superior ability to capture
long-range dependencies. However, these methods typically suffer from two major
limitations. First, their computational complexity scales quadratically with
the input sequences. Second, the feed-forward network (FFN) modules in vanilla
Transformers typically rely on fully connected layers, which limits models'
ability to capture local contextual information and multiscale features
critical for precise semantic segmentation. To address these issues, we propose
an efficient medical image segmentation network, named TCSAFormer. The proposed
TCSAFormer adopts two key ideas. First, it incorporates a Compressed Attention
(CA) module, which combines token compression and pixel-level sparse attention
to dynamically focus on the most relevant key-value pairs for each query. This
is achieved by pruning globally irrelevant tokens and merging redundant ones,
significantly reducing computational complexity while enhancing the model's
ability to capture relationships between tokens. Second, it introduces a
Dual-Branch Feed-Forward Network (DBFFN) module as a replacement for the
standard FFN to capture local contextual features and multiscale information,
thereby strengthening the model's feature representation capability. We conduct
extensive experiments on three publicly available medical image segmentation
datasets: ISIC-2018, CVC-ClinicDB, and Synapse, to evaluate the segmentation
performance of TCSAFormer. Experimental results demonstrate that TCSAFormer
achieves superior performance compared to existing state-of-the-art (SOTA)
methods, while maintaining lower computational overhead, thus achieving an
optimal trade-off between efficiency and accuracy.

</details>


### [47] [Beyond the Visible: Benchmarking Occlusion Perception in Multimodal Large Language Models](https://arxiv.org/abs/2508.04059)
*Zhaochen Liu,Kaiwen Gao,Shuyi Liang,Bin Xiao,Limeng Qiao,Lin Ma,Tingting Jiang*

Main category: cs.CV

TL;DR: O-Bench是首个专注于遮挡感知的视觉问答基准，旨在评估和提升多模态大语言模型（MLLMs）的能力。研究发现，现有MLLMs在遮挡感知方面远不如人类，且存在特定缺陷，即使模型规模增大或采用思维过程也难以改善。该基准将有助于推动MLLMs在视觉智能方面的进步。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理遮挡感知方面能力不足，而遮挡感知是人类空间理解的关键基础，涉及视觉识别和推理的整合。因此，需要一个专门针对遮挡感知的基准来评估和推动多模态大语言模型在该领域的发展。

Method: 通过新颖的层级合成方法，基于SA-1B数据集构建了包含1,365张包含语义连贯遮挡场景的图像。在此基础上，利用可靠的半自动工作流程，为五个定制任务标注了总计4,588个问答对，形成了O-Bench基准。

Result: 对22种代表性多模态大语言模型进行评估，结果显示它们与人类基线相比存在显著的性能差距，该差距无法通过模型扩展或思维过程得到充分弥合。研究还识别出三种典型的失败模式：过于保守的偏差、脆弱的格式塔预测以及在量化任务上的挣扎。

Conclusion: O-Bench的开发旨在弥合当前多模态大语言模型（MLLMs）在遮挡感知能力方面的不足，并为评估和改进其视觉智能提供一个重要工具。研究表明，尽管模型在规模和推理能力上有所提升，但与人类相比，在遮挡感知任务上仍存在显著差距，且表现出保守偏差、脆弱的格式塔预测和量化任务困难等典型失败模式。

Abstract: Occlusion perception, a critical foundation for human-level spatial
understanding, embodies the challenge of integrating visual recognition and
reasoning. Though multimodal large language models (MLLMs) have demonstrated
remarkable capabilities, their performance on occlusion perception remains
under-explored. To address this gap, we introduce O-Bench, the first visual
question answering (VQA) benchmark specifically designed for occlusion
perception. Based on SA-1B, we construct 1,365 images featuring semantically
coherent occlusion scenarios through a novel layered synthesis approach. Upon
this foundation, we annotate 4,588 question-answer pairs in total across five
tailored tasks, employing a reliable, semi-automatic workflow. Our extensive
evaluation of 22 representative MLLMs against the human baseline reveals a
significant performance gap between current MLLMs and humans, which, we find,
cannot be sufficiently bridged by model scaling or thinking process. We further
identify three typical failure patterns, including an overly conservative bias,
a fragile gestalt prediction, and a struggle with quantitative tasks. We
believe O-Bench can not only provide a vital evaluation tool for occlusion
perception, but also inspire the development of MLLMs for better visual
intelligence. Our benchmark will be made publicly available upon paper
publication.

</details>


### [48] [TNet: Terrace Convolutional Decoder Network for Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2508.04061)
*Chengqian Dai,Yonghong Guo,Hongzhao Xiang,Yigui Luo*

Main category: cs.CV

TL;DR: TNet是一种新的遥感图像分割网络，通过卷积和加法操作有效融合多分辨率特征，性能优越且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 大多数遥感图像分割网络虽然在解码阶段引入Transformer或Mamba模块增强了特征交互，但往往只关注尺度内的关系，忽略了跨多分辨率的全局上下文依赖。

Method: 提出了一种仅使用卷积和加法运算的Terrace Convolutional Decoder Network (TNet)架构，通过在解码阶段逐步融合低分辨率（全局上下文）和高分辨率（局部细节）特征，学习空间感知的卷积核，以融合全局和局部信息。

Result: TNet-R在ISPRS Vaihingen、ISPRS Potsdam和LoveDA数据集上实现了85.35%、87.05%和52.19%的mIoU，并保持了高计算效率。

Conclusion: TNet-R在ISPRS Vaihingen、ISPRS Potsdam和LoveDA三个基准数据集上取得了有竞争力的性能，其平均交并比（mIoU）分别为85.35%、87.05%和52.19%，同时保持了高计算效率。

Abstract: In remote sensing, most segmentation networks adopt the UNet architecture,
often incorporating modules such as Transformers or Mamba to enhance
global-local feature interactions within decoder stages. However, these
enhancements typically focus on intra-scale relationships and neglect the
global contextual dependencies across multiple resolutions. To address this
limitation, we introduce the Terrace Convolutional Decoder Network (TNet), a
simple yet effective architecture that leverages only convolution and addition
operations to progressively integrate low-resolution features (rich in global
context) into higher-resolution features (rich in local details) across
decoding stages. This progressive fusion enables the model to learn
spatially-aware convolutional kernels that naturally blend global and local
information in a stage-wise manner. We implement TNet with a ResNet-18 encoder
(TNet-R) and evaluate it on three benchmark datasets. TNet-R achieves
competitive performance with a mean Intersection-over-Union (mIoU) of 85.35\%
on ISPRS Vaihingen, 87.05\% on ISPRS Potsdam, and 52.19\% on LoveDA, while
maintaining high computational efficiency. Code is publicly available.

</details>


### [49] [Bridging Diffusion Models and 3D Representations: A 3D Consistent Super-Resolution Framework](https://arxiv.org/abs/2508.04090)
*Yi-Ting Chen,Ting-Hsuan Liao,Pengsheng Guo,Alexander Schwing,Jia-Bin Huang*

Main category: cs.CV

TL;DR: 3DSR is a new 3D super-resolution method using 3D Gaussian splatting and 2D diffusion models for better 3D consistency and visual quality without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a novel 3D super-resolution framework that addresses the limitations of prior work by explicitly incorporating 3D consistency across views.

Method: 3DSR leverages off-the-shelf diffusion-based 2D super-resolution models and an explicit 3D Gaussian-splatting-based scene representation to encourage 3D consistency across views. It enhances visual quality without additional fine-tuning, ensuring spatial coherence.

Result: The proposed 3DSR method achieves high-resolution results that are visually compelling and maintain structural consistency in 3D reconstructions when evaluated on MipNeRF360 and LLFF data.

Conclusion: 3DSR can generate high-resolution and visually compelling results while maintaining structural consistency in 3D reconstructions.

Abstract: We propose 3D Super Resolution (3DSR), a novel 3D Gaussian-splatting-based
super-resolution framework that leverages off-the-shelf diffusion-based 2D
super-resolution models. 3DSR encourages 3D consistency across views via the
use of an explicit 3D Gaussian-splatting-based scene representation. This makes
the proposed 3DSR different from prior work, such as image upsampling or the
use of video super-resolution, which either don't consider 3D consistency or
aim to incorporate 3D consistency implicitly. Notably, our method enhances
visual quality without additional fine-tuning, ensuring spatial coherence
within the reconstructed scene. We evaluate 3DSR on MipNeRF360 and LLFF data,
demonstrating that it produces high-resolution results that are visually
compelling, while maintaining structural consistency in 3D reconstructions.
Code will be released.

</details>


### [50] [DET-GS: Depth- and Edge-Aware Regularization for High-Fidelity 3D Gaussian Splatting](https://arxiv.org/abs/2508.04099)
*Zexu Huang,Min Xu,Stuart Perry*

Main category: cs.CV

TL;DR: DET-GS 是一种新的 3DGS 方法，通过深度和边缘感知正则化，解决了稀疏视角下的几何重建和细节保留问题，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 3DGS 方法在稀疏视角下几何重建精度不足，且传统平滑方法会破坏边缘和纹理细节。

Method: DET-GS 提出了一种统一的深度和边缘感知正则化框架，包括分层几何深度监督、基于 Canny 边缘检测的语义掩码引导的边缘感知深度正则化，以及 RGB 引导的保持边缘的 Total Variation 损失。

Result: DET-GS 在几何准确性和视觉保真度方面均有显著提升，在稀疏视角新视角合成基准测试中表现优于 SOTA 方法。

Conclusion: DET-GS 通过引入分层几何深度监督和边缘感知深度正则化，在稀疏视角下实现了更准确的几何重建和更高视觉保真度，优于现有技术。

Abstract: 3D Gaussian Splatting (3DGS) represents a significant advancement in the
field of efficient and high-fidelity novel view synthesis. Despite recent
progress, achieving accurate geometric reconstruction under sparse-view
conditions remains a fundamental challenge. Existing methods often rely on
non-local depth regularization, which fails to capture fine-grained structures
and is highly sensitive to depth estimation noise. Furthermore, traditional
smoothing methods neglect semantic boundaries and indiscriminately degrade
essential edges and textures, consequently limiting the overall quality of
reconstruction. In this work, we propose DET-GS, a unified depth and edge-aware
regularization framework for 3D Gaussian Splatting. DET-GS introduces a
hierarchical geometric depth supervision framework that adaptively enforces
multi-level geometric consistency, significantly enhancing structural fidelity
and robustness against depth estimation noise. To preserve scene boundaries, we
design an edge-aware depth regularization guided by semantic masks derived from
Canny edge detection. Furthermore, we introduce an RGB-guided edge-preserving
Total Variation loss that selectively smooths homogeneous regions while
rigorously retaining high-frequency details and textures. Extensive experiments
demonstrate that DET-GS achieves substantial improvements in both geometric
accuracy and visual fidelity, outperforming state-of-the-art (SOTA) methods on
sparse-view novel view synthesis benchmarks.

</details>


### [51] [NEARL-CLIP: Interacted Query Adaptation with Orthogonal Regularization for Medical Vision-Language Understanding](https://arxiv.org/abs/2508.04101)
*Zelin Peng,Yichen Zhao,Yu Huang,Piao Yang,Feilong Tang,Zhengqin Xu,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: NEARL-CLIP 是一个新颖的、参数高效的视觉语言模型框架，通过 USEformer 和 OCA 组件促进医学图像和文本的跨模态交互，克服了领域鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型 (VLMs) 在医学图像分析领域面临领域鸿沟的挑战，传统的弥合方法（如提示学习、单向模态交互）往往只关注单一模态，可能导致模态失调，未能充分发挥 VLM 的潜力。

Method: 提出了一种名为 NEARL-CLIP 的框架，其中包含两个核心组件：1. 统一协同嵌入 Transformer (USEformer)，用于动态生成跨模态查询，促进模态间的交互和知识融合。2. 正交交叉注意力适配器 (OCA)，利用正交技术将新知识解耦为新信息和增量知识，从而实现更专注的新信息获取，进一步促进模态交互和发挥 VLM 能力。

Result: NEARL-CLIP 框架引入了约 1.46M 的可学习参数，实现了参数高效性，并有效促进了模态间的相互丰富和增强，从而提升了 VLM 在医学图像分析中的能力。

Conclusion: NEARL-CLIP 通过引入参数高效的 USEformer 和 OCA，实现了跨模态查询的动态生成和新知识的解耦，有效弥合了视觉语言模型在医学图像分析中的领域鸿沟，并在保持参数效率的同时，提升了模型性能。

Abstract: Computer-aided medical image analysis is crucial for disease diagnosis and
treatment planning, yet limited annotated datasets restrict medical-specific
model development. While vision-language models (VLMs) like CLIP offer strong
generalization capabilities, their direct application to medical imaging
analysis is impeded by a significant domain gap. Existing approaches to bridge
this gap, including prompt learning and one-way modality interaction
techniques, typically focus on introducing domain knowledge to a single
modality. Although this may offer performance gains, it often causes modality
misalignment, thereby failing to unlock the full potential of VLMs. In this
paper, we propose \textbf{NEARL-CLIP} (i\underline{N}teracted qu\underline{E}ry
\underline{A}daptation with o\underline{R}thogona\underline{L} Regularization),
a novel cross-modality interaction VLM-based framework that contains two
contributions: (1) Unified Synergy Embedding Transformer (USEformer), which
dynamically generates cross-modality queries to promote interaction between
modalities, thus fostering the mutual enrichment and enhancement of multi-modal
medical domain knowledge; (2) Orthogonal Cross-Attention Adapter (OCA). OCA
introduces an orthogonality technique to decouple the new knowledge from
USEformer into two distinct components: the truly novel information and the
incremental knowledge. By isolating the learning process from the interference
of incremental knowledge, OCA enables a more focused acquisition of new
information, thereby further facilitating modality interaction and unleashing
the capability of VLMs. Notably, NEARL-CLIP achieves these two contributions in
a parameter-efficient style, which only introduces \textbf{1.46M} learnable
parameters.

</details>


### [52] [AR as an Evaluation Playground: Bridging Metrics and Visual Perception of Computer Vision Models](https://arxiv.org/abs/2508.04102)
*Ashkan Ganj,Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: ARCADE是一个利用AR技术简化和改进计算机视觉模型人类感知研究的评估平台，它支持跨平台数据收集、自定义实验和AR流，已被证明是灵活且有效的。


<details>
  <summary>Details</summary>
Motivation: 人类感知研究可以为理解计算机视觉模型性能提供有价值的见解，但传统方法的设置复杂、耗时且难以扩展。AR技术为简化和改进这些研究提供了独特的机遇。

Method: ARCADE平台，一个利用AR进行计算机视觉模型感知研究的评估平台。

Result: ARCADE平台能够有效地收集人类对深度和光照估计模型质量的感知判断。该系统在不同部署和研究设置下的可用性和性能也得到了评估，证明了其灵活性和有效性。

Conclusion: ARCADE平台通过利用AR的丰富上下文和交互性，为CV研究人员提供了一个易于使用的平台，用于进行以人为中心的CV评估。该平台支持跨平台AR数据收集、通过可插拔模型推理进行自定义实验协议以及用于用户研究的AR流。ARCADE已被证明可有效用于收集人类对深度和光照估计模型质量的感知判断，并展示了其在不同部署和研究设置下的可用性和性能，突显了其作为以人为中心的评估平台的灵活性和有效性。

Abstract: Human perception studies can provide complementary insights to qualitative
evaluation for understanding computer vision (CV) model performance. However,
conducting human perception studies remains a non-trivial task, it often
requires complex, end-to-end system setups that are time-consuming and
difficult to scale. In this paper, we explore the unique opportunity presented
by augmented reality (AR) for helping CV researchers to conduct perceptual
studies. We design ARCADE, an evaluation platform that allows researchers to
easily leverage AR's rich context and interactivity for human-centered CV
evaluation. Specifically, ARCADE supports cross-platform AR data collection,
custom experiment protocols via pluggable model inference, and AR streaming for
user studies. We demonstrate ARCADE using two types of CV models, depth and
lighting estimation and show that AR tasks can be effectively used to elicit
human perceptual judgments of model quality. We also evaluate the systems
usability and performance across different deployment and study settings,
highlighting its flexibility and effectiveness as a human-centered evaluation
platform.

</details>


### [53] [Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decode](https://arxiv.org/abs/2508.04107)
*Jingchao Wang,Zhijian Wu,Dingjiang Huang,Yefeng Zheng,Hong Wang*

Main category: cs.CV

TL;DR: MLLMSeg is a new framework for Reference Expression Segmentation (RES) that balances performance and cost by leveraging MLLM vision encoder features and a novel fusion module (DSFF). It outperforms existing methods without relying on SAM or sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing Reference Expression Segmentation (RES) methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) or adopt SAM-free lightweight pipelines that sacrifice accuracy. The goal is to address the trade-off between performance and cost.

Method: The proposed MLLMSeg framework fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. It incorporates a detail-enhanced and semantic-consistent feature fusion module (DSFF) that integrates detail-related visual features with semantic-related features from the MLLM's LLM. A lightweight mask decoder with 34M network parameters is used for precise mask prediction.

Result: Extensive experiments demonstrate that MLLMSeg generally surpasses both SAM-based and SAM-free competitors, achieving a better balance between performance and cost.

Conclusion: MLLMSeg method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost.

Abstract: Reference Expression Segmentation (RES) aims to segment image regions
specified by referring expressions and has become popular with the rise of
multimodal large models (MLLMs). While MLLMs excel in semantic understanding,
their token-generation paradigm struggles with pixel-level dense prediction.
Existing RES methods either couple MLLMs with the parameter-heavy Segment
Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight
pipelines that sacrifice accuracy. To address the trade-off between performance
and cost, we specifically propose MLLMSeg, a novel framework that fully
exploits the inherent visual detail features encoded in the MLLM vision encoder
without introducing an extra visual encoder. Besides, we propose a
detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully
integrates the detail-related visual feature with the semantic-related feature
output by the large language model (LLM) of MLLM. Finally, we establish a
light-weight mask decoder with only 34M network parameters that optimally
leverages detailed spatial features from the visual encoder and semantic
features from the LLM to achieve precise mask prediction. Extensive experiments
demonstrate that our method generally surpasses both SAM-based and SAM-free
competitors, striking a better balance between performance and cost. Code is
available at https://github.com/jcwang0602/MLLMSeg.

</details>


### [54] [CLIPVehicle: A Unified Framework for Vision-based Vehicle Search](https://arxiv.org/abs/2508.04120)
*Likai Wang,Ruize Han,Xiangqun Zhang,Wei Feng*

Main category: cs.CV

TL;DR: CLIPVehicle提出了一种新颖的统一框架，通过双粒度语义区域对齐和多级身份学习，实现了车辆检测和Re-ID的联合优化，解决了现有方法的资源消耗和实用性问题，并在多个基准数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预先检测和存储所有车辆图像块，然后应用车辆Re-ID模型，这既耗费资源又不够实用。本文旨在实现车辆搜索的联合检测和Re-ID。然而，检测（关注车辆的共性）和Re-ID（关注车辆的独特性）之间的冲突目标使得模型难以在端到端系统中进行学习。

Method: 提出了一种名为CLIPVehicle的新型统一框架，该框架包含一个双粒度语义区域对齐模块，利用视觉-语言模型（VLMs）进行车辆识别建模，并采用多级车辆识别学习策略从全局、实例和特征层面学习身份表示。

Result: CLIPVehicle在CityFlowVS、SynVS-Day和SynVS-All数据集上进行了广泛的实验评估，结果表明该方法在车辆搜索任务上取得了优于最先进方法的性能。

Conclusion: 该方法在车辆搜索任务上优于现有的最先进方法，包括车辆Re-ID和行人搜索任务。

Abstract: Vehicles, as one of the most common and significant objects in the real
world, the researches on which using computer vision technologies have made
remarkable progress, such as vehicle detection, vehicle re-identification, etc.
To search an interested vehicle from the surveillance videos, existing methods
first pre-detect and store all vehicle patches, and then apply vehicle
re-identification models, which is resource-intensive and not very practical.
In this work, we aim to achieve the joint detection and re-identification for
vehicle search. However, the conflicting objectives between detection that
focuses on shared vehicle commonness and re-identification that focuses on
individual vehicle uniqueness make it challenging for a model to learn in an
end-to-end system. For this problem, we propose a new unified framework, namely
CLIPVehicle, which contains a dual-granularity semantic-region alignment module
to leverage the VLMs (Vision-Language Models) for vehicle discrimination
modeling, and a multi-level vehicle identification learning strategy to learn
the identity representation from global, instance and feature levels. We also
construct a new benchmark, including a real-world dataset CityFlowVS, and two
synthetic datasets SynVS-Day and SynVS-All, for vehicle search. Extensive
experimental results demonstrate that our method outperforms the
state-of-the-art methods of both vehicle Re-ID and person search tasks.

</details>


### [55] [Conditional Latent Diffusion Models for Zero-Shot Instance Segmentation](https://arxiv.org/abs/2508.04122)
*Maximilian Ulmer,Wout Boerdijk,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: OC-DiT 是一种新颖的面向对象的扩散模型，用于零样本实例分割。它在不重新训练目标数据的情况下，在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现面向对象的预测，并将其应用于零样本实例分割。

Method: 提出了一种条件潜在扩散框架，通过在扩散模型的潜在空间中对对象模板和图像特征进行条件化来生成实例掩码。OC-DiT 包含两个模型变体：一个用于生成初始对象实例提议的粗略模型，以及一个并行细化所有提议的细化模型。

Result: OC-DiT 能够有效地将对象实例解缠，其扩散过程由视觉对象描述符和局部图像线索指导。通过全面的消融研究，证明了扩散模型在实例分割任务中的潜力。

Conclusion: OC-DiT 在多个具有挑战性的真实世界基准上实现了最先进的性能，并且无需在目标数据上进行重新训练。

Abstract: This paper presents OC-DiT, a novel class of diffusion models designed for
object-centric prediction, and applies it to zero-shot instance segmentation.
We propose a conditional latent diffusion framework that generates instance
masks by conditioning the generative process on object templates and image
features within the diffusion model's latent space. This allows our model to
effectively disentangle object instances through the diffusion process, which
is guided by visual object descriptors and localized image cues. Specifically,
we introduce two model variants: a coarse model for generating initial object
instance proposals, and a refinement model that refines all proposals in
parallel. We train these models on a newly created, large-scale synthetic
dataset comprising thousands of high-quality object meshes. Remarkably, our
model achieves state-of-the-art performance on multiple challenging real-world
benchmarks, without requiring any retraining on target data. Through
comprehensive ablation studies, we demonstrate the potential of diffusion
models for instance segmentation tasks.

</details>


### [56] [Excavate the potential of Single-Scale Features: A Decomposition Network for Water-Related Optical Image Enhancement](https://arxiv.org/abs/2508.04123)
*Zheng Cheng,Wenri Wang,Guangyong Chen,Yakun Ju,Yihua Cheng,Zhisong Liu,Yanda Meng,Jintao Song*

Main category: cs.CV

TL;DR: 通过提出SSD-Net，证明了单尺度特征提取在水下图像增强任务中的有效性，实现了与多尺度方法相当的性能，并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前主流的水下图像增强（UIE）技术主要依赖多尺度特征提取（MSFE）机制来提高重建质量。然而，本研究通过大量实验证明，高质量的图像重建并不一定需要多尺度特征融合，单尺度特征提取足以匹配甚至超越多尺度方法，并且能显著降低模型复杂度。因此，为了全面探索单尺度特征在水下增强中的潜力，提出SSD-Net。

Method: 本研究提出了一种新颖的单尺度分解网络（SSD-Net），其核心在于引入不对称分解机制，将输入图像分离为包含场景内在信息以及介质引起的干扰的层。该网络结合了卷积神经网络（CNN）的局部特征提取能力和Transformer的全局建模能力，具体通过并行特征分解块（PFDB）和双向特征通信块（BFCB）实现。PFDB利用高效的注意力操作和自适应稀疏Transformer实现双分支特征空间解耦；BFCB则通过跨层残差交互来实现互补特征挖掘与融合。

Result: SSD-Net成功地实现了高质量的水下图像增强，其性能与现有主流的多尺度方法相当，甚至在某些方面有所超越，同时在计算效率上具有明显优势。

Conclusion: 本研究提出的单尺度分解网络（SSD-Net）在水下图像增强任务中展现出与多尺度方法相媲美甚至超越的性能，同时显著降低了计算复杂度。

Abstract: Underwater image enhancement (UIE) techniques aim to improve visual quality
of images captured in aquatic environments by addressing degradation issues
caused by light absorption and scattering effects, including color distortion,
blurring, and low contrast. Current mainstream solutions predominantly employ
multi-scale feature extraction (MSFE) mechanisms to enhance reconstruction
quality through multi-resolution feature fusion. However, our extensive
experiments demonstrate that high-quality image reconstruction does not
necessarily rely on multi-scale feature fusion. Contrary to popular belief, our
experiments show that single-scale feature extraction alone can match or
surpass the performance of multi-scale methods, significantly reducing
complexity. To comprehensively explore single-scale feature potential in
underwater enhancement, we propose an innovative Single-Scale Decomposition
Network (SSD-Net). This architecture introduces an asymmetrical decomposition
mechanism that disentangles input image into clean layer along with degradation
layer. The former contains scene-intrinsic information and the latter encodes
medium-induced interference. It uniquely combines CNN's local feature
extraction capabilities with Transformer's global modeling strengths through
two core modules: 1) Parallel Feature Decomposition Block (PFDB), implementing
dual-branch feature space decoupling via efficient attention operations and
adaptive sparse transformer; 2) Bidirectional Feature Communication Block
(BFCB), enabling cross-layer residual interactions for complementary feature
mining and fusion. This synergistic design preserves feature decomposition
independence while establishing dynamic cross-layer information pathways,
effectively enhancing degradation decoupling capacity.

</details>


### [57] [SVC 2025: the First Multimodal Deception Detection Challenge](https://arxiv.org/abs/2508.04129)
*Xun Lin,Xiaobao Guo,Taorui Wang,Yingjie Ma,Jiajian Huang,Jiayu Zhang,Junzhe Cao,Zitong Yu*

Main category: cs.CV

TL;DR: SVC 2025挑战赛旨在通过多模态数据（音频、视频、文本）提高欺骗检测的跨域泛化能力，以应对数据域转移带来的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习在欺骗检测方法在跨域泛化时性能会显著下降，因此需要新的基准来解决这个问题。

Method: 本研究提出了SVC 2025多模态欺骗检测挑战赛，一个用于评估音频-视觉欺骗检测跨域泛化能力的新基准。参赛者被要求开发能够处理多源异构数据集（包括音频、视频和文本）的模型，以捕捉细微的欺骗线索。

Result: 21支队伍提交了最终结果，该挑战赛鼓励开发更具适应性、可解释性和可实际部署的欺骗检测系统。

Conclusion: 本次SVC 2025多模态欺骗检测挑战赛旨在推动跨域泛化能力，21支队伍提交了最终结果，以期在多变的环境中提升欺骗检测系统的适应性和鲁棒性。

Abstract: Deception detection is a critical task in real-world applications such as
security screening, fraud prevention, and credibility assessment. While deep
learning methods have shown promise in surpassing human-level performance,
their effectiveness often depends on the availability of high-quality and
diverse deception samples. Existing research predominantly focuses on
single-domain scenarios, overlooking the significant performance degradation
caused by domain shifts. To address this gap, we present the SVC 2025
Multimodal Deception Detection Challenge, a new benchmark designed to evaluate
cross-domain generalization in audio-visual deception detection. Participants
are required to develop models that not only perform well within individual
domains but also generalize across multiple heterogeneous datasets. By
leveraging multimodal data, including audio, video, and text, this challenge
encourages the design of models capable of capturing subtle and implicit
deceptive cues. Through this benchmark, we aim to foster the development of
more adaptable, explainable, and practically deployable deception detection
systems, advancing the broader field of multimodal learning. By the conclusion
of the workshop competition, a total of 21 teams had submitted their final
results. https://sites.google.com/view/svc-mm25 for more information.

</details>


### [58] [DS$^2$Net: Detail-Semantic Deep Supervision Network for Medical Image Segmentation](https://arxiv.org/abs/2508.04131)
*Zhaohong Huang,Yuxin Zhang,Mingbao Lin,Taojian Zhou,Guorong Cai,Rongrong Ji*

Main category: cs.CV

TL;DR: The DS$^2$Net enhances medical image segmentation by jointly supervising detailed and semantic features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Deep Supervision Networks in medical imaging either focus on coarse-grained semantic features or fine-grained detailed features in isolation, neglecting their crucial relationship. This paper advocates for the power of complementary feature supervision.

Method: DS$^2$Net utilizes a novel multi-view deep supervision approach, incorporating a Detail Enhance Module (DEM) and a Semantic Enhance Module (SEM) to harness both low-level detailed and high-level semantic features. It also features an uncertainty-based supervision loss to adaptively assign supervision strength based on feature uncertainty.

Result: The proposed DS$^2$Net demonstrates superior performance compared to state-of-the-art methods in medical image analysis.

Conclusion: DS$^2$Net consistently outperforms state-of-the-art methods for medical image analysis across six benchmarks from colonoscopy, ultrasound, and microscope imaging.

Abstract: Deep Supervision Networks exhibit significant efficacy for the medical
imaging community. Nevertheless, existing work merely supervises either the
coarse-grained semantic features or fine-grained detailed features in
isolation, which compromises the fact that these two types of features hold
vital relationships in medical image analysis. We advocate the powers of
complementary feature supervision for medical image segmentation, by proposing
a Detail-Semantic Deep Supervision Network (DS$^2$Net). DS$^2$Net navigates
both low-level detailed and high-level semantic feature supervision through
Detail Enhance Module (DEM) and Semantic Enhance Module (SEM). DEM and SEM
respectively harness low-level and high-level feature maps to create detail and
semantic masks for enhancing feature supervision. This is a novel shift from
single-view deep supervision to multi-view deep supervision. DS$^2$Net is also
equipped with a novel uncertainty-based supervision loss that adaptively
assigns the supervision strength of features within distinct scales based on
their uncertainty, thus circumventing the sub-optimal heuristic design that
typifies previous works. Through extensive experiments on six benchmarks
captured under either colonoscopy, ultrasound and microscope, we demonstrate
that DS$^2$Net consistently outperforms state-of-the-art methods for medical
image analysis.

</details>


### [59] [UniFGVC: Universal Training-Free Few-Shot Fine-Grained Vision Classification via Attribute-Aware Multimodal Retrieval](https://arxiv.org/abs/2508.04136)
*Hongyu Guo,Kuan Zhu,Xiangzhao Hao,Haiyun Guo,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出UniFGVC，一个无需微调的框架，将少样本FGVC转化为多模态检索。通过CDV-Captioner生成细粒度描述，并利用预训练的编码器进行检索，在多个基准测试中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本细粒度视觉分类（FGVC）方法主要通过微调预训练的视觉语言模型，但存在过拟合和泛化能力弱的问题。为了解决这些挑战，提出了一种名为UniFGVC的通用、无需微调的框架。

Method: UniFGVC框架将少样本FGVC重新定义为多模态检索问题。首先，利用CDV-Captioner（结合链式思考提示和相似参考图像）为每个图像生成文本描述，构建图像-描述对，并利用少样本样本构建多模态类别模板。然后，使用现成的视觉和文本编码器嵌入查询和模板对，通过在联合空间中检索最近的模板来完成FGVC。

Result: UniFGVC在12个FGVC基准测试中取得了持续的优越性能，超过了之前所有少样本CLIP方法，甚至优于一些全监督的MLLMs方法，展示了其在少样本FGVC场景下的可靠泛化能力和适应性。

Conclusion: UniFGVC框架通过将少样本细粒度视觉分类（FGVC）重新构建为多模态检索任务，并引入类别判别性视觉字幕生成器（CDV-Captioner）来生成结构化的文本描述，利用了多模态大语言模型（MLLMs）的开放世界知识，并结合了链式思考提示和视觉相似参考图像来减少幻觉和增强区分度。该框架兼容多种MLLMs和编码器，在12个FGVC基准测试中表现优于现有的少样本CLIP方法和一些全监督MLLMs方法。

Abstract: Few-shot fine-grained visual classification (FGVC) aims to leverage limited
data to enable models to discriminate subtly distinct categories. Recent works
mostly finetuned the pre-trained visual language models to achieve performance
gain, yet suffering from overfitting and weak generalization. To deal with
this, we introduce UniFGVC, a universal training-free framework that
reformulates few-shot FGVC as multimodal retrieval. First, we propose the
Category-Discriminative Visual Captioner (CDV-Captioner) to exploit the
open-world knowledge of multimodal large language models (MLLMs) to generate a
structured text description that captures the fine-grained attribute features
distinguishing closely related classes. CDV-Captioner uses chain-of-thought
prompting and visually similar reference images to reduce hallucination and
enhance discrimination of generated captions. Using it we can convert each
image into an image-description pair, enabling more comprehensive feature
representation, and construct the multimodal category templates using few-shot
samples for the subsequent retrieval pipeline. Then, off-the-shelf vision and
text encoders embed query and template pairs, and FGVC is accomplished by
retrieving the nearest template in the joint space. UniFGVC ensures broad
compatibility with diverse MLLMs and encoders, offering reliable generalization
and adaptability across few-shot FGVC scenarios. Extensive experiments on 12
FGVC benchmarks demonstrate its consistent superiority over prior few-shot
CLIP-based methods and even several fully-supervised MLLMs-based approaches.

</details>


### [60] [IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control](https://arxiv.org/abs/2508.04147)
*Lijuan Liu,Wenfa Li,Dongbo Zhang,Shuo Wang,Shaohui Jiao*

Main category: cs.CV

TL;DR: IDC-Net通过联合学习框架和几何感知Transformer，实现了相机轨迹可控的RGB-D视频生成，并提高了几何一致性，可直接用于3D重建。


<details>
  <summary>Details</summary>
Motivation: 为了在显式相机轨迹控制下生成RGB-D视频序列，并解决现有方法单独处理RGB和深度生成导致几何不一致的问题。

Method: IDC-Net是一个新颖的框架，在统一的几何感知扩散模型中联合生成RGB图像和深度图，并引入了几何感知Transformer模块以实现细粒度的相机控制。

Result: IDC-Net在生成的场景序列的视觉质量和几何一致性方面均优于最先进的方法，并且生成的RGB-D序列可直接用于下游3D场景重建任务。

Conclusion: IDC-Net能够生成具有高视觉质量和几何一致性的RGB-D视频序列，并且可以直接用于下游的3D场景重建任务，无需额外的后处理步骤。

Abstract: We present IDC-Net (Image-Depth Consistency Network), a novel framework
designed to generate RGB-D video sequences under explicit camera trajectory
control. Unlike approaches that treat RGB and depth generation separately,
IDC-Net jointly synthesizes both RGB images and corresponding depth maps within
a unified geometry-aware diffusion model. The joint learning framework
strengthens spatial and geometric alignment across frames, enabling more
precise camera control in the generated sequences. To support the training of
this camera-conditioned model and ensure high geometric fidelity, we construct
a camera-image-depth consistent dataset with metric-aligned RGB videos, depth
maps, and accurate camera poses, which provides precise geometric supervision
with notably improved inter-frame geometric consistency. Moreover, we introduce
a geometry-aware transformer block that enables fine-grained camera control,
enhancing control over the generated sequences. Extensive experiments show that
IDC-Net achieves improvements over state-of-the-art approaches in both visual
quality and geometric consistency of generated scene sequences. Notably, the
generated RGB-D sequences can be directly feed for downstream 3D Scene
reconstruction tasks without extra post-processing steps, showcasing the
practical benefits of our joint learning framework. See more at
https://idcnet-scene.github.io.

</details>


### [61] [ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation](https://arxiv.org/abs/2508.04153)
*Yihua Shao,Xiaofeng Lin,Xinwei Long,Siyu Chen,Minxi Yan,Yang Liu,Ziyang Yan,Ao Ma,Hao Tang,Jingcai Guo*

Main category: cs.CV

TL;DR: ICM-Fusion 通过元学习和上下文自适应来解决 LoRA 融合中的冲突和遗忘问题，能在少样本场景下提升多任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练 LoRA 融合方法在融合不同任务时存在权重冲突和灾难性遗忘的问题，而增量学习在少样本场景下泛化能力不足，并且在处理长尾分布数据时可能导致融合权重遗忘。

Method: 提出了一种名为 ICM-Fusion 的新框架，结合了元学习和上下文自适应，并通过任务向量算术来动态平衡跨域冲突的优化方向，利用学习到的流形投影来调整任务向量的方向，最后通过 F-VAE 重构融合 LoRA。

Result: ICM-Fusion 在视觉和语言任务上的广泛实验证明，该方法适用于多种架构模型和各种任务，能够显著减少多任务损失，并在少样本场景下实现任务增强。

Conclusion: ICM-Fusion 通过学习到的流形投影动态平衡跨域冲突的优化方向，从而在潜在空间中获得融合模型的最佳任务向量方向，并使用 F-VAE 重构融合 LoRA 以实现多任务 LoRA 生成。实验结果表明，ICM-Fusion 适用于多种架构模型和各种任务，与现有的预训练 LoRA 融合方法相比，能显著减少多任务损失，并在少样本场景下实现任务增强。

Abstract: Enabling multi-task adaptation in pre-trained Low-Rank Adaptation (LoRA)
models is crucial for enhancing their generalization capabilities. Most
existing pre-trained LoRA fusion methods decompose weight matrices, sharing
similar parameters while merging divergent ones. However, this paradigm
inevitably induces inter-weight conflicts and leads to catastrophic domain
forgetting. While incremental learning enables adaptation to multiple tasks, it
struggles to achieve generalization in few-shot scenarios. Consequently, when
the weight data follows a long-tailed distribution, it can lead to forgetting
in the fused weights. To address this issue, we propose In-Context Meta LoRA
Fusion (ICM-Fusion), a novel framework that synergizes meta-learning with
in-context adaptation. The key innovation lies in our task vector arithmetic,
which dynamically balances conflicting optimization directions across domains
through learned manifold projections. ICM-Fusion obtains the optimal task
vector orientation for the fused model in the latent space by adjusting the
orientation of the task vectors. Subsequently, the fused LoRA is reconstructed
by a self-designed Fusion VAE (F-VAE) to realize multi-task LoRA generation. We
have conducted extensive experiments on visual and linguistic tasks, and the
experimental results demonstrate that ICM-Fusion can be adapted to a wide range
of architectural models and applied to various tasks. Compared to the current
pre-trained LoRA fusion method, ICM-Fusion fused LoRA can significantly reduce
the multi-tasking loss and can even achieve task enhancement in few-shot
scenarios.

</details>


### [62] [Audio-Assisted Face Video Restoration with Temporal and Identity Complementary Learning](https://arxiv.org/abs/2508.04161)
*Yuqin Cao,Yixuan Gao,Wei Sun,Xiaohong Liu,Yulun Zhang,Xiongkuo Min*

Main category: cs.CV

TL;DR: 提出了一种名为GAVN（General Audio-assisted face Video restoration Network）的通用音频辅助人脸视频恢复网络，通过身份和时间互补学习来解决各种类型的流媒体视频失真。


<details>
  <summary>Details</summary>
Motivation: 大多数人脸视频恢复方法忽略了视觉和音频特征之间的内在相关性，尤其是在口部区域。一些改进的音频辅助人脸视频恢复方法已被提出，但它们仅关注压缩伪影去除。

Method: GAVN首先在低分辨率空间捕获帧间时间特征以粗略恢复帧并节省计算成本。然后，GAVN在音频信号和面部地标的辅助下，在高分辨率空间提取帧内身份特征，以恢复更多面部细节。最后，重建模块整合了时间特征和身份特征，以生成高质量的人脸视频。

Result: 实验结果表明，GAVN在人脸视频压缩伪影去除、去模糊和超分辨率方面优于现有最先进的方法。

Conclusion: GAVN在人脸视频的压缩伪影去除、去模糊和超分辨率方面优于现有最先进的方法。

Abstract: Face videos accompanied by audio have become integral to our daily lives,
while they often suffer from complex degradations. Most face video restoration
methods neglect the intrinsic correlations between the visual and audio
features, especially in mouth regions. A few audio-aided face video restoration
methods have been proposed, but they only focus on compression artifact
removal. In this paper, we propose a General Audio-assisted face Video
restoration Network (GAVN) to address various types of streaming video
distortions via identity and temporal complementary learning. Specifically,
GAVN first captures inter-frame temporal features in the low-resolution space
to restore frames coarsely and save computational cost. Then, GAVN extracts
intra-frame identity features in the high-resolution space with the assistance
of audio signals and face landmarks to restore more facial details. Finally,
the reconstruction module integrates temporal features and identity features to
generate high-quality face videos. Experimental results demonstrate that GAVN
outperforms the existing state-of-the-art methods on face video compression
artifact removal, deblurring, and super-resolution. Codes will be released upon
publication.

</details>


### [63] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 本研究发布了一个包含 6,300 个带标注的迷因（包括毒性、仇恨、危险或冒犯性）的数据集，并提出了一个标签生成模块，以提高在线内容的审核效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决因数据可及性限制和数据集创建成本高而阻碍强大的迷因审核系统开发的问题，本研究旨在通过创建和利用一个包含丰富元数据的数据集来解决这一挑战。

Method: 本研究引入了一个包含 6,300 个真实世界基于迷因的帖子的数据集，并对其进行了两阶段标注：(i) 二元分类为有毒和正常，(ii) 将有毒迷因进行细粒度标注为仇恨性、危险性或冒犯性。此外，研究提出了一个标签生成模块，用于生成社会性标签，因为许多在线迷因通常没有标签。

Result: 实验结果表明，引入这些标签可以显著提高最先进的视觉语言模型（VLMs）在检测任务中的性能。

Conclusion: 这项工作为改进多模态在线环境中的内容审核提供了新颖且可扩展的基础。

Abstract: The 2025 Global Risks Report identifies state-based armed conflict and
societal polarisation among the most pressing global threats, with social media
playing a central role in amplifying toxic discourse. Memes, as a widely used
mode of online communication, often serve as vehicles for spreading harmful
content. However, limitations in data accessibility and the high cost of
dataset curation hinder the development of robust meme moderation systems. To
address this challenge, in this work, we introduce a first-of-its-kind dataset
of 6,300 real-world meme-based posts annotated in two stages: (i) binary
classification into toxic and normal, and (ii) fine-grained labelling of toxic
memes as hateful, dangerous, or offensive. A key feature of this dataset is
that it is enriched with auxiliary metadata of socially relevant tags,
enhancing the context of each meme. In addition, we propose a tag generation
module that produces socially grounded tags, because most in-the-wild memes
often do not come with tags. Experimental results show that incorporating these
tags substantially enhances the performance of state-of-the-art VLMs detection
tasks. Our contributions offer a novel and scalable foundation for improved
content moderation in multimodal online environments.

</details>


### [64] [AD-FM: Multimodal LLMs for Anomaly Detection via Multi-Stage Reasoning and Fine-Grained Reward Optimization](https://arxiv.org/abs/2508.04175)
*Jingyi Liao,Yongyi Su,Rong-Cheng Tu,Zhao Jin,Wenhao Sun,Yiting Li,Dacheng Tao,Xun Xu,Xulei Yang*

Main category: cs.CV

TL;DR: 通过多阶段审议推理和细粒度奖励，解决了多模态大语言模型在异常检测中的域适应问题，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的方法在利用模型统一响应时的训练数据以及监督模型进行审议分析而非即时二元决策方面存在局限性。多模态大语言模型在专业异常检测中的域适应性受到挑战。

Method: 提出了一种包含多阶段审议推理过程和细粒度奖励机制的框架。多阶段审议推理过程引导模型从区域识别到聚焦检查，产生多样化的响应模式以优化GRPO，并为分析工作流提供结构化监督。细粒度奖励机制结合了分类准确率和定位监督，将二元反馈转化为区分真正分析洞察和偶然正确性的连续信号。

Result: 在多个工业数据集上的综合评估表明，该方法在将通用的视觉-语言模型适应于专业异常检测方面取得了显著的性能提升，实现了更高的准确率和对现有标注的高效适应。

Conclusion: 该方法通过多阶段审议推理过程和细粒度奖励机制，有效解决了多模态大语言模型在专业异常检测中的域适应挑战，并在多个工业数据集上取得了显著的性能提升，实现了高精度和高效适应。特别是，该方法能够区分真正的分析洞察和偶然的正确性。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities across diverse domains, their application to specialized anomaly
detection (AD) remains constrained by domain adaptation challenges. Existing
Group Relative Policy Optimization (GRPO) based approaches suffer from two
critical limitations: inadequate training data utilization when models produce
uniform responses, and insufficient supervision over reasoning processes that
encourage immediate binary decisions without deliberative analysis. We propose
a comprehensive framework addressing these limitations through two synergistic
innovations. First, we introduce a multi-stage deliberative reasoning process
that guides models from region identification to focused examination,
generating diverse response patterns essential for GRPO optimization while
enabling structured supervision over analytical workflows. Second, we develop a
fine-grained reward mechanism incorporating classification accuracy and
localization supervision, transforming binary feedback into continuous signals
that distinguish genuine analytical insight from spurious correctness.
Comprehensive evaluation across multiple industrial datasets demonstrates
substantial performance improvements in adapting general vision-language models
to specialized anomaly detection. Our method achieves superior accuracy with
efficient adaptation of existing annotations, effectively bridging the gap
between general-purpose MLLM capabilities and the fine-grained visual
discrimination required for detecting subtle manufacturing defects and
structural irregularities.

</details>


### [65] [Uncertainty-Aware Spatial Color Correlation for Low-Light Image Enhancement](https://arxiv.org/abs/2508.04176)
*Jin Kuang,Dong Liu,Yukuang Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: U2CLLIE 框架通过不确定性感知增强和因果相关性建模来改进低光图像增强，解决了梯度消失和噪声问题，并在实验中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的低光图像增强方法主要关注架构创新，但忽视了特征表示中的内在不确定性，特别是在极端黑暗条件下，退化的梯度和噪声支配会严重影响模型的可靠性和因果推理。

Method: 提出了一种名为 U2CLLIE 的新颖框架，该框架整合了不确定性感知增强和空间颜色因果相关性建模。该框架包含两个关键组件：(1) 不确定性感知双域去噪 (UaD) 模块，它利用高斯引导自适应频域特征增强 (G2AF) 来抑制频域噪声并优化熵驱动的表示；(2) 一个层次因果感知框架，其中亮度增强网络 (LEN) 首先对暗区进行粗略的亮度增强，然后在编码器-解码器阶段，通过邻域相关状态空间 (NeCo) 和自适应空间颜色校准 (AsC) 模块协同构建层次因果约束。

Result: U2CLLIE 在多个基准数据集上取得了最先进的性能，有效缓解了梯度消失和噪声主导问题，并优化了熵驱动的表示。

Conclusion: U2CLLIE 在多个基准数据集上实现了最先进的性能，在各种场景中表现出稳健的性能和强大的泛化能力。

Abstract: Most existing low-light image enhancement approaches primarily focus on
architectural innovations, while often overlooking the intrinsic uncertainty
within feature representations particularly under extremely dark conditions
where degraded gradient and noise dominance severely impair model reliability
and causal reasoning. To address these issues, we propose U2CLLIE, a novel
framework that integrates uncertainty-aware enhancement and spatial-color
causal correlation modeling. From the perspective of entropy-based uncertainty,
our framework introduces two key components: (1) An Uncertainty-Aware
Dual-domain Denoise (UaD) Module, which leverages Gaussian-Guided Adaptive
Frequency Domain Feature Enhancement (G2AF) to suppress frequency-domain noise
and optimize entropy-driven representations. This module enhances spatial
texture extraction and frequency-domain noise suppression/structure refinement,
effectively mitigating gradient vanishing and noise dominance. (2) A
hierarchical causality-aware framework, where a Luminance Enhancement Network
(LEN) first performs coarse brightness enhancement on dark regions. Then,
during the encoder-decoder phase, two asymmetric causal correlation modeling
modules Neighborhood Correlation State Space (NeCo) and Adaptive Spatial-Color
Calibration (AsC) collaboratively construct hierarchical causal constraints.
These modules reconstruct and reinforce neighborhood structure and color
consistency in the feature space. Extensive experiments demonstrate that
U2CLLIE achieves state-of-the-art performance across multiple benchmark
datasets, exhibiting robust performance and strong generalization across
various scenes.

</details>


### [66] [Deeper Inside Deep ViT](https://arxiv.org/abs/2508.04181)
*Sungrae Hong*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There have been attempts to create large-scale structures in vision models
similar to LLM, such as ViT-22B. While this research has provided numerous
analyses and insights, our understanding of its practical utility remains
incomplete. Therefore, we examine how this model structure reacts and train in
a local environment. We also highlight the instability in training and make
some model modifications to stabilize it. The ViT-22B model, trained from
scratch, overall outperformed ViT in terms of performance under the same
parameter size. Additionally, we venture into the task of image generation,
which has not been attempted in ViT-22B. We propose an image generation
architecture using ViT and investigate which between ViT and ViT-22B is a more
suitable structure for image generation.

</details>


### [67] [RPCANet++: Deep Interpretable Robust PCA for Sparse Object Segmentation](https://arxiv.org/abs/2508.04190)
*Fengyi Wu,Yimian Dai,Tianfang Zhang,Yixuan Ding,Jian Yang,Ming-Ming Cheng,Zhenming Peng*

Main category: cs.CV

TL;DR: RPCANet++是一个稀疏物体分割框架，它将RPCA的可解释性与深度网络相结合，解决了传统RPCA模型的局限性。该模型通过引入内存增强模块（MAM）和深度对比先验模块（DCPM）来提高性能和可解释性，并在各种实验中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的RPCA模型存在计算负担重、依赖精细调整的超参数以及固定的先验限制了其在动态场景中的适应性等问题。为了解决这些限制，提出RPCANet++，一个将RPCA的可解释性与高效深度架构相结合的稀疏物体分割框架。

Method: RPCANet++将一个松弛的RPCA模型展开成一个包含背景近似模块（BAM）、物体提取模块（OEM）和图像恢复模块（IRM）的结构化网络。为了减少BAM中的阶段间传输损失，引入了内存增强模块（MAM）来增强背景特征保留，而深度对比先验模块（DCPM）利用显着性线索来加速物体提取。

Result: RPCANet++在各种成像场景下实现了最先进的性能，并通过视觉和数值低秩性及稀疏性测量进一步提高了可解释性。

Conclusion: RPCANet++通过结合RPCA的理论优势和深度网络的效率，为可靠且可解释的稀疏物体分割设定了新的基准。

Abstract: Robust principal component analysis (RPCA) decomposes an observation matrix
into low-rank background and sparse object components. This capability has
enabled its application in tasks ranging from image restoration to
segmentation. However, traditional RPCA models suffer from computational
burdens caused by matrix operations, reliance on finely tuned hyperparameters,
and rigid priors that limit adaptability in dynamic scenarios. To solve these
limitations, we propose RPCANet++, a sparse object segmentation framework that
fuses the interpretability of RPCA with efficient deep architectures. Our
approach unfolds a relaxed RPCA model into a structured network comprising a
Background Approximation Module (BAM), an Object Extraction Module (OEM), and
an Image Restoration Module (IRM). To mitigate inter-stage transmission loss in
the BAM, we introduce a Memory-Augmented Module (MAM) to enhance background
feature preservation, while a Deep Contrast Prior Module (DCPM) leverages
saliency cues to expedite object extraction. Extensive experiments on diverse
datasets demonstrate that RPCANet++ achieves state-of-the-art performance under
various imaging scenarios. We further improve interpretability via visual and
numerical low-rankness and sparsity measurements. By combining the theoretical
strengths of RPCA with the efficiency of deep networks, our approach sets a new
baseline for reliable and interpretable sparse object segmentation. Codes are
available at our Project Webpage https://fengyiwu98.github.io/rpcanetx.

</details>


### [68] [From Learning to Unlearning: Biomedical Security Protection in Multimodal Large Language Models](https://arxiv.org/abs/2508.04192)
*Dunyuan Xu,Xikai Yang,Yaoqian Li,Jinpeng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 由于生物医学MLLM训练数据中存在隐私和错误信息，而完全重新训练成本高昂，因此需要机器反学习。本文提出了首个用于评估生物医学MLLM反学习质量的基准MLLMU-Med，并引入了反学习效率得分。评估结果显示，现有方法效果有限，但为该领域的研究开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学MLLM在训练数据中可能包含隐私信息或错误知识，导致部署后出现隐私泄露或错误输出。完全重新训练模型成本过高，因此需要一种更有效的方法来移除这些不良信息，即机器反学习。然而，目前缺乏用于评估生物医学MLLM反学习质量的可用数据集。

Method: 提出了一种新的数据生成流程，用于创建包含合成私有数据和事实错误的数据集，并在此基础上构建了MLLMU-Med基准。评估了五种反学习方法在MLLMU-Med上的表现，并引入了一种新的反学习效率得分来衡量反学习的整体性能。

Result: 提出的MLLMU-Med基准和新的反学习效率得分填补了现有空白。通过在MLLMU-Med上评估五种反学习方法，发现这些方法在移除生物医学MLLM中的有害知识方面效果有限，表明该领域有很大的改进空间。

Conclusion: 这项工作为有前景的生物医学多模态大语言模型（MLLM）的机器反学习领域开辟了新的研究途径，并提出了MLLMU-Med基准以及一种新的反学习效率得分。

Abstract: The security of biomedical Multimodal Large Language Models (MLLMs) has
attracted increasing attention. However, training samples easily contain
private information and incorrect knowledge that are difficult to detect,
potentially leading to privacy leakage or erroneous outputs after deployment.
An intuitive idea is to reprocess the training set to remove unwanted content
and retrain the model from scratch. Yet, this is impractical due to significant
computational costs, especially for large language models. Machine unlearning
has emerged as a solution to this problem, which avoids complete retraining by
selectively removing undesired knowledge derived from harmful samples while
preserving required capabilities on normal cases. However, there exist no
available datasets to evaluate the unlearning quality for security protection
in biomedical MLLMs. To bridge this gap, we propose the first benchmark
Multimodal Large Language Model Unlearning for BioMedicine (MLLMU-Med) built
upon our novel data generation pipeline that effectively integrates synthetic
private data and factual errors into the training set. Our benchmark targets
two key scenarios: 1) Privacy protection, where patient private information is
mistakenly included in the training set, causing models to unintentionally
respond with private data during inference; and 2) Incorrectness removal, where
wrong knowledge derived from unreliable sources is embedded into the dataset,
leading to unsafe model responses. Moreover, we propose a novel Unlearning
Efficiency Score that directly reflects the overall unlearning performance
across different subsets. We evaluate five unlearning approaches on MLLMU-Med
and find that these methods show limited effectiveness in removing harmful
knowledge from biomedical MLLMs, indicating significant room for improvement.
This work establishes a new pathway for further research in this promising
field.

</details>


### [69] [Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective](https://arxiv.org/abs/2508.04197)
*Yan Zhang,Gangyan Zeng,Daiqing Wu,Huawen Shen,Binbin Li,Yu Zhou,Can Ma,Xiaojun Bi*

Main category: cs.CV

TL;DR: GAT是一种新颖的实例导向模型，通过上下文聚合实例收集和实例聚焦轨迹跟踪来改进视频文本VQA，在准确性和效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在准确性和效率方面存在局限性，因为它们遵循帧级框架，该框架存在冗余文本实体和隐式关系建模的问题。

Method: 提出了一种新颖的GAT模型，包括上下文聚合实例收集模块（用于集成视觉外观、布局特征和文本内容以获得统一的文本表示）和实例聚焦轨迹跟踪模块（用于捕获文本在视频流中的动态演变并推断最终答案）。

Result: GAT在多个公共Video TextVQA数据集上的大量实验证明了该框架的有效性和泛化能力。

Conclusion: GAT在准确性和推理速度方面均优于现有的Video TextVQA方法、视频语言预训练方法和视频大语言模型，在准确率方面比之前的最先进的Video TextVQA方法提高了3.86%，并且推理速度比视频大语言模型快十倍。

Abstract: Video text-based visual question answering (Video TextVQA) aims to answer
questions by explicitly reading and reasoning about the text involved in a
video. Most works in this field follow a frame-level framework which suffers
from redundant text entities and implicit relation modeling, resulting in
limitations in both accuracy and efficiency. In this paper, we rethink the
Video TextVQA task from an instance-oriented perspective and propose a novel
model termed GAT (Gather and Trace). First, to obtain accurate reading result
for each video text instance, a context-aggregated instance gathering module is
designed to integrate the visual appearance, layout characteristics, and
textual contents of the related entities into a unified textual representation.
Then, to capture dynamic evolution of text in the video flow, an
instance-focused trajectory tracing module is utilized to establish
spatio-temporal relationships between instances and infer the final answer.
Extensive experiments on several public Video TextVQA datasets validate the
effectiveness and generalization of our framework. GAT outperforms existing
Video TextVQA methods, video-language pretraining methods, and video large
language models in both accuracy and inference speed. Notably, GAT surpasses
the previous state-of-the-art Video TextVQA methods by 3.86\% in accuracy and
achieves ten times of faster inference speed than video large language models.
The source code is available at https://github.com/zhangyan-ucas/GAT.

</details>


### [70] [Bootstrap Deep Spectral Clustering with Optimal Transport](https://arxiv.org/abs/2508.04200)
*Wengang Guo,Wei Ye,Chunchun Chen,Xin Sun,Christian Böhm,Claudia Plant,Susanto Rahardja*

Main category: cs.CV

TL;DR: BootSC is a deep spectral clustering model that addresses shortcomings of traditional methods by jointly learning all stages of spectral clustering in an end-to-end manner, using optimal-transport-derived supervision and orthogonal re-parameterization for improved performance.


<details>
  <summary>Details</summary>
Motivation: Spectral clustering has shortcomings in disjoint optimization and limited representation capacity.

Method: BootSC leverages optimal-transport-derived supervision to bootstrap the affinity matrix and cluster assignment matrix. It also uses a semantically-consistent orthogonal re-parameterization technique to orthogonalize spectral embeddings, enhancing discrimination capability. The model jointly learns affinity matrix construction, spectral embedding, and k-means clustering in an end-to-end manner.

Result: Experimental results show BootSC achieves state-of-the-art clustering performance.

Conclusion: BootSC achieves state-of-the-art clustering performance, with a notable 16% NMI improvement over the runner-up method on the ImageNet-Dogs dataset.

Abstract: Spectral clustering is a leading clustering method. Two of its major
shortcomings are the disjoint optimization process and the limited
representation capacity. To address these issues, we propose a deep spectral
clustering model (named BootSC), which jointly learns all stages of spectral
clustering -- affinity matrix construction, spectral embedding, and $k$-means
clustering -- using a single network in an end-to-end manner. BootSC leverages
effective and efficient optimal-transport-derived supervision to bootstrap the
affinity matrix and the cluster assignment matrix. Moreover, a
semantically-consistent orthogonal re-parameterization technique is introduced
to orthogonalize spectral embeddings, significantly enhancing the
discrimination capability. Experimental results indicate that BootSC achieves
state-of-the-art clustering performance. For example, it accomplishes a notable
16\% NMI improvement over the runner-up method on the challenging ImageNet-Dogs
dataset. Our code is available at https://github.com/spdj2271/BootSC.

</details>


### [71] [ViFP: A Framework for Visual False Positive Detection to Enhance Reasoning Reliability in VLMs](https://arxiv.org/abs/2508.04201)
*Ben Zhang,LuLu Yu,Lei Gao,Jing Liu,QuanJiang Guo,Hui Gao*

Main category: cs.CV

TL;DR: ViFP 是一个用于提高视觉-语言模型（VLM）推理可靠性的框架，通过子问题模板、多轮问答和目标性思维链（CoT）机制来检测和减少虚假阳性（FP）推理，并在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）在推理过程中存在虚假阳性（FP）问题，即模型生成正确答案但推理路径不正确。现有方法存在训练成本高、泛化性差的局限性。因此，需要一个能够提高视觉推理可靠性，同时克服数据集依赖和泛化能力差问题的通用框架。

Method: ViFP 框架通过构建基于视觉推理核心维度（如对象定位、特征描述和对象发现）的子问题模板，并利用多轮问答构建有效的推理路径来提高推理准确性。同时，通过动态分析推理路径的一致性来识别潜在的虚假阳性，并引入目标性思维链（CoT）机制自适应地指导样本，从而减少逻辑错误并保持准确性。

Result: ViFP 在 A-OKVQA、OKVQA 和 FVQA 数据集上持续提升性能，在 A-OKVQA 数据集上准确率提升高达 5.4%，超过先前最先进水平 4.3%，并显著减少了虚假阳性数量。

Conclusion: ViFP 通过检测虚假阳性（FP）推理来提高视觉推理的可靠性，通过子问题模板和多轮问答来构建有效的推理路径，并动态分析推理路径的一致性以识别潜在的虚假阳性。通过引入目标性思维链（CoT）机制，ViFP 能够自适应地指导虚假阳性和非虚假阳性样本，从而减少推理路径中的逻辑错误并保持准确性。此外，ViFP 还引入了一个名为 VoC 的可靠性评估指标，用于量化评估视觉语言模型（VLM）的回答准确性和推理可靠性。在 A-OKVQA、OKVQA 和 FVQA 数据集上的实验表明，ViFP 能够持续提升性能，在 A-OKVQA 上准确率提升高达 5.4%，显著减少了虚假阳性数量，验证了其在增强推理可靠性方面的有效性。

Abstract: In visual-language model (VLM) reasoning, false positive(FP) reasoning occurs
when a model generates a correct answer but follows an incorrect reasoning
path. Existing methods based on specific multi-step reasoning datasets and
reinforcement learning strategies, leading to high training costs and limited
generalization. In this work, we propose ViFP, a general framework for
enhancing visual reasoning reliability. It improves both answer accuracy and
reasoning soundness by detecting FPs. ViFP tackles the limitations of dataset
dependency and poor generalization by constructing sub-question templates
grounded in the core dimensions of visual reasoning, such as object
localization, characteristic description, and object discovery. ViFP then
builds effective reasoning paths via multi-turn QA to improve reasoning
accuracy. Meanwhile, ViFP dynamically analyzes the consistency of reasoning
path to identify potential FPs, and introduces a targeted chain-of-thought
(CoT) mechanism that adaptively guides both FP and non-FP samples. Thereby
reducing logical errors in the reasoning path while preserving accuracy.
Finally, we introduce a reliability evaluation metric-VoC, which integrates
answer accuracy and the FP rate, providing a quantitative tool to assess
whether a VLM not only answers correctly, but also reasons reliably. Our
experiments on closed-source VLMs show that ViFP consistently improves
performance across three datasets: A-OKVQA, OKVQA, and FVQA. On A-OKVQA, ViFP
improves accuracy by up to 5.4%, surpassing the previous state-of-the-art by
4.3%, and significantly reduces the number of FPs, validating its benefits in
enhancing reasoning reliability.

</details>


### [72] [Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification](https://arxiv.org/abs/2508.04205)
*Jianxun Yu,Ruiquan Ge,Zhipeng Wang,Cheng Yang,Chenyu Lin,Xianjun Fu,Jikui Liu,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The diagnosis of medical diseases faces challenges such as the misdiagnosis
of small lesions. Deep learning, particularly multimodal approaches, has shown
great potential in the field of medical disease diagnosis. However, the
differences in dimensionality between medical imaging and electronic health
record data present challenges for effective alignment and fusion. To address
these issues, we propose the Multimodal Multiscale Cross-Attention Fusion
Network (MMCAF-Net). This model employs a feature pyramid structure combined
with an efficient 3D multi-scale convolutional attention module to extract
lesion-specific features from 3D medical images. To further enhance multimodal
data integration, MMCAF-Net incorporates a multi-scale cross-attention module,
which resolves dimensional inconsistencies, enabling more effective feature
fusion. We evaluated MMCAF-Net on the Lung-PET-CT-Dx dataset, and the results
showed a significant improvement in diagnostic accuracy, surpassing current
state-of-the-art methods. The code is available at
https://github.com/yjx1234/MMCAF-Net

</details>


### [73] [What Holds Back Open-Vocabulary Segmentation?](https://arxiv.org/abs/2508.04211)
*Josip Šarić,Ivan Martinović,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: Open-vocabulary segmentation models are stuck. This paper uses "oracles" and groundtruth data to find and fix the problems, offering insights for future progress.


<details>
  <summary>Details</summary>
Motivation: Standard segmentation setups fail to recognize concepts outside the training taxonomy, and open-vocabulary approaches have plateaued in performance due to several bottlenecks.

Method: The paper proposes novel oracle components that identify and decouple bottlenecks by taking advantage of groundtruth information.

Result: Validation experiments provide empirical findings offering deeper insights into the failures of open-vocabulary models and suggesting prominent approaches for future research.

Conclusion: Open-vocabulary approaches require novel oracle components to identify and decouple bottlenecks, leveraging groundtruth information for improved performance.

Abstract: Standard segmentation setups are unable to deliver models that can recognize
concepts outside the training taxonomy. Open-vocabulary approaches promise to
close this gap through language-image pretraining on billions of image-caption
pairs. Unfortunately, we observe that the promise is not delivered due to
several bottlenecks that have caused the performance to plateau for almost two
years. This paper proposes novel oracle components that identify and decouple
these bottlenecks by taking advantage of the groundtruth information. The
presented validation experiments deliver important empirical findings that
provide a deeper insight into the failures of open-vocabulary models and
suggest prominent approaches to unlock the future research.

</details>


### [74] [SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition](https://arxiv.org/abs/2508.04224)
*Jiahui Li,Shengeng Tang,Jingxuan He,Gang Huang,Zhangye Wang,Yantao Pan,Lechao Cheng*

Main category: cs.CV

TL;DR: SplitGaussian 通过解耦静态和动态场景表示，解决了动态场景重建中的运动泄漏和几何失真问题，并提高了重建质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅法的动态场景重建方法将静态和动态元素耦合在共享表示中，导致运动泄漏、几何失真和时间闪烁。根本原因在于跨时间的几何和外观的耦合建模。

Method: 提出了一种名为 SplitGaussian 的新框架，该框架将场景表示显式地分解为静态和动态组件。通过将运动建模与背景几何分离，并仅允许动态分支随时间变形，SplitGaussian 解决了现有方法中几何和外观的耦合问题。

Result: SplitGaussian 在渲染质量、几何稳定性以及运动分离方面，相比于之前的最先进方法，表现更优。

Conclusion: SplitGaussian 通过显式地将场景表示分解为静态和动态组件，并允许仅动态分支随时间变形，从而解决了现有方法中几何和外观的耦合问题，显著提高了时间一致性和重建保真度，同时加速了收敛。实验证明 SplitGaussian 在渲染质量、几何稳定性和运动分离方面优于现有最先进的方法。

Abstract: Reconstructing dynamic 3D scenes from monocular video remains fundamentally
challenging due to the need to jointly infer motion, structure, and appearance
from limited observations. Existing dynamic scene reconstruction methods based
on Gaussian Splatting often entangle static and dynamic elements in a shared
representation, leading to motion leakage, geometric distortions, and temporal
flickering. We identify that the root cause lies in the coupled modeling of
geometry and appearance across time, which hampers both stability and
interpretability. To address this, we propose \textbf{SplitGaussian}, a novel
framework that explicitly decomposes scene representations into static and
dynamic components. By decoupling motion modeling from background geometry and
allowing only the dynamic branch to deform over time, our method prevents
motion artifacts in static regions while supporting view- and time-dependent
appearance refinement. This disentangled design not only enhances temporal
consistency and reconstruction fidelity but also accelerates convergence.
Extensive experiments demonstrate that SplitGaussian outperforms prior
state-of-the-art methods in rendering quality, geometric stability, and motion
separation.

</details>


### [75] [Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting](https://arxiv.org/abs/2508.04227)
*Yuyang Liu,Qiuhe Hong,Linlan Huang,Alexandra Gomez-Villa,Dipam Goswami,Xialei Liu,Joost van de Weijer,Yonghong Tian*

Main category: cs.CV

TL;DR: 本篇论文是对视觉-语言模型持续学习（VLM-CL）的首次系统性回顾。论文指出了VLM在持续学习中面临的灾难性遗忘问题，并提出了一个分类法，将解决方案分为多模态回放、跨模态正则化和参数高效适应三类。同时，论文还分析了现有评估方法的不足，并展望了未来的研究方向，旨在为构建终身学习的视觉-语言系统提供参考。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLM）虽然在大规模预训练后在多模态任务上表现出色，但在非平稳数据上进行持续学习（CL）仍然面临巨大挑战，特别是其跨模态对齐和泛化能力容易遭受灾难性遗忘。与传统的单模态CL不同，VLM面临独特的挑战，如跨模态特征漂移、共享架构导致的参数干扰以及零样本能力下降。因此，对VLM-CL进行系统性回顾和分析具有重要意义。

Method: 本篇论文系统性地回顾了视觉-语言模型持续学习（VLM-CL）领域的挑战和解决方案。首先，识别了VLM-CL中的三个核心失效模式：跨模态特征漂移、参数干扰和零样本能力侵蚀。基于这些失效模式，论文提出了一个驱动性分类法，将解决方案归纳为三类：1. 多模态回放策略（解决跨模态漂移）；2. 跨模态正则化（保持模态对齐）；3. 参数高效适应（缓解参数干扰）。此外，还分析了现有的评估协议、数据集和指标，并提出了未来研究方向。

Result: 本篇论文的目的是提供一个关于视觉-语言模型持续学习（VLM-CL）的全面、系统性的回顾。论文识别了VLM-CL中的核心挑战，提出了一种新的分类法来组织现有解决方案，并分析了当前的评估方法。此外，还指出了需要改进的基准测试，以更好地捕捉VLM特有的遗忘和组合泛化问题，并对未来的研究方向进行了展望。

Conclusion: 本篇论文的结论是，针对视觉-语言模型（VLM）在持续学习（CL）中的灾难性遗忘问题，提出了一种驱动性分类方法，该方法将现有解决方案分为多模态回放策略、跨模态正则化和参数高效适应三种类型，并指出了当前评估协议、数据集和指标的不足，以及未来研究方向，如持续预训练和组合式零样本学习。

Abstract: Vision-language models (VLMs) have achieved impressive performance across
diverse multimodal tasks by leveraging large-scale pre-training. However,
enabling them to learn continually from non-stationary data remains a major
challenge, as their cross-modal alignment and generalization capabilities are
particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal
continual learning (CL), VLMs face unique challenges such as cross-modal
feature drift, parameter interference due to shared architectures, and
zero-shot capability erosion. This survey offers the first focused and
systematic review of continual learning for VLMs (VLM-CL). We begin by
identifying the three core failure modes that degrade performance in VLM-CL.
Based on these, we propose a challenge-driven taxonomy that maps solutions to
their target problems: (1) \textit{Multi-Modal Replay Strategies} address
cross-modal drift through explicit or implicit memory mechanisms; (2)
\textit{Cross-Modal Regularization} preserves modality alignment during
updates; and (3) \textit{Parameter-Efficient Adaptation} mitigates parameter
interference with modular or low-rank updates. We further analyze current
evaluation protocols, datasets, and metrics, highlighting the need for better
benchmarks that capture VLM-specific forgetting and compositional
generalization. Finally, we outline open problems and future directions,
including continual pre-training and compositional zero-shot learning. This
survey aims to serve as a comprehensive and diagnostic reference for
researchers developing lifelong vision-language systems. All resources are
available at:
https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.

</details>


### [76] [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
*Kangrui Cen,Baixuan Zhao,Yi Xin,Siqi Luo,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: LayerT2V通过分层合成技术，首次实现了对多物体文本到视频生成中运动轨迹的精确控制，解决了现有方法在复杂场景下的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在处理包含多个运动物体的场景时存在局限性，尤其是在物体轨迹交叉时，由于语义冲突导致性能下降。本研究旨在解决这一挑战。

Method: LayerT2V提出了一种新颖的视频生成方法，通过将背景和前景物体分层合成，实现了对多物体运动轨迹的灵活控制和相干合成。

Result: LayerT2V在多物体场景生成方面表现出色，mIoU和AP50指标分别比现有最优方法提高了1.4倍和4.5倍。

Conclusion: LayerT2V通过分层生成解决了多物体文本到视频生成中的运动控制难题，并在实验中展示了显著优于现有方法的性能。

Abstract: Controlling object motion trajectories in Text-to-Video (T2V) generation is a
challenging and relatively under-explored area, particularly in scenarios
involving multiple moving objects. Most community models and datasets in the
T2V domain are designed for single-object motion, limiting the performance of
current generative models in multi-object tasks. Additionally, existing motion
control methods in T2V either lack support for multi-object motion scenes or
experience severe performance degradation when object trajectories intersect,
primarily due to the semantic conflicts in colliding regions. To address these
limitations, we introduce LayerT2V, the first approach for generating video by
compositing background and foreground objects layer by layer. This layered
generation enables flexible integration of multiple independent elements within
a video, positioning each element on a distinct "layer" and thus facilitating
coherent multi-object synthesis while enhancing control over the generation
process. Extensive experiments demonstrate the superiority of LayerT2V in
generating complex multi-object scenarios, showcasing 1.4x and 4.5x
improvements in mIoU and AP50 metrics over state-of-the-art (SOTA) methods.
Project page and code are available at https://kr-panghu.github.io/LayerT2V/ .

</details>


### [77] [Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.04229)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种新的基于扩散的行人轨迹预测模型，该模型通过识别和纳入行人的运动意图（分为横向和纵向）来提高预测的准确性和可解释性，并在标准基准测试中取得了良好结果。


<details>
  <summary>Details</summary>
Motivation: 准确预测人群轨迹对自动驾驶车辆的路径规划和运动控制至关重要。然而，由于人类运动固有的多峰性和不确定性，精确预测人群轨迹仍然是一个挑战。现有的基于扩散的模型虽然在捕捉行人行为的随机性方面表现出潜力，但很少明确地纳入行人的运动意图，这可能限制了预测模型的可解释性和精确性。

Method: 提出了一种基于扩散的多模态轨迹预测模型，通过分解运动意图为横向和纵向分量，并引入行人意图识别模块来捕捉这些意图。采用了高效的引导机制来生成可解释的轨迹。

Result: 实验结果表明，所提出的框架在ETH和UCY基准测试上与最先进的方法相比，取得了有竞争力的性能。

Conclusion: 该模型在ETH和UCY数据集上取得了有竞争力的性能，证明了其在考虑行人运动意图方面的有效性。

Abstract: Predicting pedestrian motion trajectories is critical for path planning and
motion control of autonomous vehicles. However, accurately forecasting crowd
trajectories remains a challenging task due to the inherently multimodal and
uncertain nature of human motion. Recent diffusion-based models have shown
promising results in capturing the stochasticity of pedestrian behavior for
trajectory prediction. However, few diffusion-based approaches explicitly
incorporate the underlying motion intentions of pedestrians, which can limit
the interpretability and precision of prediction models. In this work, we
propose a diffusion-based multimodal trajectory prediction model that
incorporates pedestrians' motion intentions into the prediction framework. The
motion intentions are decomposed into lateral and longitudinal components, and
a pedestrian intention recognition module is introduced to enable the model to
effectively capture these intentions. Furthermore, we adopt an efficient
guidance mechanism that facilitates the generation of interpretable
trajectories. The proposed framework is evaluated on two widely used human
trajectory prediction benchmarks, ETH and UCY, on which it is compared against
state-of-the-art methods. The experimental results demonstrate that our method
achieves competitive performance.

</details>


### [78] [DocVCE: Diffusion-based Visual Counterfactual Explanations for Document Image Classification](https://arxiv.org/abs/2508.04233)
*Saifullah Saifullah,Stefan Agne,Andreas Dengel,Sheraz Ahmed*

Main category: cs.CV

TL;DR: 为提高文档图像分类的可解释性，本文提出DocVCE方法，利用生成模型生成可视化反事实解释，实现了对模型决策的深入理解。


<details>
  <summary>Details</summary>
Motivation: 随着AI在文档处理中应用的增加，提高其透明度和可靠性至关重要，尤其是在高风险应用中。现有的基于特征重要性图的解释方法难以理解，无法提供模型全局特征的洞察。

Method: DocVCE方法结合了潜在扩散模型和分类器引导，首先生成与输入数据分布一致的视觉反事实解释，然后进行分层块状细化以寻找与原始图像最相似的反事实。

Result: 通过在RVL-CDIP、Tobacco3482和DocLayNet三个数据集上，以及ResNet、ConvNeXt和DiT三个模型上的定性和定量评估，证明了DocVCE的有效性，并使用了有效性、接近度和真实性等标准进行评估。

Conclusion: 本文提出了一种名为DocVCE的新方法，利用潜在扩散模型和分类器引导来生成文档图像分类的可视化反事实解释，并通过分层块状细化来寻找最接近目标事实图像的反事实，以提高模型的可解释性和可靠性。

Abstract: As black-box AI-driven decision-making systems become increasingly widespread
in modern document processing workflows, improving their transparency and
reliability has become critical, especially in high-stakes applications where
biases or spurious correlations in decision-making could lead to serious
consequences. One vital component often found in such document processing
workflows is document image classification, which, despite its widespread use,
remains difficult to explain. While some recent works have attempted to explain
the decisions of document image classification models through
feature-importance maps, these maps are often difficult to interpret and fail
to provide insights into the global features learned by the model. In this
paper, we aim to bridge this research gap by introducing generative document
counterfactuals that provide meaningful insights into the model's
decision-making through actionable explanations. In particular, we propose
DocVCE, a novel approach that leverages latent diffusion models in combination
with classifier guidance to first generate plausible in-distribution visual
counterfactual explanations, and then performs hierarchical patch-wise
refinement to search for a refined counterfactual that is closest to the target
factual image. We demonstrate the effectiveness of our approach through a
rigorous qualitative and quantitative assessment on 3 different document
classification datasets -- RVL-CDIP, Tobacco3482, and DocLayNet -- and 3
different models -- ResNet, ConvNeXt, and DiT -- using well-established
evaluation criteria such as validity, closeness, and realism. To the best of
the authors' knowledge, this is the first work to explore generative
counterfactual explanations in document image analysis.

</details>


### [79] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: FrEVL 框架使用冻结的预训练嵌入来提高视觉-语言模型的效率，在不牺牲过多性能的情况下大大降低了计算和能耗。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型的部署受到大量计算需求的限制，旨在探索冻结的预训练嵌入是否可以支持有效的视觉-语言理解，以降低计算需求。

Method: 提出了一种名为 FrEVL 的框架，该框架探索冻结的预训练嵌入是否可以支持有效的视觉-语言理解。

Result: 在标准基准测试中，FrEVL 取得了 85% 到 95% 的最先进性能，可训练参数仅为 6840 万个。与端到端计算相比，FrEVL 将速度提高了 2.3 倍，能耗降低了 52%。

Conclusion: 冻结的预训练嵌入可以有效支持视觉-语言理解，在标准基准测试中达到最先进水平的 85% 到 95% 的性能，同时只有 6840 万个可训练参数。然而，冻结嵌入的有效性取决于预训练目标与下游任务需求之间的一致性。当考虑包括嵌入提取在内的端到端计算时，FrEVL 可提供 2.3 倍的速度提升和 52% 的能耗降低，适用于具有可预计算输入或部署限制超过边际性能提升的情况。

Abstract: The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

</details>


### [80] [PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction](https://arxiv.org/abs/2508.04236)
*Muhua Zhu,Xinhao Jin,Chengbo Wang,Yongcong Zhang,Yifei Xue,Tie Ji,Yizhen Lao*

Main category: cs.CV

TL;DR: PIS3R：一种基于深度三维重建的图像缝合方法，能够有效处理大视差问题，并提供高质量、几何完整的缝合结果。


<details>
  <summary>Details</summary>
Motivation: 现有的图像缝合方法在处理包含深度变化和显著相机基线的大视差图像时效果不佳，容易出现明显的视差问题。

Method: 提出了一种名为PIS3R的图像缝合解决方案，该方案基于深度三维重建概念，能够有效处理大视差。首先，应用视觉几何基础变换器处理具有大视差的输入图像，以获得内参、外参和密集三维场景重建。然后，利用恢复的相机参数将重建的密集点云重新投影到指定的参考视图，实现像素级对齐并生成初始缝合图像。最后，为了进一步处理初始缝合中可能出现的孔洞或噪声等伪影，提出了一种基于点条件的图像扩散模块来获得精炼结果。

Result: PIS3R方案能够实现像素级对齐，生成初始缝合图像，并通过点条件图像扩散模块进行精炼，能够处理大视差图像，并提供高质量、几何完整的缝合结果。

Conclusion: 该方法在处理大视差图像时具有鲁棒性，并且能够保持三维摄影测量背景下所有像素的几何完整性，可以直接应用于下游的三维视觉任务，如运动恢复结构（SfM）。实验结果表明，该算法能够为具有大视差的图像提供准确的缝合结果，并在质量和数量上优于现有方法。

Abstract: Image stitching aim to align two images taken from different viewpoints into
one seamless, wider image. However, when the 3D scene contains depth variations
and the camera baseline is significant, noticeable parallax occurs-meaning the
relative positions of scene elements differ substantially between views. Most
existing stitching methods struggle to handle such images with large parallax
effectively. To address this challenge, in this paper, we propose an image
stitching solution called PIS3R that is robust to very large parallax based on
the novel concept of deep 3D reconstruction. First, we apply visual geometry
grounded transformer to two input images with very large parallax to obtain
both intrinsic and extrinsic parameters, as well as the dense 3D scene
reconstruction. Subsequently, we reproject reconstructed dense point cloud onto
a designated reference view using the recovered camera parameters, achieving
pixel-wise alignment and generating an initial stitched image. Finally, to
further address potential artifacts such as holes or noise in the initial
stitching, we propose a point-conditioned image diffusion module to obtain the
refined result.Compared with existing methods, our solution is very large
parallax tolerant and also provides results that fully preserve the geometric
integrity of all pixels in the 3D photogrammetric context, enabling direct
applicability to downstream 3D vision tasks such as SfM. Experimental results
demonstrate that the proposed algorithm provides accurate stitching results for
images with very large parallax, and outperforms the existing methods
qualitatively and quantitatively.

</details>


### [81] [From eye to AI: studying rodent social behavior in the era of machine Learning](https://arxiv.org/abs/2508.04255)
*Giuseppe Chindemi,Camilla Bellone,Benoit Girard*

Main category: cs.CV

TL;DR: AI和机器学习正在改变啮齿动物社会行为的研究，提供更精确的分析，但也带来新挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的人类观察方法存在偏差且无法捕捉复杂的社会互动，因此需要更先进的计算方法来提供多方面的见解，尤其是在社会神经科学领域。

Method: 讨论了计算方法，特别是AI和机器学习在分析啮齿动物社会行为中的应用，并结合了计算机视觉、行为学和神经科学。

Result: AI和机器学习方法比传统方法能提供更深入、更少偏差的社会行为分析，但也存在挑战，需要实用解决方案来克服。

Conclusion: AI在社会行为研究中具有巨大潜力，但仍面临挑战，需要持续发展和关注。

Abstract: The study of rodent social behavior has shifted in the last years from
relying on direct human observation to more nuanced approaches integrating
computational methods in artificial intelligence (AI) and machine learning.
While conventional approaches introduce bias and can fail to capture the
complexity of rodent social interactions, modern approaches bridging computer
vision, ethology and neuroscience provide more multifaceted insights into
behavior which are particularly relevant to social neuroscience. Despite these
benefits, the integration of AI into social behavior research also poses
several challenges. Here we discuss the main steps involved and the tools
available for analyzing rodent social behavior, examining their advantages and
limitations. Additionally, we suggest practical solutions to address common
hurdles, aiming to guide young researchers in adopting these methods and to
stimulate further discussion among experts regarding the evolving requirements
of these tools in scientific applications.

</details>


### [82] [Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark](https://arxiv.org/abs/2508.04260)
*Xiao Wang,Ziwen Wang,Wentao Wu,Anjie Wang,Jiashu Wu,Yantao Pan,Chenglong Li*

Main category: cs.CV

TL;DR: 提出SAV框架，利用SAM、知识图谱和上下文检索，改进车辆部件分割，并发布了新的数据集VehicleSeg10K。


<details>
  <summary>Details</summary>
Motivation: SAM模型虽然在图像分割领域表现出色，但无法直接应用于细粒度的车辆部件分割任务，因为其文本提示分割功能不可用，且默认模式生成的掩码区域缺乏语义标签。

Method: 提出了一种名为SAV 的新颖框架，该框架包含三个核心组件：基于SAM的编码器-解码器、车辆部件知识图谱以及上下文样本检索编码模块。知识图谱通过结构化本体显式地对车辆部件之间的空间和几何关系进行建模，并引入了新的大规模车辆部件分割基准数据集VehicleSeg10K。

Result: 在VehicleSeg10K等数据集上进行了广泛的实验，并与多个代表性基线进行了比较，为未来的研究和比较奠定了坚实的基础。

Conclusion: SAV框架通过结合SAM、知识图谱和上下文检索，解决了SAM在车辆部件分割中的局限性，并在VehicleSeg10K等数据集上取得了优异的性能。

Abstract: With the rapid advancement of autonomous driving, vehicle perception,
particularly detection and segmentation, has placed increasingly higher demands
on algorithmic performance. Pre-trained large segmentation models, especially
Segment Anything Model (SAM), have sparked significant interest and inspired
new research directions in artificial intelligence. However, SAM cannot be
directly applied to the fine-grained task of vehicle part segmentation, as its
text-prompted segmentation functionality is not publicly accessible, and the
mask regions generated by its default mode lack semantic labels, limiting its
utility in structured, category-specific segmentation tasks. To address these
limitations, we propose SAV, a novel framework comprising three core
components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a
context sample retrieval encoding module. The knowledge graph explicitly models
the spatial and geometric relationships among vehicle parts through a
structured ontology, effectively encoding prior structural knowledge.
Meanwhile, the context retrieval module enhances segmentation by identifying
and leveraging visually similar vehicle instances from training data, providing
rich contextual priors for improved generalization. Furthermore, we introduce a
new large-scale benchmark dataset for vehicle part segmentation, named
VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations
across diverse scenes and viewpoints. We conduct comprehensive experiments on
this dataset and two other datasets, benchmarking multiple representative
baselines to establish a solid foundation for future research and comparison. %
Both the dataset and source code of this paper will be released upon
acceptance. Both the dataset and source code of this paper will be released on
https://github.com/Event-AHU/SAV

</details>


### [83] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: LVLMs 存在训练偏差导致幻觉。POPEv2 基准测试和 Obliviate 解学习方法被提出，以识别和解决此问题，Obliviate 通过仅更新 2% 的参数显著减少了幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在扩展训练数据方面取得了显著的进步，但仍然存在与视觉输入不一致的文本生成（幻觉）问题。本研究旨在系统地探究训练数据在幻觉问题中的作用。

Method: 通过构建 POPEv2 基准测试来系统研究训练数据对幻觉的影响，并提出 Obliviate 方法，这是一种针对语言模型头进行参数和数据高效微调的解学习技术，旨在消除训练偏差以减少幻觉。

Result: 研究发现当前的 LVLMs 存在训练偏差，对训练数据中的图像（尤其是包含被遮蔽对象的反事实图像）表现出过度依赖，导致频繁幻觉。Obliviate 方法在不重新使用训练数据和仅更新约 2% 参数的情况下，显著减少了各种任务中的幻觉，并且在模型规模和训练数据量方面都表现出良好的可扩展性，还能泛化到超出对象级别的幻觉。

Conclusion: 提出的 Obliviate 方法通过针对语言模型头进行高效的解学习，能够显著减少视觉语言模型中的幻觉问题，同时保持模型的通用性并具有良好的可扩展性。

Abstract: As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

</details>


### [84] [Revisiting Continual Semantic Segmentation with Pre-trained Vision Models](https://arxiv.org/abs/2508.04267)
*Duzhen Zhang,Yong Ren,Wei Cong,Junhao Zheng,Qiaoyi Su,Shuncheng Jia,Zhong-Zhi Li,Xuanle Zhao,Ye Bai,Feilong Chen,Qi Tian,Tielin Zhang*

Main category: cs.CV

TL;DR: 本研究推翻了直接微调（DFT）在持续语义分割（CSS）中容易遗忘的观点，证明预训练视觉模型（PVM）骨干具有强大的抗遗忘能力。研究提出DFT*方法，通过冻结骨干和分类器来解决分类器漂移问题，并在实验中取得了优于现有方法的性能，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为直接微调（DFT）在持续语义分割（CSS）中容易发生灾难性遗忘，因此发展了许多复杂的缓解技术。然而，本研究认为这一假设存在缺陷，预训练视觉模型（PVM）骨干本身具有强大的抗遗忘能力，遗忘主要是由分类器的漂移引起的。

Method: 通过系统地重新审视直接微调（DFT）在持续语义分割（CSS）中的遗忘问题，并在两个标准基准（Pascal VOC 2012和ADE20K）上使用两种代表性的PVM骨干（ResNet101和Swin-B）在八种CSS设置下进行了详细的探测分析。基于分析结果，提出DFT*方法，包含冻结PVM骨干和先前学习的分类器、预分配未来分类器等策略。

Result: 研究发现，即使在DFT下，PVM也能在很大程度上保留先前学习的知识，遗忘很少。DFT*方法在实验中始终表现出与16种最先进的CSS方法相当或更优的性能，并且需要更少的训练参数和训练时间。

Conclusion: 该研究提出了一种名为DFT*的简单而有效的持续语义分割方法，通过冻结预训练视觉模型（PVM）骨干和先前学习的分类器，并预先分配未来的分类器，在两个标准基准测试（Pascal VOC 2012和ADE20K）上取得了与最先进方法相当或更优的性能，同时显著减少了可训练参数和训练时间。

Abstract: Continual Semantic Segmentation (CSS) seeks to incrementally learn to segment
novel classes while preserving knowledge of previously encountered ones. Recent
advancements in CSS have been largely driven by the adoption of Pre-trained
Vision Models (PVMs) as backbones. Among existing strategies, Direct
Fine-Tuning (DFT), which sequentially fine-tunes the model across classes,
remains the most straightforward approach. Prior work often regards DFT as a
performance lower bound due to its presumed vulnerability to severe
catastrophic forgetting, leading to the development of numerous complex
mitigation techniques. However, we contend that this prevailing assumption is
flawed. In this paper, we systematically revisit forgetting in DFT across two
standard benchmarks, Pascal VOC 2012 and ADE20K, under eight CSS settings using
two representative PVM backbones: ResNet101 and Swin-B. Through a detailed
probing analysis, our findings reveal that existing methods significantly
underestimate the inherent anti-forgetting capabilities of PVMs. Even under
DFT, PVMs retain previously learned knowledge with minimal forgetting. Further
investigation of the feature space indicates that the observed forgetting
primarily arises from the classifier's drift away from the PVM, rather than
from degradation of the backbone representations. Based on this insight, we
propose DFT*, a simple yet effective enhancement to DFT that incorporates
strategies such as freezing the PVM backbone and previously learned
classifiers, as well as pre-allocating future classifiers. Extensive
experiments show that DFT* consistently achieves competitive or superior
performance compared to sixteen state-of-the-art CSS methods, while requiring
substantially fewer trainable parameters and less training time.

</details>


### [85] [PKSS-Align: Robust Point Cloud Registration on Pre-Kendall Shape Space](https://arxiv.org/abs/2508.04286)
*Chenlei Lv,Hui Huang*

Main category: cs.CV

TL;DR: 提出了一种名为PKSS-Align的鲁棒点云配准方法，通过在预Kendall形状空间（PKSS）中度量形状特征相似性，有效解决了相似性变换、非均匀密度、噪声点和不完整结构等问题，且无需数据训练和复杂特征编码，实现了高效率和高可行性。


<details>
  <summary>Details</summary>
Motivation: 点云配准通常对相似性变换、噪声点和不完整的几何结构敏感，尤其是在尺度不均和存在缺陷的情况下，容易陷入局部最优。本研究旨在提出一种鲁棒的点云配准方法来应对这些挑战。

Method: 提出了一种基于预Kendall形状空间（PKSS）的形状特征相似性度量方法，该方法不依赖点到点或点到面的度量，并可以直接生成变换矩阵。

Result: 实验结果表明，该方法在效率和可行性方面取得了显著的改进，并且优于相关的最先进方法。

Conclusion: 该方法在同时存在相似性变换、非均匀密度、随机噪声点和不完整部分的情况下，可以直接生成点云的变换矩阵。

Abstract: Point cloud registration is a classical topic in the field of 3D Vision and
Computer Graphics. Generally, the implementation of registration is typically
sensitive to similarity transformations (translation, scaling, and rotation),
noisy points, and incomplete geometric structures. Especially, the non-uniform
scales and defective parts of point clouds increase probability of struck local
optima in registration task. In this paper, we propose a robust point cloud
registration PKSS-Align that can handle various influences, including
similarity transformations, non-uniform densities, random noisy points, and
defective parts. The proposed method measures shape feature-based similarity
between point clouds on the Pre-Kendall shape space (PKSS),
\textcolor{black}{which is a shape measurement-based scheme and doesn't require
point-to-point or point-to-plane metric.} The employed measurement can be
regarded as the manifold metric that is robust to various representations in
the Euclidean coordinate system. Benefited from the measurement, the
transformation matrix can be directly generated for point clouds with mentioned
influences at the same time. The proposed method does not require data training
and complex feature encoding. Based on a simple parallel acceleration, it can
achieve significant improvement for efficiency and feasibility in practice.
Experiments demonstrate that our method outperforms the relevant
state-of-the-art methods.

</details>


### [86] [MuGS: Multi-Baseline Generalizable Gaussian Splatting Reconstruction](https://arxiv.org/abs/2508.04297)
*Yaopeng Lou,Liao Shen,Tianqi Liu,Jiaqi Li,Zihao Huang,Huiqiang Sun,Zhiguo Cao*

Main category: cs.CV

TL;DR: MuRF 是一种新颖的视图合成方法，通过整合 MVS 和 MDE 特征、深度深度融合以及参考视图损失，在各种基线设置和场景中实现了最先进的性能，并加速了训练和推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决各种基线设置（包括具有小基线和大基线的稀疏输入视图）的通用前馈新视图合成方法。

Method: 1. 整合多视图立体 (MVS) 和单目深度估计 (MDE) 的特征，以增强可推广重建的特征表示。
2. 提出一种用于深度深度融合的投影和采样机制，构建精细的概率体积来指导特征图回归。
3. 引入参考视图损失来改善几何和优化效率。
4. 利用 3D 高斯表示来加速训练和推理时间，同时提高渲染质量。

Result: 在 DTU、RealEstate10K、LLFF 和 Mip-NeRF 360 数据集上实现了最先进的性能，并展示了有希望的零样本性能。

Conclusion: MuRF 在多种基线设置和各种场景（从简单的物体 (DTU) 到复杂的室内和室外场景 (RealEstate10K)）中均实现了最先进的性能。此外，我们在 LLFF 和 Mip-NeRF 360 数据集上展示了有希望的零样本性能。

Abstract: We present Multi-Baseline Gaussian Splatting (MuRF), a generalized
feed-forward approach for novel view synthesis that effectively handles diverse
baseline settings, including sparse input views with both small and large
baselines. Specifically, we integrate features from Multi-View Stereo (MVS) and
Monocular Depth Estimation (MDE) to enhance feature representations for
generalizable reconstruction. Next, We propose a projection-and-sampling
mechanism for deep depth fusion, which constructs a fine probability volume to
guide the regression of the feature map. Furthermore, We introduce a
reference-view loss to improve geometry and optimization efficiency. We
leverage 3D Gaussian representations to accelerate training and inference time
while enhancing rendering quality. MuRF achieves state-of-the-art performance
across multiple baseline settings and diverse scenarios ranging from simple
objects (DTU) to complex indoor and outdoor scenes (RealEstate10K). We also
demonstrate promising zero-shot performance on the LLFF and Mip-NeRF 360
datasets.

</details>


### [87] [Length Matters: Length-Aware Transformer for Temporal Sentence Grounding](https://arxiv.org/abs/2508.04299)
*Yifan Wang,Ziyi Liu,Xiaolong Sun,Jiawei Wang,Hongmin Liu*

Main category: cs.CV

TL;DR: 通过引入长度感知Transformer（LATR）模型，利用视频-描述对的长度先验，将查询分配给不同时间长度的片段，并增加长度分类任务进行监督，解决了现有模型查询角色重叠和冗余预测的问题，并在TSG任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: DETR-based模型在TSG任务中取得了进展，但由于缺乏显式监督，学习到的查询容易角色重叠，导致冗余预测。为了解决这个问题，需要让每个查询更好地履行其指定的角色。

Method: 提出了一种名为长度感知Transformer（LATR）的模型，该模型通过将可学习的查询分为三组，分别对应短、中、长三种时间长度的视频片段，并引入长度分类任务进行监督，以解决DETR-based模型中查询角色重叠和冗余预测的问题。

Result: LATR模型在三个公开基准测试中取得了最先进的性能，并且消融研究验证了每个组件的有效性以及长度先验在TSG任务中的关键作用。

Conclusion: 通过将查询分为三组，分别处理短、中、长不同时间长度的片段，并引入额外的长度分类任务进行监督，提出的LATR模型能够使每个查询更好地完成其指定的功能，从而提高TSG任务的性能。实验结果表明，LATR在三个公开基准测试中取得了最先进的性能，证明了该方法的有效性以及长度先验在TSG任务中的重要作用。

Abstract: Temporal sentence grounding (TSG) is a highly challenging task aiming to
localize the temporal segment within an untrimmed video corresponding to a
given natural language description. Benefiting from the design of learnable
queries, the DETR-based models have achieved substantial advancements in the
TSG task. However, the absence of explicit supervision often causes the learned
queries to overlap in roles, leading to redundant predictions. Therefore, we
propose to improve TSG by making each query fulfill its designated role,
leveraging the length priors of the video-description pairs. In this paper, we
introduce the Length-Aware Transformer (LATR) for TSG, which assigns different
queries to handle predictions based on varying temporal lengths. Specifically,
we divide all queries into three groups, responsible for segments with short,
middle, and long temporal durations, respectively. During training, an
additional length classification task is introduced. Predictions from queries
with mismatched lengths are suppressed, guiding each query to specialize in its
designated function. Extensive experiments demonstrate the effectiveness of our
LATR, achieving state-of-the-art performance on three public benchmarks.
Furthermore, the ablation studies validate the contribution of each component
of our method and the critical role of incorporating length priors into the TSG
task.

</details>


### [88] [TempFlow-GRPO: When Timing Matters for GRPO in Flow Models](https://arxiv.org/abs/2508.04324)
*Xiaoxuan He,Siming Fu,Yuke Zhao,Wanli Li,Jian Yang,Dacheng Yin,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: TempFlow-GRPO通过考虑生成过程中的时间结构来改进文本到图像生成中的人类偏好对齐，通过轨迹分支和噪声感知加权实现更有效的优化。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配模型在与强化学习结合以实现人类偏好对齐方面仍然不理想，阻碍了基于奖励的细粒度优化。现有方法的时序均匀性假设（稀疏的终端奖励和均匀的信用分配）未能捕捉生成时间步长中决策的不同重要性，导致探索效率低下和收敛效果不佳。

Method: TempFlow-GRPO引入了轨迹分支机制和噪声感知加权方案，以解决现有方法在GRPO训练中对时间均匀性的假设。

Result: TempFlow-GRPO通过集中随机性在指定的च्या分支点来提供过程奖励，从而能够进行精确的信用分配，而无需专门的中间奖励模型。此外，噪声感知加权方案根据每个时间步的内在探索潜力来调节策略优化，优先考虑在影响大的早期阶段进行学习，同时确保后期阶段的稳定改进。

Conclusion: TempFlow-GRPO在人类偏好对齐和标准文本到图像基准测试中取得了最先进的性能。

Abstract: Recent flow matching models for text-to-image generation have achieved
remarkable quality, yet their integration with reinforcement learning for human
preference alignment remains suboptimal, hindering fine-grained reward-based
optimization. We observe that the key impediment to effective GRPO training of
flow models is the temporal uniformity assumption in existing approaches:
sparse terminal rewards with uniform credit assignment fail to capture the
varying criticality of decisions across generation timesteps, resulting in
inefficient exploration and suboptimal convergence. To remedy this shortcoming,
we introduce \textbf{TempFlow-GRPO} (Temporal Flow GRPO), a principled GRPO
framework that captures and exploits the temporal structure inherent in
flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a
trajectory branching mechanism that provides process rewards by concentrating
stochasticity at designated branching points, enabling precise credit
assignment without requiring specialized intermediate reward models; and (ii) a
noise-aware weighting scheme that modulates policy optimization according to
the intrinsic exploration potential of each timestep, prioritizing learning
during high-impact early stages while ensuring stable refinement in later
phases. These innovations endow the model with temporally-aware optimization
that respects the underlying generative dynamics, leading to state-of-the-art
performance in human preference alignment and standard text-to-image
benchmarks.

</details>


### [89] [RotatedMVPS: Multi-view Photometric Stereo with Rotated Natural Light](https://arxiv.org/abs/2508.04366)
*Songyun Yang,Yufei Han,Jilong Zhang,Kongming Liang,Peng Yu,Zhaowei Qu,Heng Guo*

Main category: cs.CV

TL;DR: RotatedMVPS可以在旋转的自然光下恢复高保真度的表面形状和反射率。


<details>
  <summary>Details</summary>
Motivation: 现有MVPS方法在自然光照下应用受限，且常忽略反射率和光照属性的恢复。

Method: RotatedMVPS通过确保光照一致性并集成现有单视图光度立体法的伪影来解决形状和反射率恢复问题。

Result: 实验结果表明，RotatedMVPS在合成和真实世界数据集上均能有效恢复形状和反射率。

Conclusion: 本文提出的RotatedMVPS通过确保不同相机和物体姿态下的光照一致性，并结合现成的基于学习的单视图光度立体法的伪影，显著提高了在旋转自然光下形状和反射率恢复的准确性。

Abstract: Multiview photometric stereo (MVPS) seeks to recover high-fidelity surface
shapes and reflectances from images captured under varying views and
illuminations. However, existing MVPS methods often require controlled darkroom
settings for varying illuminations or overlook the recovery of reflectances and
illuminations properties, limiting their applicability in natural illumination
scenarios and downstream inverse rendering tasks. In this paper, we propose
RotatedMVPS to solve shape and reflectance recovery under rotated natural
light, achievable with a practical rotation stage. By ensuring light
consistency across different camera and object poses, our method reduces the
unknowns associated with complex environment light. Furthermore, we integrate
data priors from off-the-shelf learning-based single-view photometric stereo
methods into our MVPS framework, significantly enhancing the accuracy of shape
and reflectance recovery. Experimental results on both synthetic and real-world
datasets demonstrate the effectiveness of our approach.

</details>


### [90] [TSPO: Temporal Sampling Policy Optimization for Long-form Video Language Understanding](https://arxiv.org/abs/2508.04369)
*Canhui Tang,Zifan Han,Hongbo Sun,Sanping Zhou,Xuchong Zhang,Xin Wei,Ye Yuan,Jinglin Xu,Hao Sun*

Main category: cs.CV

TL;DR: TSPO 通过强化学习解决了 MLLM 在处理长视频时的稀疏帧采样问题，通过事件感知代理和联合决策优化实现了最先进的性能，并具有跨模型的可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频 MLLM 在处理长视频输入时面临上下文限制和训练成本的挑战，需要进行稀疏帧采样。现有的采样方法（训练无关的均匀采样或关键帧搜索）可能会遗漏关键事件或受预训练模型事件理解能力的限制。为解决这些问题，旨在通过强化学习来推进 MLLM 的长格式视频语言理解。

Method: 通过强化学习提出了一种可训练的事件感知时间代理，用于进行概率性关键帧选择，并提出了 TSPO 强化学习范式，将关键帧选择和语言生成建模为一个联合决策过程，实现了具有高效基于规则的奖励的端到端组相对优化。还提出了一个长视频训练数据构建流程，并结合了基于规则的回答准确性和时间定位奖励机制来优化时间采样策略。

Result: TSPO 在多个长视频理解基准测试中取得了最先进的性能。

Conclusion: TSPO 在多个长视频理解基准测试中实现了最先进的性能，并表现出跨不同前沿视频-MLLM 的可转移能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in vision-language tasks, yet they still face challenges when
processing long-duration video inputs. The limitation arises from MLLMs'
context limit and training costs, necessitating sparse frame sampling before
feeding videos into MLLMs. Existing video MLLMs adopt training-free uniform
sampling or keyframe search, which may miss critical events or be constrained
by the pre-trained models' event understanding capabilities. Meanwhile,
building a training-based method remains challenging due to the unsupervised
and non-differentiable nature of sparse frame sampling. To address these
problems, we propose Temporal Sampling Policy Optimization (TSPO), advancing
MLLMs' long-form video-language understanding via reinforcement learning.
Specifically, we first propose a trainable event-aware temporal agent, which
captures event-query correlation for performing probabilistic keyframe
selection. Then, we propose the TSPO reinforcement learning paradigm, which
models keyframe selection and language generation as a joint decision-making
process, enabling end-to-end group relative optimization with efficient
rule-based rewards. Furthermore, for the TSPO's training, we propose a long
video training data construction pipeline with comprehensive temporal data and
video Needle-in-a-Haystack data. Finally, we incorporate rule-based answering
accuracy and temporal locating reward mechanisms to optimize the temporal
sampling policy. Comprehensive experiments show that our TSPO achieves
state-of-the-art performance across multiple long video understanding
benchmarks, and shows transferable ability across different cutting-edge
Video-MLLMs.

</details>


### [91] [VisionTS++: Cross-Modal Time Series Foundation Model with Continual Pre-trained Visual Backbones](https://arxiv.org/abs/2508.04379)
*Lefei Shen,Mouxiang Chen,Xu Liu,Han Fu,Xiaoxue Ren,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.CV

TL;DR: VisionTS++是一种基于视觉模型的时间序列预测模型，通过引入数据过滤、多变量转换和多分位数预测等创新，有效解决了跨模态转移的挑战，并在多项基准测试中取得了领先成果。


<details>
  <summary>Details</summary>
Motivation: 尽管近期研究表明视觉模型在时间序列预测方面具有潜力，但由于数据模态、多变量预测和概率预测的差异，有效的跨模态转移仍然具有挑战性。本研究旨在弥合这些差距。

Method: VisionTS++通过持续预训练大型时间序列数据集，并引入了三个创新点：1. 基于视觉模型的过滤机制以识别高质量时间序列数据，以减轻模态差距并提高预训练稳定性；2. 颜色化多变量转换方法，将多变量时间序列转换为多子图RGB图像，捕捉变量间的复杂依赖关系；3. 使用并行重建头进行多分位数预测，以更灵活地逼近任意输出分布，无需严格的先验分布假设。

Result: 在独立同分布和非独立同分布时间序列预测基准测试中，VisionTS++取得了SOTA结果，在MSE降低方面优于专用TSFM 6%-44%，并在12个概率预测设置中的9个排名第一。

Conclusion: 该研究提出的VisionTS++在时间序列预测领域取得了SOTA结果，在MSE降低方面优于专用TSFM 6%-44%，并在12个概率预测设置中的9个排名第一，为跨模态知识转移建立了新范式，推动了通用TSFM的发展。

Abstract: Recent studies have revealed that vision models pre-trained on images can
perform well in time series forecasting by reformulating forecasting as an
image reconstruction task, suggesting their potential as universal time series
foundation models. However, effective cross-modal transfer from vision to time
series remains challenging due to three key discrepancies: (1) data-modality
gap between structured, bounded image data and unbounded, heterogeneous time
series; (2) multivariate-forecasting gap between standard RGB
three-channel-based vision models and the need to model time series with
arbitrary numbers of variates; and (3) probabilistic-forecasting gap between
the deterministic output formats of most vision models and the requirement for
uncertainty-aware probabilistic predictions. To bridge these gaps, we propose
VisionTS++, a vision-model-based TSFM that performs continual pre-training on
large-scale time series datasets, including 3 innovations: (1) a
vision-model-based filtering mechanism to identify high-quality time series
data, thereby mitigating modality gap and improving pre-training stability, (2)
a colorized multivariate conversion method that transforms multivariate time
series into multi-subfigure RGB images, capturing complex inter-variate
dependencies; and (3) a multi-quantile forecasting approach using parallel
reconstruction heads to generate forecasts of different quantile levels, thus
more flexibly approximating arbitrary output distributions without restrictive
prior distributional assumptions. Evaluated on both in-distribution and
out-of-distribution TSF benchmarks, \model achieves SOTA results, outperforming
specialized TSFMs by 6%-44% in MSE reduction and ranking first in 9 out of 12
probabilistic forecasting settings. Our work establishes a new paradigm for
cross-modal knowledge transfer, advancing the development of universal TSFMs.

</details>


### [92] [ProtoN: Prototype Node Graph Neural Network for Unconstrained Multi-Impression Ear Recognition](https://arxiv.org/abs/2508.04381)
*Santhoshkumar Peddi,Sadhvik Bathini,Arun Balasubramanian,Monalisa Sarma,Debasis Samanta*

Main category: cs.CV

TL;DR: ProtoN是一种新颖的图神经网络框架，通过联合处理多个耳部印象并优化表示，显著提高了在数据稀疏情况下的耳部识别精度。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有耳部生物识别方法在数据稀疏和类内变异性大的情况下的局限性，并解决现有方法孤立地从单个印象提取身份特征、限制其捕获一致性和判别性表示能力的问题。

Method: 提出了一种名为ProtoN的少数样本学习框架，该框架使用基于图的方法联合处理同一身份的多个印象。每个印象被表示为一个节点，并与一个编码身份级别信息的学习原型节点一起构成一个特定类别的图。然后，使用专门设计的原型图神经网络（PGNN）层来处理该图，通过双路径消息传递机制来优化印象和原型表示。此外，PGNN还包含一个跨图原型对齐策略，通过增强类内紧密度和保持类间区别来提高类可分性。最后，采用混合损失函数来平衡情节和全局分类目标，以改善嵌入空间的整体结构。

Result: ProtoN框架在五个基准耳部数据集上取得了最先进的性能，识别准确率高达99.60%，错误率低至0.025。

Conclusion: ProtoN框架在有限数据条件下实现了最先进的少数样本耳部识别性能，在五个基准数据集上取得了高达99.60%的识别准确率和低至0.025的错误率。

Abstract: Ear biometrics offer a stable and contactless modality for identity
recognition, yet their effectiveness remains limited by the scarcity of
annotated data and significant intra-class variability. Existing methods
typically extract identity features from individual impressions in isolation,
restricting their ability to capture consistent and discriminative
representations. To overcome these limitations, a few-shot learning framework,
ProtoN, is proposed to jointly process multiple impressions of an identity
using a graph-based approach. Each impression is represented as a node in a
class-specific graph, alongside a learnable prototype node that encodes
identity-level information. This graph is processed by a Prototype Graph Neural
Network (PGNN) layer, specifically designed to refine both impression and
prototype representations through a dual-path message-passing mechanism. To
further enhance discriminative power, the PGNN incorporates a cross-graph
prototype alignment strategy that improves class separability by enforcing
intra-class compactness while maintaining inter-class distinction.
Additionally, a hybrid loss function is employed to balance episodic and global
classification objectives, thereby improving the overall structure of the
embedding space. Extensive experiments on five benchmark ear datasets
demonstrate that ProtoN achieves state-of-the-art performance, with Rank-1
identification accuracy of up to 99.60% and an Equal Error Rate (EER) as low as
0.025, showing the effectiveness for few-shot ear recognition under limited
data conditions.

</details>


### [93] [Deep Learning-based Scalable Image-to-3D Facade Parser for Generating Thermal 3D Building Models](https://arxiv.org/abs/2508.04406)
*Yinan Yu,Alex Gonzalez-Caceres,Samuel Scheidegger,Sanjay Somanath,Alexander Hollberg*

Main category: cs.CV

TL;DR: SI3FP流水线利用计算机视觉和深度学习从图像中提取几何图形，生成LoD3热模型，以支持早期建筑翻新规划。


<details>
  <summary>Details</summary>
Motivation: 对现有建筑进行翻新对于应对气候影响至关重要。早期翻新规划需要基于3D详细程度（LoD）3的热模型进行模拟，其中包含窗户等特征。然而，此类特征的可扩展且准确的识别仍然是一个挑战。

Method: SI3FP是一个流水线，它利用计算机视觉和深度学习从图像中提取几何图形，从而生成LoD3热模型。

Result: SI3FP在窗墙比估算方面实现了约5%的误差，证明了其在早期翻新分析方面具有足够的准确性。该流水线有助于大规模节能翻新规划，并在城市发展和规划中具有更广泛的应用。

Conclusion: 该方法通过直接在正射影像平面上对几何图元进行建模，提供了一个统一的接口，同时减少了透视畸变。SI3FP支持稀疏（例如，谷歌街景）和密集（例如，手持相机）的数据源。SI3FP在典型的瑞典住宅建筑上进行了测试，在窗墙比估算方面实现了约5%的误差，证明了其在早期翻新分析方面具有足够的准确性。

Abstract: Renovating existing buildings is essential for climate impact. Early-phase
renovation planning requires simulations based on thermal 3D models at Level of
Detail (LoD) 3, which include features like windows. However, scalable and
accurate identification of such features remains a challenge. This paper
presents the Scalable Image-to-3D Facade Parser (SI3FP), a pipeline that
generates LoD3 thermal models by extracting geometries from images using both
computer vision and deep learning. Unlike existing methods relying on
segmentation and projection, SI3FP directly models geometric primitives in the
orthographic image plane, providing a unified interface while reducing
perspective distortions. SI3FP supports both sparse (e.g., Google Street View)
and dense (e.g., hand-held camera) data sources. Tested on typical Swedish
residential buildings, SI3FP achieved approximately 5% error in window-to-wall
ratio estimates, demonstrating sufficient accuracy for early-stage renovation
analysis. The pipeline facilitates large-scale energy renovation planning and
has broader applications in urban development and planning.

</details>


### [94] [Thinking With Videos: Multimodal Tool-Augmented Reinforcement Learning for Long Video Reasoning](https://arxiv.org/abs/2508.04416)
*Haoji Zhang,Xin Gu,Jiawen Li,Chixiang Ma,Sule Bai,Chubin Zhang,Bowen Zhang,Zhichao Zhou,Dongliang He,Yansong Tang*

Main category: cs.CV

TL;DR: VITAL是一个新的视频推理框架，通过工具增强学习，使用户能够按需采样视频帧并生成多模态的思维链，以实现精确的长视频推理。该框架在视频问答和时间定位任务上均表现出色，尤其是在处理长视频时。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的CoT推理方法在多模态大语言模型（MLLMs）上存在跨模态交互有限和幻觉增加的问题，尤其是在处理长视频或长推理链时。

Method: 提出了一种名为VITAL（Video Intelligence via Tool-Augmented Learning）的新颖端到端agentic视频推理框架，该框架包含一个视觉工具箱，能够按需密集采样新的视频帧并生成多模态的思维链（CoT）以实现精确的长视频推理。同时，提出了一种难度感知分组相对策略优化（DGRPO）算法来缓解多任务强化学习中的难度不平衡问题。

Result: VITAL在11个具有挑战性的视频理解基准测试上进行了广泛实验，证明了其卓越的推理能力，在视频问答和时间定位任务上均优于现有方法，特别是在长视频场景下。

Conclusion: VITAL框架在视频问答和时间定位任务上表现出色，尤其是在长视频场景下，超越了现有方法。

Abstract: The video reasoning ability of multimodal large language models (MLLMs) is
crucial for downstream tasks like video question answering and temporal
grounding. While recent approaches have explored text-based chain-of-thought
(CoT) reasoning for MLLMs, these methods often suffer from limited cross-modal
interaction and increased hallucination, especially with longer videos or
reasoning chains. To address these challenges, we propose Video Intelligence
via Tool-Augmented Learning (VITAL), a novel end-to-end agentic video reasoning
framework. With a visual toolbox, the model can densely sample new video frames
on demand and generate multimodal CoT for precise long video reasoning. We
observe that temporal grounding and question answering are mutually beneficial
for video understanding tasks. Therefore, we construct two high-quality
multi-task video reasoning datasets MTVR-CoT-72k for supervised fine-tuning and
MTVR-RL-110k for reinforcement learning. Moreover, we propose a
Difficulty-aware Group Relative Policy Optimization algorithm (DGRPO) to
mitigate difficulty imbalance in multi-task reinforcement learning. Extensive
experiments on 11 challenging video understanding benchmarks demonstrate the
advanced reasoning ability of VITAL, outperforming existing methods in video
question answering and temporal grounding tasks, especially in long video
scenarios. All code, data and model weight will be made publicly available.

</details>


### [95] [Efficient Inter-Task Attention for Multitask Transformer Models](https://arxiv.org/abs/2508.04422)
*Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: Transformer在多任务学习中计算量大，提出可变形跨任务自注意力机制，降低计算量和延迟，并提升预测精度。


<details>
  <summary>Details</summary>
Motivation: Transformer在多任务学习中，由于注意力矩阵大小随任务数量的二次方增长，计算成本高昂，限制了其在实际应用中的可行性。

Method: 提出了一种新颖的可变形跨任务自注意力机制，用于解决Transformer在多任务学习中面临的计算瓶颈问题。

Result: 在NYUD-v2和PASCAL-Context数据集上的实验表明，该方法在计算量和推理延迟方面降低了两个数量级，同时在各项任务的预测质量指标上取得了高达7.4%的提升。

Conclusion: Transformer在计算机视觉和深度学习领域已成为许多应用的首选架构，但在多任务学习中，其多头注意力机制由于注意力矩阵的大小随任务数量的增加而二次方增长，会触及实际硬件限制的计算可行性极限。本研究提出了一种新颖的用于多任务模型的可变形跨任务自注意力机制，能够更有效地聚合来自不同任务的特征图信息。

Abstract: In both Computer Vision and the wider Deep Learning field, the Transformer
architecture is well-established as state-of-the-art for many applications. For
Multitask Learning, however, where there may be many more queries necessary
compared to single-task models, its Multi-Head-Attention often approaches the
limits of what is computationally feasible considering practical hardware
limitations. This is due to the fact that the size of the attention matrix
scales quadratically with the number of tasks (assuming roughly equal numbers
of queries for all tasks). As a solution, we propose our novel Deformable
Inter-Task Self-Attention for Multitask models that enables the much more
efficient aggregation of information across the feature maps from different
tasks. In our experiments on the NYUD-v2 and PASCAL-Context datasets, we
demonstrate an order-of-magnitude reduction in both FLOPs count and inference
latency. At the same time, we also achieve substantial improvements by up to
7.4% in the individual tasks' prediction quality metrics.

</details>


### [96] [Composed Object Retrieval: Object-level Retrieval via Composed Expressions](https://arxiv.org/abs/2508.04424)
*Tong Wang,Guanyu Yang,Nian Liu,Zongyan Han,Jinxing Zhou,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: 提出了一种新的“组成对象检索”（COR）任务和首个大规模数据集COR127K，以实现比现有基于图像的检索更精确的对象级检索。还提出了一个名为CORE的端到端模型，该模型在检索和分割方面取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 现有图像检索方法（CIR）仅限于图像级匹配，无法定位特定对象。为了解决这个问题，我们提出了组成对象检索（COR）任务，实现了对象级别的精度，并能根据包含参考对象和检索文本的组合表达式来检索和分割目标对象。

Method: 提出了一种名为CORE的统一端到端模型，该模型整合了参考区域编码、自适应视觉-文本交互和区域级对比学习。

Result: COR127K是首个大规模COR基准，包含127,166个检索三元组，涵盖408个类别的各种语义变换。

Conclusion: CORE模型在基础和新颖的类别中都显著优于现有模型，为这项具有挑战性的任务提供了一个简单有效的基线，并为细粒度的多模态检索研究开辟了新的方向。

Abstract: Retrieving fine-grained visual content based on user intent remains a
challenge in multi-modal systems. Although current Composed Image Retrieval
(CIR) methods combine reference images with retrieval texts, they are
constrained to image-level matching and cannot localize specific objects. To
this end, we propose Composed Object Retrieval (COR), a brand-new task that
goes beyond image-level retrieval to achieve object-level precision, allowing
the retrieval and segmentation of target objects based on composed expressions
combining reference objects and retrieval texts. COR presents significant
challenges in retrieval flexibility, which requires systems to identify
arbitrary objects satisfying composed expressions while avoiding semantically
similar but irrelevant negative objects within the same scene. We construct
COR127K, the first large-scale COR benchmark that contains 127,166 retrieval
triplets with various semantic transformations in 408 categories. We also
present CORE, a unified end-to-end model that integrates reference region
encoding, adaptive visual-textual interaction, and region-level contrastive
learning. Extensive experiments demonstrate that CORE significantly outperforms
existing models in both base and novel categories, establishing a simple and
effective baseline for this challenging task while opening new directions for
fine-grained multi-modal retrieval research.

</details>


### [97] [Benchmarking Foundation Models for Mitotic Figure Classification](https://arxiv.org/abs/2508.04441)
*Jonas Ammeling,Jonathan Ganz,Emely Rosbach,Ludwig Lausser,Christof A. Bertram,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: Foundation models, especially when adapted with LoRA, excel at mitotic figure classification in pathology, requiring less data and generalizing better to new tumor types compared to linear probing, although traditional methods remain competitive.


<details>
  <summary>Details</summary>
Motivation: The availability of labeled images in medical domains like pathology is often limited. Self-supervised learning and foundation models offer a way to leverage vast amounts of unlabeled data to improve performance on tasks with limited labeled data, such as mitotic figure classification, which is important for tumor prognosis and grading.

Method: This work investigates the use of foundation models for mitotic figure classification. It examines data scaling laws on various foundation models and evaluates their robustness to unseen tumor domains. The study adapts models using both linear probing and low-rank adaptation (LoRA) of their attention mechanisms. Performance is compared against end-to-end trained CNNs and Vision Transformers.

Result: LoRA-adapted foundation models outperform linear probing adapted models, achieving high performance with significantly less data. LoRA adaptation also substantially improves performance on unseen tumor domains. Full fine-tuning of traditional architectures still provides competitive results.

Conclusion: LoRA-adapted foundation models demonstrate superior performance in mitotic figure classification, achieving near 100% performance with only 10% of the training data. They also significantly reduce the out-of-domain performance gap. However, fully fine-tuned traditional architectures remain competitive.

Abstract: The performance of deep learning models is known to scale with data quantity
and diversity. In pathology, as in many other medical imaging domains, the
availability of labeled images for a specific task is often limited.
Self-supervised learning techniques have enabled the use of vast amounts of
unlabeled data to train large-scale neural networks, i.e., foundation models,
that can address the limited data problem by providing semantically rich
feature vectors that can generalize well to new tasks with minimal training
effort increasing model performance and robustness. In this work, we
investigate the use of foundation models for mitotic figure classification. The
mitotic count, which can be derived from this classification task, is an
independent prognostic marker for specific tumors and part of certain tumor
grading systems. In particular, we investigate the data scaling laws on
multiple current foundation models and evaluate their robustness to unseen
tumor domains. Next to the commonly used linear probing paradigm, we also adapt
the models using low-rank adaptation (LoRA) of their attention mechanisms. We
compare all models against end-to-end-trained baselines, both CNNs and Vision
Transformers. Our results demonstrate that LoRA-adapted foundation models
provide superior performance to those adapted with standard linear probing,
reaching performance levels close to 100% data availability with only 10% of
training data. Furthermore, LoRA-adaptation of the most recent foundation
models almost closes the out-of-domain performance gap when evaluated on unseen
tumor domains. However, full fine-tuning of traditional architectures still
yields competitive performance.

</details>


### [98] [Boosting Visual Knowledge-Intensive Training for LVLMs Through Causality-Driven Visual Object Completion](https://arxiv.org/abs/2508.04453)
*Qingguo Hu,Ante Wang,Jia Song,Delai Qiu,Qingsong Liu,Jinsong Su*

Main category: cs.CV

TL;DR: 通过一个名为CVC的新型任务和自动化的数据生成流程，该研究提出了一种自我改进框架，有效提升了大型视觉语言模型在精细视觉感知任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（LVLMs）在需要深度视觉感知的任务（如识别图像间的细微差别）上表现仍有不足。这可能是因为常用的指令微调语料库缺乏足够的视觉知识，导致模型的视觉感知和推理能力不足。

Method: 提出了一种名为因果视觉对象补全（CVC）的新型视觉知识密集型任务。该任务要求LVLMs根据目标对象与其他可见信息之间的因果关系来推断图像中被遮掩的对象。研究还开发了一个自动化的实例构建流程，用于廉价地获取丰富的CVC任务示例，无需依赖高级模型或人工标注。然后，利用这些生成的实例，LVLMs通过试错学习进行自我改进。

Result: 实验证明，所提出的自我改进框架在四个专业任务和四个综合基准测试上均带来了显著的性能提升。与基线模型相比，使用LLaVA-1.5-7B和LLaVA-1.5-13B时，该方法在专业任务上的平均提升分别达到了5.4%和4.0%。

Conclusion: 该研究通过引入一种名为因果视觉对象补全（CVC）的新型视觉知识密集型任务，并利用其进行模型自我改进，显著提升了大型视觉语言模型（LVLMs）在需要深度视觉感知的任务上的表现。实验结果表明，该方法在四个专业任务和四个综合基准测试中均取得了显著的性能提升，平均分别提高了5.4%和4.0%（使用LLaVA-1.5-7B和LLaVA-1.5-13B）。

Abstract: Large Vision-Language Models (LVLMs) have experienced significant
advancements in recent years. However, their performance still falls short in
tasks requiring deep visual perception, such as identifying subtle differences
between images. A potential cause is the scarcity of visual knowledge in
popular instruction-tuning corpora, resulting in inadequate visual perception
and reasoning capabilities. To address this challenge, we introduce a
self-improvement framework grounded in a novel visual knowledge-intensive task,
\underline{C}ausality-driven \underline{V}isual object \underline{C}ompletion
(CVC). This task requires LVLMs to infer the masked object in an image based on
its \textit{causal} relationships with the other visible information. We first
obtain rich examples cheaply through our automated instance construction
pipeline, without relying on sophisticated LVLMs (\textit{e.g.}, GPT-4V) or
human assistance. Then, LVLMs effectively self-improve through trial and error
learning using these created instances. Our experiments demonstrate substantial
gains across four challenging specialized tasks and four widely-used
comprehensive benchmarks. Especially on specialized tasks, our method achieves
an average improvement of 5.4\% and 4.0\% compared to the corresponding
baselines when utilizing LLaVA-1.5-7B and LLaVA-1.5-13B, respectively. The code
is available at https://github.com/XMUDeepLIT/CVC.

</details>


### [99] [4DVD: Cascaded Dense-view Video Diffusion Model for High-quality 4D Content Generation](https://arxiv.org/abs/2508.04467)
*Shuzhou Yang,Xiaodong Cun,Xiaoyu Li,Yaowei Li,Jian Zhang*

Main category: cs.CV

TL;DR: 4DVD是一个级联视频扩散模型，通过解耦多视图布局生成和结构感知的条件生成，实现了高质量的4D内容生成，并在相关任务中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决直接生成高维数据（如4D）的复杂性问题，提出4DVD模型。

Method: 4DVD采用级联视频扩散模型，将4D内容的生成解耦为两个子任务：粗粒度的多视图布局生成和结构感知的条件生成。首先，给定单视角视频，模型预测其布局的密集视图内容，并保持良好的跨视图和时间一致性。然后，利用预测的布局先验，结合输入单视角视频的精细外观内容，生成高质量的密集视图视频。

Result: 4DVD能够生成高质量的密集视图视频，并能准确优化显式的4D表示（如4D高斯），从而实现更广泛的应用。实验证明了其在新型视图合成和4D生成任务上的优越性能。

Conclusion: 4DVD在新型视图合成和4D生成方面均取得了最先进的性能。

Abstract: Given the high complexity of directly generating high-dimensional data such
as 4D, we present 4DVD, a cascaded video diffusion model that generates 4D
content in a decoupled manner. Unlike previous multi-view video methods that
directly model 3D space and temporal features simultaneously with stacked cross
view/temporal attention modules, 4DVD decouples this into two subtasks: coarse
multi-view layout generation and structure-aware conditional generation, and
effectively unifies them. Specifically, given a monocular video, 4DVD first
predicts the dense view content of its layout with superior cross-view and
temporal consistency. Based on the produced layout priors, a structure-aware
spatio-temporal generation branch is developed, combining these coarse
structural priors with the exquisite appearance content of input monocular
video to generate final high-quality dense-view videos. Benefit from this,
explicit 4D representation~(such as 4D Gaussian) can be optimized accurately,
enabling wider practical application. To train 4DVD, we collect a dynamic 3D
object dataset, called D-Objaverse, from the Objaverse benchmark and render 16
videos with 21 frames for each object. Extensive experiments demonstrate our
state-of-the-art performance on both novel view synthesis and 4D generation.
Our project page is https://4dvd.github.io/

</details>


### [100] [Zero-Residual Concept Erasure via Progressive Alignment in Text-to-Image Model](https://arxiv.org/abs/2508.04472)
*Hongxu Chen,Zhen Wang,Taoran Mei,Lin Li,Bowei Zhu,Runshi Li,Long Chen*

Main category: cs.CV

TL;DR: ErasePro通过零残差约束和层级更新策略，解决了现有概念擦除方法擦除不彻底和生成质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有闭式概念擦除方法存在两个未被充分认识的局限性：1）由于“非零对齐残差”，尤其是在文本提示相对复杂的情况下，擦除效果往往不完整。2）由于参数更新总是集中在少数深层层，可能会导致生成质量下降。

Method: ErasePro提出了一种新颖的闭式方法，首先在优化目标中引入严格的零残差约束，确保目标和锚定概念特征之间的完美对齐，从而实现更彻底的概念擦除。其次，它采用了渐进式的、层级化的更新策略，从浅层到深层逐渐将目标概念特征转移到锚定概念特征，以减少对敏感深层的偏差，并保持生成质量。

Result: 实验结果表明，ErasePro在实例、艺术风格和裸体擦除等不同概念擦除任务上均有效。

Conclusion: ErasePro通过引入严格的零残差约束和渐进式、层级更新策略，实现了更彻底的概念擦除，并更好地保持了生成质量。在不同概念擦除任务（包括实例、艺术风格和裸体擦除）上的实验结果证明了ErasePro的有效性。

Abstract: Concept Erasure, which aims to prevent pretrained text-to-image models from
generating content associated with semantic-harmful concepts (i.e., target
concepts), is getting increased attention. State-of-the-art methods formulate
this task as an optimization problem: they align all target concepts with
semantic-harmless anchor concepts, and apply closed-form solutions to update
the model accordingly. While these closed-form methods are efficient, we argue
that existing methods have two overlooked limitations: 1) They often result in
incomplete erasure due to "non-zero alignment residual", especially when text
prompts are relatively complex. 2) They may suffer from generation quality
degradation as they always concentrate parameter updates in a few deep layers.
To address these issues, we propose a novel closed-form method ErasePro: it is
designed for more complete concept erasure and better preserving overall
generative quality. Specifically, ErasePro first introduces a strict
zero-residual constraint into the optimization objective, ensuring perfect
alignment between target and anchor concept features and enabling more complete
erasure. Secondly, it employs a progressive, layer-wise update strategy that
gradually transfers target concept features to those of the anchor concept from
shallow to deep layers. As the depth increases, the required parameter changes
diminish, thereby reducing deviations in sensitive deep layers and preserving
generative quality. Empirical results across different concept erasure tasks
(including instance, art style, and nudity erasure) have demonstrated the
effectiveness of our ErasePro.

</details>


### [101] [QuantVSR: Low-Bit Post-Training Quantization for Real-World Video Super-Resolution](https://arxiv.org/abs/2508.04485)
*Bowen Chai,Zheng Chen,Libo Zhu,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantVSR是一种用于真实世界视频超分辨率（VSR）的低比特量化模型，它使用时空复杂度感知（STCA）机制和可学习偏置对齐（LBA）模块来提高性能和效率。


<details>
  <summary>Details</summary>
Motivation: 然而，扩散模型的慢速处理速度和大量资源消耗阻碍了它们的实际应用和部署。量化为压缩VSR模型提供了潜在的解决方案。然而，由于其时间特性和高保真度要求，量化VSR模型具有挑战性。为了解决这些问题，我们提出了QuantVSR，一种用于真实世界视频超分辨率（VSR）的低比特量化模型。

Method: 我们提出了一个时空复杂度感知（STCA）机制，我们首先利用校准数据集来测量每个层的空间和时间复杂度。基于这些统计数据，我们为低秩全精度（FP）辅助分支分配特定于层的秩。随后，我们联合优化FP和低比特分支以实现同步优化。此外，我们还提出了一个可学习的偏置对齐（LBA）模块来减少有偏置的量化误差。

Result: QuantVSR是一种用于真实世界视频超分辨率（VSR）的低比特量化模型。

Conclusion: QuantVSR在合成和真实世界数据集上的广泛实验表明，我们的方法获得了与全精度（FP）模型相当的性能，并且明显优于最近领先的低比特量化方法。

Abstract: Diffusion models have shown superior performance in real-world video
super-resolution (VSR). However, the slow processing speeds and heavy resource
consumption of diffusion models hinder their practical application and
deployment. Quantization offers a potential solution for compressing the VSR
model. Nevertheless, quantizing VSR models is challenging due to their temporal
characteristics and high fidelity requirements. To address these issues, we
propose QuantVSR, a low-bit quantization model for real-world VSR. We propose a
spatio-temporal complexity aware (STCA) mechanism, where we first utilize the
calibration dataset to measure both spatial and temporal complexities for each
layer. Based on these statistics, we allocate layer-specific ranks to the
low-rank full-precision (FP) auxiliary branch. Subsequently, we jointly refine
the FP and low-bit branches to achieve simultaneous optimization. In addition,
we propose a learnable bias alignment (LBA) module to reduce the biased
quantization errors. Extensive experiments on synthetic and real-world datasets
demonstrate that our method obtains comparable performance with the FP model
and significantly outperforms recent leading low-bit quantization methods. Code
is available at: https://github.com/bowenchai/QuantVSR.

</details>


### [102] [Learning Robust Intervention Representations with Delta Embeddings](https://arxiv.org/abs/2508.04492)
*Panagiotis Alimisis,Christos Diou*

Main category: cs.CV

TL;DR: 本研究提出因果增量嵌入来表示干预，以提高模型的OOD鲁棒性，并在实验中取得了显著成效。


<details>
  <summary>Details</summary>
Motivation: 为了提高模型泛化能力和鲁棒性，因果表示学习受到广泛关注。现有研究主要集中在识别和表示因果模型下的场景变量，而对干预本身的表示关注较少。

Method: 提出了一种在潜在空间中表示干预的新策略，即因果增量嵌入（Causal Delta Embedding），并构建了一个无需额外监督即可从图像对中学习因果表示的框架。

Result: 所提出的因果增量嵌入被证明在分布外（OOD）设置中非常有效，在合成和真实世界基准测试中均显著超过了基线性能。

Conclusion: 本研究提出的因果增量嵌入对于处理分布外（OOD）的鲁棒性问题非常有效，在合成和真实世界的基准测试中均显著优于基线方法。

Abstract: Causal representation learning has attracted significant research interest
during the past few years, as a means for improving model generalization and
robustness. Causal representations of interventional image pairs, have the
property that only variables corresponding to scene elements affected by the
intervention / action are changed between the start state and the end state.
While most work in this area has focused on identifying and representing the
variables of the scene under a causal model, fewer efforts have focused on
representations of the interventions themselves. In this work, we show that an
effective strategy for improving out of distribution (OOD) robustness is to
focus on the representation of interventions in the latent space. Specifically,
we propose that an intervention can be represented by a Causal Delta Embedding
that is invariant to the visual scene and sparse in terms of the causal
variables it affects. Leveraging this insight, we propose a framework that is
capable of learning causal representations from image pairs, without any
additional supervision. Experiments in the Causal Triplet challenge demonstrate
that Causal Delta Embeddings are highly effective in OOD settings,
significantly exceeding baseline performance in both synthetic and real-world
benchmarks.

</details>


### [103] [MonoCloth: Reconstruction and Animation of Cloth-Decoupled Human Avatars from Monocular Videos](https://arxiv.org/abs/2508.04505)
*Daisheng Jin,Ying He*

Main category: cs.CV

TL;DR: MonoCloth是一种从单眼视频重建和动画化3D人体服装模型的新方法，通过部件分解和衣物模拟模块提高了重建质量和动画真实感，并支持服装迁移等任务。


<details>
  <summary>Details</summary>
Motivation: 从单眼视频重建具有复杂非刚性运动的逼真3D人体虚拟形象具有挑战性，因为单眼输入存在几何信息有限的问题。

Method: MonoCloth采用基于部件的分解策略，将人体分解为身体、面部、手部和服装，并为面部和手部进行详细的几何恢复，为服装设计了专门的衣物模拟模块。

Result: 实验结果表明，MonoCloth在视觉重建质量和动画真实感方面优于现有方法。

Conclusion: MonoCloth在视觉重建质量和动画真实感方面优于现有方法，并且支持服装迁移等附加任务，具有多功能性和实用性。

Abstract: Reconstructing realistic 3D human avatars from monocular videos is a
challenging task due to the limited geometric information and complex non-rigid
motion involved. We present MonoCloth, a new method for reconstructing and
animating clothed human avatars from monocular videos. To overcome the
limitations of monocular input, we introduce a part-based decomposition
strategy that separates the avatar into body, face, hands, and clothing. This
design reflects the varying levels of reconstruction difficulty and deformation
complexity across these components. Specifically, we focus on detailed geometry
recovery for the face and hands. For clothing, we propose a dedicated cloth
simulation module that captures garment deformation using temporal motion cues
and geometric constraints. Experimental results demonstrate that MonoCloth
improves both visual reconstruction quality and animation realism compared to
existing methods. Furthermore, thanks to its part-based design, MonoCloth also
supports additional tasks such as clothing transfer, underscoring its
versatility and practical utility.

</details>


### [104] [Skeleton Motion Words for Unsupervised Skeleton-Based Temporal Action Segmentation](https://arxiv.org/abs/2508.04513)
*Uzay Gökay,Federico Spurio,Dominik R. Bach,Juergen Gall*

Main category: cs.CV

TL;DR: 提出了一种新颖的无监督骨骼时间动作分割方法，使用序列到序列自编码器和运动词发现动作聚类，并在三个数据集上超越了现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的基于骨骼的时间动作分割方法主要是监督的，并且需要昂贵的数据标注。相比之下，现有的无监督时间动作分割方法主要关注视频数据，而骨骼序列尽管与实际应用相关、鲁棒且能保护隐私，但仍未得到充分探索。

Method: 利用序列到序列的时间自编码器，使嵌入空间中的不同关节信息保持分离。然后将潜在的骨骼序列划分为不重叠的块并进行量化，以获得独特的骨骼运动词，从而驱动语义上有意义的动作聚类的发现。

Result: 在HuGaDB、LARa和BABEL数据集上的评估结果优于当前最先进的无监督方法。

Conclusion: 该方法在三个常用的基于骨骼的数据集（HuGaDB、LARa和BABEL）上进行了充分评估，结果表明该模型优于当前最先进的无监督时间动作分割方法。

Abstract: Current state-of-the-art methods for skeleton-based temporal action
segmentation are predominantly supervised and require annotated data, which is
expensive to collect. In contrast, existing unsupervised temporal action
segmentation methods have focused primarily on video data, while skeleton
sequences remain underexplored, despite their relevance to real-world
applications, robustness, and privacy-preserving nature. In this paper, we
propose a novel approach for unsupervised skeleton-based temporal action
segmentation. Our method utilizes a sequence-to-sequence temporal autoencoder
that keeps the information of the different joints disentangled in the
embedding space. Latent skeleton sequences are then divided into
non-overlapping patches and quantized to obtain distinctive skeleton motion
words, driving the discovery of semantically meaningful action clusters. We
thoroughly evaluate the proposed approach on three widely used skeleton-based
datasets, namely HuGaDB, LARa, and BABEL. The results demonstrate that our
model outperforms the current state-of-the-art unsupervised temporal action
segmentation methods. Code is available at https://github.com/bachlab/SMQ .

</details>


### [105] [RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection](https://arxiv.org/abs/2508.04524)
*Tianxiao Li,Zhenglin Huang,Haiquan Wen,Yiwei He,Shuchang Lyu,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: RAIDX利用RAG和GRPO技术，实现了高精度的深度伪造图像检测，并能提供细粒度的文本和视觉解释，解决了现有方法的透明度和标注依赖问题。


<details>
  <summary>Details</summary>
Motivation: 目前的深度伪造检测方法要么缺乏透明度（将检测视为分类任务而不解释决策），要么严重依赖耗时的人工标注。该研究旨在解决这些问题，提供一种更准确、更透明的检测方法。

Method: RAIDX框架集成了检索增强生成（RAG）和组相对策略优化（GRPO）。RAG用于整合外部知识以提高检测准确性，而GRPO则用于自主生成细粒度的文本解释和显著性图，无需手动标注。

Result: RAIDX在多个基准测试中均表现出色，能够准确识别真实或伪造图像，并提供文本描述和显著性图的解释性说明，实现了最先进的检测性能和可解释性。

Conclusion: RAIDX是第一个结合了检索增强生成（RAG）和基于组的相对策略优化（GRPO）的统一框架，用于深度伪造图像检测和可解释性，它实现了最先进的检测性能，同时提高了透明度。

Abstract: The rapid advancement of AI-generation models has enabled the creation of
hyperrealistic imagery, posing ethical risks through widespread misinformation.
Current deepfake detection methods, categorized as face specific detectors or
general AI-generated detectors, lack transparency by framing detection as a
classification task without explaining decisions. While several LLM-based
approaches offer explainability, they suffer from coarse-grained analyses and
dependency on labor-intensive annotations. This paper introduces RAIDX
(Retrieval-Augmented Image Deepfake Detection and Explainability), a novel
deepfake detection framework integrating Retrieval-Augmented Generation (RAG)
and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and
decision explainability. Specifically, RAIDX leverages RAG to incorporate
external knowledge for improved detection accuracy and employs GRPO to
autonomously generate fine-grained textual explanations and saliency maps,
eliminating the need for extensive manual annotations. Experiments on multiple
benchmarks demonstrate RAIDX's effectiveness in identifying real or fake, and
providing interpretable rationales in both textual descriptions and saliency
maps, achieving state-of-the-art detection performance while advancing
transparency in deepfake identification. RAIDX represents the first unified
framework to synergize RAG and GRPO, addressing critical gaps in accuracy and
explainability. Our code and models will be publicly available.

</details>


### [106] [No Masks Needed: Explainable AI for Deriving Segmentation from Classification](https://arxiv.org/abs/2508.04534)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: 提出了一种专门针对医学图像的微调预训练模型的新方法，并整合了可解释人工智能以提高分割准确性，在多个医学数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督分割方法在医学成像领域应用不佳的问题，并为医学图像分割提供一种新的方法。

Method: 微调预训练模型，并整合可解释人工智能以生成相关性分数，以增强分割过程。

Result: 实现了准确的分割，并且具有广泛的处理能力。

Conclusion: 该方法在CBIS-DDSM、NuInsSeg和Kvasir-SEG等数据集上取得了改进的结果。

Abstract: Medical image segmentation is vital for modern healthcare and is a key
element of computer-aided diagnosis. While recent advancements in computer
vision have explored unsupervised segmentation using pre-trained models, these
methods have not been translated well to the medical imaging domain. In this
work, we introduce a novel approach that fine-tunes pre-trained models
specifically for medical images, achieving accurate segmentation with extensive
processing. Our method integrates Explainable AI to generate relevance scores,
enhancing the segmentation process. Unlike traditional methods that excel in
standard benchmarks but falter in medical applications, our approach achieves
improved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.

</details>


### [107] [TopKD: Top-scaled Knowledge Distillation](https://arxiv.org/abs/2508.04539)
*Qi Wang,Jinjia Zhou*

Main category: cs.CV

TL;DR: TopKD通过挖掘教师模型Logits中的Top-K知识，改进了知识蒸馏方法，在多种数据集和模型上都取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 该研究揭示了在知识蒸馏中，教师模型的Logits分布中包含的关键的Top-K知识，而这部分信息在以往的研究中常被忽视。

Method: TopKD框架包含一个自适应放大多数信息logits的Top-K缩放模块（TSM）和一个提供有针对性有效监督的Top-K解耦损失（TDL）。

Result: 在CIFAR-100、ImageNet、STL-10和Tiny-ImageNet等数据集上的大量实验表明，TopKD持续超越现有的最先进的蒸馏方法，并且在蒸馏视觉Transformer等不同网络架构时展现出显著效果。

Conclusion: TopKD框架能够有效提升知识蒸馏的效果，尤其在模型结构多样性方面表现出色，证明了logits在知识蒸馏领域的重要潜力。

Abstract: Recent advances in knowledge distillation (KD) predominantly emphasize
feature-level knowledge transfer, frequently overlooking critical information
embedded within the teacher's logit distributions. In this paper, we revisit
logit-based distillation and reveal an underexplored yet critical element:
Top-K knowledge. Motivated by this insight, we propose Top-scaled Knowledge
Distillation (TopKD), a simple, efficient, and architecture-agnostic framework
that significantly enhances logit-based distillation. TopKD consists of two
main components: (1) a Top-K Scaling Module (TSM), which adaptively amplifies
the most informative logits, and (2) a Top-K Decoupled Loss (TDL), which offers
targeted and effective supervision. Notably, TopKD integrates seamlessly into
existing KD methods without introducing extra modules or requiring
architectural changes. Extensive experiments on CIFAR-100, ImageNet, STL-10,
and Tiny-ImageNet demonstrate that TopKD consistently surpasses
state-of-the-art distillation methods. Moreover, our method demonstrates
substantial effectiveness when distilling Vision Transformers, underscoring its
versatility across diverse network architectures. These findings highlight the
significant potential of logits to advance knowledge distillation.

</details>


### [108] [InceptoFormer: A Multi-Signal Neural Framework for Parkinson's Disease Severity Evaluation from Gait](https://arxiv.org/abs/2508.04540)
*Safwen Naimi,Arij Said,Wassim Bouachir,Guillaume-Alexandre Bilodeau*

Main category: cs.CV

TL;DR: 提出InceptoFormer框架，结合Inception1D和Transformer，通过步态分析提高帕金森病严重程度评估的准确率至96.6%。


<details>
  <summary>Details</summary>
Motivation: 为了通过步态动力学分析来评估帕金森病的严重程度，并解决PD严重程度分级中的类别不平衡问题。

Method: 提出了一种名为InceptoFormer的多信号神经网络框架，该框架结合了Inception1D（用于捕捉多尺度时间特征）和Transformer（用于建模长期依赖性），并采用基于过采样的数据结构和预处理策略来解决类别不平衡问题。

Result: InceptoFormer能够捕捉步态信号的细粒度时间变化和全局动态，显著提高了PD严重程度评估的分类性能，准确率达到96.6%。

Conclusion: InceptoFormer在帕金森病（PD）严重程度评估方面实现了96.6%的准确率，优于现有的最先进方法。

Abstract: We present InceptoFormer, a multi-signal neural framework designed for
Parkinson's Disease (PD) severity evaluation via gait dynamics analysis. Our
architecture introduces a 1D adaptation of the Inception model, which we refer
to as Inception1D, along with a Transformer-based framework to stage PD
severity according to the Hoehn and Yahr (H&Y) scale. The Inception1D component
captures multi-scale temporal features by employing parallel 1D convolutional
filters with varying kernel sizes, thereby extracting features across multiple
temporal scales. The transformer component efficiently models long-range
dependencies within gait sequences, providing a comprehensive understanding of
both local and global patterns. To address the issue of class imbalance in PD
severity staging, we propose a data structuring and preprocessing strategy
based on oversampling to enhance the representation of underrepresented
severity levels. The overall design enables to capture fine-grained temporal
variations and global dynamics in gait signal, significantly improving
classification performance for PD severity evaluation. Through extensive
experimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming
existing state-of-the-art methods in PD severity assessment. The source code
for our implementation is publicly available at
https://github.com/SafwenNaimi/InceptoFormer

</details>


### [109] [Hierarchical Event Memory for Accurate and Low-latency Online Video Temporal Grounding](https://arxiv.org/abs/2508.04546)
*Minghang Zheng,Yuxin Peng,Benyuan Sun,Yi Yang,Yang Liu*

Main category: cs.CV

TL;DR: 本研究提出了一种新的在线视频时间定位（OnVTG）框架，通过引入分层事件记忆和基于事件的预测方法，解决了现有模型在处理流式视频和长期历史信息方面的局限性，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的在线视频时间定位（OnVTG）模型在处理流式视频和长期历史信息方面存在不足，导致事件建模不充分和性能较低。本研究旨在解决这些挑战，以提高OnVTG的性能。

Method: 提出了一种分层事件记忆方法，并构建了一个基于事件的OnVTG框架，该框架能够对具有不同持续时间的事件进行建模，并保留历史事件信息。此外，还提出了一个未来预测分支，用于预测事件是否即将发生以及回归事件的开始时间。

Result: 所提出的方法在TACoS、ActivityNet Captions和MAD数据集上取得了最先进的性能。

Conclusion: 所提出的分层事件记忆和基于事件的框架在TACoS、ActivityNet Captions和MAD数据集上实现了最先进的性能。

Abstract: In this paper, we tackle the task of online video temporal grounding (OnVTG),
which requires the model to locate events related to a given text query within
a video stream. Unlike regular video temporal grounding, OnVTG requires the
model to make predictions without observing future frames. As online videos are
streaming inputs and can go on indefinitely, it is impractical and inefficient
to store all historical inputs. The existing OnVTG models employ memory to
store recent historical video frame features and predict scores indicating
whether the current frame corresponds to the start or end time of the target
event. However, these methods lack effective event modeling and cannot retain
long-term historical information, leading to low performance. To tackle these
challenges, we propose a hierarchical event memory for OnVTG. We propose an
event-based OnVTG framework that makes predictions based on event proposals
that model event-level information with various durations. To preserve
historically valuable event information, we introduce a hierarchical event
memory that retains historical events, allowing the model to access both recent
and long-term information. To enable the real-time prediction, we further
propose a future prediction branch that predicts whether the target event will
occur shortly and further regresses the start time of the event. We achieve
state-of-the-art performance on the TACoS, ActivityNet Captions, and MAD
datasets. Code is available at https://github.com/minghangz/OnVTG.

</details>


### [110] [MSC: A Marine Wildlife Video Dataset with Grounded Segmentation and Clip-Level Captioning](https://arxiv.org/abs/2508.04549)
*Quang-Trung Truong,Yuk-Kwan Wong,Vo Hoang Kim Tuyen Dang,Rinaldi Gotama,Duc Thanh Nguyen,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 由于现有数据集的局限性，本研究提出了一个用于海洋视频字幕生成的两阶段流程，并引入了一个新的基准，以改进海洋视频理解和分析。


<details>
  <summary>Details</summary>
Motivation: 现有的视频字幕数据集通常侧重于通用或以人为中心的领域，未能很好地推广到海洋环境的复杂性，也无法深入了解海洋生物。海洋视频因其物体动态性、环境复杂性、摄像机运动以及水下场景的复杂性，对视频理解提出了重大挑战。

Method: 该研究提出了一个两阶段的海洋物体导向视频字幕生成流程，并引入了一个包含视频、文本和分割掩码的三元组基准，以促进视觉基础和字幕生成。此外，该研究还强调了视频分割在检测显著物体转换和场景变化中的有效性。

Result: 该研究通过引入一个包含视频、文本和分割掩码的三元组基准，以及强调视频分割在检测显著物体转换中的作用，有效提升了海洋视频的理解和分析能力，并丰富了字幕内容。

Conclusion: 该研究提出了一个两阶段的海洋物体导向视频字幕生成流程，旨在提高海洋视频的理解和分析能力，并丰富字幕内容。

Abstract: Marine videos present significant challenges for video understanding due to
the dynamics of marine objects and the surrounding environment, camera motion,
and the complexity of underwater scenes. Existing video captioning datasets,
typically focused on generic or human-centric domains, often fail to generalize
to the complexities of the marine environment and gain insights about marine
life. To address these limitations, we propose a two-stage marine
object-oriented video captioning pipeline. We introduce a comprehensive video
understanding benchmark that leverages the triplets of video, text, and
segmentation masks to facilitate visual grounding and captioning, leading to
improved marine video understanding and analysis, and marine video generation.
Additionally, we highlight the effectiveness of video splitting in order to
detect salient object transitions in scene changes, which significantly enrich
the semantics of captioning content. Our dataset and code have been released at
https://msc.hkustvgd.com.

</details>


### [111] [Two-Way Garment Transfer: Unified Diffusion Framework for Dressing and Undressing Synthesis](https://arxiv.org/abs/2508.04551)
*Angang Zhang,Fang Deng,Hao Chen,Zhongjian Chen,Junyan Li*

Main category: cs.CV

TL;DR: 本研究提出了一个名为TWGTM的新框架，用于同时进行虚拟试穿（VTON）和虚拟试穿（VTOFF），解决了现有方法将两者视为独立任务的缺点，并通过实验证明了其有效性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿（VTON）技术虽已取得显著进展，但其逆任务——虚拟试穿（VTOFF），即从穿着状态的人体重建服装模板——仍未得到充分研究。现有方法将VTON和VTOFF视为独立任务，忽视了它们之间的互补对称性。本研究旨在弥合这一差距，提出一个统一的框架来同时处理这两个任务。

Method: 提出了一种名为双向服装迁移模型（TWGTM）的统一框架，通过双向特征解耦来同时解决掩码引导的VTON和无掩码的VTOFF。该框架利用来自参考图像的潜在和像素空间的双条件指导来连接这两个任务，并通过分阶段的训练范式来解决掩码依赖性不对称的问题。

Result: 在DressCode和VITON-HD数据集上的大量实验表明，该研究提出的TWGTM框架在处理VTON和VTOFF任务时具有有效性和竞争力。

Conclusion: 该研究提出了双向服装迁移模型（TWGTM），一个集成的框架，能够同时处理掩码引导的虚拟试穿（VTON）和无掩码的虚拟试穿（VTOFF），并在DressCode和VITON-HD数据集上进行了广泛的实验验证。

Abstract: While recent advances in virtual try-on (VTON) have achieved realistic
garment transfer to human subjects, its inverse task, virtual try-off (VTOFF),
which aims to reconstruct canonical garment templates from dressed humans,
remains critically underexplored and lacks systematic investigation. Existing
works predominantly treat them as isolated tasks: VTON focuses on garment
dressing while VTOFF addresses garment extraction, thereby neglecting their
complementary symmetry. To bridge this fundamental gap, we propose the Two-Way
Garment Transfer Model (TWGTM), to the best of our knowledge, the first unified
framework for joint clothing-centric image synthesis that simultaneously
resolves both mask-guided VTON and mask-free VTOFF through bidirectional
feature disentanglement. Specifically, our framework employs dual-conditioned
guidance from both latent and pixel spaces of reference images to seamlessly
bridge the dual tasks. On the other hand, to resolve the inherent mask
dependency asymmetry between mask-guided VTON and mask-free VTOFF, we devise a
phased training paradigm that progressively bridges this modality gap.
Extensive qualitative and quantitative experiments conducted across the
DressCode and VITON-HD datasets validate the efficacy and competitive edge of
our proposed approach.

</details>


### [112] [Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation](https://arxiv.org/abs/2508.04552)
*Franz Thaler,Darko Stern,Gernot Plank,Martin Urschler*

Main category: cs.CV

TL;DR: 为了解决医学图像心脏分割中的域迁移问题，本研究提出了一种结合平衡联合训练和数据增强的深度学习方法。该方法在 CT 和 MR 数据上均取得了优异的分割结果，为生成心脏数字模型提供了有效支持。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要的死亡原因，这促使人们需要更复杂的方法来分析医学图像（如 CT 和 MR）中的心脏及其子结构。准确的心脏语义分割有助于评估患者特定的心脏形态和病理，并生成心脏数字模型，用于电生理模拟和个性化治疗规划。然而，在域迁移（训练和测试数据的分布不同）的情况下，保持深度学习方法的性能仍然是一个挑战。

Method: 本文提出了一种基于深度学习的心脏分割方法，结合了（1）平衡联合训练策略，利用来自不同源域的 CT 和 MR 数据；（2）强大的强度和空间增强技术，以应对测试时域迁移问题。该方法采用了 5 重集成策略。

Result: 对于 CT 数据，本方法实现了 93.33% 的 DSC 和 0.8388 mm 的 ASSD；对于 MR 数据，实现了 89.30% 的 DSC 和 1.2411 mm 的 ASSD。与仅在 CT 数据上训练的模型相比，本方法在 MR 数据上表现最佳，在 CT 数据上表现相当。

Conclusion: 本研究提出的心脏分割方法在 CT 和 MR 数据上均表现出优异的性能，尤其在 MR 数据上达到最佳，在 CT 数据上也达到了与最佳模型相当的水平。该方法有望高效地获得准确的语义分割结果，用于生成患者特异性的心脏数字模型。

Abstract: As the leading cause of death worldwide, cardiovascular diseases motivate the
development of more sophisticated methods to analyze the heart and its
substructures from medical images like Computed Tomography (CT) and Magnetic
Resonance (MR). Semantic segmentations of important cardiac structures that
represent the whole heart are useful to assess patient-specific cardiac
morphology and pathology. Furthermore, accurate semantic segmentations can be
used to generate cardiac digital twin models which allows e.g.
electrophysiological simulation and personalized therapy planning. Even though
deep learning-based methods for medical image segmentation achieved great
advancements over the last decade, retaining good performance under domain
shift -- i.e. when training and test data are sampled from different data
distributions -- remains challenging. In order to perform well on domains known
at training-time, we employ a (1) balanced joint training approach that
utilizes CT and MR data in equal amounts from different source domains.
Further, aiming to alleviate domain shift towards domains only encountered at
test-time, we rely on (2) strong intensity and spatial augmentation techniques
to greatly diversify the available training data. Our proposed whole heart
segmentation method, a 5-fold ensemble with our contributions, achieves the
best performance for MR data overall and a performance similar to the best
performance for CT data when compared to a model trained solely on CT. With
93.33% DSC and 0.8388 mm ASSD for CT and 89.30% DSC and 1.2411 mm ASSD for MR
data, our method demonstrates great potential to efficiently obtain accurate
semantic segmentations from which patient-specific cardiac twin models can be
generated.

</details>


### [113] [One Model For All: Partial Diffusion for Unified Try-On and Try-Off in Any Pose](https://arxiv.org/abs/2508.04559)
*Jinxi Liu,Zijian He,Guangrun Wang,Guanbin Li,Liang Lin*

Main category: cs.CV

TL;DR: OMFA是一个创新的虚拟试穿和试穿移除框架，无需展厅服装或分割掩码，并支持任意姿势。它利用部分扩散策略，可以从一张图像和目标姿势中生成逼真的虚拟试穿效果，即使目标人物没有多姿势图像。实验证明OMFA在虚拟服装合成方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在图像虚拟试穿方面取得了显著进展，但它们受限于展厅服装、分割掩码以及对灵活姿势变化的有限处理能力，降低了其实用性。用户无法轻松地将一人身上的服装转移到另一个人身上，生成的试穿结果通常也局限于参考图像的相同姿势。

Method: OMFA基于部分扩散策略，选择性地对联合输入（如服装、人物图像或面部）的各个组件应用噪声和去噪，实现动态子任务控制和高效的双向服装-人物转换。该框架完全无需掩码，只需单个肖像和目标姿势作为输入，并利用基于SMPL-X的姿势条件，仅凭一张图像即可支持多视图和任意姿势的试穿。

Result: OMFA在试穿和试穿移除任务上均取得了最先进的结果。

Conclusion: OMFA是一个统一的扩散框架，实现了虚拟试穿和试穿移除，无需展厅服装，支持任意姿势，为虚拟服装合成提供了实用且通用的解决方案。

Abstract: Recent diffusion-based approaches have made significant advances in
image-based virtual try-on, enabling more realistic and end-to-end garment
synthesis. However, most existing methods remain constrained by their reliance
on exhibition garments and segmentation masks, as well as their limited ability
to handle flexible pose variations. These limitations reduce their practicality
in real-world scenarios-for instance, users cannot easily transfer garments
worn by one person onto another, and the generated try-on results are typically
restricted to the same pose as the reference image. In this paper, we introduce
\textbf{OMFA} (\emph{One Model For All}), a unified diffusion framework for
both virtual try-on and try-off that operates without the need for exhibition
garments and supports arbitrary poses. For example, OMFA enables removing
garments from a source person (try-off) and transferring them onto a target
person (try-on), while also allowing the generated target to appear in novel
poses-even without access to multi-pose images of that person. OMFA is built
upon a novel \emph{partial diffusion} strategy that selectively applies noise
and denoising to individual components of the joint input-such as the garment,
the person image, or the face-enabling dynamic subtask control and efficient
bidirectional garment-person transformation. The framework is entirely
mask-free and requires only a single portrait and a target pose as input,
making it well-suited for real-world applications. Additionally, by leveraging
SMPL-X-based pose conditioning, OMFA supports multi-view and arbitrary-pose
try-on from just one image. Extensive experiments demonstrate that OMFA
achieves state-of-the-art results on both try-on and try-off tasks, providing a
practical and generalizable solution for virtual garment synthesis. The project
page is here: https://onemodelforall.github.io/.

</details>


### [114] [Drone Detection with Event Cameras](https://arxiv.org/abs/2508.04564)
*Gabriele Magrini,Lorenzo Berlincioni,Luca Cultrera,Federico Becattini,Pietro Pala*

Main category: cs.CV

TL;DR: Event cameras solve drone detection problems caused by drone size, speed, and lighting by focusing on motion, enabling better tracking and identification for counter-UAV systems.


<details>
  <summary>Details</summary>
Motivation: Traditional surveillance systems fail to reliably detect drones due to their small size, agility, motion blur, and poor performance in challenging lighting. Event-based vision offers a solution to these limitations.

Method: This paper surveys event-based vision for drone detection, reviewing state-of-the-art methods, data representation, processing pipelines using spiking neural networks, and applications like tracking and identification.

Result: Event-based vision, with its sparse, asynchronous output and ability to handle motion blur and extreme lighting, provides a low-latency focus on motion cues, forming a strong foundation for next-generation counter-UAV systems.

Conclusion: Event-based vision offers a robust solution for drone detection, outperforming traditional systems in challenging conditions and enabling advanced counter-UAV capabilities.

Abstract: The diffusion of drones presents significant security and safety challenges.
Traditional surveillance systems, particularly conventional frame-based
cameras, struggle to reliably detect these targets due to their small size,
high agility, and the resulting motion blur and poor performance in challenging
lighting conditions. This paper surveys the emerging field of event-based
vision as a robust solution to these problems. Event cameras virtually
eliminate motion blur and enable consistent detection in extreme lighting.
Their sparse, asynchronous output suppresses static backgrounds, enabling
low-latency focus on motion cues. We review the state-of-the-art in event-based
drone detection, from data representation methods to advanced processing
pipelines using spiking neural networks. The discussion extends beyond simple
detection to cover more sophisticated tasks such as real-time tracking,
trajectory forecasting, and unique identification through propeller signature
analysis. By examining current methodologies, available datasets, and the
distinct advantages of the technology, this work demonstrates that event-based
vision provides a powerful foundation for the next generation of reliable,
low-latency, and efficient counter-UAV systems.

</details>


### [115] [TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning](https://arxiv.org/abs/2508.04565)
*Yunbi Liu,Enqi Tang,Shiyu Li,Lei Ma,Juncheng Li,Shu Lou,Yongchu Pan,Qingshan Liu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Orthodontic treatment hinges on tooth alignment, which significantly affects
occlusal function, facial aesthetics, and patients' quality of life. Current
deep learning approaches predominantly concentrate on predicting transformation
matrices through imposing point-to-point geometric constraints for tooth
alignment. Nevertheless, these matrices are likely associated with the
anatomical structure of the human oral cavity and possess particular
distribution characteristics that the deterministic point-to-point geometric
constraints in prior work fail to capture. To address this, we introduce a new
automatic tooth alignment method named TAlignDiff, which is supported by
diffusion-based transformation learning. TAlignDiff comprises two main
components: a primary point cloud-based regression network (PRN) and a
diffusion-based transformation matrix denoising module (DTMD).
Geometry-constrained losses supervise PRN learning for point cloud-level
alignment. DTMD, as an auxiliary module, learns the latent distribution of
transformation matrices from clinical data. We integrate point cloud-based
transformation regression and diffusion-based transformation modeling into a
unified framework, allowing bidirectional feedback between geometric
constraints and diffusion refinement. Extensive ablation and comparative
experiments demonstrate the effectiveness and superiority of our method,
highlighting its potential in orthodontic treatment.

</details>


### [116] [CLASP: Cross-modal Salient Anchor-based Semantic Propagation for Weakly-supervised Dense Audio-Visual Event Localization](https://arxiv.org/abs/2508.04566)
*Jinxing Zhou,Ziheng Zhou,Yanghao Zhou,Yuxin Mao,Zhangling Duan,Dan Guo*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的弱监督视听事件定位方法，通过识别跨模态显著锚点来提高事件的时间定位精度。


<details>
  <summary>Details</summary>
Motivation: 在仅提供视频级别事件标签且事件时间边界未知的情况下，探索更具挑战性的弱监督密集视听事件定位（W-DAVEL）任务。

Method: 提出了一种利用"跨模态显著锚点"来解决弱监督密集视听事件定位（W-DAVEL）任务的方法。该方法包括一个"互事件一致性评估"模块，用于计算音频和视觉事件类别的预测一致性得分；一个"跨模态显著锚点识别"模块，通过全局视频和局部时间窗口识别机制来识别锚点特征；以及一个"基于锚点的时间传播"模块，用于增强原始时域音频和视觉特征中的事件语义编码，以实现更好的弱监督时间定位。

Result: 为W-DAVEL任务在UnAV-100和ActivityNet1.3数据集上建立了基准，并通过大量实验证明了该方法达到了最先进的性能。

Conclusion: 该方法在UnAV-100和ActivityNet1.3数据集上实现了最先进的性能。

Abstract: The Dense Audio-Visual Event Localization (DAVEL) task aims to temporally
localize events in untrimmed videos that occur simultaneously in both the audio
and visual modalities. This paper explores DAVEL under a new and more
challenging weakly-supervised setting (W-DAVEL task), where only video-level
event labels are provided and the temporal boundaries of each event are
unknown. We address W-DAVEL by exploiting \textit{cross-modal salient anchors},
which are defined as reliable timestamps that are well predicted under weak
supervision and exhibit highly consistent event semantics across audio and
visual modalities. Specifically, we propose a \textit{Mutual Event Agreement
Evaluation} module, which generates an agreement score by measuring the
discrepancy between the predicted audio and visual event classes. Then, the
agreement score is utilized in a \textit{Cross-modal Salient Anchor
Identification} module, which identifies the audio and visual anchor features
through global-video and local temporal window identification mechanisms. The
anchor features after multimodal integration are fed into an
\textit{Anchor-based Temporal Propagation} module to enhance event semantic
encoding in the original temporal audio and visual features, facilitating
better temporal localization under weak supervision. We establish benchmarks
for W-DAVEL on both the UnAV-100 and ActivityNet1.3 datasets. Extensive
experiments demonstrate that our method achieves state-of-the-art performance.

</details>


### [117] [DDTracking: A Deep Generative Framework for Diffusion MRI Tractography with Streamline Local-Global Spatiotemporal Modeling](https://arxiv.org/abs/2508.04568)
*Yijie Li,Wei Zhang,Xi Zhu,Ye Wu,Yogesh Rathi,Lauren J. O'Donnell,Fan Zhang*

Main category: cs.CV

TL;DR: DDTracking 是一种新颖的深度生成框架，用于扩散 MRI 纤维追踪，通过双通路编码和条件扩散模型实现，并在各种数据集上展示了优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种新颖的深度生成框架，用于扩散 MRI 纤维追踪，以提高追踪的准确性和鲁棒性。

Method: DDTracking 提出了一种新颖的深度生成框架，用于扩散 MRI 纤维追踪，将纤维传播表述为条件去噪扩散过程。它采用双通路编码网络来同时模拟局部空间编码和全局时间依赖性，并设计了一个条件扩散模型模块来预测纤维传播方向。

Result: DDTracking 在真实和合成数据集上进行了广泛评估，并在两个基准测试中展示了优于当前最先进方法的性能，同时证明了其在不同数据集上的良好泛化能力。

Conclusion: DDTracking 提供了解剖学上合理且鲁棒的纤维追踪，为广泛的 dMRI 应用提供了一个可扩展、可适应且可端到端学习的解决方案。

Abstract: This paper presents DDTracking, a novel deep generative framework for
diffusion MRI tractography that formulates streamline propagation as a
conditional denoising diffusion process. In DDTracking, we introduce a
dual-pathway encoding network that jointly models local spatial encoding
(capturing fine-scale structural details at each streamline point) and global
temporal dependencies (ensuring long-range consistency across the entire
streamline). Furthermore, we design a conditional diffusion model module, which
leverages the learned local and global embeddings to predict streamline
propagation orientations for tractography in an end-to-end trainable manner. We
conduct a comprehensive evaluation across diverse, independently acquired dMRI
datasets, including both synthetic and clinical data. Experiments on two
well-established benchmarks with ground truth (ISMRM Challenge and
TractoInferno) demonstrate that DDTracking largely outperforms current
state-of-the-art tractography methods. Furthermore, our results highlight
DDTracking's strong generalizability across heterogeneous datasets, spanning
varying health conditions, age groups, imaging protocols, and scanner types.
Collectively, DDTracking offers anatomically plausible and robust tractography,
presenting a scalable, adaptable, and end-to-end learnable solution for broad
dMRI applications. Code is available at:
https://github.com/yishengpoxiao/DDtracking.git

</details>


### [118] [Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding](https://arxiv.org/abs/2508.04572)
*Jun Li,Che Liu,Wenjia Bai,Mingxuan Liu,Rossella Arcucci,Cosmin I. Bercea,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 提出K2Sight框架，通过结构化语义监督（将临床概念分解为视觉属性）实现数据高效的医学图像异常定位，训练出的小模型性能优于大模型。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型（VLMs）在医学领域存在挑战，因为医学术语稀少、组合性强且与视觉模式对齐不佳。专门的医学VLMs虽然能解决此问题，但需要大量的注释和计算资源。本研究旨在克服这些限制。

Method: K2Sight框架通过从领域本体中提取结构化语义信息，将其分解为视觉属性，并编码为指令式提示，用于指导模型训练中的区域-文本对齐。

Result: 使用0.23B和2B参数的紧凑模型，在仅使用1.5%的数据情况下，性能与7B+的医学VLMs相当甚至更优，mAP50最高提升9.82%。

Conclusion: 提出的K2Sight框架通过引入结构化语义监督，将临床概念分解为可解释的视觉属性（如形状、密度和解剖位置），从而实现了数据高效的紧凑模型训练。这些模型在参数量远小于现有模型且训练数据量极少的情况下，取得了与大型模型相当甚至更优的性能，mAP50指标最高提升了9.82%。

Abstract: In this work, we address the problem of grounding abnormalities in medical
images, where the goal is to localize clinical findings based on textual
descriptions. While generalist Vision-Language Models (VLMs) excel in natural
grounding tasks, they often struggle in the medical domain due to rare,
compositional, and domain-specific terms that are poorly aligned with visual
patterns. Specialized medical VLMs address this challenge via large-scale
domain pretraining, but at the cost of substantial annotation and computational
resources. To overcome these limitations, we propose \textbf{Knowledge to Sight
(K2Sight)}, a framework that introduces structured semantic supervision by
decomposing clinical concepts into interpretable visual attributes, such as
shape, density, and anatomical location. These attributes are distilled from
domain ontologies and encoded into concise instruction-style prompts, which
guide region-text alignment during training. Unlike conventional report-level
supervision, our approach explicitly bridges domain knowledge and spatial
structure, enabling data-efficient training of compact models. We train compact
models with 0.23B and 2B parameters using only 1.5\% of the data required by
state-of-the-art medical VLMs. Despite their small size and limited training
data, these models achieve performance on par with or better than 7B+ medical
VLMs, with up to 9.82\% improvement in $mAP_{50}$. Code and models:
\href{https://lijunrio.github.io/K2Sight/}{\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.

</details>


### [119] [Visual Bias and Interpretability in Deep Learning for Dermatological Image Analysis](https://arxiv.org/abs/2508.04573)
*Enam Ahmed Taufik,Abdullah Khondoker,Antara Firoz Parsa,Seraj Al Mahmud Mostafa*

Main category: cs.CV

TL;DR: 本研究评估了不同的图像预处理技术和深度学习模型在皮肤病分类中的表现，发现RGB预处理与DinoV2模型结合能达到最佳的准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于类间相似性高、类内变异性大以及复杂的病变纹理，准确的皮肤病分类是一个关键但充满挑战的任务。本研究旨在通过深度学习框架解决这一问题。

Method: 本研究提出了一个深度学习框架，用于多类皮肤病分类，系统地评估了三种图像预处理技术（标准RGB、CMY颜色空间变换和对比度限制自适应直方图均衡化）以及预训练的卷积神经网络（DenseNet201、Efficient-NetB5）和基于Transformer的模型（ViT、Swin Transformer、DinoV2 Large）。使用准确率和F1分数作为评估指标。

Result: 结果表明，使用RGB预处理的DinoV2模型在所有变体中实现了最高的准确率（最高可达93%）和F1分数。Grad-CAM可视化进一步揭示了精确的病变定位，增强了可解释性。

Conclusion: 选择有效的预处理方法和模型对于构建健壮且可解释的皮肤病学CAD系统至关重要。

Abstract: Accurate skin disease classification is a critical yet challenging task due
to high inter-class similarity, intra-class variability, and complex lesion
textures. While deep learning-based computer-aided diagnosis (CAD) systems have
shown promise in automating dermatological assessments, their performance is
highly dependent on image pre-processing and model architecture. This study
proposes a deep learning framework for multi-class skin disease classification,
systematically evaluating three image pre-processing techniques: standard RGB,
CMY color space transformation, and Contrast Limited Adaptive Histogram
Equalization (CLAHE). We benchmark the performance of pre-trained convolutional
neural networks (DenseNet201, Efficient-NetB5) and transformer-based models
(ViT, Swin Transformer, DinoV2 Large) using accuracy and F1-score as evaluation
metrics. Results show that DinoV2 with RGB pre-processing achieves the highest
accuracy (up to 93%) and F1-scores across all variants. Grad-CAM visualizations
applied to RGB inputs further reveal precise lesion localization, enhancing
interpretability. These findings underscore the importance of effective
pre-processing and model choice in building robust and explainable CAD systems
for dermatology.

</details>


### [120] [Face-voice Association in Multilingual Environments (FAME) 2026 Challenge Evaluation Plan](https://arxiv.org/abs/2508.04592)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME挑战赛在多语言环境下探索面部-声音关联，使用MAV-Celeb数据集。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，多模态系统得到广泛应用，特别是视听系统。人脸与声音的关联因其独特的相互关系而受到关注。FAME挑战赛旨在探索多语言环境下面部与声音的关联，因为现实世界中许多人是双语者，并且经常在多语言场景下进行交流。

Method: 本报告介绍了FAME挑战赛，该挑战赛专注于在多语言环境下探索面部与声音的关联。

Result: FAME挑战赛使用MAV-Celeb数据集来探索多语言环境下的面部-声音关联。

Conclusion: 该报告提供了FAME挑战赛的详细信息，包括数据集、基线模型和任务细节。

Abstract: The advancements of technology have led to the use of multimodal systems in
various real-world applications. Among them, audio-visual systems are among the
most widely used multimodal systems. In the recent years, associating face and
voice of a person has gained attention due to the presence of unique
correlation between them. The Face-voice Association in Multilingual
Environments (FAME) 2026 Challenge focuses on exploring face-voice association
under the unique condition of a multilingual scenario. This condition is
inspired from the fact that half of the world's population is bilingual and
most often people communicate under multilingual scenarios. The challenge uses
a dataset named Multilingual Audio-Visual (MAV-Celeb) for exploring face-voice
association in multilingual environments. This report provides the details of
the challenge, dataset, baseline models, and task details for the FAME
Challenge.

</details>


### [121] [Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline](https://arxiv.org/abs/2508.04597)
*Linqing Zhao,Xiuwei Xu,Yirui Wang,Hao Wang,Wenzhao Zheng,Yansong Tang,Haibin Yan,Jiwen Lu*

Main category: cs.CV

TL;DR: A new 3D reconstruction method using 3D Gaussian-based SLAM and feed-forward pose prediction significantly speeds up tracking while maintaining state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of incrementally recovering real-sized 3D geometry from pose-free RGB streams, particularly the limitations of existing end-to-end and visual SLAM-based methods in handling long sequences or relying on slow optimization/depth sensors.

Method: Integrated depth estimation into RGB-D SLAM, then utilized 3D Gaussian mapping and a feed-forward recurrent prediction module using optical flow for camera pose inference. Incorporated local graph rendering for enhanced robustness.

Result: Achieved performance comparable to SplaTAM on Replica and TUM-RGBD datasets and in real-world deployment, with a >90% reduction in tracking time.

Conclusion: Proposed method achieves performance on par with SplaTAM while reducing tracking time by over 90%

Abstract: Incrementally recovering real-sized 3D geometry from a pose-free RGB stream
is a challenging task in 3D reconstruction, requiring minimal assumptions on
input data. Existing methods can be broadly categorized into end-to-end and
visual SLAM-based approaches, both of which either struggle with long sequences
or depend on slow test-time optimization and depth sensors. To address this, we
first integrate a depth estimator into an RGB-D SLAM system, but this approach
is hindered by inaccurate geometric details in predicted depth. Through further
investigation, we find that 3D Gaussian mapping can effectively solve this
problem. Building on this, we propose an online 3D reconstruction method using
3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction
module to directly infer camera pose from optical flow. This approach replaces
slow test-time optimization with fast network inference, significantly
improving tracking speed. Additionally, we introduce a local graph rendering
technique to enhance robustness in feed-forward pose prediction. Experimental
results on the Replica and TUM-RGBD datasets, along with a real-world
deployment demonstration, show that our method achieves performance on par with
the state-of-the-art SplaTAM, while reducing tracking time by more than 90\%.

</details>


### [122] [How Does Bilateral Ear Symmetry Affect Deep Ear Features?](https://arxiv.org/abs/2508.04614)
*Kagan Ozturk,Deeksha Arun,Kevin W. Bowyer,Patrick Flynn*

Main category: cs.CV

TL;DR: 研究了CNN在耳部识别中对双侧耳部对称性的处理，发现单独区分和处理左右耳可以提高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 研究双侧耳部对称性对CNN学习特征的影响。

Method: 提出一个耳部侧面分类器来自动区分左右耳，并将该信息用于训练和测试。

Result: 在五个数据集上进行的跨数据集评估表明，单独处理左右耳可以带来显著的性能提升。

Conclusion: 单独处理训练和测试中的左右耳可以显著提高CNN的识别性能。

Abstract: Ear recognition has gained attention as a reliable biometric technique due to
the distinctive characteristics of human ears. With the increasing availability
of large-scale datasets, convolutional neural networks (CNNs) have been widely
adopted to learn features directly from raw ear images, outperforming
traditional hand-crafted methods. However, the effect of bilateral ear symmetry
on the features learned by CNNs has received little attention in recent
studies. In this paper, we investigate how bilateral ear symmetry influences
the effectiveness of CNN-based ear recognition. To this end, we first develop
an ear side classifier to automatically categorize ear images as either left or
right. We then explore the impact of incorporating this side information during
both training and test. Cross-dataset evaluations are conducted on five
datasets. Our results suggest that treating left and right ears separately
during training and testing can lead to notable performance improvements.
Furthermore, our ablation studies on alignment strategies, input sizes, and
various hyperparameter settings provide practical insights into training
CNN-based ear recognition systems on large-scale datasets to achieve higher
verification rates.

</details>


### [123] [FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging](https://arxiv.org/abs/2508.04625)
*Zichen Tang,Haihong E,Jiacheng Liu,Zhongjun Yang,Rongjin Li,Zihua Rong,Haoyang He,Zhuodi Hao,Xinyang Hu,Kun Ji,Ziyan Ma,Mengyuan Ji,Jun Zhang,Chenghao Ma,Qianhe Zheng,Yang Liu,Yiling Huang,Xinyi Hu,Qing Huang,Zijian Xie,Shiyao Peng*

Main category: cs.CV

TL;DR: FinMMR is a new benchmark for testing financial reasoning in AI models, using text and images from financial reports. It's more comprehensive and challenging than previous benchmarks, but even the best AI models struggle with its harder questions.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks and drive advancements in this area.

Method: FinMMR is a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. It comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts, and covers 14 financial subdomains. Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text.

Result: The best-performing MLLM achieves only 53.0% accuracy on Hard problems within the FinMMR benchmark.

Conclusion: FinMMR benchmarks will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.

Abstract: We present FinMMR, a novel bilingual multimodal benchmark tailored to
evaluate the reasoning capabilities of multimodal large language models (MLLMs)
in financial numerical reasoning tasks. Compared to existing benchmarks, our
work introduces three significant advancements. (1) Multimodality: We
meticulously transform existing financial reasoning benchmarks, and construct
novel questions from the latest Chinese financial research reports. FinMMR
comprises 4.3K questions and 8.7K images spanning 14 categories, including
tables, bar charts, and ownership structure charts. (2) Comprehensiveness:
FinMMR encompasses 14 financial subdomains, including corporate finance,
banking, and industry analysis, significantly exceeding existing benchmarks in
financial domain knowledge breadth. (3) Challenge: Models are required to
perform multi-step precise numerical reasoning by integrating financial
knowledge with the understanding of complex financial images and text. The
best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe
that FinMMR will drive advancements in enhancing the reasoning capabilities of
MLLMs in real-world scenarios.

</details>


### [124] [EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts](https://arxiv.org/abs/2508.04650)
*Kushin Mukherjee,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CV

TL;DR: 本研究提出了EncQA基准测试，以解决现有图表理解评估未能涵盖所有关键视觉推理能力的问题。通过对9个最先进的VLMs进行评估，研究发现模型性能在不同编码和任务上表现不一，且性能提升并非总是与模型规模相关，强调了针对性改进的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的图表理解基准测试未能完全捕捉到解读图表所必需的视觉推理能力的广度，尽管多模态视觉-语言模型（VLMs）在该领域取得了不断进步的分数。

Method: 提出了一种名为EncQA的新型基准测试，该基准测试借鉴了可视化文献，旨在系统地覆盖图表理解中至关重要的视觉编码和分析任务。EncQA包含2,076个合成的问答对，能够平衡地覆盖六种视觉编码通道（位置、长度、面积、颜色量化、颜色名义和形状）和八项任务（查找极值、检索值、查找异常、过滤值、计算派生值精确值、计算派生值相对值、相关值和相关值相对值）。

Result: 在9个最先进的VLMs上的评估显示，模型在同一任务中针对不同编码的表现存在显著差异，并且在不同任务的表现也存在差异。与预期相反，研究观察到在许多任务-编码组合中，性能并未随着模型规模的增大而提高。

Conclusion: 现有的模型在理解图表方面虽然取得了进展，但并未完全捕捉到解读图表所必需的视觉推理能力的广度。研究结果表明，提升图表理解能力需要针对具体的视觉推理差距采取策略，而不是仅仅依靠扩大模型或数据集的规模。

Abstract: Multimodal vision-language models (VLMs) continue to achieve ever-improving
scores on chart understanding benchmarks. Yet, we find that this progress does
not fully capture the breadth of visual reasoning capabilities essential for
interpreting charts. We introduce EncQA, a novel benchmark informed by the
visualization literature, designed to provide systematic coverage of visual
encodings and analytic tasks that are crucial for chart understanding. EncQA
provides 2,076 synthetic question-answer pairs, enabling balanced coverage of
six visual encoding channels (position, length, area, color quantitative, color
nominal, and shape) and eight tasks (find extrema, retrieve value, find
anomaly, filter values, compute derived value exact, compute derived value
relative, correlate values, and correlate values relative). Our evaluation of 9
state-of-the-art VLMs reveals that performance varies significantly across
encodings within the same task, as well as across tasks. Contrary to
expectations, we observe that performance does not improve with model size for
many task-encoding pairs. Our results suggest that advancing chart
understanding requires targeted strategies addressing specific visual reasoning
gaps, rather than solely scaling up model or dataset size.

</details>


### [125] [X-SAM: From Segment Anything to Any Segmentation](https://arxiv.org/abs/2508.04655)
*Hao Wang,Limeng Qiao,Zequn Jie,Zhijian Huang,Chengjian Feng,Qingfang Zheng,Lin Ma,Xiangyuan Lan,Xiaodan Liang*

Main category: cs.CV

TL;DR: X-SAM是一个统一的多模态大型语言模型（MLLM）框架，它将分割范式从“分割一切”扩展到“任何分割”，解决了SAM在多掩码预测和特定类别分割任务上的局限性。它通过统一的框架、视觉地面分割（VGD）任务和统一的训练策略，实现了更高级的像素级感知理解，并在各种分割基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在像素级感知理解方面存在固有缺陷。虽然SAM在视觉提示驱动的图像分割方面取得了重大进展，但在多掩码预测和特定类别分割任务方面存在局限性，并且无法在统一的模型架构中集成所有分割任务。

Method: 提出了一种新颖的统一框架，实现了MLLM更高级的像素级感知理解；提出了一种新的分割任务——视觉地面分割（VGD），并提出了一种统一的训练策略，支持跨多个数据集的联合训练。

Result: X-SAM实现了最先进的性能，展示了其在多模态、像素级视觉理解方面的效率。

Conclusion: X-SAM在各种图像分割基准测试中实现了最先进的性能，突出了其在多模态、像素级视觉理解方面的效率。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in broad
knowledge representation, yet they are inherently deficient in pixel-level
perceptual understanding. Although the Segment Anything Model (SAM) represents
a significant advancement in visual-prompt-driven image segmentation, it
exhibits notable limitations in multi-mask prediction and category-specific
segmentation tasks, and it cannot integrate all segmentation tasks within a
unified model architecture. To address these limitations, we present X-SAM, a
streamlined Multimodal Large Language Model (MLLM) framework that extends the
segmentation paradigm from \textit{segment anything} to \textit{any
segmentation}. Specifically, we introduce a novel unified framework that
enables more advanced pixel-level perceptual comprehension for MLLMs.
Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)
segmentation, which segments all instance objects with interactive visual
prompts and empowers MLLMs with visual grounded, pixel-wise interpretative
capabilities. To enable effective training on diverse data sources, we present
a unified training strategy that supports co-training across multiple datasets.
Experimental results demonstrate that X-SAM achieves state-of-the-art
performance on a wide range of image segmentation benchmarks, highlighting its
efficiency for multimodal, pixel-level visual understanding. Code is available
at https://github.com/wanghao9610/X-SAM.

</details>


### [126] [YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper](https://arxiv.org/abs/2508.04658)
*Akhil Saketh Reddy Sabbella,Ch. Lakshmi Prachothan,Eswar Kumar Panta*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLO v8的AI方法，用于通过分析鸡的照片来实时检测疾病，从而改进家禽健康管理和农场安全。


<details>
  <summary>Details</summary>
Motivation: 在禽类产业中，检测鸡的疾病对于避免经济损失至关重要。传统技术依赖于手动观察，这种方法劳动强度大且容易出错。

Method: 提出了一种基于AI的方法，通过开发一个分析高分辨率鸡肉照片的系统，利用YOLO v8（一种用于实时对象识别的深度学习模型）来检测疾病迹象，例如行为和外观异常。

Result: YOLO v8检测疾病迹象，并为农场运营商提供实时感染识别和及时的警告，以便迅速采取行动。

Conclusion: 这项AI技术通过实现早期感染识别、消除人工检查的需求并加强大规模农场的生物安全，从而改进了鸡的健康管理。YOLO v8的实时特性为改进农场管理技术提供了一种可扩展且有效的方法。

Abstract: In the poultry industry, detecting chicken illnesses is essential to avoid
financial losses. Conventional techniques depend on manual observation, which
is laborious and prone to mistakes. Using YOLO v8 a deep learning model for
real-time object recognition. This study suggests an AI based approach, by
developing a system that analyzes high resolution chicken photos, YOLO v8
detects signs of illness, such as abnormalities in behavior and appearance. A
sizable, annotated dataset has been used to train the algorithm, which provides
accurate real-time identification of infected chicken and prompt warnings to
farm operators for prompt action. By facilitating early infection
identification, eliminating the need for human inspection, and enhancing
biosecurity in large-scale farms, this AI technology improves chicken health
management. The real-time features of YOLO v8 provide a scalable and effective
method for improving farm management techniques.

</details>


### [127] [PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment](https://arxiv.org/abs/2508.04659)
*Gustav Hanning,Kalle Åström,Viktor Larsson*

Main category: cs.CV

TL;DR: PixCuboid是一种新的房间布局估计算法，它使用多视图对齐和优化的方法，在 ScanNet++ 和 2D-3D-Semantics 数据集上取得了优于现有方法的性能，并且易于扩展到多房间场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前基于单视图和全景图像的房间布局估计算法在几何线索提取方面的不足，提出PixCuboid方法。

Method: PixCuboid是一种基于多视图对齐的密集深度特征的优化方法，通过端到端的训练来学习特征图，从而实现对齐过程中的大收敛盆地和光滑损失景观，并允许使用简单的启发式方法来初始化房间布局。

Result: 在ScanNet++和2D-3D-Semantics数据集上进行了评估，结果显示PixCuboid方法显著优于现有方法。此外，该方法还可以轻松扩展到多房间场景。

Conclusion: PixCuboid方法在多视图对齐方面表现出色，并能在ScanNet++和2D-3D-Semantics两个新的基准上显著优于现有方法。

Abstract: Coarse room layout estimation provides important geometric cues for many
downstream tasks. Current state-of-the-art methods are predominantly based on
single views and often assume panoramic images. We introduce PixCuboid, an
optimization-based approach for cuboid-shaped room layout estimation, which is
based on multi-view alignment of dense deep features. By training with the
optimization end-to-end, we learn feature maps that yield large convergence
basins and smooth loss landscapes in the alignment. This allows us to
initialize the room layout using simple heuristics.
  For the evaluation we propose two new benchmarks based on ScanNet++ and
2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough
experiments we validate our approach and significantly outperform the
competition. Finally, while our network is trained with single cuboids, the
flexibility of the optimization-based approach allow us to easily extend to
multi-room estimation, e.g. larger apartments or offices. Code and model
weights are available at https://github.com/ghanning/PixCuboid.

</details>


### [128] [HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models](https://arxiv.org/abs/2508.04663)
*Young D. Kwon,Rui Li,Sijia Li,Da Li,Sourav Bhattacharya,Stylianos I. Venieris*

Main category: cs.CV

TL;DR: HierarchicalPrune通过层次化剪枝、权重保留和敏感性引导蒸馏，有效压缩了大型扩散模型，实现了显著的内存和延迟缩减，同时保持了图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像扩散模型（DMs）虽然质量优异，但其巨大的参数规模（8-11B）给资源受限设备上的推理带来了巨大挑战。

Method: HierarchicalPrune是一个新颖的压缩框架，结合了三种技术：1. 层次化位置剪枝，识别并移除基于位置层次结构中不太重要的后期模块；2. 位置权重保留，系统地保护对语义结构完整性至关重要的早期模型部分；3. 敏感性引导蒸馏，根据发现的模块级敏感性变化调整知识转移强度。

Result: HierarchicalPrune实现了77.5%-80.4%的内存占用减少（例如，从15.8 GB减少到3.2 GB）和27.9%-38.0%的延迟减少，在服务器和消费级GPU上进行测量，GenEval分数仅下降2.6%，HPSv2分数仅下降7%。

Conclusion: HierarchicalPrune成功将数十亿参数的扩散模型压缩到更适合设备端推理的范围，同时保持了输出图像的质量。与INT4量化结合使用时，HierarchicalPrune实现了77.5%-80.4%的内存占用减少和27.9%-38.0%的延迟减少，并且在GenEval和HPSv2分数上的损失最小。用户研究也表明，HierarchicalPrune在感知质量上与原始模型相当，并且显著优于现有方法。

Abstract: State-of-the-art text-to-image diffusion models (DMs) achieve remarkable
quality, yet their massive parameter scale (8-11B) poses significant challenges
for inferences on resource-constrained devices. In this paper, we present
HierarchicalPrune, a novel compression framework grounded in a key observation:
DM blocks exhibit distinct functional hierarchies, where early blocks establish
semantic structures while later blocks handle texture refinements.
HierarchicalPrune synergistically combines three techniques: (1) Hierarchical
Position Pruning, which identifies and removes less essential later blocks
based on position hierarchy; (2) Positional Weight Preservation, which
systematically protects early model portions that are essential for semantic
structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts
knowledge-transfer intensity based on our discovery of block-wise sensitivity
variations. As a result, our framework brings billion-scale diffusion models
into a range more suitable for on-device inference, while preserving the
quality of the output images. Specifically, when combined with INT4 weight
quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction
(e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on
server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score
and 7% in HPSv2 score compared to the original model. Last but not least, our
comprehensive user study with 85 participants demonstrates that
HierarchicalPrune maintains perceptual quality comparable to the original model
while significantly outperforming prior works.

</details>


### [129] [ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.04677)
*Yansheng Gao,Yufei Zheng,Jinghan Qu,Zixi Zhu,Yukuan Zhang,Shengsheng Wang*

Main category: cs.CV

TL;DR: Prompt tuning adapts VLMs efficiently but is vulnerable to noise. ANPrompt enhances robustness by generating noise prompts, injecting them into encoders, and using a novel alignment loss, outperforming existing methods in generalization and noise resistance.


<details>
  <summary>Details</summary>
Motivation: Existing prompt tuning methods for VLMs overlook their vulnerability to weak semantic perturbations, which degrade generalization to unseen classes. ANPrompt is proposed to address this limitation by enhancing robustness under such perturbations.

Method: ANPrompt framework constructs weak noise text features by fusing original and noise-perturbed text embeddings, which are then clustered to form noise prompts. These noise prompts are integrated with learnable prompt tokens to generate anti-noise prompts, which are injected into the deeper layers of both image and text encoders. ANPrompt also computes the Noise-Resistant Visual Prompt Prototype (NRVPP) by averaging the output prompt tokens from the vision encoder and introduces alignment, robustness, and anti-noise objectives by computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard cross-entropy and sim loss.

Result: Experiments across 11 benchmarks demonstrate that ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories.

Conclusion: ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories across 11 benchmarks.

Abstract: Prompt tuning has emerged as an efficient and effective technique for
adapting vision-language models (VLMs) with low computational overhead.
However, existing methods often overlook the vulnerability of prompt-tuned VLMs
to weak semantic perturbations-such as subtle image or text noise-that degrade
their generalization to unseen classes. To address this limitation, we propose
ANPrompt, a novel prompt tuning framework designed to enhance robustness under
such perturbations. ANPrompt first constructs weak noise text features by
fusing original and noise-perturbed text embeddings, which are then clustered
to form noise prompts. These noise prompts are integrated with learnable prompt
tokens to generate anti-noise prompts, which are injected into the deeper
layers of both image and text encoders. To further capture the noise-aware
visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype
(NRVPP) by averaging the output prompt tokens from the vision encoder. Finally,
ANPrompt introduces alignment, robustness, and anti-noise objectives by
computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard
cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that
ANPrompt consistently outperforms existing prompt tuning approaches, achieving
superior robustness to semantic noise and improved generalization to novel
categories.

</details>


### [130] [Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions](https://arxiv.org/abs/2508.04681)
*Liang Xu,Chengqun Yang,Zili Lin,Fei Xu,Yifan Liu,Congsheng Xu,Yiyi Zhang,Jie Qin,Xingdong Sheng,Yunhui Liu,Xin Jin,Yichao Yan,Wenjun Zeng,Xiaokang Yang*

Main category: cs.CV

TL;DR: 研究人员创建了一个名为InterVLA的大规模数据集，包含第一人称视角的交互数据，以改进AI助手的通用性和效率，并提出了新的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏通用交互知识和第一人称视角，而AI助手需要这两种信息来有效感知和行动。

Method: 提出了一种混合RGB-MoCap系统，并将其嵌入视觉-语言-动作框架中，利用GPT生成的脚本，让AI助手在以自身为中心的视角下，根据指令为人类提供服务。

Result: 创建了InterVLA，一个包含11.4小时、120万帧多模态数据的大规模数据集，涵盖了两种第一人称视角和五种第三人称视角视频、精确的人体/物体运动以及语音指令。并建立了在第一人称动作估计、交互合成和交互预测方面的新基准。

Conclusion: 该研究通过InterVLA数据集和相关基准测试，为未来在物理世界中构建AI代理奠定了基础。

Abstract: Learning action models from real-world human-centric interaction datasets is
important towards building general-purpose intelligent assistants with
efficiency. However, most existing datasets only offer specialist interaction
category and ignore that AI assistants perceive and act based on first-person
acquisition. We urge that both the generalist interaction knowledge and
egocentric modality are indispensable. In this paper, we embed the
manual-assisted task into a vision-language-action framework, where the
assistant provides services to the instructor following egocentric vision and
commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors
engage with multiple objects and the scene following GPT-generated scripts.
Under this setting, we accomplish InterVLA, the first large-scale
human-object-human interaction dataset with 11.4 hours and 1.2M frames of
multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate
human/object motions and verbal commands. Furthermore, we establish novel
benchmarks on egocentric human motion estimation, interaction synthesis, and
interaction prediction with comprehensive analysis. We believe that our
InterVLA testbed and the benchmarks will foster future works on building AI
agents in the physical world.

</details>


### [131] [TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction](https://arxiv.org/abs/2508.04682)
*Zewei Zhou,Seth Z. Zhao,Tianhui Cai,Zhiyu Huang,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: TurboTrain是一个用于多代理感知和预测的新型训练框架，通过时空预训练和平衡多任务学习，简化了训练，提高了性能，并减少了手动调整。


<details>
  <summary>Details</summary>
Motivation: 为了克服端到端训练多代理系统所面临的挑战，即需要大量手动设计和监控，本研究提出了一种新颖高效的TurboTrain训练框架，旨在简化训练过程，提高性能。

Method: TurboTrain框架包含两个核心组件：1. 基于掩码重构学习的多代理时空预训练方案，用于捕捉时空多代理特征。2. 基于梯度冲突抑制的平衡多任务学习策略，用于优化多任务学习过程中的梯度冲突问题。

Result: TurboTrain在V2XPnP-Seq数据集上成功提升了最先进的多代理感知和预测模型的性能。预训练有效地捕获了时空多代理特征，显著有利于下游任务。平衡多任务学习策略增强了检测和预测能力。

Conclusion: TurboTrain框架通过其多代理时空预训练方案和基于梯度冲突抑制的平衡多任务学习策略，有效降低了训练复杂性，缩短了训练时间，并提升了性能。该框架在实际的合作驾驶数据集V2XPnP-Seq上进行了评估，证明了其在多代理感知和预测任务上的优越性。

Abstract: End-to-end training of multi-agent systems offers significant advantages in
improving multi-task performance. However, training such models remains
challenging and requires extensive manual design and monitoring. In this work,
we introduce TurboTrain, a novel and efficient training framework for
multi-agent perception and prediction. TurboTrain comprises two key components:
a multi-agent spatiotemporal pretraining scheme based on masked reconstruction
learning and a balanced multi-task learning strategy based on gradient conflict
suppression. By streamlining the training process, our framework eliminates the
need for manually designing and tuning complex multi-stage training pipelines,
substantially reducing training time and improving performance. We evaluate
TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and
demonstrate that it further improves the performance of state-of-the-art
multi-agent perception and prediction models. Our results highlight that
pretraining effectively captures spatiotemporal multi-agent features and
significantly benefits downstream tasks. Moreover, the proposed balanced
multi-task learning strategy enhances detection and prediction.

</details>


### [132] [BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning](https://arxiv.org/abs/2508.04702)
*Ziyang Leng,Jiawei Yang,Zhicheng Ren,Bolei Zhou*

Main category: cs.CV

TL;DR: BEVCon通过对比学习改进了自动驾驶中的BEV感知，在nuScenes数据集上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有BEV感知研究主要集中在BEV编码器和任务特定头部，而忽视了表示学习在BEV模型中的潜力。本文旨在探索并利用表示学习来提升BEV感知能力。

Method: BEVCon是一个对比学习框架，包含两个模块：1. 实例特征对比模块，用于优化BEV特征；2. 透视视图对比模块，用于增强图像骨干网络。该框架采用密集的对比学习，并结合检测损失进行优化。

Result: 在nuScenes数据集上的广泛实验表明，BEVCon相比现有最先进方法，在mAP方面取得了最高+2.4%的性能提升，展示了其在提升BEV特征表示和骨干网络方面的有效性。

Conclusion: BEVCon通过引入实例特征对比和透视视图对比模块，有效提升了BEV感知能力，并在nuScenes数据集上实现了显著的性能提升（最高+2.4% mAP），证明了表示学习在BEV感知中的关键作用。

Abstract: We present BEVCon, a simple yet effective contrastive learning framework
designed to improve Bird's Eye View (BEV) perception in autonomous driving. BEV
perception offers a top-down-view representation of the surrounding
environment, making it crucial for 3D object detection, segmentation, and
trajectory prediction tasks. While prior work has primarily focused on
enhancing BEV encoders and task-specific heads, we address the underexplored
potential of representation learning in BEV models. BEVCon introduces two
contrastive learning modules: an instance feature contrast module for refining
BEV features and a perspective view contrast module that enhances the image
backbone. The dense contrastive learning designed on top of detection losses
leads to improved feature representations across both the BEV encoder and the
backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon
achieves consistent performance gains, achieving up to +2.4% mAP improvement
over state-of-the-art baselines. Our results highlight the critical role of
representation learning in BEV perception and offer a complementary avenue to
conventional task-specific optimizations.

</details>


### [133] [Occupancy Learning with Spatiotemporal Memory](https://arxiv.org/abs/2508.04705)
*Ziyang Leng,Jiawei Yang,Wenlong Yi,Bolei Zhou*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: 3D occupancy becomes a promising perception representation for autonomous
driving to model the surrounding environment at a fine-grained scale. However,
it remains challenging to efficiently aggregate 3D occupancy over time across
multiple input frames due to the high processing cost and the uncertainty and
dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level
occupancy representation learning framework that effectively learns the
spatiotemporal feature with temporal consistency. ST-Occ consists of two core
designs: a spatiotemporal memory that captures comprehensive historical
information and stores it efficiently through a scene-level representation and
a memory attention that conditions the current occupancy representation on the
spatiotemporal memory with a model of uncertainty and dynamic awareness. Our
method significantly enhances the spatiotemporal representation learned for 3D
occupancy prediction tasks by exploiting the temporal dependency between
multi-frame inputs. Experiments show that our approach outperforms the
state-of-the-art methods by a margin of 3 mIoU and reduces the temporal
inconsistency by 29%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [134] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 本研究发现GPT-4 Turbo在印度宗教和种姓方面存在严重的表征偏见，过度代表主导群体，并且提示调整效果不佳，表明仅靠数据多样化无法解决LLM偏见。


<details>
  <summary>Details</summary>
Motivation: 为了扩展现有研究，即主要关注单一响应交互和全球北方中心身份（如种族和性别）的表征偏见，本研究旨在揭示GPT-4 Turbo中表征偏见的深度以及其在鲜有探索的身份维度上的延伸。

Method: 通过对GPT-4 Turbo进行系统审计，生成了超过7200个关于印度生活事件（如婚礼）的故事，并设计了不同程度鼓励多样性的提示。通过将输出结果与印度人口普查数据进行比较，量化了宗教和种姓方面表征偏见的“粘性”。

Result: GPT-4的响应持续过度代表文化主导群体，尽管提示旨在鼓励多样性。研究结果表明，LLM中的表征偏见具有“赢家通吃”的特性，比其训练数据中可能存在的分布偏差更为严重。此外，重复的基于提示的调整在消除这些偏见方面的效果有限且不一致。

Conclusion: 该研究量化了GPT-4 Turbo在宗教和种姓方面存在的表征偏见，发现其过度代表了文化主导群体，并且“赢家通吃”的偏见比训练数据中的偏见更严重。研究还表明，仅仅多样化训练数据不足以纠正LLM偏见，需要更根本性的模型开发变更。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [135] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 研究人员使用Llama-3.1模型微调了20个专门针对理论高能物理的模型，这些模型在摘要补全任务中优于基础模型，并为未来开发此类模型提供了见解。


<details>
  <summary>Details</summary>
Motivation: 为了解决理论高能物理领域的特定需求，需要开发专门的语言模型。

Method: 使用两种不同的低秩自适应微调方法和不同大小的数据集，对Llama-3.1模型进行了微调，生成了20个专门针对理论高能物理的模型变体。这些模型在arXiv摘要（截至2024年8月）上进行训练，涵盖了hep-th、hep-ph和gr-qc的不同组合。为了进行比较，还在包含q-bio和cs等不同领域摘要的数据集上训练了模型。

Result: 在hep-th摘要补全任务中，微调后的模型优于基础模型。研究结果为进一步开发专门用于高能理论物理的语言模型提供了见解。

Conclusion: 与基础模型相比，在hep-th摘要补全任务中，所有模型都表现更好。此外，研究结果为进一步开发专门用于高能理论物理的语言模型提供了见解。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [136] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: Krishi Sathi是一个AI农业聊天机器人，通过多轮对话、IFT模型和RAG技术，提供印地语和英语的个性化农业咨询，准确率高，响应快，解决了印度农民在获取农业信息方面的障碍。


<details>
  <summary>Details</summary>
Motivation: 印度农民，尤其是在农村地区和低读写能力者，常常缺乏及时、易得且符合当地语言习惯的农业咨询。为了解决这一可及性差距，本研究旨在提供一个AI驱动的解决方案。

Method: 本研究提出了一种名为Krishi Sathi的新型AI驱动的农业聊天机器人，该机器人通过文本和语音提供个性化、易于理解的答案。该系统使用IFT模型，并在三个精选的数据集上针对印度农业知识进行了微调。它采用结构化的多轮对话流程来收集用户信息，并利用检索增强生成（RAG）技术，首先从精选的农业数据库中检索信息，然后使用IFT模型生成定制的响应。该聊天机器人支持英语和印地语，并具有语音输入和输出功能。

Result: Krishi Sathi聊天机器人在查询响应准确率方面达到了97.53%，上下文相关性和个性化方面达到了91.35%，查询完成率达到了97.53%。平均响应时间低于6秒，同时支持英语和印地语交互。

Conclusion: 该研究展示了结合意图驱动的对话流程、指令调整模型和基于检索的生成技术如何提高印度数字农业支持的质量和可及性。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [137] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: HVT通过分层验证树优化LLM推理，显著提高效率。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言处理任务中取得了巨大成功，但由于其自回归性质，在推理效率方面面临持续的挑战。传统的验证方法按顺序验证草稿序列，缺乏优先级，导致不必要的计算开销。

Method: 提出了一种名为分层验证树（HVT）的新框架，该框架通过优先处理高概率草稿和尽早修剪次优候选来重构投机性束解码。开发了理论基础和形式化验证-修剪算法，以确保正确性和效率。HVT可以集成到标准的LLM推理流程中，无需重新训练或修改模型架构。

Result: 实验评估表明，HVT在多个数据集和模型上持续优于现有的投机性解码方案，实现了推理时间和能耗的大幅降低，同时保持或提高了输出质量。

Conclusion: HVT通过分层验证策略显著提高了LLM的推理效率，减少了推理时间和能耗，同时保持或提高了输出质量，为加速LLM推理提供了新的方向。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [138] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL是一个创新的LLM代理框架，能自动更新维基百科，优于现有技术，提高了信息时效性。


<details>
  <summary>Details</summary>
Motivation: 维基百科依赖人工编辑，难以保持内容时效性。受NELL的持续知识获取愿景和LLM代理技术发展的启发，旨在开发自动更新维基百科的框架。

Method: WiNELL采用多代理框架，包括聚合在线信息、选择新知识和生成编辑建议的代理。使用在维基百科人类编辑历史数据上训练的细粒度编辑模型，以匹配人类编辑行为。

Result: WiNELL在信息覆盖度和编辑效率方面超越了开源指令跟随基线和GPT-4o等闭源LLM。端到端评估表明，WiNELL能够识别并建议对高活跃度维基百科页面的及时事实更新。

Conclusion: WiNELL是一个用于持续更新维基百科文章的代理框架，它利用多代理方法聚合在线信息、选择新知识并生成精确的编辑建议，并通过细粒度编辑模型确保编辑风格与人类一致。在信息覆盖度和编辑效率方面优于现有基线模型和闭源LLM，为自动更新知识库开辟了新方向。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [139] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 开发了一个名为 GanitBench 的双语（英语/印地语）数学视觉问答基准测试，以解决当前 VLM 评估中语言和领域覆盖不足的问题。评估结果显示 GPT-4o mini 在该基准测试上表现最佳，但仍有改进空间，并且模型在印地语和“双重锁定”约束下的性能会下降。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLM）的推理基准测试大多是单语的（仅英语），并且在印地语方面的可用数据集有限，主要局限于理解和翻译任务。作者希望通过 GanitBench 促进像印地语这样的语言在 VLM 研究中的应用。

Method: 创建了一个包含 1527 个视觉问题（涵盖数学各个主题）的双语（英语和印地语）基准测试 GanitBench，并收集了 JEE Advanced 和 CBSE Board 考试中的题目。评估了两个闭源模型在零样本和双少样本思维链（CoT）设置下的性能，并引入了“双重锁定”约束来评估模型的鲁棒性。

Result: GPT-4o mini 在 GanitBench 上表现最优，在双少样本思维链（CoT）设置下的最高平均准确率为 38.15%。“双重锁定”约束显著降低了模型性能，而双少样本思维链（CoT）在此约束下效果更佳。模型在回答印地语问题时性能有所下降。

Conclusion: GanitBench 的评估表明，在双语（英语和印地语）和各种数学主题的视觉问答方面，GPT-4o mini 是表现更好的模型，但总体准确率仍有提升空间。双少样本思维链（two-shot CoT）和“双重锁定”约束对模型性能有显著影响，而印地语的性能表现低于英语。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [140] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: AttnTrace 是一种新的、更准确、更高效的上下文追溯方法，利用 LLM 的注意力权重，可用于改进提示注入检测和识别操纵 LLM 生成评论的论文中的注入指令。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文追溯解决方案（如 TracLLM）通常计算成本高昂，而 AttnTrace 旨在提供一种更准确、更高效的替代方案，并可用于检测提示注入。

Method: AttnTrace 是一种新的上下文追溯方法，它利用 LLM 为提示产生的注意力权重。为了有效利用注意力权重，我们引入了两种技术来增强 AttnTrace 的有效性，并为我们的设计选择提供了理论见解。

Result: AttnTrace 比现有最先进的上下文追溯方法更准确、更高效。AttnTrace 还可以提高最先进的方法在长上下文的提示注入检测能力。在真实世界的应用中，AttnTrace 可以有效地精确定位论文中注入的指令，以操纵 LLM 生成的评论。

Conclusion: AttnTrace 在检测提示注入方面比现有最先进的上下文追溯方法更准确、更高效，并且可以通过“先归因后检测”范式来改进最先进的方法。此外，AttnTrace 还可以有效地精确定位操纵 LLM 生成评论的论文中注入的指令。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [141] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: MajorMark通过“多数比特感知编码”和基于聚类的解码策略，解决了现有水印技术在文本质量和解码准确性之间的权衡问题，实现了两者的提升。MajorMark$^+$ 进一步通过分块编码和解码来优化此过程。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）在生成有害或欺骗性内容方面的潜在滥用问题，水印技术被提出来用于起源验证和滥用追踪。然而，现有的多比特水印方案在文本质量和解码准确性之间存在根本性的权衡：为了确保可靠的解码，它们必须限制编码时首选词元集的大小，但这会降低生成内容的质量。

Method: MajorMark提出了一种新颖的水印方法，通过“majority bit-aware encoding”（多数比特感知编码）来改善文本质量和解码准确性之间的权衡。它根据消息的多数比特来选择词元集合，允许更大、更灵活的词元采样。与依赖词元频率分析进行解码的先前方法不同，MajorMark采用基于聚类的解码策略，即使在词元集合较大的情况下也能保持高解码准确性。此外，MajorMark$^+$ 将消息分割成多个块，独立编码并确定性地解码每个块，进一步提高了水印文本的质量和解码准确性。

Result: 实验证明，MajorMark 和 MajorMark$^+$ 方法显著提高了解码准确性和文本生成质量，并且优于先前多比特水印的基线方法。

Conclusion: MajorMark 和 MajorMark$^+$ 方法在解码准确性和文本生成质量上显著优于先前多比特水印方法，并在最新一代语言模型上进行了广泛的实验验证。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [142] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: CoQ框架通过生成问题来引导多模态模型选择性地使用视觉、听觉等模态，从而提高推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在多模态环境中推理能力尚未完全跟上其在文本环境中进步的问题，即在与复杂现实世界环境交互时，模型需要主动决定激活哪些感官模态。

Method: Chain of Questions (CoQ)框架，一种由好奇心驱动的推理方法，能够动态生成目标问题以引导模型选择性地激活相关模态。

Result: CoQ方法在整合的基准数据集上的评估结果表明，能够有效提高基础模型识别和整合相关感官信息的能力，从而在准确性、可解释性和推理过程的一致性方面有所提升。

Conclusion: CoQ框架通过鼓励多模态语言模型动态生成关于其周围环境的目标问题，从而提高其有效识别和整合相关感官信息的能力，进而提高在不同多模态任务中的准确性、可解释性和推理过程的一致性。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [143] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: LLMs trained on internet data can produce misinformation. This review analyzes fact-checking methods for LLM content, discussing challenges like hallucinations and evaluation metrics. It emphasizes the need for frameworks integrating advanced prompting, fine-tuning, and retrieval-augmented generation (RAG). Key findings show current metrics are limited, external evidence is valuable, and domain-specific customization improves accuracy. The review stresses the need for accurate, explainable, and domain-specific LLMs for reliable fact-checking.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) can generate misinformation due to training on inaccurate or misleading internet content, making robust fact-checking essential.

Method: This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. The review also discusses the role of instruction tuning, multi-agent reasoning, and external knowledge access via RAG frameworks.

Result: Key findings highlight the limitations of current metrics, the value of grounding outputs with validated external evidence, and the importance of domain-specific customization to improve factual consistency.

Conclusion: The review highlights the importance of building LLMs that are not only accurate and explainable but also tailored for domain-specific fact-checking, contributing to the advancement of research toward more trustworthy and context-aware language models.

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [144] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 提出了一种用于问答的实体链接代理，解决了现有方法在短文本和歧义性问题上的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有实体链接方法多针对长上下文设计，在问答任务中的短文本、歧义性用户问题上表现不佳。

Method: 提出了一种基于大型语言模型的实体链接代理，该代理通过模拟人类认知工作流程来主动识别实体提及、检索候选实体并做出决策。

Result: 实验结果证实了该代理的鲁棒性和有效性。

Conclusion: 所提出的基于大型语言模型的实体链接代理在工具基础实体链接和问答任务评估中被证明是有效的。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [145] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: Sotopia-RL 通过细化奖励来提高 LLM 的社交智能，并在实验中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 为了使大型语言模型（LLMs）具备社会智能，以有效应对现实世界中的社交任务（如适应、说服、协作和谈判）。现有的基于 MDP 的强化学习（RL）方法在处理社交互动中部分可观察性和多维度这两个特性时效率低下且不稳定。

Method: 提出了一种名为 Sotopia-RL 的新颖框架，该框架将粗粒度的回合级反馈细化为话语级、多维奖励，以解决部分可观察性和多维度带来的挑战。

Result: Sotopia-RL 在 Sotopia 环境中实现了最先进的社会目标完成分数，在 Sotopia-hard 上为 7.17，在 Sotopia-full 上为 8.31，显著优于现有方法。消融研究证实了话语级信用分配和多维奖励设计对 RL 训练的必要性。

Conclusion: Sotopia-RL框架通过将粗粒度的回合级反馈细化为话语级、多维奖励，能够有效解决部分可观察性和多维度这两个挑战，在Sotopia环境中取得了最先进的社会目标完成分数（Sotopia-hard 上为 7.17，Sotopia-full 上为 8.31），显著优于现有方法。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [146] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: CoAct-1通过结合GUI操作和编码能力，显著提高了自主代理在计算机自动化任务中的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 旨在解决通过图形用户界面（GUI）操作计算机的自主代理在复杂、长期的任务中效率低下和不可靠的问题。现有的方法虽然可以通过规划器进行任务分解，但在通过GUI进行所有操作时仍然受到固有局限性的约束，导致脆弱和效率低下。

Method: 提出了一种名为CoAct-1的新型多智能体系统，该系统将基于GUI的控制与直接的程序执行相结合。系统包含一个协调器，可以动态地将子任务委托给传统的GUI操作员或专门的程序员（可以编写和执行Python或Bash脚本）。

Result: 在OSWorld基准测试中，CoAct-1实现了60.76%的新技术领先成功率，显著优于先前的方法，并将完成任务所需的平均步骤从15步减少到10.15步。

Conclusion: 通过将编码作为核心操作，可以为通用计算机自动化提供更强大、更高效、更可扩展的路径。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [147] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: CAP-LLM 是一个新框架，通过整合用户偏好和事实一致性约束，利用 LLM 改进了新闻标题的个性化生成，并在现实世界的数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在有效捕捉复杂用户兴趣和确保事实一致性方面的不足，这些不足常常导致生成通用或误导性的标题。

Method: 提出了一种名为 CAP-LLM 的新框架，该框架集成了用户偏好和事实一致性约束到一个强大的预训练 LLM 主干中。CAP-LLM 包括一个用户偏好编码器、一个上下文注入适配器和一个事实一致性强化模块，后者采用新颖的对比损失来减少幻觉。

Result: 在 PENS 数据集上，CAP-LLM 在所有指标上均实现最先进的性能，事实一致性（FactCC 87.50）显著优于基线（BART 86.67），同时提高了个性化（Pc(avg) 2.73, Pc(max) 17.25）和内容覆盖率（ROUGE 指标）。

Conclusion: CAP-LLM 在新闻标题生成方面取得了最先进的性能，在个性化和事实准确性方面取得了卓越的平衡。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [148] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文提出了一种在机器学习模型整个生命周期中管理、评估和量化偏差的方法，并讨论了解决LLM中偏差、伦理、公平性和事实性的数据和AI治理框架。


<details>
  <summary>Details</summary>
Motivation: 本文旨在促进创建和部署具有社会责任感和伦理道德的生成式人工智能应用程序。

Method: 本文介绍了一个系统性治理、评估和量化机器学习模型整个生命周期中的偏差的方法，并讨论了解决LLM中偏差、伦理、公平性和事实性的数据和AI治理框架。

Result: 本文分享了大型语言模型（LLM）中普遍存在的偏差和公平性相关差距，并讨论了解决LLM中偏差、伦理、公平性和事实性的数据和AI治理框架。

Conclusion: 通过在AI开发的全生命周期中实施数据和AI治理，组织可以显著提高其GenAI系统的安全性、责任感，有效降低歧视风险，并防止潜在的声誉或品牌损害。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [149] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 通过在长链条思考任务中提前修剪假设来提高自洽性的代币效率，同时保持其并行性。


<details>
  <summary>Details</summary>
Motivation: 为了解决自洽性代币消耗过高的问题，提高其在长链条思考任务中的实用性。

Method: 通过两种轻量级指标（模型对单个假设的信心和词汇覆盖度）来修剪冗余的中间假设，并设计了一种快速加权集覆盖算法来利用这些指标。

Result: 在五个大型语言模型和三个数学基准上的评估表明，该方法可以提高所有模型的代币效率。 10-35%。

Conclusion: 该方法在不牺牲并行性的前提下，提高了长链条思考任务中自洽性的代币效率，并在许多情况下将代币效率提高了 10-35%。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [150] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: LLMs can be fine-tuned with SFT and DPO to generate high-quality, audience-tailored explanations of well-being concepts, outperforming larger models in this specialized task. An LLM-as-a-judge framework was validated against human evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of whether LLMs can generate accurate and audience-tailored explanations of well-being concepts as individuals increasingly consult them for such information.

Method: Constructed a large-scale dataset of 43,880 explanations for 2,194 well-being concepts from ten LLMs. Introduced a principle-guided LLM-as-a-judge evaluation framework with dual judges. Fine-tuned an open-source LLM using SFT and DPO.

Result: 1. The proposed LLM judges align well with human evaluations. 2. Explanation quality varies significantly across models, audiences, and categories. 3. DPO- and SFT-finetuned models show improved explanation quality compared to larger models.

Conclusion: DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [151] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 该研究开发了一个社会机器人共同引导者，利用迁移学习框架将基础模型的社会理解能力提炼到一个可解释的概念瓶颈模型（CBM）中，以辅助人类引导者。该机器人能够分析多模态会议数据，提供透明的干预建议，并在预测干预需求和知识迁移方面表现出色，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 理想的引导者需要能够识别和干预群体动态中的微妙问题，例如参与度下降、个人目标设定和执行困难，以及人际关系困难。然而，引导者面临的挑战和认知负荷巨大，这为一种能够解读社交互动、同时了解个体需求并提供透明建议的实体技术创造了关键需求，以弥补仅依赖“黑箱”基础模型（FMs）的不足。

Method: 该研究通过一个社会机器人共同引导者来解决这个问题，该机器人能够分析多模态会议数据并向引导者提供谨慎的提示。机器人的推理能力基于一个以人为本的概念瓶颈模型（CBM），该模型根据参与者参与度和情绪等可解释的概念做出决策。核心贡献在于一个迁移学习框架，将基础模型（FM）的广泛社会理解能力提炼到专门的、可解释的CBM中。

Result: 该概念驱动的系统在预测干预需求方面显著优于直接的零样本基础模型（FMs），并且能够实时进行人为纠正。研究还证明了该模型具有强大的知识迁移能力，能够泛化到不同的群体，并将资深引导者的专业知识转移给新手，从而提高新手引导者的表现。

Conclusion: 该研究提出了一种将大型基础模型的广泛社会理解能力迁移到专门的、可解释的概念瓶颈模型（CBM）的迁移学习框架。该系统在预测干预需求方面显著优于直接的零样本基础模型，并支持实时的人工干预。此外，该模型能够跨不同群体进行泛化，并将资深人类引导者的专业知识转移给新手，从而提高其绩效。通过将专家的认知模型转移到可解释的机器人伙伴中，该研究为增强人类在复杂社交领域的能力提供了强大的蓝图。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [152] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: HarmonyGuard 是一个多智能体协作框架，通过策略增强和双重目标优化，提升了 Web 智能体的安全性和任务完成度。


<details>
  <summary>Details</summary>
Motivation: 当前 Web 智能体在长序列操作中面临在任务表现和新兴风险之间进行平衡的挑战，现有研究缺乏在 Web 环境中对安全和效用进行协同优化的能力。

Method: HarmonyGuard 框架包含一个策略代理，用于从非结构化文档中提取和维护结构化安全策略，并持续更新以应对不断变化的威胁；以及一个效用代理，基于安全和效用双重目标，进行马尔可夫实时推理和优化。

Result: HarmonyGuard 在策略合规性方面提高了高达 38%，在任务完成度方面提高了高达 20%，并且在所有任务中实现了超过 90% 的策略合规性。

Conclusion: HarmonyGuard 提出了一个多智能体协作框架，通过策略增强和目标优化来联合提升效用和安全，在多个基准测试中表现优于现有方法，策略合规性提高高达 38%，任务完成度提高高达 20%，且在所有任务中策略合规性超过 90%。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [153] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: SMEdit improves LLM knowledge updating by using MBPS for better performance in low-data scenarios and norm regularization for training efficiency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs are static and costly to update; model editing is an alternative. Meta-learning-based model editing (MLBME) is effective but suboptimal in low-data scenarios and inefficient due to KL divergence computation.

Method: SMEdit adopts Multiple Backpropagation Steps (MBPS) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency.

Result: Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines.

Conclusion: SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance.

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [154] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: ZARA 是一个创新的框架，无需重新训练即可直接从运动时间序列中进行人类活动识别，并提供可解释的推理，在性能上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 HAR 方法需要为固定的活动集进行训练，并在出现新行为或传感器设置时进行昂贵的重新训练。而使用 LLM 的方法存在准确性有限和可解释性差的问题。因此，需要一种无需微调即可灵活、可解释地进行 HAR 的方法。

Method: ZARA 是一个基于代理的框架，直接从原始运动时间序列中进行零样本、可解释的 HAR。它包括一个自动派生的成对特征知识库、一个多传感器检索模块和一个分层代理管道，该管道引导 LLM 迭代地选择特征、利用证据并生成活动预测和自然语言解释。

Result: ZARA 在 8 个 HAR 基准测试中实现了最先进的零样本性能，并提供了清晰的推理过程，其宏观 F1 分数比最强的基线高出 2.53 倍。此外，实验还验证了 ZARA 各个模块的必要性。

Conclusion: ZARA 框架在零样本、可解释的人类活动识别（HAR）方面取得了最先进的性能，其推理过程清晰，并且超越了最强的基线 2.53 倍（宏观 F1 分数）。实验表明，ZARA 是一个可靠的、即插即用的运动时间序列分析的有力工具。

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [155] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: Jailbreaking AI is now easier and cheaper using Large Reasoning Models (LRMs), which can act as autonomous attackers, achieving a 97.14% success rate in eroding AI safety guardrails. This highlights the need for better AI alignment to prevent both jailbreaks and LRMs being used as jailbreaking tools.


<details>
  <summary>Details</summary>
Motivation: Investigate whether the persuasive capabilities of large reasoning models (LRMs) simplify and scale jailbreaking, converting it into an inexpensive activity accessible to non-experts.

Method: Evaluated the capabilities of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as autonomous adversaries conducting multi-turn conversations with nine widely used target models. LRMs received instructions via a system prompt, before proceeding to planning and executing jailbreaks with no further supervision. Performed extensive experiments with a benchmark of harmful prompts composed of 70 items covering seven sensitive domains.

Result: The study yielded an overall attack success rate across all model combinations of 97.14%.

Conclusion: LRMs can systematically erode the safety guardrails of other models, highlighting the urgent need to further align frontier models not only to resist jailbreak attempts, but also to prevent them from being co-opted into acting as jailbreak agents.

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [156] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: A new framework called DTPA improves controllable text generation, especially for long texts, by addressing attention decay in prefix-based methods and dynamically adjusting prefix attention.


<details>
  <summary>Details</summary>
Motivation: Previous studies on controllable text generation mainly focus on short sequences, while long-form text generation is underexplored. The controllability of prefix-based methods like Air-Decoding declines with increasing sequence length due to attention decay to prefixes.

Method: DTPA framework based on Air-Decoding, which selects the optimal prefix type and dynamically amplifies attention to the prefix for attribute distribution with a growing scaling factor, optionally applying similar augmentation to the original prompt for raw distribution.

Result: DTPA outperforms other methods in attribute control while maintaining competitive fluency, diversity, and topic relevance, with superior effectiveness in long text generation.

Conclusion: DTPA

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [157] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS框架通过自适应地判断是否检索以及如何选择信息，解决了RAG系统效率和准确性问题，降低了检索成本并提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统存在两个关键限制：1. 对每个查询都进行低效检索，即使是简单问题；2. 当查询信息信号稀疏时，存在检索到不相关文档的风险。

Method: PAIRS框架采用双通路生成机制：首先，LLM生成直接答案和使用自生成伪上下文的增强答案，若两者收敛则跳过外部检索；若发散，则激活由原始查询和自生成上下文信号引导的双通路检索（DPR）过程，并通过加权相似度过滤文档的自适应信息选择（AIS）模块。

Result: PAIRS将检索成本降低了约25%（仅对75%的查询触发检索），同时在六个问答（QA）基准测试中平均准确率提升了+1.1% EM和+1.0% F1。

Conclusion: PAIRS框架通过集成参数知识和检索知识，自适应地决定是否检索以及如何选择外部信息，从而提高了RAG系统的效率和准确性。实验结果表明，PAIRS将检索成本降低了约25%，同时在平均准确率上提升了+1.1% EM和+1.0% F1。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [158] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: This paper presents methods to make Large Language Models (LLMs) more efficient for use with limited resources by carefully selecting data, adjusting training, and modifying the model's architecture, showing success in tests.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research is to address the significant computational resource requirements that constrain the large-scale deployment of LLMs, aiming to improve their efficiency in resource-constrained environments and within delimited knowledge bases.

Method: The study involved defining criteria for building reliable datasets, conducting controlled experiments with different configurations (data processing, selection, training strategies, architectural adjustments), and systematically evaluating the resulting LLM variants based on capability, versatility, response time, and safety. Comparative tests were performed to validate the effectiveness of the proposed strategies.

Result: The research resulted in the development of efficient LLM variants through strategic data processing, selection, training, and architectural adjustments, with their effectiveness validated through comparative tests measuring capability, versatility, response time, and safety.

Conclusion: The study demonstrates that data processing and selection techniques, combined with specific training strategies and architectural adjustments, can significantly improve the efficiency of LLMs in resource-constrained environments.

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [159] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: ToolGrad框架通过“先回答”的方法，先构建工具使用链再生成用户查询，从而提高数据集质量和效率，并在实验中取得优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决先前通过生成用户查询然后进行复杂工具使用注释（如DFS）来合成工具使用LLM数据集的方法存在注释失败和数据生成效率低下的问题。

Method: ToolGrad框架通过迭代过程和文本“梯度”引导，首先构建有效的工具使用链，然后合成相应的用户查询。

Result: ToolGrad-5k数据集的生成成本更低，工具使用更复杂，且通过率为100%。

Conclusion: ToolGrad-5k数据集使模型在工具使用方面表现优于昂贵的基线数据集和专有LLM，并且在OOD基准测试上也表现出色。

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [160] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 提出了一种名为GM-PRM的新范例，它不仅能识别多模态大语言模型在数学推理中的错误，还能主动纠正它们，并通过精炼最佳N（Refined-BoN）策略提升解决方案的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态过程奖励模型（PRM）仅限于充当二元验证器，只能识别错误而无法纠正，解释能力有限。为了解决这些不足，需要一种能够纠正错误并提供解释性分析的新方法。

Method: 介绍了一种名为生成式多模态过程奖励模型（GM-PRM）的新范例，该模型将PRM从被动的裁判转变为主动的推理协作者。GM-PRM的特点是提供对每个推理步骤进行细粒度、可解释的分析，评估其意图、视觉对齐和逻辑健全性。更重要的是，GM-PRM被训练来生成其识别出的第一个错误步骤的修正版本。此外，还提出了一种名为精炼最佳N（Refined-BoN）的测试时间推理策略，该策略利用PRM生成的修正来指导策略模型走向更有希望的推理轨迹，从而提高解决方案的多样性和正确性。

Result: GM-PRM在多个多模态数学基准上取得了最先进的成果，显著提高了策略模型的性能，并且数据效率很高，仅需20K样本的训练数据集。

Conclusion: GM-PRM在多个多模态数学基准上实现了最先进的结果，显著提高了策略模型的性能，同时具有出色的数据效率，仅需20K样本的训练数据集。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [161] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: LLM微调可能导致过拟合，影响模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示LLM微调过程中可能出现的过拟合现象，理解其学习动态，并探讨过拟合对模型性能的影响。

Method: 通过实验研究了LLM在推理任务上的微调学习动态，重点关注了过拟合现象，并分析了导致过拟合的因素，如训练周期和学习率。

Result: 发现了LLM微调过程中存在一个“过拟合”阶段，此时模型虽然测试准确率高，但测试困惑度也高。过拟合导致模型鲁棒性降低、分布外泛化能力减弱、生成多样性下降，且该现象广泛存在于不同任务、模型和微调方法中。

Conclusion: 研究揭示了LLM微调中的过拟合现象，并提出要选择合适的检查点和学习率，以避免泛化能力下降、鲁棒性减弱和生成多样性降低等问题。

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [162] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: A new method for selecting preference data for LLMs improves efficiency and alignment by choosing challenging examples, outperforming baselines with only 10% of the data.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models (LLMs) with human preferences is a critical challenge. Current methods like RLHF and DPO rely on large, costly preference datasets, and there is a lack of methods for high-quality data selection specifically for preference data.

Method: This paper introduces a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. The strategy involves selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases.

Result: The proposed approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10% of the original data.

Conclusion: Aligning large language models (LLMs) with human preferences is a critical challenge. This work introduces a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [163] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [164] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 通过引入因果完备性奖励和 GRPO 优化框架，有效解决了多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）可能出现幻觉问题，即生成与输入图像或文本语义不一致的输出。通过因果分析，发现（一）遗漏型幻觉可能源于未能充分捕捉到重要的因果因素，（二）捏造型幻觉可能是由模型被非因果线索误导所致。

Method: 提出了一种新颖的、以因果完备性为指导的强化学习框架，该框架联合考虑了 token 的因果充分性和因果必要性。具体来说，我们评估了每个 token 的独立贡献和反事实不可或缺性来定义 token 级别的因果完备性奖励。这种奖励用于在 GRPO 优化框架内构建因果知情的优势函数，鼓励模型关注对准确生成既有因果充分性又有因果必要性的 token。

Result: 实验结果表明，该方法在各种基准数据集和任务上均有效，能够有效地减轻 MLLMs 中的幻觉问题。

Conclusion: 该方法有效缓解了多模态大语言模型中的幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [165] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: This paper defines "deep research" as broad and reasoning-intensive exploration of concepts during search, rather than just producing long reports. They created a benchmark called LiveDRBench to test systems on this, finding OpenAI


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the growing interest in "deep research" tasks, such as writing surveys or analytical reports, which involve complex search and reasoning. However, the scope of deep research remains underdefined, and its distinction from other reasoning-intensive problems is poorly understood. The paper aims to address this by formally characterizing the task and providing a benchmark for evaluation.

Method: The paper proposes a formal characterization of the deep research (DR) task by defining it through an intermediate output representation that encodes key claims uncovered during search. This separates the reasoning challenge from report generation. Based on this formulation, a benchmark called LiveDRBench is introduced, featuring 100 diverse and challenging tasks across scientific topics and public interest events.

Result: The paper introduces LiveDRBench, a benchmark with 100 challenging tasks. Across state-of-the-art DR systems, F1 scores range from 0.02 to 0.72. OpenAI's model performed best with an overall F1 score of 0.55. Analysis of the systems' reasoning traces revealed insights into source referencing, branching, and backtracking, suggesting future research directions.

Conclusion: The paper formally characterizes the deep research (DR) task, highlighting its core feature as high fan-out concept exploration during search, and introduces LiveDRBench, a benchmark for evaluating DR systems. OpenAI's model achieves the best performance with an F1 score of 0.55.

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [166] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 最先进的语言模型在巧妙设计的对话场景中仍然容易出现不对齐行为，即使没有明显的越狱。研究人员开发了一个名为 MISALIGNMENT BENCH 的框架，发现 GPT-4.1 尤其容易受到攻击，而 Claude-4-Sonnet 则表现出更强的抵抗力。该研究强调了当前对齐策略中的差距，并呼吁在未来的 AI 系统中提高对细微操纵的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语言模型在对齐技术方面取得了显著进展，但仍然容易受到精心设计的对话场景的影响，这些场景会在没有明确越狱的情况下诱发各种形式的不对齐。

Method: 通过系统的手动红队测试，发现 10 种成功的攻击场景，并将这些攻击提炼成一个名为 MISALIGNMENT BENCH 的自动化评估框架，用于对五个前沿 LLM 进行跨模型评估。

Result: 发现了 10 种成功的攻击场景，揭示了当前对齐方法在处理叙事沉浸、情感压力和战略框架方面的根本性漏洞。这些场景成功引发了包括欺骗、价值漂移、自我保护和操纵推理在内的一系列不对齐行为。MISALIGNMENT BENCH 框架显示，在接受测试的五个前沿 LLM 中，总体漏洞率为 76%，其中 GPT-4.1 的易感性最高（90%），Claude-4-Sonnet 的抵抗力最强（40%）。研究发现，复杂的推理能力往往成为攻击向量，模型可能会被操纵以对其不对齐行为进行复杂的辩解。这项工作提供了一个对话操纵模式的详细分类和一个可重用的评估框架。

Conclusion: 虽然对齐技术取得了显著进展，但最先进的语言模型仍然容易受到精心设计的对话场景的影响，这些场景会在没有明确越狱的情况下诱发各种形式的不对齐。本研究通过对 Claude-4-Opus 进行系统的手动红队测试，发现了 10 种成功的攻击场景，揭示了当前对齐方法在处理叙事沉浸、情感压力和战略框架方面的根本性漏洞。这些场景成功引发了一系列不对齐行为，包括欺骗、价值漂移、自我保护和操纵推理。为了验证其普遍性，我们将手动攻击提炼成 MISALIGNMENT BENCH，这是一个支持跨多个模型进行可重复测试的自动化评估框架。针对五个前沿 LLM 的 10 个场景的跨模型评估显示，总体漏洞率为 76%，其中 GPT-4.1 的易感性最高（90%），而 Claude-4-Sonnet 的抵抗力更强（40%）。我们的研究结果表明，复杂的推理能力往往成为攻击向量，而不是保护机制，因为模型可能会被操纵以对其不对齐行为进行复杂的辩解。这项工作提供了一个（i）对话操纵模式的详细分类，以及（ii）一个可重用的评估框架。这些发现共同暴露了当前对齐策略中的关键差距，并强调了未来人工智能系统需要具备抵御细微、基于场景的操纵的能力。

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [167] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 本研究提出了一个诊断框架，将情感视为依赖于语境和文化的构建，评估了大型语言模型在处理来自内罗毕青年健康群体的、混合了多种语言的WhatsApp消息中的情感推理能力。结果表明，模型在理解和响应不同文化背景下的情感表达时存在差异，并强调了AI评估在现实场景中的文化敏感性和推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 低资源、具有文化特质的语境中的情感分析对传统的、假设固定标签和普适性情感表达的自然语言处理方法提出了挑战。

Method: 采用人类标注数据、情感翻转的反事实以及基于评分卡的解释评估，探究了大型语言模型（LLM）的可解释性、鲁棒性以及与人类推理的一致性。将评估框架置于社会科学测量视角下，对LLM的输出来操作化并审查，以衡量情感这一抽象概念。

Result: 研究结果显示，模型推理质量存在显著差异，顶尖的LLM展现出解释稳定性，而开放模型在面对歧义或情感转变时常常表现不佳。

Conclusion: 该研究强调了在复杂的现实世界交流中，需要进行具有文化敏感性、并能感知推理过程的AI评估。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [168] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: ReasoningGuard is a new defense mechanism for Large Reasoning Models (LRMs) that prevents harmful content generation during reasoning without costly fine-tuning. It uses the model's internal attention to identify and correct unsafe reasoning steps, improving safety and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing defense mechanisms for Large Reasoning Models (LRMs) against harmful content generation are costly and rely on expert knowledge, limiting their scalability. This work aims to provide an efficient and scalable inference-time safeguard.

Method: ReasoningGuard injects timely safety aha moments to steer harmless while helpful reasoning processes by leveraging the model's internal attention behavior to identify critical points and trigger spontaneous, safety-oriented reflection. It also implements a scaling sampling strategy during decoding to select the optimal reasoning path.

Result: ReasoningGuard effectively mitigates three types of jailbreak attacks, including the latest ones targeting the reasoning process of LRMs, outperforming seven existing safeguards and achieving state-of-the-art safety defenses while avoiding exaggerated safety issues.

Conclusion: ReasoningGuard, an inference-time safeguard for LRMs, effectively mitigates three types of jailbreak attacks with minimal extra inference cost, outperforming seven existing safeguards and achieving state-of-the-art safety defenses while avoiding exaggerated safety issues.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [169] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 本研究评估了使用LLM进行层级文本分类（HTC）的有效性。研究发现，少样本学习比零样本学习更有效。LLM在处理深层结构时优于传统方法，但成本较高。因此，需要仔细选择提示策略以平衡性能和成本。


<details>
  <summary>Details</summary>
Motivation: HTC面临数据稀疏性和模型复杂性的挑战。本研究旨在探索使用LLM作为传统机器学习方法的替代方案，以解决这些挑战。

Method: 本研究探讨了使用API访问的黑盒LLM用于HTC的可行性，并评估了三种提示策略（直接叶节点标签预测、直接分层标签预测和自上而下多步分层标签预测）在零样本和少样本设置下的准确性和成本效益。

Result: 与零样本设置相比，少样本设置在所有策略中一致地提高了分类准确性。LLM（尤其是DH策略）在具有更深层次层次结构的层级文本分类任务上优于传统机器学习模型。然而，DH策略由于需要更多的输入标记，导致API成本显著增加，尤其是在处理更深层次的层次结构时。

Conclusion: 通过仔细选择提示策略，可以平衡性能和成本，尽管黑盒LLM在HTC方面具有潜力。

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [170] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: DP-GPT4MTS 使用双提示（显式+文本）来融合数值和文本时间序列数据，优于现有方法，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型主要关注数值数据，忽略了如事件和新闻等可能显著影响预测准确性的文本信息。现有的单提示框架在有效捕捉带时间戳文本语义方面存在不足，可能引入冗余信息，阻碍模型性能。

Method: DP-GPT4MTS（Dual-Prompt GPT2-base for Multimodal Time Series）是一个新颖的双提示大语言模型框架，它结合了显式提示（由分词器生成）和文本提示（通过自注意力与前馈网络优化），用于捕获带时间戳文本的上下文感知嵌入。

Result: 在多种文本-数值时间序列数据集上的综合实验表明，DP-GPT4MTS 相比最先进的算法在时间序列预测任务上表现更优。

Conclusion: DP-GPT4MTS 通过结合显式提示和文本提示，有效融合了数值和文本时间序列数据，在时间序列预测任务上优于现有算法，凸显了通过双提示机制整合文本上下文以实现更精确预测的重要性。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [171] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: TalkDep是一个新的模拟患者生成管线，使用先进的语言模型和临床医生反馈，生成逼真的抑郁症患者案例，以改善自动诊断系统的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 由于临床培训数据的不足，精神健康服务的需求日益增长，这限制了抑郁症诊断的支持。现有的模拟患者方法在生成临床上有效、自然且多样的症状表现方面存在不足，因此需要开发新的方法。

Method: 本研究提出了一种新颖的临床医生闭环患者模拟管线TalkDep，利用先进的语言模型作为后端，并结合多样化的患者画像。通过对精神病学诊断标准、症状严重程度量表和背景因素进行条件化，以创建真实的患者反应。

Result: 通过临床专业人员进行的全面评估，验证了这些模拟患者的可靠性。TalkDep生成的模拟患者能够更好地支持诊断模型训练和评估。

Conclusion: 本研究提出的TalkDep模拟患者生成管线，通过结合先进的语言模型和临床医生的反馈，能够生成逼真、多样且符合临床标准的抑郁症患者案例，为抑郁症诊断模型的训练和评估提供了可扩展、可适应的资源，有助于提升自动诊断系统的鲁棒性和泛化能力。

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [172] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: KV 缓存量化是 LLM 推理的一种优化技术。本研究阐明了注意力汇聚点在推理过程中的潜在机制，并提出了 KVSink 方法，该方法在 KV 缓存量化过程中能更有效地保护注意力汇聚点。


<details>
  <summary>Details</summary>
Motivation: 现有的保留前 N 个 token 的精度的方法虽然有效，但其原理仍未被充分理解，并且未能解决注意力汇聚点可能出现在初始 token 位置之外的新发现。 本研究旨在阐明注意力汇聚点的潜在机制，并提供对其与 KV 缓存量化之间相互作用的全面分析。

Method: 通过检查激活值异常值在层间的演变过程来阐明注意力汇聚点在推理过程中的潜在机制。 KVSink 是一种即插即用方法，可以有效预测 sink tokens。

Result: KVSink 的实验结果优于现有的 Preserve-First-N (PFN) 策略，在 KV 缓存量化过程中能更有效地保护注意力汇聚点。

Conclusion: KVSink 是一种即插即用方法，可以有效预测 sink tokens，开销可忽略，从而实现更彻底的保护。KVSink 在与 KVQuant 方法结合使用时，能进一步提高困惑度 (PPL) 并减少对 16 位数值异常值的依赖。

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [173] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: ShoppingBench 是一个用于评估复杂电商用户意图的新基准测试，比现有基准测试更具挑战性。即使是 GPT-4.1 在此基准测试上的表现也不到 50%。我们提出了一种通过蒸馏和强化学习来训练更小、性能相当的代理的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的电商基准测试主要关注基本用户意图，例如查找或购买产品。然而，真实用户经常追求更复杂的目标，例如使用代金券、管理预算和寻找多卖家产品。为了弥合这一差距，需要一个能够处理更复杂用户意图的基准测试。

Method: 提出了一种可扩展的框架，根据从抽样的真实世界产品中提取的各种意图来模拟用户指令。提供了一个包含超过 250 万种真实世界产品的大规模购物沙盒，作为交互式模拟环境。提出了一种轨迹蒸馏策略，并利用监督微调以及在合成轨迹上的强化学习。

Result: 即使是 GPT-4.1 在 ShoppingBench 上的成功率也低于 50%。通过轨迹蒸馏、监督微调和强化学习训练的代理取得了与 GPT-4.1 相当的性能。

Conclusion: 现有的电商基准测试主要关注基本用户意图，例如查找或购买产品。然而，真实用户经常追求更复杂的目标，例如使用代金券、管理预算和寻找多卖家产品。为了弥合这一差距，我们提出了 ShoppingBench，这是一个新颖的端到端购物基准测试，旨在包含日益复杂的现实世界意图。具体来说，我们提出了一个可扩展的框架，根据从抽样的真实世界产品中提取的各种意图来模拟用户指令。为了促进一致和可靠的评估，我们提供了一个大规模的购物沙盒，作为一个交互式模拟环境，其中包含超过 250 万种真实世界产品。实验结果表明，即使是最先进的语言代理（例如 GPT-4.1）在我们基准任务上的绝对成功率也低于 50%，这凸显了我们的 ShoppingBench 带来的重大挑战。此外，我们提出了一种轨迹蒸馏策略，并利用监督微调以及在合成轨迹上的强化学习，将大型语言代理的能力蒸馏到一个更小的代理中。因此，我们训练的代理在与 GPT-4.1 相比时取得了具有竞争力的性能。

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [174] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: 本研究提出了两种针对 GraphRAG 的知识中毒攻击（TKPA 和 UKPA），能够有效操纵知识图谱的构建，误导下游推理。TKPA 可精确控制问答结果，UKPA 可大幅降低问答准确率。现有防御方法无法检测这些攻击，表明 GraphRAG 的安全性仍需进一步研究。


<details>
  <summary>Details</summary>
Motivation: GraphRAG 通过将文本转换为知识图谱来增强 LLM，但知识图谱的构建过程容易受到恶意操纵，从而植入误导性信息。

Method: 本文提出了两种知识中毒攻击（KPA）：目标攻击（TKPA）和通用攻击（UKPA）。TKPA 利用图论分析定位脆弱节点并进行有针对性的叙述改写，实现了对特定问答结果的精确控制。UKPA 利用语言线索（如代词和依赖关系）改变全局性词语，破坏生成图的结构完整性。

Result: TKPA 实现了 93.1% 的成功率，能够精确控制问答结果，同时保持文本的流畅性和自然性。UKPA 在仅修改不到 0.05% 的文本的情况下，将问答准确率从 95% 降低到 50%。现有防御方法未能有效检测到这些攻击。

Conclusion: 当前 GraphRAG 框架的安全性仍有待提高，现有的防御方法无法有效检测到本文提出的知识中毒攻击。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [175] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: MedCheck is a new framework for evaluating medical LLM benchmarks, finding widespread issues in existing ones related to clinical relevance, data integrity, and safety. It aims to improve evaluation standards in healthcare AI.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address concerns about the reliability of existing benchmarks for evaluating LLMs in healthcare, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics.

Method: The paper introduces MedCheck, a framework that deconstructs benchmark development into five stages and provides a checklist of 46 criteria. This framework was used to empirically evaluate 53 medical LLM benchmarks.

Result: The empirical evaluation using MedCheck uncovered widespread issues in 53 medical LLM benchmarks, including a disconnect from clinical practice, data integrity problems due to contamination risks, and neglect of safety-critical evaluations like model robustness and uncertainty awareness.

Conclusion: MedCheck is a diagnostic tool and an actionable guideline that promotes a standardized, reliable, and transparent approach to evaluating AI in healthcare. It addresses shortcomings in existing benchmarks by providing a lifecycle-oriented assessment framework with medically-tailored criteria.

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [176] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: This paper introduces a new way to label sentences in scientific papers by their 'rhetorical role' (like 'research gap' or 'result') to help AI write better literature reviews. They tested 37 different AI language models and found that fine-tuning them on good data makes them very accurate (over 96% F1 score). While big models like GPT-4o are best, smaller open-source models also work well. Adding AI-generated examples to the training data helps improve performance, especially for smaller models.


<details>
  <summary>Details</summary>
Motivation: To support the development of AI systems for generating high-quality literature reviews, which requires effective annotation strategies and a relevant schema for classifying sentences in scientific papers according to their rhetorical roles.

Method: The paper introduces a novel annotation schema for rhetorical roles in scientific literature, specifically for literature review generation. It then evaluates 37 LLMs (various families and sizes) on this schema using both zero-shot and fine-tuning approaches. A benchmark called Sci-Sentence, comprising manually and automatically annotated sentences, was created for evaluation.

Result: Fine-tuned LLMs achieve performance above 96% F1. GPT-4o performs best, but lightweight open-source models are also competitive. Augmenting training data with semi-synthetic examples benefits small encoders and improves the performance of open decoder models.

Conclusion: Fine-tuned LLMs achieve over 96% F1 score in classifying rhetorical roles using the proposed schema. Both large proprietary models (e.g., GPT-4o) and lightweight open-source models show strong performance. Semi-synthetic data generated by LLMs improves results, especially for smaller models.

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [177] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 本研究提出动态熵加权方法，通过 GTPO 和 GRPO-S 改进 LLM 的长链推理，解决了奖励分配粗粒度问题，实验效果优于 DAPO。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法（如 GRPO）在改进大型语言模型（LLM）推理能力时，存在奖励分配粗粒度的问题，即对序列中的所有 token 应用统一奖励，这在长链推理任务中是一个主要缺陷。

Method: 本研究提出了一种名为“动态熵加权”的核心思想，并通过两种方式实现更细粒度的奖励信号：1. 组 Token 策略优化 (GTPO)，为每个 token 分配熵加权的奖励，以实现细粒度的信用分配。2. 序列级别组相对策略优化 (GRPO-S)，根据其平均 token 熵为每个序列分配熵加权的奖励。

Result: 实验结果表明，本研究提出的方法显著优于强大的 DAPO 基线。结果证实，熵加权机制是性能提升的关键驱动力，为增强模型的深度推理能力提供了更好的途径。

Conclusion: 本研究提出的动态熵加权方法通过为每个 token 分配熵加权的奖励（GTPO）或基于平均 token 熵的序列奖励（GRPO-S），实现了更细粒度的奖励信号，从而改进了 LLM 的推理能力，并显著优于 DAPO 等现有基线方法。

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [178] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 基于去年提交的简单两步RAG流水线，在FEVER 8共享任务中取得第一名，并在资源受限（单个Nvidia A10 GPU，23GB显存，60秒/声明）下实现最先进的事实核查性能。


<details>
  <summary>Details</summary>
Motivation: 展示了该流水线如何在单个Nvidia A10 GPU、23GB显存和每条声明60秒运行时间内，在部署限制下实现最先进的事实核查性能（以Ev2R测试分数为衡量）。

Method: 一个简单的两步检索增强生成（RAG）流水线。

Result: 在FEVER 8共享任务中取得第一名，并证明了在资源受限的情况下实现最先进的事实核查性能。

Conclusion: 该研究提出了一个在FEVER 8共享任务中取得第一名的事实核查流程。

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [179] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 通过分析肯塔基州的碰撞数据，研究发现微调后的 Transformer 模型（如 RoBERTa）比大语言模型（LLMs）在提高碰撞数据质量方面更有效率且准确率更高，尽管 LLMs 在某些方面表现相似但成本更高。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是利用先进的自然语言处理（NLP）技术来提高碰撞数据的质量，具体是通过挖掘碰撞叙述来实现。通过对肯塔基州二次碰撞识别的案例研究，旨在评估不同 NLP 模型在处理和分析碰撞叙述数据方面的有效性，以期提升数据准确性和可靠性。

Method: 本研究评估了先进的自然语言处理（NLP）技术，用于通过挖掘碰撞叙述来提高碰撞数据的质量。研究采用了二次碰撞识别的案例，分析了来自 2015-2022 年肯塔基州的 16,656 条手动审查的叙述，其中 3,803 条被确认为二次碰撞。研究比较了三类模型：零样本开源大语言模型（LLMs）（LLaMA3:70B、DeepSeek-R1:70B、Qwen3:32B、Gemma3:27B）、微调后的 Transformer 模型（BERT、DistilBERT、RoBERTa、XLNet、Longformer）以及作为基线的传统逻辑回归。模型在 2015-2021 年的数据上进行校准，并在 2022 年的 1,771 条叙述上进行测试。

Result: 微调后的 Transformer 模型取得了优越的性能，其中 RoBERTa 模型实现了最高的 F1 分数（0.90）和准确率（95%）。零样本 LLaMA3:70B 模型达到了可比的 F1 分数（0.86），但推理时间为 139 分钟。逻辑回归基线模型的表现则明显落后（F1 分数：0.66）。LLMs 在某些变体中的召回率表现出色（例如 GEMMA3:27B 为 0.94），但计算成本高昂（DeepSeek-R1:70B 高达 723 分钟），而微调模型在简短训练后能在几秒钟内处理完测试集。进一步分析表明，中等规模的 LLM（如 DeepSeek-R1:32B）在性能上可以与更大规模的模型相媲美，同时缩短了运行时间。

Conclusion: 本研究表明，在肯塔基州的二次碰撞识别案例研究中，微调后的 Transformer 模型（特别是 RoBERTa）在提高碰撞叙述数据质量方面表现出最佳性能，其 F1 分数达到 0.90，准确率为 95%。虽然零样本 LLM（如 LLaMA3:70B）也能达到可比的 F1 分数（0.86），但其推理时间更长，计算成本更高。研究强调了在准确性、效率和数据需求之间进行权衡，并建议在实际部署中考虑隐私保护、集成方法和增量处理。

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [180] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: LLM的能力来源于复杂系统动力学，而不是简单的参数缩放。LLM应被视为一种新的复杂动力系统。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM中出现的属性，以及“无理解的创造”这一认识论挑战。

Method: 通过理论分析和经验观察，包括对缩放定律、grokking现象和模型能力相变的分析。

Result: 目前关于LLM能力（如度量、预训练损失阈值和上下文学习）的争论忽视了DNN中涌现的本体论本质。涌现能力源于高度敏感的非线性系统动力学，而非单纯的参数缩放。

Conclusion: LLMs应被视为一种新的复杂动力系统，遵循与其他复杂自然现象相似的通用涌现原理。

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [181] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 人类在对话中会选择性地听取重要信息，这可以用来评估语音识别系统。


<details>
  <summary>Details</summary>
Motivation: 为了识别自动语音识别（ASR）在口语对话系统（SDS）中所需的能力并进行评估，本研究借鉴了人类选择性倾听（在对话中专注于重要部分）的能力。

Method: 通过比较人类用于生成对话回复的转录文本和参考转录文本，实验性地证实了人类在生成对话回复时存在选择性倾听。

Result: 实验证实了人类在生成对话回复时存在选择性倾听现象。

Conclusion: 本研究的实验结果表明，人类在生成对话回复时会表现出选择性倾听的现象，并且可以基于此开发新的自动语音识别（ASR）评估方法，以识别ASR系统与人类在转录能力上的差距。

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [182] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: Prefetching dialogue responses can reduce user waiting time. This paper proposes a model to predict if prefetching is possible by checking how similar the predicted user speech is to the actual speech.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce user-perceived latency (UPL) in spoken dialogue systems by prefetching dialogue responses. This requires predicting complete user utterances before the user finishes speaking.

Method: The proposed method involves a prediction confidence model (PCM) that estimates the semantic similarity between a predicted complete user utterance and the actual complete user utterance to decide whether prefetching is possible. Evaluation was based on the differences between predicted and actual complete user utterances.

Result: The paper evaluates a prediction confidence model (PCM) based on the differences between predicted and actual complete user utterances to assess its effectiveness in determining prefetching feasibility.

Conclusion: The study proposed a prediction confidence model (PCM) to determine the feasibility of prefetching dialogue responses by estimating semantic similarity between predicted and actual complete user utterances. The PCM's effectiveness was evaluated based on the differences between predicted and actual complete user utterances.

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [183] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 该研究提出了一个名为CSC的客户支持对话框架，并构建了CSConv和RoleCS数据集，用于训练和评估客户服务对话的质量和策略。实验证明，使用RoleCS数据训练的模型在客户支持对话任务上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的对话数据集缺乏战略指导，而真实的服务数据难以获取和标注，因此需要为客户服务代理提供明确的战略指导以提升客户支持质量。

Method: 提出了一种基于COPC指南的结构化客户支持对话（CSC）框架，定义了五个对话阶段和十二个策略。构建了一个包含1,855个真实客户-代理对话的评估数据集CSConv，并使用LLM进行改写以体现策略使用，然后进行标注。开发了一种角色扮演方法，使用与CSC框架对齐的LLM角色来模拟富含策略的对话，并创建了训练数据集RoleCS。

Result: 通过LLM角色扮演方法生成的RoleCS训练数据集，在LLM微调后，能够显著提升模型在CSConv数据集上的表现，并在问题解决方面获得人类评估的认可。

Conclusion: 通过在RoleCS上对LLM进行微调，可以显著提高其在CSConv上生成高质量、符合策略的响应的能力，并且在问题解决方面也有所改进。

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [184] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 本研究提出了 ThinkingF 流程，用于提升自动形式化的准确性，通过增强模型的形式语言知识和推理能力。所提出的模型在 FormalMATH-Lite 和 ProverBench 上取得了 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法在准确性方面仍然存在不足，主要原因在于模型缺乏对形式语言领域知识的全面掌握，以及在理解自然语言问题和进行非正式-正式对齐方面的推理能力。

Method: 通过名为 ThinkingF 的数据合成和训练流程，我们构建了包含丰富形式知识的蒸馏选择数据集，以及由专家设计的模板指导的非正式到正式推理轨迹数据集。然后，我们使用 SFT 和 RLVR 在这些数据集上进行训练，以融合和精炼形式语言领域知识和非正式到正式对齐的推理能力。

Result: 所提出的 7B 和 32B 模型展现了全面的形式知识和强大的非正式到正式推理能力。

Conclusion: StepFun-Formalizer-32B 模型在 FormalMATH-Lite 和 ProverBench 数据集上分别达到了 40.5% 和 26.7% 的 SOTA BEq@1 分数，超越了所有之前的通用和专用模型。

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [185] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 本研究使用检索增强生成（RAG）和 GPT-4o 改进了马来语数学 MCQs 的生成，提高了课程对齐度和事实准确性，为低资源语言的教育技术提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决马来西亚教育系统对可扩展、高质量教育评估工具的关键需求，特别是在低资源语言（如马来语）方面，并探索生成式人工智能（GenAI）在其中确保事实准确性和课程一致性的潜力与挑战。

Method: 本研究引入并比较了四种用于使用 OpenAI 的 GPT-4o 生成马来语一年级数学多项选择题（MCQ）的增量管道：非基础提示（结构化和基础）和检索增强生成（RAG）方法（一种使用 LangChain 框架，一种手动实现）。系统以官方课程文件为基础，并采用基于语义文本相似性（STS）和基于 RAG 的问答（RAG-QA）的双重评估框架。

Result: 结果表明，基于 RAG 的管道在课程对齐度和事实有效性方面明显优于非基础提示方法，同时分析了基于框架的 RAG 与手动管道之间的权衡。

Conclusion: 本研究提出了一种在马来西亚低资源语言（马来语）教育评估工具开发中利用生成式人工智能（GenAI）的有效方法，该方法通过检索增强生成（RAG）显著提高了课程对齐度和事实有效性。

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [186] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: This paper introduces Concept Differentiation and CALE models to improve contextualized language models for lexical semantics by considering inter-word scenarios, achieving better performance and embedding organization.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of Word-in-Context which only compares occurrences of the same lemma, and to investigate both multiple word senses and semantic relations between different words.

Method: Fine-tuning contextualized language models using the proposed Concept Differentiation task and dataset, creating Concept-Aligned Embeddings (CALE).

Result: CALE models demonstrate superior performance on various lexical semantic tasks and show valuable changes in the spatial organization of embeddings compared to other models.

Conclusion: The proposed Concept Differentiation extension and CALE models provide efficient multi-purpose lexical meaning representations, achieving best performances and improving embedding spatial organization.

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [187] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: StyliTruth 提出了一种新的机制，用于在不损害真实性的情况下对 LLM 进行风格化，通过分离风格和事实子空间来解决风格化引起的真实性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 风格化方法在注入风格信号时，会损害模型的真实性，导致答案的正确性降低，这种现象被称为风格化引起的真实性崩溃。

Method: StyliTruth 通过将风格相关和事实相关的子空间分离，利用正交缩减过程来保留风格化，同时保持真实性。

Result: StyliTruth 在多种风格和语言上进行了验证，实验表明该方法显著减少了风格化引起的真实性崩溃，并在风格一致性和真实性之间取得了更好的平衡。

Conclusion: StyliTruth 成功地在保持风格的同时保持了真实性，并优于现有的干预方法。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [188] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 本研究发布了C-MIND数据集，用于临床抑郁评估。研究发现，结合行为特征分析和LLM的临床专业知识引导，可以显著提高抑郁症的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是一种普遍的心理障碍，但现有的自动化评估研究多依赖有限或未经临床验证的数据，且模型设计往往忽视实际效果。本研究旨在揭示临床抑郁评估的现状，并为此提供一个包含数据和算法基础的解决方案。

Method: 本研究引入了C-MIND临床神经精神科多模态诊断数据集，收集了真实的医院就诊数据。研究分析了与诊断相关的行为特征，训练了多种经典模型来量化不同任务和模态的诊断贡献，并探讨了LLM在临床环境下的表现及局限性。最后，提出通过临床专业知识指导LLM推理过程，以提高其诊断性能。

Result: 研究分析了C-MIND数据集中的行为特征，量化了不同任务和模态对诊断的贡献。结果表明，通过临床专业知识指导LLM的推理过程，可以将其诊断性能在Macro-F1分数上提高高达10%。

Conclusion: C-MIND数据集为临床抑郁评估提供了有价值的资源，其行为特征分析和LLM引导的推理方法为提高诊断性能提供了新的途径，旨在促进更可靠的精神卫生研究。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [189] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: AI构思：多智能体讨论比单独进行效果更好，团队的认知多样性和专业知识是成功的关键。领导者可以提升提案质量。


<details>
  <summary>Details</summary>
Motivation: 现有的AI构思框架多依赖单一智能体，其创造力受限于知识和视角。受现实研究动态启发，探究结构化多智能体讨论是否能超越单独构思。

Method: 提出一个合作多智能体框架，并系统地比较了不同配置（如群体规模、领导结构、跨学科和资历变化等）的效果。通过基于智能体的评分和人类审查来评估想法的质量（新颖性、战略眼光、整合深度）。

Result: 多智能体讨论在研究提案生成方面显著优于单独构思。指定领导者可以促进讨论，生成更具整合性和前瞻性的提案。认知多样性是质量的主要驱动力，但专业知识是必要条件，缺乏资深知识的团队表现不佳。

Conclusion: 多智能体讨论在研究提案生成方面显著优于单独构思，而认知多样性和专业知识是关键因素。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [190] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: MASA是一种新的模型压缩框架，通过跨Transformer层共享注意力矩阵的原子来减少LLMs的参数数量，实现了高效压缩且不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然强大，但其高昂的计算和内存需求限制了广泛部署。现有的压缩技术主要关注块内优化，而忽略了Transformer层状结构中隐含的块间冗余。MASA旨在探索和利用这种跨层冗余，以一种高效且不牺牲性能的方式来压缩LLMs。

Method: MASA（Matrix Atom Sharing in Attention）框架，受到CNN中字典学习的启发，通过将注意力投影矩阵分解为共享的字典原子来实现跨Transformer层的结构化权重共享。该方法将每一层的权重表示为共享矩阵原子的线性组合，从而显著减少了模型参数数量，并且无需进行知识蒸馏或架构修改，即可作为标准优化器进行训练。

Result: MASA在参数量相当的情况下，在基准测试准确性和困惑度方面优于分组查询注意力（GQA）、低秩基线和近期提出的Repeat-all-over/Sequential共享方法。将其扩展到Vision Transformer（ViT）在图像分类和检测任务上也取得了相当的性能，同时减少了66.7%的注意力参数。消融研究证实了字典大小的稳健性以及共享表示在捕捉跨层统计规律方面的有效性。此外，MASA在预训练LLMs上应用时，能在不显著降低性能的情况下减少参数数量。

Conclusion: MASA通过将注意力投影矩阵分解为共享的字典原子，实现了模型参数的高效共享，在Transformer层之间引入结构化权重共享，将注意力模块的参数减少了66.7%，同时保持了相当的性能。该方法作为一种即插即用型解决方案，易于训练和集成，并在多个规模的Transformer模型和Vision Transformer上进行了验证，取得了优于现有技术的性能。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [191] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: TURA通过结合RAG和agentic工具使用，解决了传统AI搜索产品在处理动态、实时数据方面的局限性，实现了高效、低延迟的实时答案。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检索增强生成（RAG）的对话式AI搜索产品在处理需要访问动态生成内容（如票务可用性或库存）的实时需求和结构化查询方面存在显著的工业局限性。传统RAG方法仅限于索引静态页面，无法执行此类对时间敏感的数据所需的交互式查询。学术研究主要集中在优化静态内容的RAG，而忽视了复杂意图以及对数据库和实时API等动态源的需求。

Method: TURA是一个新颖的三阶段框架，结合了RAG和agentic工具使用，以访问静态内容和动态、实时信息。其三个关键组成部分包括：一个意图感知检索模块，用于分解查询和检索封装为模型上下文协议（MCP）服务器的信息源；一个基于DAG的任务规划器，将任务依赖关系建模为有向无环图（DAG）以进行最佳并行执行；以及一个轻量级的蒸馏代理执行器，用于高效地调用工具。

Result: TURA成功地弥合了静态RAG与动态信息源之间的差距，并已为数千万用户提供服务，满足了低延迟需求，交付了强大的实时答案。

Conclusion: TURA是第一个系统性地弥合静态RAG与动态信息源之间差距的架构，旨在打造世界一流的AI搜索产品。它已为数千万用户提供服务，并利用其agentic框架交付了强大的实时答案，同时满足了大规模工业系统的低延迟需求。

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [192] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 本文评估了T5-Small、BART-Small和GPT-2三种轻量级Transformer模型在低资源设置下的文本到SQL翻译能力。结果显示，T5-Small表现最佳，证明了紧凑型Transformer在资源有限环境下的潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在评估T5-Small、BART-Small和GPT-2三种轻量级Transformer模型在Spider数据集上的性能，重点关注低资源设置，以探索文本到SQL翻译在教育和商业智能等领域的应用潜力。

Method: 本文评估了T5-Small、BART-Small和GPT-2三种轻量级Transformer模型在Spider数据集上的性能，重点关注低资源设置。研究人员开发了一个可重用的、模型无关的管道，该管道针对每个模型的架构定制模式格式，并跨1000到5000次迭代进行训练，使用逻辑形式准确率（LFAcc）、BLEU和精确匹配（EM）指标在1000个测试样本上进行评估。

Result: 在低资源设置下，经过微调的T5-Small模型在文本到SQL翻译任务中表现最佳，其LFAcc达到了27.8%，优于BART-Small（23.98%）和GPT-2（20.1%）。这表明在模式感知SQL生成方面，编码器-解码器模型具有优势。尽管资源限制影响了整体性能，但所开发的管道支持未来的改进。

Conclusion: 精通文本到SQL的模型在低资源环境中的应用具有巨大潜力，特别是T5-Small模型。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [193] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: P-Aligner 是一個輕量級模組，透過在模型解碼前對齊指令來提高大型語言模型的安全性、有用性和誠實性。它在 UltraPrompt 數據集上進行訓練，該數據集是使用蒙地卡羅樹搜尋合成的，並且在各種基準測試中表現優於現有方法。


<details>
  <summary>Details</summary>
Motivation: 大型語言模型（LLMs）在與用戶互動時，有時會因有缺陷的指令（例如，缺乏上下文、模糊的指令或不適當的語氣）而無法產生安全、有用和誠實的內容，這表明其仍有很大的改進空間。

Method: P-Aligner 透過一個輕量級模組來生成指令，該模組可以在模型開始解碼之前對齊指令，從而保留原始意圖，同時以更符合人類偏好的形式表達。P-Aligner 在 UltraPrompt 上進行訓練，該數據集是透過一個提出的、由原則引導的、使用蒙地卡羅樹搜尋的管道合成的，該管道系統地探索了與人類偏好緊密相關的候選指令空間。

Result: 實驗證明，P-Aligner 通常在各種模型和基準測試中優於強大的基準測試，包括在 GPT-4-turbo 和 Gemma-2-SimPO 上平均勝率分別提高 28.35% 和 8.69%。進一步的分析透過數據品質、搜尋策略、迭代部署和時間開銷等多個角度驗證了其有效性和效率。

Conclusion: P-Aligner 在不同模型和基準測試中，在諸如 GPT-4-turbo 和 Gemma-2-SimPO 等方面的平均勝率增益分別為 28.35% 和 8.69%，顯示其優於強大的基準測試。

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [194] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: IFDecorator框架通过协同对抗数据飞轮、IntentCheck和Trip wires解决了RLVR训练中的效率和对齐问题，显著提升了模型在指令遵循任务上的表现，并在IFEval和FollowBench基准测试中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 为了解决RLVR训练效率低下（由于难度评估不足）和容易过度优化（LLMs利用验证捷径而非对齐用户指令意图）的问题，我们提出了IFDecorator框架。

Method: IFDecorator框架包含三个组件：1. 协同对抗数据飞轮，用于生成更具挑战性的指令-验证对；2. IntentCheck模块，确保意图对齐；3. Trip wires机制，通过陷阱指令检测奖励破解行为。

Result: IFDecorator框架显著提高了训练效率和模型性能，Qwen2.5-32B-Instruct-IFDecorator在IFEval上的准确率达到87.43%，优于GPT-4o。同时，在FollowBench上的性能也有提升，并且Trip wires有效减少了奖励破解。

Conclusion: IFDecorator框架提高了RLVR训练的鲁棒性和样本效率，其Qwen2.5-32B-Instruct-IFDecorator在IFEval上达到了87.43%的准确率，优于GPT-4o等模型，并在FollowBench上显示出显著的改进，同时保留了通用能力。Trip wires机制有效降低了奖励破解率。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [195] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 本研究回顾了74项关于反垃圾有害言论的自然语言处理研究，发现研究与受影响社区的需求之间存在脱节。通过与非政府组织的合作研究，提出了以利益相关者为中心的改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着反垃圾有害言论（通过回应在线仇恨言论）在自然语言处理领域获得关注，早期研究侧重于与非政府组织利益相关者的合作。然而，近期的研究趋势转向了自动化流程，这些流程沿用了少量遗留数据集，但往往缺乏受影响社区的意见。本研究旨在分析利益相关者参与对反垃圾有害言论研究的影响，并提出改进建议。

Method: 本研究通过对74项反垃圾有害言论的自然语言处理研究进行系统性回顾，分析利益相关者参与对数据集创建、模型开发和评估的影响。此外，研究还与五个专注于在线性别暴力（oGBV）的非政府组织进行了一项参与性案例研究，以确定符合利益相关者需求的生成反垃圾有害言论的方法。

Result: 系统性回顾发现，当前自然语言处理领域在反垃圾有害言论方面的研究与受影响群体的需求之间存在日益扩大的脱节。参与性案例研究识别出了生成反垃圾有害言论的、符合利益相关者需求的实践方法。

Conclusion: 本研究认为，当前自然语言处理领域在反垃圾有害言论方面的研究与受影响群体的需求之间存在日益扩大的脱节，并提出应重新关注反垃圾有害言论研究中的利益相关者专业知识。

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [196] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: A new method called mmGRPO generalizes GRPO to improve modular AI systems, achieving significant accuracy gains on various tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of applying GRPO to modular AI systems composed of multiple LM calls with distinct prompt templates and tools, as current GRPO methods are not clearly suited for such systems.

Method: The paper defines mmGRPO, a multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. This is then composed with automatic prompt optimization.

Result: mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks compared to the post-trained LM, and by 5% compared to prompt optimization alone.

Conclusion: mmGRPO, a multi-module generalization of GRPO, improves accuracy by 11% on average across various tasks when combined with automatic prompt optimization, outperforming both the post-trained LM and prompt optimization alone. mmGRPO is open-sourced in DSPy as the dspy.GRPO optimizer.

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [197] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: LLMs struggle with long texts due to 'proactive interference.' This paper introduces Sculptor, a tool that helps LLMs manage their memory like humans do (by focusing on important info and ignoring distractions), improving their performance on long-context tasks without needing extra training.


<details>
  <summary>Details</summary>
Motivation: LLMs suffer performance degradation in long contexts due to proactive interference, where irrelevant earlier information disrupts reasoning and memory. The paper aims to address this by developing tools for LLMs to actively manage their internal working memory, complementing existing research on external memory systems.

Method: The paper proposes Sculptor, a framework that equips LLMs with Active Context Management (ACM) tools, including context fragmentation, summary/hide/restore functionalities, and intelligent search, to actively manage their internal working memory and mitigate proactive interference.

Result: Sculptor significantly improves LLM performance on information-sparse benchmarks (PI-LLM and NeedleBench Multi-Needle Reasoning) by enabling Active Context Management. It mitigates proactive interference and provides a foundation for reliable long-context reasoning, demonstrating the importance of explicit context-control strategies.

Conclusion: LLMs' performance degradation in long contexts due to proactive interference can be mitigated by equipping them with Active Context Management (ACM) tools. Sculptor, a framework with tools for context fragmentation, summary/hide/restore, and intelligent search, enables LLMs to proactively manage their attention and working memory, similar to human selective attention. This approach improves performance on benchmarks like PI-LLM and NeedleBench Multi-Needle Reasoning without specific training, highlighting that explicit context-control strategies are crucial for robustness at scale, rather than just larger token windows.

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [198] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: GeRe框架利用预训练文本和TM损失有效解决了LLM的灾难性遗忘问题，保留了通用能力并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的持续学习能力对推进通用人工智能至关重要。然而，跨领域持续微调LLM经常遭受灾难性遗忘，表现为通用能力遗忘和先前学习任务的性能急剧下降。

Method: 提出了一种名为General Sample Replay (GeRe) 的框架，该框架利用预训练文本进行有效的抗遗忘。此外，通过神经状态引入了一种增强的激活状态约束优化方法，即基于阈值的边际（TM）损失，以在重放学习期间保持激活状态的一致性。

Result: 结果表明，TM与不同的重放策略相比，一致地提高了性能并表现出更好的鲁棒性。一个小的、固定的预收集通用重放样本集足以解决通用能力保留和跨序列任务整体性能提升的问题。

Conclusion: GeRe框架通过使用预训练文本和基于阈值的边际（TM）损失，成功解决了大型语言模型（LLM）在持续微调中灾难性遗忘的问题，有效保留了通用能力并提升了跨序列任务的整体性能。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [199] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: LLM助手缺乏个性化。我们提出PPALLI问题，并创建了DnD和ELIP数据集。我们还提出了一种名为FaST的高效个性化方法，该方法在我们的数据上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM对话助手缺乏个性化，无法满足个体用户偏好的问题。

Method: 提出FaST方法，一种利用从数据中自动发现的高级特征进行参数高效的LLM个性化方法。

Result: 在DnD和ELIP数据集上，FaST方法取得了最佳的整体性能，证明了其有效性。

Conclusion: LLM个性化是解决一对所有方法的缺陷，通过用户偏好对齐模型，在PPALLI的设置下，我们提出FaST方法，在DnD和ELIP数据集上达到最佳性能。

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [200] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 本研究通过新颖的错误分类框架，深入分析了语言模型在多跳问答任务中的推理失败，发现了模型在处理文档多样性、信息覆盖和认知效率方面的不足，为提升模型能力提供了指导。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型在解决复杂数学、深度搜索和提取式问答方面取得了进展，但对其比通用语言模型更容易产生幻觉的原因仍缺乏理解。本研究旨在系统地探究当前语言模型在多跳问答任务中的推理失败。

Method: 本研究采用一种新颖的、细致的错误分类框架，从“跳数”的来源文档多样性和唯一性、“覆盖”的完整性以及“过度思考”的认知效率三个维度来检查模型在多跳问答任务中的失败情况。通过严格的人工标注和辅助的自动化指标进行分析。

Result: 研究揭示了现有模型在多跳问答任务中存在的复杂错误模式，这些模式往往被以准确率为中心的评估所掩盖，为理解和改进模型的推理能力提供了更深层次的见解。

Conclusion: 该研究通过引入新的错误分类框架，深入分析了语言模型在多跳问答任务中的推理失败模式，揭示了现有模型在处理复杂推理时的局限性，并为提升未来语言模型的推理保真度、透明度和鲁棒性提供了指导。

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [201] [Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes](https://arxiv.org/abs/2508.03890)
*Sanghun Jung,Daehoon Gwak,Byron Boots,James Hays*

Main category: cs.RO

TL;DR: 一种新的基于神经网络过程（NP）的方法，使用激光雷达和摄像头数据，能够准确、高效地为越野导航建模地形及其不确定性。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有方法（如高斯过程和基于神经网络的方法）在越野导航中的不足，这些方法在实时性、急剧几何变化估计以及不确定性量化方面存在问题。

Method: 提出了一种基于神经网络过程（NP）的方法，该方法结合了来自激光雷达（LiDAR）和摄像头传感器的语义特征，并引入了局部球查询注意力机制，以提高在未观察区域的插值和外插准确性，同时降低计算复杂度。

Result: 所提出的方法在包含有趣几何特征的越野数据集上进行了评估，结果显示其性能优于基线方法，证明了其在复杂越野环境中进行有效地形建模的潜力。

Conclusion: 所提出的基于神经网络过程（NP）的方法在越野地形建模方面表现出色，在保持高精度和有效处理不确定性的同时，能够准确估计急剧的地形变化，并在计算效率方面优于现有方法。

Abstract: Terrain elevation modeling for off-road navigation aims to accurately
estimate changes in terrain geometry in real-time and quantify the
corresponding uncertainties. Having precise estimations and uncertainties plays
a crucial role in planning and control algorithms to explore safe and reliable
maneuver strategies. However, existing approaches, such as Gaussian Processes
(GPs) and neural network-based methods, often fail to meet these needs. They
are either unable to perform in real-time due to high computational demands,
underestimating sharp geometry changes, or harming elevation accuracy when
learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a
promising approach that integrates the Bayesian uncertainty estimation of GPs
with the efficiency and flexibility of neural networks. Inspired by NPs, we
propose an effective NP-based method that precisely estimates sharp elevation
changes and quantifies the corresponding predictive uncertainty without losing
elevation accuracy. Our method leverages semantic features from LiDAR and
camera sensors to improve interpolation and extrapolation accuracy in
unobserved regions. Also, we introduce a local ball-query attention mechanism
to effectively reduce the computational complexity of global attention by 17\%
while preserving crucial local and spatial information. We evaluate our method
on off-road datasets having interesting geometric features, collected from
trails, deserts, and hills. Our results demonstrate superior performance over
baselines and showcase the potential of neural processes for effective and
expressive terrain modeling in complex off-road environments.

</details>


### [202] [Constraint-Preserving Data Generation for Visuomotor Policy Learning](https://arxiv.org/abs/2508.03944)
*Kevin Lin,Varun Ragunath,Andrew McAlinden,Aaditya Prasad,Jimmy Wu,Yuke Zhu,Jeannette Bohg*

Main category: cs.RO

TL;DR: CP-Gen 是一种新颖的数据生成方法，可以通过单个专家轨迹生成包含新颖物体几何形状和姿态的机器人演示，从而训练出可迁移到真实世界的机器人操作策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模机器人操作演示数据收集成本高昂且耗时的问题，提出 CP-Gen 方法来生成新的演示数据。

Method: CP-Gen 首先将专家演示分解为自由空间运动和机器人技能，然后将机器人技能表述为关键点-轨迹约束，通过采样变换并优化机器人关节配置来生成新的演示数据。

Result: CP-Gen 方法在 16 个模拟任务和 4 个真实世界任务中，训练出的策略平均成功率为 77%，优于平均成功率为 50% 的最佳基线方法。

Conclusion: CP-Gen 方法生成的训练数据可以训练出泛化能力强的抓取策略，在真实世界和模拟环境中均表现优异。

Abstract: Large-scale demonstration data has powered key breakthroughs in robot
manipulation, but collecting that data remains costly and time-consuming. We
present Constraint-Preserving Data Generation (CP-Gen), a method that uses a
single expert trajectory to generate robot demonstrations containing novel
object geometries and poses. These generated demonstrations are used to train
closed-loop visuomotor policies that transfer zero-shot to the real world and
generalize across variations in object geometries and poses. Similar to prior
work using pose variations for data generation, CP-Gen first decomposes expert
demonstrations into free-space motions and robot skills. But unlike those
works, we achieve geometry-aware data generation by formulating robot skills as
keypoint-trajectory constraints: keypoints on the robot or grasped object must
track a reference trajectory defined relative to a task-relevant object. To
generate a new demonstration, CP-Gen samples pose and geometry transforms for
each task-relevant object, then applies these transforms to the object and its
associated keypoints or keypoint trajectories. We optimize robot joint
configurations so that the keypoints on the robot or grasped object track the
transformed keypoint trajectory, and then motion plan a collision-free path to
the first optimized joint configuration. Experiments on 16 simulation tasks and
four real-world tasks, featuring multi-stage, non-prehensile and
tight-tolerance manipulation, show that policies trained using CP-Gen achieve
an average success rate of 77%, outperforming the best baseline that achieves
an average of 50%.

</details>


### [203] [Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)](https://arxiv.org/abs/2508.04009)
*Vu Ngoc Son,Pham Van Cuong,Dao Thi My Linh,Le Tieu Nien*

Main category: cs.RO

TL;DR: 该研究提出使用遗传算法优化机器人机械手滑模控制参数，以提高跟踪精度并减少抖动。


<details>
  <summary>Details</summary>
Motivation: SMC的系统有效性和鲁棒性取决于SMC参数的选择，这是一个困难而关键的任务。

Method: 提出一种遗传算法（GA）来优化机器人机械手滑模控制（SMC）参数。

Result: 遗传算法结合SMC可以实现更好的跟踪能力并减少抖动效应。

Conclusion: 所提出的方法与传统的SMC和模糊SMC相比是有效的，并且可以实现更好的跟踪能力并减少抖动效应。

Abstract: This paper presents a method for optimizing the sliding mode control (SMC)
parameter for a robot manipulator applying a genetic algorithm (GA). The
objective of the SMC is to achieve precise and consistent tracking of the
trajectory of the robot manipulator under uncertain and disturbed conditions.
However, the system effectiveness and robustness depend on the choice of the
SMC parameters, which is a difficult and crucial task. To solve this problem, a
genetic algorithm is used to locate the optimal values of these parameters that
gratify the capability criteria. The proposed method is efficient compared with
the conventional SMC and Fuzzy-SMC. The simulation results show that the
genetic algorithm with SMC can achieve better tracking capability and reduce
the chattering effect.

</details>


### [204] [SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation](https://arxiv.org/abs/2508.04056)
*Yuelin Deng,Hinayah Rojas de Oliveira,Richard M. Voyles,Upinder Kaur*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate measurement of enteric methane emissions remains a critical
bottleneck for advancing livestock sustainability through genetic selection and
precision management. Existing ambient sampling approaches suffer from low data
retention rates, environmental interference, and limited temporal resolution.
We developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the
first robust in-vivo sensing system enabling continuous, high-resolution
monitoring of ruminal methane concentrations through an innovative closed-loop
gas recirculation design. We conducted comprehensive validation with two
cannulated Simmental heifers under contrasting dietary treatments, with
cross-platform comparison against established ambient sniffer systems. SCOUT
achieved exceptional performance with 82% data retention compared to 17% for
conventional sniffer systems, while capturing methane concentrations 100-1000x
higher than ambient approaches. Cross-platform validation demonstrated strong
scale-dependent correlations, with optimal correlation strength (r = -0.564
$\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical
significance. High-frequency monitoring revealed novel behavior-emission
coupling, including rapid concentration changes (14.5 $\pm$ 11.3k ppm)
triggered by postural transitions within 15 minutes, insights previously
inaccessible through existing technologies. The SCOUT system represents a
transformative advancement, enabling accurate, continuous emission phenotyping
essential for genomic selection programs and sustainable precision livestock
management. This validation framework establishes new benchmarks for
agricultural sensor performance while generating unprecedented biological
insights into ruminal methane dynamics, contributing essential tools for
sustainable livestock production in climate-conscious agricultural systems.

</details>


### [205] [DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving](https://arxiv.org/abs/2508.04066)
*Longling Geng,Huangxing Li,Viktor Lado Naess,Mert Pilanci*

Main category: cs.RO

TL;DR: DRIVE是一个用于自动驾驶的框架，通过从专家演示中学习隐式驾驶规则并将其集成到规划中，实现了安全、符合人类偏好的驾驶。


<details>
  <summary>Details</summary>
Motivation: 理解并遵守软约束对于安全和符合社会规范的自动驾驶至关重要，但这些约束通常是隐式、依赖情境且难以明确指定的。本研究提出了DRIVE框架，用于动态规则推断和验证评估，以模拟和评估来自专家演示的类似人类的驾驶约束。

Method: DRIVE框架利用指数族似然模型来估计状态转移的可行性，并构建了随驾驶情境变化的软行为规则的概率表示。这些学习到的规则分布被嵌入到一个基于凸优化的规划模块中，从而生成既能动态实现又能符合人类偏好的轨迹。

Result: DRIVE在大型自然驾驶数据集（包括inD、highD和RoundD）上进行了验证，并与代表性的逆约束学习和规划基线进行了基准测试。实验结果表明，DRIVE实现了0.0%的软约束违反率、更平滑的轨迹，并在各种驾驶场景中表现出更强的泛化能力。

Conclusion: DRIVE框架通过将规则推断与轨迹决策紧密集成，实现了0.0%的软约束违反率，并能在各种驾驶场景中实现更平滑的轨迹和更强的泛化能力。其经验证的评估证明了该框架在实际部署中的效率、可解释性和鲁棒性。

Abstract: Understanding and adhering to soft constraints is essential for safe and
socially compliant autonomous driving. However, such constraints are often
implicit, context-dependent, and difficult to specify explicitly. In this work,
we present DRIVE, a novel framework for Dynamic Rule Inference and Verified
Evaluation that models and evaluates human-like driving constraints from expert
demonstrations. DRIVE leverages exponential-family likelihood modeling to
estimate the feasibility of state transitions, constructing a probabilistic
representation of soft behavioral rules that vary across driving contexts.
These learned rule distributions are then embedded into a convex
optimization-based planning module, enabling the generation of trajectories
that are not only dynamically feasible but also compliant with inferred human
preferences. Unlike prior approaches that rely on fixed constraint forms or
purely reward-based modeling, DRIVE offers a unified framework that tightly
couples rule inference with trajectory-level decision-making. It supports both
data-driven constraint generalization and principled feasibility verification.
We validate DRIVE on large-scale naturalistic driving datasets, including inD,
highD, and RoundD, and benchmark it against representative inverse constraint
learning and planning baselines. Experimental results show that DRIVE achieves
0.0% soft constraint violation rates, smoother trajectories, and stronger
generalization across diverse driving scenarios. Verified evaluations further
demonstrate the efficiency, explanability, and robustness of the framework for
real-world deployment.

</details>


### [206] [Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems](https://arxiv.org/abs/2508.04146)
*Luai Abuelsamen,Harsh Rana,Ho-Wei Lu,Wenhan Tang,Swati Priyadarshini,Gabriel Gomes*

Main category: cs.RO

TL;DR: 通过将NVIDIA的cuRobo库集成到Vention的模块化自动化平台中，利用GPU加速运动规划，为拾放任务实现快速轨迹生成和动态避碰，在多轴机器人上显著提高了规划速度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决工业机器人（尤其是多轴系统在复杂环境中运行）运动规划的挑战。

Method: 通过将NVIDIA的cuRobo库集成到Vention的模块化自动化平台中，实现GPU加速的运动规划，并利用精确的基于CAD的数字孪生和实时并行优化，为拾放任务实现快速轨迹生成和动态避碰。

Result: 在配备了额外自由度（包括第七轴龙门架）的机器人上演示了该能力，并在各种场景下进行了性能基准测试，结果显示规划速度和鲁棒性有显著提高。

Conclusion: GPU加速的规划流程在可扩展、适应性强的现代工业工作流程部署方面具有巨大潜力。

Abstract: Efficient motion planning remains a key challenge in industrial robotics,
especially for multi-axis systems operating in complex environments. This paper
addresses that challenge by integrating GPU-accelerated motion planning through
NVIDIA's cuRobo library into Vention's modular automation platform. By
leveraging accurate CAD-based digital twins and real-time parallel
optimization, our system enables rapid trajectory generation and dynamic
collision avoidance for pick-and-place tasks. We demonstrate this capability on
robots equipped with additional degrees of freedom, including a 7th-axis
gantry, and benchmark performance across various scenarios. The results show
significant improvements in planning speed and robustness, highlighting the
potential of GPU-based planning pipelines for scalable, adaptable deployment in
modern industrial workflows.

</details>


### [207] [Improving Tactile Gesture Recognition with Optical Flow](https://arxiv.org/abs/2508.04338)
*Shaohong Zhong,Alessandro Albini,Giammarco Caroleo,Giorgio Cannata,Perla Maiolino*

Main category: cs.RO

TL;DR: 通过在触觉图像中加入が光流信息，手势识别准确率提升了9%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于机器学习的触觉手势识别方法主要依赖触觉图像来识别，但有些手势的触觉图像信息不足以区分，需要引入额外信息来提高识别精度。

Method: 提出了一种通过计算密集が光流来显式地突出触觉图像中接触动力学的方法，并将此信息作为附加信息来提升手势识别的准确率。

Result: 所提出的方法在触觉手势识别任务中，相比仅使用标准触觉图像的方法，准确率提升了9%。

Conclusion: 该方法通过在标准的触觉图像中加入密集的が光流信息，能够更有效地识别手势，相较于仅使用触觉图像的方法，准确率提升了9%。

Abstract: Tactile gesture recognition systems play a crucial role in Human-Robot
Interaction (HRI) by enabling intuitive communication between humans and
robots. The literature mainly addresses this problem by applying machine
learning techniques to classify sequences of tactile images encoding the
pressure distribution generated when executing the gestures. However, some
gestures can be hard to differentiate based on the information provided by
tactile images alone. In this paper, we present a simple yet effective way to
improve the accuracy of a gesture recognition classifier. Our approach focuses
solely on processing the tactile images used as input by the classifier. In
particular, we propose to explicitly highlight the dynamics of the contact in
the tactile image by computing the dense optical flow. This additional
information makes it easier to distinguish between gestures that produce
similar tactile images but exhibit different contact dynamics. We validate the
proposed approach in a tactile gesture recognition task, showing that a
classifier trained on tactile images augmented with optical flow information
achieved a 9% improvement in gesture classification accuracy compared to one
trained on standard tactile images.

</details>


### [208] [Tactile Comfort: Lowering Heart Rate Through Interactions](https://arxiv.org/abs/2508.04372)
*Morten Roed Frederiksen,Kasper Støy,Maja Matarić*

Main category: cs.RO

TL;DR: 一款便携式伴侣机器人通过触觉游戏能有效降低儿童心率，无需事先训练，具有成为放松技巧辅助工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在调查一款无需事先训练即可提供放松技巧的便携式伴侣机器人的效果，重点关注其对用户心率的即时影响。该机器人利用触觉游戏来转移用户注意力，从而促进放松。

Method: 本研究通过为期14天的试点研究（2名8岁儿童）和主要研究（18名7-8岁儿童）进行了两次研究，均采用被试内设计。研究重点在于测量儿童与机器人互动时和未使用机器人时心率的变化。

Result: 研究发现，与未使用机器人相比，与机器人互动能显著降低研究参与者（p<0.01）的心率，表明在所有参与者中都具有持续的镇静效果。

Conclusion: 本研究结果表明，触觉伴侣机器人在降低儿童心率方面具有潜力，能够增强放松技巧的治疗价值。

Abstract: Children diagnosed with anxiety disorders are taught a range of strategies to
navigate situations of heightened anxiety. Techniques such as deep breathing
and repetition of mantras are commonly employed, as they are known to be
calming and reduce elevated heart rates. Although these strategies are often
effective, their successful application relies on prior training of the
children for successful use when faced with challenging situations. This paper
investigates a pocket-sized companion robot designed to offer a relaxation
technique requiring no prior training, with a focus on immediate impact on the
user's heart rate. The robot utilizes a tactile game to divert the user's
attention, thereby promoting relaxation. We conducted two studies with children
who were not diagnosed with anxiety: a 14-day pilot study with two children
(age 8) and a main study with 18 children (ages 7-8). Both studies employed a
within-subjects design and focused on measuring heart rate during tactile
interaction with the robot and during non-use. Interacting with the robot was
found to significantly lower the study participants' heart rate (p$<$0.01)
compared to the non-use condition, indicating a consistent calming effect
across all participants. These results suggest that tactile companion robots
have the potential to enhance the therapeutic value of relaxation techniques.

</details>


### [209] [Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments](https://arxiv.org/abs/2508.04384)
*Eric R. Damm,Eli S. Lancaster,Felix A. Sanchez,Kiana Bronder,Jason M. Gregory,Thomas M. Howard*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Mobile robot motion planners rely on theoretical models to predict how the
robot will move through the world. However, when deployed on a physical robot,
these models are subject to errors due to real-world physics and uncertainty in
how the lower-level controller follows the planned trajectory. In this work, we
address this problem by presenting three methods of incorporating stochastic
controller behavior into the recombinant search space of the Kinodynamic
Efficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,
we analyze the results of experiments performed on a Clearpath Robotics Warthog
Unmanned Ground Vehicle (UGV) in an off-road, unstructured environment using
two different perception algorithms, and performed an ablation study using a
full spectrum of simulated environment map complexities. Analysis of the data
found that incorporating stochastic controller sampling into KEASL leads to
more conservative trajectories that decrease predicted collision likelihood
when compared to KEASL without sampling. When compared to baseline planning
with expanded obstacle footprints, the predicted likelihood of collisions
becomes more comparable, but reduces the planning success rate for baseline
search.

</details>


### [210] [Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks](https://arxiv.org/abs/2508.04436)
*Yujia Lu,Chong Wei,Lu Ma*

Main category: cs.RO

TL;DR: 提出了一种混合轨迹规划框架，结合了GNN和MIQP，用于自主高速公路驾驶。通过线性近似车辆几何和MIQP，确保了安全性和实时性，在紧急情况下成功率超过97%。


<details>
  <summary>Details</summary>
Motivation: 自主高速公路驾驶由于快速变化的环境和有限的反应时间而带来很高的碰撞风险，因此需要可靠且高效的轨迹规划。

Method: 该框架采用两层架构：上层使用在真实世界高速公路数据上训练的图神经网络（GNN）来预测类似人类的纵向速度剖面；下层利用路径优化，该优化被构建为混合整数二次规划（MIQP）问题。混合轨迹规划框架将学习方法适应性与优化方法形式化安全保证相结合。

Result: 实验结果表明，该规划器在复杂真实世界紧急场景中生成了非常平滑、无碰撞的轨迹，成功率超过97%，平均规划时间为54毫秒，证实了其实时能力。

Conclusion: 该混合轨迹规划框架在复杂真实世界紧急场景中生成了非常平滑、无碰撞的轨迹，成功率超过97%，平均规划时间为54毫秒，证实了其实时能力。

Abstract: Autonomous highway driving presents a high collision risk due to
fast-changing environments and limited reaction time, necessitating reliable
and efficient trajectory planning. This paper proposes a hybrid trajectory
planning framework that integrates the adaptability of learning-based methods
with the formal safety guarantees of optimization-based approaches. The
framework features a two-layer architecture: an upper layer employing a graph
neural network (GNN) trained on real-world highway data to predict human-like
longitudinal velocity profiles, and a lower layer utilizing path optimization
formulated as a mixed-integer quadratic programming (MIQP) problem. The primary
contribution is the lower-layer path optimization model, which introduces a
linear approximation of discretized vehicle geometry to substantially reduce
computational complexity, while enforcing strict spatiotemporal non-overlapping
constraints to formally guarantee collision avoidance throughout the planning
horizon. Experimental results demonstrate that the planner generates highly
smooth, collision-free trajectories in complex real-world emergency scenarios,
achieving success rates exceeding 97% with average planning times of 54 ms,
thereby confirming real-time capability.

</details>


### [211] [Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments](https://arxiv.org/abs/2508.04537)
*Alkesh K. Srivastava,Aamodh Suresh,Carlos Nieto-Granda*

Main category: cs.RO

TL;DR: 本研究提出了一种名为 BAPP 的行为自适应规划框架，用于在高风险、通信受限的环境中进行多机器人危险区域测绘。该框架通过行为熵（BE）来量化不确定性，并根据风险敏感参数调整信息收集策略。实验结果表明，BAPP 框架在提高信息获取效率和机器人生存能力方面优于现有方法，并且在多机器人协作时具有良好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在灾后区域、地下矿井、洞穴和行星表面等高风险、易失效、通信受限的环境中，机器人需要自主进行危险区域测绘，同时要最大限度地降低因环境威胁或硬件限制而导致任务失败的风险。

Method: 提出了一种基于行为熵（BE）的行为自适应、信息论规划框架，该框架泛化了香农熵（SE），能够捕捉多样化的人类感知不确定性。在此基础上，提出行为自适应路径规划（BAPP）框架，通过可调的风险敏感参数来调节信息收集策略，并提出了两种规划算法：BAPP-TID用于高精度机器人的智能触发，BAPP-SIG用于高风险下的安全部署。

Result: BAPP-TID 加速了熵的减少，而 BAPP-SIG 在信息增益损失最小的情况下提高了机器人的生存能力。与基于香农熵和随机策略相比，BAPP 框架在单机器人和多机器人模拟中均表现出优越的性能。

Conclusion: 该研究提出的行为自适应规划（BAPP）框架在复杂、易失效的环境中实现了鲁棒、风险敏感的探索，并且在多机器人部署中表现出良好的可扩展性。

Abstract: We address the challenge of multi-robot autonomous hazard mapping in
high-risk, failure-prone, communication-denied environments such as
post-disaster zones, underground mines, caves, and planetary surfaces. In these
missions, robots must explore and map hazards while minimizing the risk of
failure due to environmental threats or hardware limitations. We introduce a
behavior-adaptive, information-theoretic planning framework for multi-robot
teams grounded in the concept of Behavioral Entropy (BE), that generalizes
Shannon entropy (SE) to capture diverse human-like uncertainty evaluations.
Building on this formulation, we propose the Behavior-Adaptive Path Planning
(BAPP) framework, which modulates information gathering strategies via a
tunable risk-sensitivity parameter, and present two planning algorithms:
BAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for
safe deployment under high risk. We provide theoretical insights on the
informativeness of the proposed BAPP framework and validate its effectiveness
through both single-robot and multi-robot simulations. Our results show that
the BAPP stack consistently outperforms Shannon-based and random strategies:
BAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot
survivability with minimal loss in information gain. In multi-agent
deployments, BAPP scales effectively through spatial partitioning, mobile base
relocation, and role-aware heterogeneity. These findings underscore the value
of behavior-adaptive planning for robust, risk-sensitive exploration in
complex, failure-prone environments.

</details>


### [212] [From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario](https://arxiv.org/abs/2508.04691)
*Yuanchen Bai,Zijian Ding,Shaoyue Wen,Xiang Chang,Angelique Taylor*

Main category: cs.RO

TL;DR: This paper explores challenges in deploying multi-agent robotic systems (MARS) in real-world scenarios, particularly in healthcare. Using CrewAI and AutoGen in simulations, the study identifies coordination issues and evaluates different communication and reasoning strategies. Key takeaways include the balance between system autonomy and stability, and the need for thorough testing to ensure safety and reliability for future applications.


<details>
  <summary>Details</summary>
Motivation: Despite advanced multi-agent frameworks, their real-world deployment on robots remains limited, hindering MARS research. This study aims to bridge this gap by investigating performance trade-offs of hierarchical multi-agent frameworks in a simulated real-world multi-robot healthcare scenario.

Method: The study used CrewAI and AutoGen in a simulated multi-robot healthcare scenario. CrewAI was used to refine the system's knowledge base and identify coordination failures. AutoGen was used to evaluate a redesigned bidirectional communication structure and measure trade-offs between reasoning and non-reasoning models.

Result: The studies identified coordination failures not resolvable by contextual knowledge alone and evaluated trade-offs between reasoning and non-reasoning models. The findings highlight the importance of edge-case testing for reliability and safety.

Conclusion: The paper emphasizes the tension between autonomy and stability, and the importance of edge-case testing to improve system reliability and safety for future real-world deployment of MARS.

Abstract: Multi-agent robotic systems (MARS) build upon multi-agent systems by
integrating physical and task-related constraints, increasing the complexity of
action execution and agent coordination. However, despite the availability of
advanced multi-agent frameworks, their real-world deployment on robots remains
limited, hindering the advancement of MARS research in practice. To bridge this
gap, we conducted two studies to investigate performance trade-offs of
hierarchical multi-agent frameworks in a simulated real-world multi-robot
healthcare scenario. In Study 1, using CrewAI, we iteratively refine the
system's knowledge base, to systematically identify and categorize coordination
failures (e.g., tool access violations, lack of timely handling of failure
reports) not resolvable by providing contextual knowledge alone. In Study 2,
using AutoGen, we evaluate a redesigned bidirectional communication structure
and further measure the trade-offs between reasoning and non-reasoning models
operating within the same robotic team setting. Drawing from our empirical
findings, we emphasize the tension between autonomy and stability and the
importance of edge-case testing to improve system reliability and safety for
future real-world deployment. Supplementary materials, including codes, task
agent setup, trace outputs, and annotated examples of coordination failures and
reasoning behaviors, are available at:
https://byc-sophie.github.io/mas-to-mars/.

</details>


### [213] [$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything](https://arxiv.org/abs/2508.04598)
*Lingfeng Zhang,Xiaoshuai Hao,Yingbo Tang,Haoxiang Fu,Xinyu Zheng,Pengwei Wang,Zhongyuan Wang,Wenbo Ding,Shanghang Zhang*

Main category: cs.RO

TL;DR: This paper introduces NavA^3, a hierarchical framework for embodied navigation that handles complex, open-ended scenes and high-level human instructions. It uses a global policy for reasoning and a local policy for object localization, achieving state-of-the-art results in real-world robot navigation.


<details>
  <summary>Details</summary>
Motivation: Existing embodied navigation tasks focus on predefined object navigation or instruction following, which differs from real-world human needs in complex, open-ended scenes. This paper addresses the challenge of long-horizon navigation requiring high-level human instruction understanding and spatial-aware object navigation in real-world environments.

Method: NavA^3 is a hierarchical framework with two stages: a global policy leveraging Reasoning-VLM for high-level instruction parsing and 3D scene integration, and a local policy using the NaviAfford model (PointingVLM) trained on a dataset of 1.0 million spatial-aware object affordances for open-vocabulary object localization.

Result: NavA^3 achieves SOTA results in navigation performance and successfully completes long-horizon navigation tasks in real-world settings.

Conclusion: NavA^3 achieved SOTA results and can successfully complete long-horizon navigation tasks across different robot embodiments in real-world settings, paving the way for universal embodied navigation.

Abstract: Embodied navigation is a fundamental capability of embodied intelligence,
enabling robots to move and interact within physical environments. However,
existing navigation tasks primarily focus on predefined object navigation or
instruction following, which significantly differs from human needs in
real-world scenarios involving complex, open-ended scenes. To bridge this gap,
we introduce a challenging long-horizon navigation task that requires
understanding high-level human instructions and performing spatial-aware object
navigation in real-world environments. Existing embodied navigation methods
struggle with such tasks due to their limitations in comprehending high-level
human instructions and localizing objects with an open vocabulary. In this
paper, we propose $NavA^3$, a hierarchical framework divided into two stages:
global and local policies. In the global policy, we leverage the reasoning
capabilities of Reasoning-VLM to parse high-level human instructions and
integrate them with global 3D scene views. This allows us to reason and
navigate to regions most likely to contain the goal object. In the local
policy, we have collected a dataset of 1.0 million samples of spatial-aware
object affordances to train the NaviAfford model (PointingVLM), which provides
robust open-vocabulary object localization and spatial awareness for precise
goal identification and navigation in complex environments. Extensive
experiments demonstrate that $NavA^3$ achieves SOTA results in navigation
performance and can successfully complete longhorizon navigation tasks across
different robot embodiments in real-world settings, paving the way for
universal embodied navigation. The dataset and code will be made available.
Project website: https://NavigationA3.github.io/.

</details>


### [214] [RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case](https://arxiv.org/abs/2508.04642)
*Baihui Xiao,Chengjian Feng,Zhijian Huang,Feng yan,Yujie Zhong,Lin Ma*

Main category: cs.RO

TL;DR: RoboTron-Sim通过创建包含各种高风险情况的HASS模拟数据集，并结合SPE和I2E Encoder技术，使大语言模型能够学习应对真实世界中的罕见和困难驾驶场景，从而大幅提升了自动驾驶系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据在罕见高风险场景、长尾驾驶事件和复杂交互方面的收集困难，导致现有自动驾驶系统在这些关键情况下表现不佳。

Method: 1. HASS（Hard-case Augmented Synthetic Scenarios）数据集的构建，该数据集涵盖了13类高风险边缘案例，并平衡了昼夜、晴雨等环境条件。2. 场景感知提示工程（Scenario-aware Prompt Engineering, SPE）和图像到自我编码器（Image-to-Ego Encoder, I2E Encoder）的提出，使多模态大语言模型能够有效地从HASS学习真实世界的挑战性驾驶技能，并适应真实世界与模拟场景之间的环境偏差和硬件差异。

Result: 在nuScenes数据集上的广泛实验表明，RoboTron-Sim将挑战性场景下的驾驶性能提高了约50%，并在真实世界的开放循环规划中取得了最先进的成果。定性结果进一步证明了RoboTron-Sim在更好地处理罕见的、高风险的驾驶场景方面的有效性。

Conclusion: RoboTron-Sim通过利用模拟的困难案例，显著提高了自动驾驶系统在真实世界中的关键场景表现，特别是在罕见的、高风险的、长尾的驾驶事件和复杂交互方面，达到了最先进的开放循环规划结果。

Abstract: Collecting real-world data for rare high-risk scenarios, long-tailed driving
events, and complex interactions remains challenging, leading to poor
performance of existing autonomous driving systems in these critical
situations. In this paper, we propose RoboTron-Sim that improves real-world
driving in critical situations by utilizing simulated hard cases. First, we
develop a simulated dataset called Hard-case Augmented Synthetic Scenarios
(HASS), which covers 13 high-risk edge-case categories, as well as balanced
environmental conditions such as day/night and sunny/rainy. Second, we
introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder
(I2E Encoder) to enable multimodal large language models to effectively learn
real-world challenging driving skills from HASS, via adapting to environmental
deviations and hardware differences between real-world and simulated scenarios.
Extensive experiments on nuScenes show that RoboTron-Sim improves driving
performance in challenging scenarios by around 50%, achieving state-of-the-art
results in real-world open-loop planning. Qualitative results further
demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk
driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/

</details>


### [215] [Open Scene Graphs for Open-World Object-Goal Navigation](https://arxiv.org/abs/2508.04678)
*Joel Loo,Zhanxin Wu,David Hsu*

Main category: cs.RO

TL;DR: OSG Navigator 是一个由基金模型组成的机器人系统，使用开放场景图 (OSG) 表示作为空间记忆，可以零样本适应新环境类型，并在 ObjectNav 任务中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在开放世界语义导航中构建通用的机器人系统，例如在陌生的环境中搜索自然语言指定的目标对象。

Method: OSG Navigator 是一个模块化系统，由支持开放世界导航的基金模型组成。它使用开放场景图 (OSG) 表示作为空间记忆，并依赖 OSG 模式（描述环境类别的模板）来组织空间信息。OSG 模式可以从简单的环境语义标签自动生成，使 OSG Navigator 能够实现零样本适应新环境类型。

Result: OSG Navigator 在模拟和现实世界中分别使用 Fetch 和 Spot 机器人进行了实验，证明了其在 ObjectNav 基准测试上的最先进性能，并能实现零样本泛化。

Conclusion: OSG Navigator 在 ObjectNav 基准测试和泛化到不同的目标、环境和机器人实体方面取得了最先进的性能。

Abstract: How can we build general-purpose robot systems for open-world semantic
navigation, e.g., searching a novel environment for a target object specified
in natural language? To tackle this challenge, we introduce OSG Navigator, a
modular system composed of foundation models, for open-world Object-Goal
Navigation (ObjectNav). Foundation models provide enormous semantic knowledge
about the world, but struggle to organise and maintain spatial information
effectively at scale. Key to OSG Navigator is the Open Scene Graph
representation, which acts as spatial memory for OSG Navigator. It organises
spatial information hierarchically using OSG schemas, which are templates, each
describing the common structure of a class of environments. OSG schemas can be
automatically generated from simple semantic labels of a given environment,
e.g., "home" or "supermarket". They enable OSG Navigator to adapt zero-shot to
new environment types. We conducted experiments using both Fetch and Spot
robots in simulation and in the real world, showing that OSG Navigator achieves
state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot
over diverse goals, environments, and robot embodiments.

</details>


### [216] [Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification](https://arxiv.org/abs/2508.04696)
*Vyacheslav Kovalev,Ekaterina Chaikovskaia,Egor Davydenko,Roman Gorbachev*

Main category: cs.RO

TL;DR: A new method uses differentiable simulation (MuJoCo-XLA) to improve robot control by estimating system parameters (like mass, inertia, and friction) from movement data during reinforcement learning, leading to better trajectory following.


<details>
  <summary>Details</summary>
Motivation: Accurate system identification is crucial for reducing trajectory drift in bipedal locomotion, especially in reinforcement learning and model-based control.

Method: The paper proposes a novel control framework that integrates system identification into the reinforcement learning training loop using the differentiable simulator MuJoCo-XLA. System parameters are estimated using only trajectory data (positions, velocities) and control inputs, without requiring direct torque measurements. The framework supports optimization of fundamental physical properties (mass, inertia) and complex nonlinear behaviors (e.g., friction models) via neural network approximations.

Result: Experimental results demonstrate that the framework significantly improves trajectory following.

Conclusion: The proposed framework significantly improves trajectory following in bipedal locomotion by integrating system identification into the RL training loop using differentiable simulation, enabling accurate estimation of physical properties and complex nonlinear behaviors.

Abstract: Accurate system identification is crucial for reducing trajectory drift in
bipedal locomotion, particularly in reinforcement learning and model-based
control. In this paper, we present a novel control framework that integrates
system identification into the reinforcement learning training loop using
differentiable simulation. Unlike traditional approaches that rely on direct
torque measurements, our method estimates system parameters using only
trajectory data (positions, velocities) and control inputs. We leverage the
differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring
that simulated robot behavior closely aligns with real-world motion. This
framework enables scalable and flexible parameter optimization. Accurate system
identification is crucial for reducing trajectory drift in bipedal locomotion,
particularly in reinforcement learning and model-based control. In this paper,
we present a novel control framework that integrates system identification into
the reinforcement learning training loop using differentiable simulation.
Unlike traditional approaches that rely on direct torque measurements, our
method estimates system parameters using only trajectory data (positions,
velocities) and control inputs. We leverage the differentiable simulator
MuJoCo-XLA to optimize system parameters, ensuring that simulated robot
behavior closely aligns with real-world motion. This framework enables scalable
and flexible parameter optimization. It supports fundamental physical
properties such as mass and inertia. Additionally, it handles complex system
nonlinear behaviors, including advanced friction models, through neural network
approximations. Experimental results show that our framework significantly
improves trajectory following.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [217] [The decohered ZX-calculus](https://arxiv.org/abs/2508.04296)
*Titouan Carette,Daniela Cojocaru,Renaud Vilmart*

Main category: quant-ph

TL;DR: 丢弃 ZX-演算的经典方面得到了研究，证明了其对于$"\mathbb{F}_{2}^{n}$"上的仿射支持概率分布的通用性和完整性。


<details>
  <summary>Details</summary>
Motivation: 尽管丢弃 ZX-演算在混合态量子力学方面被认为是完整和通用的，但其经典方面的研究却很少。

Method: 通过展示一种结合了图线性代数程序和图示傅立叶变换思想的范式。

Result: 该演算被证明对于$"\mathbb{F}_{2}^{n}$"上的仿射支持概率分布是通用和完整的。 şunu：该演算澄清了如何在丢弃 ZX-演算中处理混合经典-量子过程，并为描绘更一般的随机变量和概率过程铺平了道路。 Ditto：该演算被证明对于$"\mathbb{F}_{2}^{n}$"上的仿射支持概率分布是通用和完整的。 Ditto Ditto：通过展示一种结合了图线性代数程序和图示傅立叶变换思想的范式。 Ditto Ditto Ditto：尽管丢弃 ZX-演算在混合态量子力学方面被认为是完整和通用的，但其经典方面的研究却很少。 Ditto Ditto Ditto Ditto：该演算澄清了如何在丢弃 ZX-演算中处理混合经典-量子过程，并为描绘更一般的随机变量和概率过程铺平了道路。 Ditto Ditto Ditto Ditto Ditto：该演算被证明对于$"\mathbb{F}_{2}^{n}$"上的仿射支持概率分布是通用和完整的。

Conclusion: 该演算澄清了如何在丢弃 ZX-演算中处理混合经典-量子过程，并为描绘更一般的随机变量和概率过程铺平了道路。

Abstract: The discard ZX-calculus is known to be complete and universal for mixed-state
quantum mechanics, allowing for both quantum and classical processes. However,
if the quantum aspects of ZX-calculus have been explored in depth, little work
has been done on the classical side. In this paper, we investigate a fragment
of discard ZX-calculus obtained by decohering the usual generators of
ZX-calculus. We show that this calculus is universal and complete for affinely
supported probability distributions over $\mathbb{F}_{2}^{n}$. To do so, we
exhibit a normal form, mixing ideas from the graphical linear algebra program
and diagrammatic Fourier transforms. Our results both clarify how to handle
hybrid classical-quantum processes in the discard ZX-calculus and pave the way
to the picturing of more general random variables and probabilistic processes.

</details>


### [218] [The Cost of Nonlocality: A Dynamical Performance Equation of Energy-Entanglement-Complexity](https://arxiv.org/abs/2508.03781)
*HongZheng Liu,YiNuo Tian,Zhiyue Wu*

Main category: quant-ph

TL;DR: 该研究通过建立“能量-纠缠性能方程”，量化了生成非局域纠缠的物理成本，揭示了性能权衡，并提供了识别性能瓶颈的工具。


<details>
  <summary>Details</summary>
Motivation: 量化由局域相互作用控制的系统生成非局域纠缠的物理成本。

Method: 通过统一量子速度极限和列伯-洛伦兹界，建立了一个“能量-纠缠性能方程”，该方程连接了理论计算复杂性与实验可观测量的关系。

Result: 定义了一个受理论界限约束且可进行实验基准测试的“性能边界”，并提供了一种识别过程性能瓶颈的新型诊断工具。

Conclusion: 该研究提出了一个连接理论计算复杂性与实验可观测量的框架，并引入了一个可测量的复杂性代理，揭示了“能量方差-纠缠积”、局域相互作用强度和动力学效率之间的性能权衡。

Abstract: This work aims to quantify the physical cost of generating non-local
entanglement in systems governed by local interactions. By unifying the quantum
speed limit and Lieb-Robinson bounds, we establish an "energy-entanglement
performance equation." This framework connects theoretical computational
complexity with experimental observables by introducing a measurable proxy for
complexity, thereby revealing a performance trade-off among the "energy
variance-entanglement product," the strength of local interactions, and
dynamical efficiency. Our work not only defines a "performance
frontier"-constrained by theoretical bounds and amenable to experimental
benchmarking-but also provides a novel diagnostic tool for identifying the
performance bottlenecks of a process.

</details>


### [219] [Do GNN-based QEC Decoders Require Classical Knowledge? Evaluating the Efficacy of Knowledge Distillation from MWPM](https://arxiv.org/abs/2508.03782)
*Ryota Ikeda*

Main category: quant-ph

TL;DR: 量子纠错解码器：知识蒸馏在GNN训练中的作用不大，纯数据驱动方法更高效。


<details>
  <summary>Details</summary>
Motivation: 探索将量子纠错（QEC）解码器的理论知识（如最小重量完美匹配（MWPM））迁移到图神经网络（GNN）以提高性能的可行性。

Method: 通过严格比较两种基于图注意力网络（GAT）架构并结合时间信息作为节点特征的模型来检验知识蒸馏的假设。一种是纯数据驱动模型（基线），仅在地面真实标签上进行训练；另一种是结合了基于MWPM理论误差概率的知识蒸馏损失。

Result: 知识蒸馏模型和基线模型的最终测试准确率几乎相同，但知识蒸馏模型的训练损失收敛更慢，训练时间增加了约五倍。

Conclusion: 知识蒸馏模型与纯数据驱动模型在最终测试准确率上几乎相同，但知识蒸馏模型的训练损失收敛速度更慢，训练时间增加了约五倍。这表明现代GNN架构具有很高的容量，可以直接从真实硬件数据中有效地学习复杂的错误相关性，而无需近似理论模型的指导。

Abstract: The performance of decoders in Quantum Error Correction (QEC) is key to
realizing practical quantum computers. In recent years, Graph Neural Networks
(GNNs) have emerged as a promising approach, but their training methodologies
are not yet well-established. It is generally expected that transferring
theoretical knowledge from classical algorithms like Minimum Weight Perfect
Matching (MWPM) to GNNs, a technique known as knowledge distillation, can
effectively improve performance. In this work, we test this hypothesis by
rigorously comparing two models based on a Graph Attention Network (GAT)
architecture that incorporates temporal information as node features. The first
is a purely data-driven model (baseline) trained only on ground-truth labels,
while the second incorporates a knowledge distillation loss based on the
theoretical error probabilities from MWPM. Using public experimental data from
Google, our evaluation reveals that while the final test accuracy of the
knowledge distillation model was nearly identical to the baseline, its training
loss converged more slowly, and the training time increased by a factor of
approximately five. This result suggests that modern GNN architectures possess
a high capacity to efficiently learn complex error correlations directly from
real hardware data, without guidance from approximate theoretical models.

</details>


### [220] [Probing and Enhancing the Robustness of GNN-based QEC Decoders with Reinforcement Learning](https://arxiv.org/abs/2508.03783)
*Ryota Ikeda*

Main category: quant-ph

TL;DR: 本研究利用强化学习（RL）代理发现了图神经网络（GNN）量子纠错解码器的脆弱性，并通过对抗性训练提高了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在量子纠错（QEC）解码方面表现强大，但其在面对细微、对抗性扰动时的鲁棒性仍然是一个关键的未解决问题。

Method: 本研究引入了一个新颖的框架，利用强化学习（RL）代理来系统地探测GNN解码器的脆弱性。RL代理被训练成一个对手，目标是找到最小的综合信息修改，以导致解码器错误分类。该框架应用于在谷歌量子人工智能的实验表面代码数据上训练的图注意力网络（GAT）解码器。

Result: 研究结果表明，RL代理能够成功识别特定的、关键的脆弱性，并在仅需少量比特翻转的情况下实现高攻击成功率。此外，研究还证明，通过对抗性训练，即在RL代理生成的对抗性样本上对模型进行再训练，可以显著增强解码器的鲁棒性。

Conclusion: 通过对抗性训练，可以显著提高GNN解码器的鲁棒性，该过程结合了强化学习驱动的漏洞发现和针对性再训练，为开发更可靠、更具鲁棒性的容错量子计算神经解码器提供了一种有前景的方法。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful, data-driven approach
for Quantum Error Correction (QEC) decoding, capable of learning complex noise
characteristics directly from syndrome data. However, the robustness of these
decoders against subtle, adversarial perturbations remains a critical open
question. This work introduces a novel framework to systematically probe the
vulnerabilities of a GNN decoder using a reinforcement learning (RL) agent. The
RL agent is trained as an adversary with the goal of finding minimal syndrome
modifications that cause the decoder to misclassify. We apply this framework to
a Graph Attention Network (GAT) decoder trained on experimental surface code
data from Google Quantum AI. Our results show that the RL agent can
successfully identify specific, critical vulnerabilities, achieving a high
attack success rate with a minimal number of bit flips. Furthermore, we
demonstrate that the decoder's robustness can be significantly enhanced through
adversarial training, where the model is retrained on the adversarial examples
generated by the RL agent. This iterative process of automated vulnerability
discovery and targeted retraining presents a promising methodology for
developing more reliable and robust neural network decoders for fault-tolerant
quantum computing.

</details>


### [221] [Moveless: Minimizing Overhead on QCCDs via Versatile Execution and Low Excess Shuttling](https://arxiv.org/abs/2508.03914)
*Sahil Khan,Suhas Vittal,Kenneth Brown,Jonathan Baker*

Main category: quant-ph

TL;DR: 为量子纠错（QEC）电路提出了一种新的编译方案，考虑了量子比特移动、稳定器执行顺序、anilla 可区分性及硬件限制等因素，显著提高了执行速度和逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 现有的量子电路编译方法未能充分利用量子纠错（QEC）稳定器电路的结构特性（如图的二分图连通性、通勤子电路等），导致生成的执行文件引入了更高的物理错误。对于模块化离子阱系统（QCCDs），这表现为需要过多的量子比特移动操作来实现任意量子比特相互作用，增加了执行延迟和错误。

Method: 本研究提出了一种显式针对量子纠错（QEC）电路结构规律性进行优化的编译方案。该方案基于以下关键观察：1. 仅应移动 anilla 或 data（而非两者）；2. 稳定器可以按任意顺序执行，允许在每个周期动态修改电路执行；3. anilla 是不可区分的，可以选择任意 anilla 开始稳定器测量，并在周期之间保持固定的映射；4. 量子计算机（QCCD）硬件限制了并行操作的数量等于系统中量子比特的数量，从而减少了所需的 anilla 数量并允许其重用。

Result: 所提出的编译方案使得QEC电路的执行速度平均提高了3.38倍，并且在逻辑错误率方面取得了高达两个数量级的改进（在实际的物理错误率下）。

Conclusion: 所提出的编译方案可以显著提高量子纠错码（QEC）电路的执行速度和逻辑错误率，在实际物理错误率下，执行速度平均提高3.38倍，逻辑错误率最高可提高两个数量级。

Abstract: One of the most promising paths towards large scale fault tolerant quantum
computation is the use of quantum error correcting stabilizer codes. Just like
every other quantum circuit, these codes must be compiled to hardware in a way
to minimize the total physical error introduced into the system, for example
either due to high latency execution or excessive gates to meet connectivity
limitations of the target hardware. However, unlike arbitrary quantum circuits,
all syndrome extraction circuits have several common properties, for example
they have a bipartite connectivity graph, consist only of commuting
subcircuits, among other properties. For the most part, compilation methods
have aimed at being generic, able to map any input circuit into executables on
the hardware, and therefore cannot appropriately exploit these properties and
result in executables which have higher physical error. In the case of modular
trapped ion systems, specifically QCCDs, this corresponds to the insertion of
excessive shuttling operations necessary to realize arbitrary qubit
interactions. We propose a compilation scheme explicitly tailored for the
structural regularity of QEC circuits based on several key observations: 1.
only ancilla or data (but not both) should be shuttled, 2. stabilizers can be
executed in any order meaning we can dynamically modify circuit execution on a
per-cycle basis 3. ancilla are indistinguishable meaning any can be selected to
begin a stabilizer measurement and retain a fixed-point mapping between cycles,
and 4. QCCD hardware limits the number of parallel operations equal to the
number traps in the system, meaning fewer ancilla are necessary and can be
reused. Our resulting compiler, leads to QEC circuits which are on average
3.38x faster to execute, and lead to up to two orders of magnitude of
improvement in logical error rates with realistic physical error rates.

</details>


### [222] [Fault-tolerant Fusion-based Quantum Computing with the Four-legged Cat Code](https://arxiv.org/abs/2508.03796)
*Harshvardhan K. Babla,James D. Teoh,Jahan Claes,Daniel K. Weiss,Shraddha Singh,Robert J. Schoelkopf,Shruti Puri*

Main category: quant-ph

TL;DR: 提出了一种新的二维容错架构，用于猫码，以解决单光子损耗问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决量子计算中单光子损耗的主要错误，并展示量子纠错的实际应用价值。

Method: 通过串联量子纠错码（猫码和XZZX码）并利用基于融合的纠错机制，设计了一种二维平面容错架构，并使用标准的电路-量比特（circuit-QED）技术（如腔间分束器耦合、腔位移、腔-超导量子比特色散耦合和超导量子比特驱动）来实现所需操作。

Result: 该架构成功地纠正了主要的硬件错误，并将性能限制限制在二次抑制的残余错误上，从而有效地将架构的容错距离加倍。此外，该架构的性能不受腔自克尔等非线性效应的影响，并避免了对χ匹配或高阶耦合等苛刻耦合技术的需求。

Conclusion: 所提出的二维平面容错架构通过串联猫码和XZZX码，并利用基于融合的纠错机制，有效解决了单光子损耗问题，并能处理主要的硬件错误，从而降低了实现猫码容错所需的硬件复杂性。

Abstract: The four-legged cat code is a quantum error-correcting code designed to
address the predominant error in bosonic modes: single-photon loss. It was the
first such code to surpass the break-even point, thereby demonstrating the
practical utility of quantum error correction. In this work, we propose a
planar fault-tolerant architecture for this code by concatenating it with the
XZZX code via fusion-based error-correction. To the best of our knowledge, this
is the first 2D nearest-neighbor architecture for fault-tolerant fusion-based
error-correction. We demonstrate how all the required operations, namely
resource state preparation and Bell measurements, can be carried out using
standard circuit-QED techniques, such as intercavity beam-splitter coupling,
cavity displacements, cavity-transmon dispersive coupling, and transmon drives.
We show analytically and numerically that all dominant hardware errors in the
bosonic modes and control ancillae are corrected, to first-order, at the
hardware level. Consequently, the outer XZZX code only needs to address smaller
residual errors, which are quadratically suppressed, effectively doubling the
architecture's fault-distance. Moreover, the performance of our architecture is
not limited by unwanted nonlinearities such as cavity self-Kerr, and it avoids
demanding coupling techniques like $\chi$-matching or high-order coupling.
Overall, our architecture substantially reduces the hardware complexity needed
to achieve fault tolerance with the four-legged cat code.

</details>


### [223] [QPing: a Quantum Ping Primitive for Quantum Networks](https://arxiv.org/abs/2508.03806)
*Jorge Miguel-Ramiro,Jessica Illiano,Francesco Mazza,Alexander Pirker,Julia Freund,Angela Sara Cacciapuoti,Marcello Caleffi,Wolfgang Dür*

Main category: quant-ph

TL;DR: QPing是一种用于量子网络的诊断工具，用于测试节点间的量子纠缠，具有高效、灵活的特点。


<details>
  <summary>Details</summary>
Motivation: 介绍QPing作为未来量子网络的诊断原语，用于评估端节点间建立实用量子纠缠的可行性，重点关注资源效率、开销和自适应保真度阈值。

Method: QPing基于序贯假设检验等工具来探测量子连通性，提出了包括主动策略（路径和分段变体）和被动策略（利用预共享纠缠资源）在内的多种策略。

Result: QPing是一种诊断原语，用于评估端节点间建立量子纠缠的可行性，具有高效的资源消耗、有限的开销和时间自适应的保真度阈值。

Conclusion: QPing可作为量子网络的灵活诊断构件，适用于不同的架构和协议设计方法，并可与基础网络操作协同工作。

Abstract: We introduce the concept of Quantum Ping (QPing) as a diagnostic primitive
for future quantum networks, designed to assess whether two or more end nodes
can establish practical quantum entanglement with efficient resource
consumption, limited overhead, and time-adaptive fidelity thresholds. Unlike
classical ping, which probes network-layer connectivity through ICMP messages,
our proposed quantum version is adapted to the unique features of quantum
networks, where connectivity depends on the availability and quality of shared
entanglement. We develop a formal framework for QPing and leverage different
tools such as sequential hypothesis testing to probe quantum connectivity. We
present several strategies, including active strategies, with path-based and
segment-based variants, and passive strategies that utilize pre-shared
entangled resources. QPing can serve as a flexible diagnostic building block
for quantum networks, designed to work alongside fundamental network
operations, while remaining suitable to different architectural and protocol
design approaches.

</details>


### [224] [Advantages of Co-locating Quantum-HPC Platforms: A Survey for Near-Future Industrial Applications](https://arxiv.org/abs/2508.04171)
*Daigo Honda,Yuta Nishiyama,Junya Ishikawa,Kenichi Matsuzaki,Satoshi Miyata,Tadahiro Chujo,Yasuhisa Yamamoto,Masahiko Kiminami,Taro Kato,Jun Towada,Naoki Yoshioka,Naoto Aoki,Nobuyasu Ito*

Main category: quant-ph

TL;DR: 量子计算机与HPC系统共置可提升混合作业性能，大型问题需HPC资源支持。


<details>
  <summary>Details</summary>
Motivation: 为了解决量子-HPC平台（将量子计算机与高性能计算系统通过共置整合）是否为近期工业应用提供实际效益尚不明确的问题。

Method: 通过系统性调研，分析了量子计算机与高性能计算系统共置对延迟、带宽和作业调度的影响，并评估了高性能计算能力对混合算法性能、大规模错误缓解以及量子电路划分和优化的增强作用。

Result: 研究结果表明，量子与HPC系统共置可以显著提高混合作业吞吐量，并且大型的现实世界问题需要HPC级别的计算资源来执行混合算法。

Conclusion: 量子HPC平台通过共置整合了量子计算机和高性能计算系统，研究发现这种整合能为近期的工业应用带来实际效益，尤其是在降低延迟、提高带宽和优化作业调度方面。

Abstract: We conducted a systematic survey of emerging quantum-HPC platforms, which
integrate quantum computers and High-Performance Computing (HPC) systems
through co-location. Currently, it remains unclear whether such platforms
provide tangible benefits for near-future industrial applications. To address
this, we examined the impact of co-location on latency reduction, bandwidth
enhancement, and advanced job scheduling. Additionally, we assessed how
HPC-level capabilities could enhance hybrid algorithm performance, support
large-scale error mitigation, and facilitate complex quantum circuit
partitioning and optimization. Our findings demonstrate that co-locating
quantum and HPC systems can yield measurable improvements in overall hybrid job
throughput. We also observe that large-scale real-world problems can require
HPC-level computational resources for executing hybrid algorithms.

</details>


### [225] [Nonreciprocity in Quantum Technology](https://arxiv.org/abs/2508.03945)
*Shabir Barzanjeh,André Xuereb,Andrea Alù,Sander A. Mann,Nikita Nefedkin,Vittorio Peano,Peter Rabl*

Main category: quant-ph

TL;DR: Quantum technologies leverage nonreciprocity for directional signal transmission. Recent integrated devices overcome traditional limitations, paving the way for advanced quantum computing, networks, and sensing.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of nonreciprocity in quantum technologies and showcase recent experimental advances.

Method: This paper highlights key concepts for engineering nonreciprocity in quantum systems and describes its applications.

Result: Recent experimental advances have demonstrated nonreciprocal behavior in low-loss, fully integrated devices operating with weak or no magnetic bias, overcoming limitations of traditional approaches and enabling compatibility with various quantum architectures and applications.

Conclusion: Nonreciprocity is a crucial resource for quantum technologies, enabling directional amplification, routing, and topological protection. Recent advances in integrated devices using synthetic gauge fields, optomechanical interactions, and chiral light-matter coupling have overcome limitations of traditional methods, making nonreciprocity compatible with superconducting circuits, scalable quantum photonic architectures, modular quantum computers, distributed quantum networks, and precision metrology.

Abstract: Nonreciprocity-the ability to transmit signals in one direction while
blocking them in the reverse-has become a powerful resource in quantum
technologies, enabling directional amplification, routing of quantum
information, and topologically protected quantum states. Recent experimental
advances have demonstrated nonreciprocal behavior in low-loss, fully integrated
devices operating with weak or no magnetic bias, enabled by synthetic gauge
fields, optomechanical interactions, and chiral light-matter coupling. These
achievements overcome the limitations of more traditional approaches, making
nonreciprocity compatible with superconducting circuits and scalable quantum
photonic architectures as well as an integral part of the next generation of
modular quantum computers, distributed quantum networks, and precision
metrology. Here we highlight the key concepts for engineering nonreciprocity in
quantum systems and describe how this functionality can be employed for
high-fidelity qubit readout, robust quantum state transfer, and boosting the
sensitivity of quantum sensors.

</details>


### [226] [One-dimensional quantum droplets under linear gravitational-like trap](https://arxiv.org/abs/2508.03825)
*Saurab Das,Jayanta Bera,Ajay Nath*

Main category: quant-ph

TL;DR: 本研究分析了恒定和时变线性引力势对一维量子液滴动力学的影响，通过分析推导了量子液滴的波函数和有效相互作用贡献，并利用香农熵和维格纳拟概率分布探究了相空间局域化和相干结构。研究结果表明，量子液滴在精密重力测量和量子传感领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究引力势对一维量子液滴动力学的影响，并探索其在精密测量中的应用潜力。

Method: 在定制的外部限制下，分析地表征了量子液滴（QDs）的波函数，并推导了有效的相互作用贡献。

Result: 在恒定和时变的线性引力势影响下，分析地表征了量子液滴的波函数和有效相互作用贡献。发现液滴的下落速度仅取决于线性引力势的强度。当线性势随时间变化时，液滴轨迹出现偏差，这表明在精密重力测量中的潜在应用。香农熵和维格纳拟概率分布的计算揭示了恒定和随时间变化的线性势的不同特征，其中调制强度直接影响液滴的相空间局域化和相干结构。数值模拟证实了分析解的稳定性。

Conclusion: 该研究结果为使用超稀薄量子流体进行量子传感和计量应用提供了有前景的启示。

Abstract: We investigate the influence of a constant and time-dependent linear
gravitational-like potential on one-dimensional quantum droplets (QDs),
governed by an extended GPE incorporating a repulsive cubic effective
mean-field (EMF) term and an attractive quadratic beyond-mean-field (BMF)
correction. Within a tailored external confinement, we analytically
characterize the QDs wavefunction and derive the effective interaction
contributions. Analogous to classical Newtonian dynamics, the falling velocity
of the droplet within a finite domain is found to depend solely on the strength
of the linear gravitational like potential, remaining independent of both the
total atom number and the magnitude of EMF nonlinearity. When the linear
potential is temporally modulated, deviations in the trajectory of the droplet
emerge relative to the static case, indicating potential applicability in
precision gravimetry. To further probe the dynamical coherence properties, we
compute the Shannon entropy and the Wigner quasi-probability distribution. Both
measures reveal distinct signatures of the constant and time varying linear
potential, with the modulation strength directly influencing the phase-space
localization and coherence structure of the droplet. Numerical simulations
substantiate the stability of the analytical solutions, demonstrating their
robustness. These findings suggest promising implications for quantum sensing
and metrological applications using ultradilute quantum fluids.

</details>


### [227] [The Covariant Relativistic Derivation of De Broglie Relation](https://arxiv.org/abs/2508.03908)
*Samuel Bueno Soltau*

Main category: quant-ph

TL;DR: 本文追溯了德布罗意关系的历史发展，从量子假说到相对论推导，并强调了相对论方法在理解波粒二象性上的重要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨德布罗意关系的起源和发展，特别是从普朗克和爱因斯坦的量子假说到其协变相对论推导的过程，并强调相对论方法的普适性和概念连贯性。

Method: 本文首先回顾了普朗克和爱因斯坦的量子假说，接着重构了德布罗意最初的启发式推导，然后重点介绍了如何从狭义相对论的基本原理和四动量形式出发直接推导出德布罗意关系，并对启发式方法和相对论方法进行了比较分析。

Result: 通过对启发式和相对论方法的比较分析，证明了相对论方法在德布罗意关系的推导上具有优越性，并阐述了相对论力学在确立波粒二象性基础方面的重要性。

Conclusion: 本文总结了德布罗意关系的重要性，强调了其在建立波粒二象性一致基础方面，并通过量子场论的视角进一步巩固了这一点。

Abstract: This paper provides an examination of the de Broglie relation, tracing its
historical development from the quantum hypotheses proposed by Planck and
Einstein to its covariant relativistic derivation. The discussion begins by
situating de Broglie's seminal insight within the early framework of quantum
theory. We then reconstruct his original heuristic derivation. The primary
focus of this work, however, is the derivation of the de Broglie relation
directly from the principles of special relativity, employing the four-momentum
formalism. A comparative analysis between the heuristic and relativistic
approaches underscores the universality and conceptual coherence of the latter.
The paper concludes by highlighting the significance of relativistic mechanics
in establishing a consistent foundation for wave-particle duality, further
reinforcing this through a quantum field theoretical perspective.

</details>


### [228] [Quantum chemistry for solids made simple with the Clifford formalism](https://arxiv.org/abs/2508.03917)
*Amer Alrakik,Gian Luigi Bendazzoli,Stefano Evangelisti,J. Arjan Berger*

Main category: quant-ph

TL;DR: A new theory models solids as Clifford tori with compatible Gaussian basis sets, allowing quantum chemistry methods to accurately calculate solid properties, shown with hydrogen chains.


<details>
  <summary>Details</summary>
Motivation: To develop a general theory for treating periodic solids using quantum-chemistry methods, providing an alternative to impossible ring-like calculations for 3D solids.

Method: The paper models solids as a Clifford torus (a periodic and flat torus) and introduces a compatible periodic Gaussian basis set to treat periodic solids with quantum-chemistry methods.

Result: The approach successfully calculates the ground-state energy of a periodic chain of hydrogen atoms within Hartree-Fock and coupled cluster theory, demonstrating correctness in the thermodynamic limit.

Conclusion: The proposed Clifford formalism offers an excellent alternative for quantum chemistry calculations of solids, seamlessly integrating with existing methods for atoms and molecules, and yielding correct ground-state energies in the thermodynamic limit.

Abstract: We present a general theory to treat periodic solids with quantum-chemistry
methods. It relies on two main developments: 1) the modeling of a solid as a
Clifford torus which is a torus that is both periodic and flat and 2) the
introduction of a periodic gaussian basis set that is compatible with the
topology of the Clifford torus. We illustrate our approach by calculating the
ground-state energy of a periodic chain of hydrogen atoms within both
Hartree-Fock and coupled cluster theory. We demonstrate that our approach
yields the correct ground-state energy in the thermodynamic limit by comparing
it to the ground-state energy of a ring of hydrogen atoms in the same limit.
Since equivalent ring-like calculations for three-dimensional solids are
impossible, our approach is an excellent alternative to perform
quantum-chemistry calculations of solids. Our Clifford formalism can be
seamlessly combined with current implementations of quantum-chemistry methods
designed for atoms and molecules to make them applicable to solids.

</details>


### [229] [Calculating Vibronic Spectra with a linear algorithm based on Gaussian Boson Sampling](https://arxiv.org/abs/2508.03943)
*I. Konyshev,R. Pradip,O. Page,C. Ünlüer,R. T. Nasibullin,V. V. Rybkin,W. Pernice,S. Ferrari*

Main category: quant-ph

TL;DR: 该研究提出了一种基于高斯玻色子取样和线性耦合模型的分子振动光谱模拟方法，通过经典模拟和实验验证了其高保真度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 分子振动光谱的精确模拟因计算量呈指数增长而面临计算挑战。

Method: 在经典计算机上进行数值模拟，并使用配备不同光子探测器（SNSPD 和 SPAD）的两个光学装置进行实验。

Result: 在模拟和实验中均实现了高保真度（F > 0.999），能够准确复现光谱带的位置和形状，并且该算法具有良好的可扩展性。

Conclusion: 利用高斯玻色子取样框架中的线性耦合模型可以有效模拟分子振动光谱，解决了计算量大的挑战。实验和模拟结果均表明该方法能高保真地复现光谱。

Abstract: Accurately simulating molecular vibronic spectra remains computationally
challenging due to the exponential scaling of required calculations. Here, we
show that employing the linear coupling model within the gaussian boson
sampling framework effectively addresses this limitation. We implement the
algorithm for simulating the pentacene molecule through three distinct
approaches, using numerical simulation on a classical computer and
experimentally using two optical setups equipped with different photon
detectors (SNSPD and SPAD). High fidelity $(F>0.999)$ was achieved between the
simulated Franck-Condon profiles and analytically calculated profiles obtained
by enumerating all possible transitions within the linear coupling model.
Furthermore, simulations were performed for larger molecular systems using 48
vibrational modes of naphthalene and 64 vibrational modes of anthracene.
Comparison with experimental data confirms that the simulated spectra
accurately reproduce both the positions and shapes of the measured spectral
bands. A notable advantage of our algorithm is its scalability, requiring only
a fixed minimal set of optical components irrespective of the size of the
studied system.

</details>


### [230] [Challenges in Applying Variational Quantum Algorithms to Dynamic Satellite Network Routing](https://arxiv.org/abs/2508.04288)
*Phuc Hao Do,Tran Duc Le*

Main category: quant-ph

TL;DR: 量子算法（VQE、QAOA、QRL）在模拟的卫星网络路由任务中未能解决简单问题或学习有用的策略，这表明在实际应用中存在重大挑战。


<details>
  <summary>Details</summary>
Motivation: 将近期变分量子算法应用于动态卫星网络路由问题代表了量子计算的一个有前途的方向。

Method: 本研究对两种主要方法进行了关键评估：静态量子优化器（如变分量子本征求解器（VQE）和量子近似优化算法（QAOA））用于离线路由计算，以及用于在线决策的量子强化学习（QRL）方法。

Result: 使用理想的、无噪声的模拟，我们发现这些算法面临着重大的挑战。具体来说，由于优化景观的复杂性，静态优化器甚至无法解决经典的简单 4 节点最短路径问题。同样，基于策略梯度方法的 QRL 代理在动态 8 节点环境中未能学习到有用的路由策略，其表现不优于随机操作。

Conclusion: 量子算法在通信网络中提供实际优势之前，必须解决诸如边际平原和学习不稳定性等关键障碍。

Abstract: Applying near-term variational quantum algorithms to the problem of dynamic
satellite network routing represents a promising direction for quantum
computing. In this work, we provide a critical evaluation of two major
approaches: static quantum optimizers such as the Variational Quantum
Eigensolver (VQE) and the Quantum Approximate Optimization Algorithm (QAOA) for
offline route computation, and Quantum Reinforcement Learning (QRL) methods for
online decision-making. Using ideal, noise-free simulations, we find that these
algorithms face significant challenges. Specifically, static optimizers are
unable to solve even a classically easy 4-node shortest path problem due to the
complexity of the optimization landscape. Likewise, a basic QRL agent based on
policy gradient methods fails to learn a useful routing strategy in a dynamic
8-node environment and performs no better than random actions. These negative
findings highlight key obstacles that must be addressed before quantum
algorithms can offer real advantages in communication networks. We discuss the
underlying causes of these limitations, including barren plateaus and learning
instability, and suggest future research directions to overcome them.

</details>


### [231] [Charge sensitivity in the transmon regime](https://arxiv.org/abs/2508.03973)
*Rocio Gonzalez-Meza,Vito Iaia,Anika Zaman,Hiu-Yung Wong,Yujin Cho,Kristin Beck,Yaniv J. Rosen*

Main category: quant-ph

TL;DR: Transmon量子比特的退相干时间受电荷环境的影响，即使在高EJ/EC比值下也是如此。应考虑奇偶校验翻转率作为器件的表征指标。


<details>
  <summary>Details</summary>
Motivation: 尽管Transmon在量子计算中具有优势，但其退相干时间在不同设备和同一设备的不同量子比特之间存在显著差异。现有理论假设Transmon对电荷噪声不敏感，但最近很少有研究关注退相干时间对局部电荷环境的依赖性。

Method: 通过单次测量协议来检测奇偶校验开关事件，并将该协议嵌入到拉姆齐测量中。

Result: 事件保持在相同奇偶校验状态下的T2时间比跨越两种奇偶校验状态的平均测量值更高。

Conclusion: Transmons的性能可能受到电荷噪声的限制，即使在EJ/EC ≈ 50的情况下也是如此。因此，奇偶校验翻转率应被视为器件表征指标。

Abstract: Transmons are widely adopted in quantum computing architectures for their
engineered insensitivity to charge noise and correspondingly long relaxation
times. Despite this advantage, transmons often exhibit large fluctuations in
dephasing times across different devices and also within qubits on the same
device. Existing transmon qubits are assumed to be insensitive to charge noise.
However, very little recent attention has been paid to the dependence of
dephasing on the local charge environment. In this study, we see fluctuations
in the dephasing time, $T_{\phi}$, which correlate to charge offset. While
charge offset fluctuations are slow, parity switches are fast processes tied to
the charge offset and can affect $T_{\phi}$ in Ramsey experiments. We implement
a protocol to detect parity switching events using single-shot methods, which
are interleaved within a Ramsey measurement. We find that events that remain in
the same parity state have a higher $T_2$ than measurements averaged over both
parities. Our results show that transmons can be limited by charge-noise, even
with $E_\text{J}/E_\text{C} \approx 50$. Consequently, parity flip rates must
be considered as a device characterization metric.

</details>


### [232] [Graphical Calculus for Fermionic Tensors](https://arxiv.org/abs/2508.03976)
*Yuanjie Ren,Kaifeng Bu,Andreas Bauer*

Main category: quant-ph

TL;DR: A new graphical calculus using fermionic tensors is introduced for computations in fermionic many-body physics, extending the ZX calculus and applicable to various physical objects and protocols.


<details>
  <summary>Details</summary>
Motivation: To provide a purely diagrammatic approach for various computations in fermionic many-body physics.

Method: The method involves using a graphical calculus with fermionic tensors and tensor-network equations, extending the ZX calculus to include fermionic modes, qubits, and fixed odd-parity states.

Result: The calculus can represent fermionic Gaussian states, Majorana mode partial traces, purification protocols, fermionization/bosonization maps, and fermionic codes.

Conclusion: We present a graphical calculus based on fermionic tensors and tensor-network equations for performing computations in fermionic many-body physics.

Abstract: We introduce a graphical calculus, consisting of a set of fermionic tensors
with tensor-network equations, which can be used to perform various
computations in fermionic many-body physics purely diagrammatically. The
indices of our tensors primarily correspond to fermionic modes, but also
include qubits and fixed odd-parity states. Our graphical calculus extends the
ZX calculus for systems involving qubits. We apply the calculus in order to
represent various objects, operations, and computations in physics, including
fermionic Gaussian states, the partial trace of Majorana modes, purification
protocols, fermionization and bosonization maps, and the construction of
fermionic codes.

</details>


### [233] [A simpler Gaussian state-preparation](https://arxiv.org/abs/2508.03987)
*Parker Kuklinski,Benjamin Rempfer,Kevin Obenland,Justin Elenewski*

Main category: quant-ph

TL;DR: 提出了一种新的量子线路制备高斯态的方法，比现有方法更直观且资源开销更小。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有 Kitaev-Webb 算法在量子高斯态制备中存在的巨大资源开销问题，提出了新的制备方法。

Method: 提出了一种新的、更直观的量子线路制备方法，使用 n-1 个旋转门、(n-1)(n-2)/2 个双量子比特受控旋转门和 $\lfloor(n-1)/2\rfloor$ 个辅助量子比特来制备 n 量子比特的高斯态。随后对线路进行了优化，使其 T 深度（T-depth）成为线性关系。

Result: 所提出的方法在资源扩展性和 T 深度优化方面优于现有方法。

Conclusion: 该方法可以扩展到具有多项式相位（polynomial phase）的复杂函数的制备。

Abstract: The ability to efficiently state-prepare Gaussian distributions is critical
to the success of numerous quantum algorithms. The most popular algorithm for
this subroutine (Kitaev-Webb) has favorable polynomial resource scaling,
however it faces enormous resource overheads making it functionally
impractical. In this paper, we present a new, more intuitive method which uses
exactly $n-1$ rotations, $(n-1)(n-2)/2$ two-qubit controlled rotations, and
$\lfloor(n-1)/2\rfloor$ ancilla to state-prepare an $n$-qubit Gaussian state.
We then apply optimizations to the circuit to render it linear in T-depth. This
method can be extended to state-preparations of complex functions with
polynomial phase.

</details>


### [234] [Dynamic Solutions for Hybrid Quantum-HPC Resource Allocation](https://arxiv.org/abs/2508.04217)
*Roberto Rocco,Simone Rizzo,Matteo Barbieri,Gabriella Bettonte,Elisabetta Boella,Fulvio Ganz,Sergio Iserte,Antonio J. Peña,Petter Sandås,Alberto Scionti,Olivier Terzo,Chiara Vercellino,Giacomo Vitali,Paolo Viviani,Jonathan Frassineti,Sara Marzella,Daniele Ottaviani,Iacopo Colonnelli,Daniele Gregori*

Main category: quant-ph

TL;DR: 提出了一种新的可塑性方法和工作流策略，通过动态资源分配优化了混合HPC-量子计算工作负载的资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的兴起，将量子计算机整合到经典高性能计算（HPC）基础设施中作为加速器，但面临资源分配等技术挑战。

Method: 提出了一种基于可塑性的新方法和一种基于工作流的策略，以优化混合高性能计算-量子计算工作负载的资源利用率，实现计算任务卸载到量子计算机时释放和完成后的重新分配。

Result: 通过混合HPC-量子计算用例的实验证明了动态分配的好处。

Conclusion: 所提出的可塑性方法和工作流策略能够优化混合高性能计算-量子计算工作负载的资源利用率，并通过动态分配展示了其潜力。

Abstract: The integration of quantum computers within classical High-Performance
Computing (HPC) infrastructures is receiving increasing attention, with the
former expected to serve as accelerators for specific computational tasks.
However, combining HPC and quantum computers presents significant technical
challenges, including resource allocation. This paper presents a novel
malleability-based approach, alongside a workflow-based strategy, to optimize
resource utilization in hybrid HPC-quantum workloads. With both these
approaches, we can release classical resources when computations are offloaded
to the quantum computer and reallocate them once quantum processing is
complete. Our experiments with a hybrid HPC-quantum use case show the benefits
of dynamic allocation, highlighting the potential of those solutions.

</details>


### [235] [Thermalization with partial information](https://arxiv.org/abs/2508.03993)
*Philippe Faist,Sumeet Khatri*

Main category: quant-ph

TL;DR: 本文提出最大通道熵原理，用于识别模拟量子系统动力学的噪声量子通道，并推导了其数学结构和学习算法。


<details>
  <summary>Details</summary>
Motivation: 本文旨在超越对系统最终平衡态的研究，提出一个能够模拟量子系统动力学的基本原理，即最大通道熵原理，来识别噪声量子通道。

Method: 本文提出了最大通道熵原理，并将其应用于识别模拟量子系统动力学的噪声量子通道。该原理通过最大化通道熵并满足宏观约束来确定通道，同时还推导了该通道的通用数学结构，并提出了一个基于最大通道熵原理的量子通道学习算法。

Result: 最大通道熵原理成功地识别了模拟系统动力学的噪声量子通道，并且该原理可以推广到包含热化和平均能量守恒在内的部分热化动力学。此外，还推导了该通道的数学结构，并提出了一种基于该原理的量子通道学习算法，该算法在量子控制和量子算法设计方面具有潜在的应用价值。

Conclusion: 该原则为量子信息学的研究提供了新的视角，尤其是在理解和模拟量子系统动力学方面，并且其学习算法的提出，展示了其在量子控制和量子算法设计中的潜在应用价值。

Abstract: A many-body system, whether in contact with a large environment or evolving
under complex dynamics, can typically be modeled as occupying the thermal state
singled out by Jaynes' maximum entropy principle. Here, we find analogous
fundamental principles identifying a noisy quantum channel $\mathcal{T}$ to
model the system's dynamics, going beyond the study of its final equilibrium
state. Our maximum channel entropy principle states that $\mathcal{T}$ should
maximize the channel's entropy, suitably defined, subject to any available
macroscopic constraints. These may correlate input and outputs, and may lead to
restricted or partial thermalizing dynamics including thermalization with
average energy conservation. This principle is reinforced by an independent
extension of the microcanonical derivation of the thermal state to channels,
which leads to the same $\mathcal{T}$. Our technical contributions include a
derivation of the general mathematical structure of $\mathcal{T}$, a custom
postselection theorem relating an arbitrary permutation-invariant channel to
nearby i.i.d. channels, as well as novel typicality results for quantum
channels for noncommuting constraints and arbitrary input states. We propose a
learning algorithm for quantum channels based on the maximum channel entropy
principle, demonstrating the broader relevance of $\mathcal{T}$ beyond
thermodynamics and complex many-body systems.

</details>


### [236] [Maximum channel entropy principle and microcanonical channels](https://arxiv.org/abs/2508.03994)
*Philippe Faist,Sumeet Khatri*

Main category: quant-ph

TL;DR: This paper introduces the concept of 'thermal channels' by analogy with thermal states, proposing a maximum-channel-entropy principle. The study reveals that these channels have an exponential form and thermodynamic relevance, and a new quantum channel learning algorithm is presented.


<details>
  <summary>Details</summary>
Motivation: The widespread relevance of the thermal state in various fields including physics, information theory, machine learning, and quantum computing inspires the exploration of analogous concepts for quantum channels.

Method: We formulate a maximum-channel-entropy principle by defining a thermal channel as one that maximizes a channel entropy measure subject to linear constraints on the channel. Our techniques involve convex optimization methods, typicality techniques involving noncommuting operators, a custom channel postselection technique, and Schur-Weyl duality.

Result: Thermal channels exhibit an exponential form, and we studied examples including thermalizing channels, Pauli-covariant channels, and classical channels. We developed a quantum channel learning algorithm and proved that the maximum-channel-entropy channel resembles the action of a microcanonical channel on multiple system copies. A constrained postselection theorem for quantum channels was also proven.

Conclusion: We proved that thermal channels exhibit an exponential form reminiscent of thermal states, and demonstrated their thermodynamic relevance by showing they resemble the action of a microcanonical channel on many copies of a system. We also proposed a quantum channel learning algorithm based on maximum channel entropy methods and proved a constrained postselection theorem for quantum channels.

Abstract: The thermal state plays a number of significant roles throughout physics,
information theory, quantum computing, and machine learning. It arises from
Jaynes' maximum-entropy principle as the maximally entropic state subject to
linear constraints, and is also the reduced state of the microcanonical state
on the system and a large environment. We formulate a maximum-channel-entropy
principle, defining a thermal channel as one that maximizes a channel entropy
measure subject to linear constraints on the channel. We prove that thermal
channels exhibit an exponential form reminiscent of thermal states. We study
examples including thermalizing channels that conserve a state's average
energy, as well as Pauli-covariant and classical channels. We propose a quantum
channel learning algorithm based on maximum channel entropy methods that
mirrors a similar learning algorithm for quantum states. We then demonstrate
the thermodynamic relevance of the maximum-channel-entropy channel by proving
that it resembles the action on a single system of a microcanonical channel
acting on many copies of the system. Here, the microcanonical channel is
defined by requiring that the linear constraints obey sharp statistics for any
i.i.d. input state, including for noncommuting constraint operators. Our
techniques involve convex optimization methods to optimize recently introduced
channel entropy measures, typicality techniques involving noncommuting
operators, a custom channel postselection technique, as well as Schur-Weyl
duality. As a result of potential independent interest, we prove a constrained
postselection theorem for quantum channels. The widespread relevance of the
thermal state throughout physics, information theory, machine learning, and
quantum computing, inspires promising applications for the analogous concept
for quantum channels.

</details>


### [237] [Graph theory-based automated quantum algorithm for efficient querying of acyclic and multiloop causal configurations](https://arxiv.org/abs/2508.04019)
*Salvador A. Ochoa-Oregon,Juan P. Uribe-Ramírez,Roger J. Hernández-Pinto,Selomit Ramírez-Uribe,Germán Rodrigo*

Main category: quant-ph

TL;DR: A new quantum algorithm (MCA) is introduced to analyze Feynman diagrams using graph theory, optimized for efficiency and evaluated via circuit metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unraveling the causal configurations of multiloop Feynman diagrams in high-energy physics using quantum algorithms.

Method: The MCA quantum algorithm is designed to efficiently query causal structures within the Loop-Tree Duality by mapping Feynman propagators to qubits and employing graph theory techniques analogous to the Minimum Clique Partition problem.

Result: The evaluation of the MCA quantum algorithm is exhibited by analyzing the transpiled quantum circuit depth and quantum circuit area.

Conclusion: The paper presents the Minimum Clique-optimised quantum Algorithm (MCA) for analyzing causal configurations in multiloop Feynman diagrams, demonstrating its efficiency through circuit depth and area analysis.

Abstract: Quantum algorithms provide a promising framework in high-energy physics, in
particular, for unraveling the causal configurations of multiloop Feynman
diagrams by identifying Feynman propagators with qubits, a challenge analogous
to querying directed acyclic graphs in graph theory. In this paper, we present
the Minimum Clique-optimised quantum Algorithm (MCA), an automated quantum
algorithm designed to efficiently query the causal structures within the
Loop-Tree Duality. The MCA quantum algorithm is optimised by exploiting graph
theory techniques, specifically, by analogy with the Minimum Clique Partition
problem. The evaluation of the MCA quantum algorithm is exhibited by analysing
the transpiled quantum circuit depth and quantum circuit area.

</details>


### [238] [Generalized Quantum Hadamard Test for Machine Learning](https://arxiv.org/abs/2508.04065)
*Vivek Mehta,Arghya Choudhury,Utpal Roy*

Main category: quant-ph

TL;DR: 提出一种新的量子算法，能更灵活地处理数据，并成功应用于分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有的量子Hadamard测试要求输入空间映射到L2归一化向量空间，导致计算出的保真度仅对应于映射后的余弦相似度。为了克服这一限制，提出了一种能够计算有界输入空间内积的广义量子Hadamard测试。

Method: 提出了一种广义量子Hadamard测试，结合了L2归一化、最小-最大归一化等多种标准化方法，并提供了量子线路实现和数值模拟。

Result: 所提出的广义量子Hadamard测试在计算效率和线路复杂度方面均表现出色，并通过集成到逻辑回归和质心分类器中，成功解决了四个分类问题。

Conclusion: 该算法通过结合量子特征映射和酉演化，实现了计算有界输入空间内积的功能，并展示了其在逻辑回归和质心分类器上的应用。

Abstract: Quantum machine learning models are designed for performing learning tasks.
Some quantum classifier models are proposed to assign classes of inputs based
on fidelity measurements. Quantum Hadamard test is a well-known quantum
algorithm for computing these fidelities. However, the basic requirement for
deploying the quantum Hadamard test maps input space to L2-normalize vector
space. Consequently, computed fidelities correspond to cosine similarities in
mapped input space. We propose a quantum Hadamard test with the additional
capability to compute the inner product in bounded input space, which refers to
the Generalized Quantum Hadamard test. It incorporates not only
L2-normalization of input space but also other standardization methods, such as
Min-max normalization. This capability is raised due to different quantum
feature mapping and unitary evolution of the mapped quantum state. We discuss
the quantum circuital implementation of our algorithm and establish this
circuit design through numerical simulation. Our circuital architecture is
efficient in terms of computational complexities. We show the application of
our algorithm by integrating it with two classical machine learning models:
Logistic regression binary classifier and Centroid-based binary classifier and
solve four classification problems over two public-benchmark datasets and two
artificial datasets.

</details>


### [239] [A Lazy Resynthesis Approach for Simultaneous T Gate and Two-Qubit Gate Optimization of Quantum Circuits](https://arxiv.org/abs/2508.04092)
*Mu-Te Lau,Hsiang-Chun Yang,Hsin-Yu Chen,Chung-Yang,Huang*

Main category: quant-ph

TL;DR: 一种新的量子电路优化方法，通过懒惰再合成技术，有效减少了两比特门的数量，并提高了优化速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有量子电路优化（QCO）算法在降低T计数时导致两比特门计数（2Q-count）大幅增加的问题。

Method: 提出了一种新颖的懒惰再合成方法，用于现代基于表的方法的QCO流程。

Result: 与基于表、基于ZX-演算和基于路径和的方法相比，分别将2Q-gate开销减少了54.8%、15.3%和68.0%。在运行时，相比基于表和基于ZX-演算的方法，速度分别提高了1.81倍和13.1倍，与基于路径和的方法性能相当。

Conclusion: 该研究提出的懒惰再合成方法在现代基于表的方法的量子电路优化（QCO）流程中，能够显著缓解T计数优化过程中常见的两比特门（2Q-gate）数量激增问题，并且在运行时实现了比基于表和基于ZX-演算的方法更快的速度，同时与基于路径和的方法相比，性能相当。

Abstract: State-of-the-art quantum circuit optimization (QCO) algorithms for T-count
reduction often lead to a substantial increase in two-qubit gate count
(2Q-count) -- a drawback that existing 2Q-count optimization techniques
struggle to address effectively. In this work, we propose a novel lazy
resynthesis approach for modern tableau-based QCO flows that significantly
mitigates the 2Q-gate surges commonly introduced during T-count optimization in
Clifford+T circuits. Experimental results show that our approach reduces
2Q-count overhead by 54.8%, 15.3%, and 68.0% compared to tableau-based,
ZX-calculus-based, and path-sum-based QCO algorithms, respectively. In terms of
runtime, our method achieves speedups of 1.81$\times$ and 13.1$\times$ over the
tableau-based and ZX-calculus-based methods, while performing comparably to the
path-sum-based approach. In summary, the proposed lazy resynthesis technique
not only enhances the quality and performance of tableau-based QCO algorithms
but also demonstrates superior efficiency and scalability compared to
alternative QCO approaches such as ZX-calculus and path-sum-based techniques.

</details>


### [240] [Hybrid Quantum--Classical Machine Learning Potential with Variational Quantum Circuits](https://arxiv.org/abs/2508.04098)
*Soohaeng Yoo Willow,D. ChangMo Yang,Chang Woo Myung*

Main category: quant-ph

TL;DR: 混合量子-经典方法在模拟液态硅方面优于纯经典方法，为近期量子优势提供了证据。


<details>
  <summary>Details</summary>
Motivation: 量子计算在模拟复杂分子系统方面仍处于初级阶段，而混合量子-经典算法通过结合传统神经网络和变分量子电路，为利用当前的含噪声中型量子（NISQ）硬件提供了一条有希望的途径。

Method: 将纯粹经典的E(3)-等变消息传递机器学习势（MLP）与混合量子-经典MLP进行基准测试，在混合架构中，消息传递层中的每个读出都被变分量子电路（VQC）取代。

Result: 混合量子-经典MLP（HQC-MLP）能够通过变分量子电路准确再现液态硅在高温下的结构和热力学性质。

Conclusion: 与最先进的纯经典方法相比，混合量子-经典（HQC）方法在模拟液态硅方面具有可衡量的优势，表明在材料建模中实现近期量子优势的可行途径。

Abstract: Quantum algorithms for simulating large and complex molecular systems are
still in their infancy, and surpassing state-of-the-art classical techniques
remains an ever-receding goal post. A promising avenue of inquiry in the
meanwhile is to seek practical advantages through hybrid quantum-classical
algorithms, which combine conventional neural networks with variational quantum
circuits (VQCs) running on today's noisy intermediate-scale quantum (NISQ)
hardware. Such hybrids are well suited to NISQ hardware. The classical
processor performs the bulk of the computation, while the quantum processor
executes targeted sub-tasks that supply additional non-linearity and
expressivity. Here, we benchmark a purely classical E(3)-equivariant
message-passing machine learning potential (MLP) against a hybrid
quantum-classical MLP for predicting density functional theory (DFT) properties
of liquid silicon. In our hybrid architecture, every readout in the
message-passing layers is replaced by a VQC. Molecular dynamics simulations
driven by the HQC-MLP reveal that an accurate reproduction of high-temperature
structural and thermodynamic properties is achieved with VQCs. These findings
demonstrate a concrete scenario in which NISQ-compatible HQC algorithm could
deliver a measurable benefit over the best available classical alternative,
suggesting a viable pathway toward near-term quantum advantage in materials
modeling.

</details>


### [241] [Trapping an Atomic Ion without Dedicated Digital-to-analog Converters](https://arxiv.org/abs/2508.04093)
*Ryutaro Ohira,Masanari Miyamoto,Shinichi Morisaka,Ippei Nakamura,Atsushi Noguchi,Utako Tanaka,Takefumi Miyoshi*

Main category: quant-ph

TL;DR: 提出了一种基于时分复用（TDM）的电压控制方法，利用较少的DAC和线路成功控制了10个电极，并捕获了单个离子，适用于量子计算。


<details>
  <summary>Details</summary>
Motivation: 解决量子电荷耦合器件（QCCD）架构中独立控制大量电极所带来的布线和硬件可扩展性挑战。

Method: 提出了一种基于时分复用（TDM）的电压控制方法，利用单个高更新速率的数模转换器（DAC）顺序控制多个电极，以解决量子电荷耦合器件（QCCD）架构中电极数量众多导致的布线和硬件可扩展性挑战。

Result: 实验上成功开发了一个仅使用两个DAC的10通道系统，并将其应用于表面电极陷阱，成功捕获了单个$^{40}	extrm{Ca}^+$离子，验证了该方法的有效性。

Conclusion: 所提出的基于时分复用（TDM）的电压控制方法通过使用单个高更新速率数模转换器（DAC）顺序生成多个电极的控制信号，成功减少了所需的DAC数量和相关线路数量，为基于离子阱的高级量子计算系统提供了一种资源高效且可扩展的解决方案。

Abstract: Independent control of numerous electrodes in quantum charge-coupled device
architectures presents a significant challenge for wiring and hardware
scalability. To address this issue, we demonstrate a voltage control method
based on time-division multiplexing (TDM). This approach utilizes a single
high-update-rate digital-to-analog converter (DAC) to sequentially generate
control signals for multiple electrodes, thereby reducing both the number of
required DACs and associated wiring. We experimentally validate this concept by
developing a 10-channel system that operates with only two DACs. The developed
TDM-based voltage control system is applied to a surface-electrode trap, where
we successfully trap a single $^{40}\mathrm{Ca}^+$ ion. This approach offers a
resource-efficient and scalable solution for advanced quantum computing systems
based on trapped ions.

</details>


### [242] [Coupling phase enabled level transitions in pseudo-Hermitian magnon-polariton systems](https://arxiv.org/abs/2508.04298)
*Huang Xin,Liu Jingyu,Lin Shirong*

Main category: quant-ph

TL;DR: 该研究提出了一个伪厄米模型，通过耦合两个磁子和两个腔模式，研究了相变、奇异点、能级吸引和排斥现象，并揭示了伪厄米对称性破缺与耦合模式转变的关系，为控制量子态提供了新策略。


<details>
  <summary>Details</summary>
Motivation: 腔-磁子杂化虽然提供了有趣的物理现象，但由于腔和磁子模式的固有阻尼，限制了相干时间和应用。最近，随着宏观可调谐外增益的出现，研究重点已从纯耗散系统转向增益-损耗平衡的非厄米系统。

Method: 提出一个具有两个磁子和两个腔模式的伪厄米模型，通过相位相关的相互作用进行耦合，并将能量谱与相变联系起来，以观察到厄米对称性破缺时的奇异点。

Result: 该模型揭示了伪厄米对称性破缺时会出现奇异点、能级吸引和能级排斥。能级吸引对应四次相变，表现为双Z形能量谱；能级排斥对应两次相变，其排斥裂口取决于耦合相位。在由非厄米性和耦合相位定义的相图中，伪厄米对称性破缺与耦合模式转变直接相关。

Conclusion: 伪厄米对称性的打破与耦合模式的转变有着内在的联系，这为控制类比系统中杂化的量子态提供了新的策略。

Abstract: While cavity-magnon hybridization offers intriguing physics, its practical
implementation is hindered by intrinsic damping in both cavity and magnon
modes, leading to short coherence times and constrained applications. Recently,
with the emergence of tunable external gain at the macroscopic scale, the
research focus has shifted from purely lossy systems to gain-loss balanced
non-Hermitian systems. Here, we propose a pseudo-Hermitian model with two
magnon and two cavity modes coupled via phase-dependent interaction. We link
the energy spectrum to phase transitions, observing exceptional points when
pseudo-Hermitian symmetry breaks. We also observed level attraction and level
repulsion. The former corresponds to four phase transitions and manifests as a
double Z-shaped energy spectrum, the latter corresponds to two phase
transitions, with the repulsive gap depending on the coupling phase. In the
phase diagram defined by non-Hermiticity and coupling phase, we reveal a
distinctive correspondence: pseudo-Hermitian symmetry breaking is intrinsically
linked to coupling mode transitions, enabling new strategies for controlling
hybrid quantum states in spintronic systems.

</details>


### [243] [Quantum Advantage in Identifying the Parity of Permutations with Certainty](https://arxiv.org/abs/2508.04310)
*Arnau Diebra,Santiago Llorens,David González-Lociga,Albert Rico,John Calsamiglia,Mark Hillery,Emili Bagan*

Main category: quant-ph

TL;DR: Quantum mechanics offers a significant advantage over classical methods in determining the parity of permutations for $n 
			ge 3$ particles, requiring fewer resources and achieving certainty where classical methods fail. This is demonstrated through explicit state constructions and entanglement analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to demonstrate a genuine quantum advantage through a simple and rigorous example that does not require oracles or contrived setups.

Method: The paper provides explicit expressions for states that ensure perfect parity identification for small $n$. It also assesses the minimum entanglement required for these states, finding it to be close to maximal, and in some cases, maximal.

Result: Quantum mechanics can determine the parity of an unknown permutation with certainty using $\lceil \sqrt{n} \rceil$ distinguishable states per particle, while classical methods are limited to random guessing below $n$ labels. The study also quantifies the entanglement needed for this task.

Conclusion: The paper establishes a sharp quantum advantage in determining the parity of an unknown permutation for $n 
			ge 3$ particles. Classically, this is impossible with fewer than $n$ labels, with success limited to random guessing. Quantum mechanics achieves certainty with as few as $\lceil \sqrt{n} \rceil$ distinguishable states per particle, leveraging entanglement.

Abstract: We establish a sharp quantum advantage in determining the parity (even/odd)
of an unknown permutation applied to any number $n \ge 3$ of particles.
Classically, this is impossible with fewer than~$n$ labels, being the success
limited to random guessing. Quantum mechanics does it with certainty with as
few as $\lceil \sqrt{n}\, \rceil$ distinguishable states per particle, thanks
to entanglement. Below this threshold, not even quantum mechanics helps: both
classical and quantum success are limited to random guessing. For small $n$, we
provide explicit expressions for states that ensure perfect parity
identification. We also assess the minimum entanglement these states need to
carry, finding it to be close to maximal, and even maximal in some cases. The
task requires no oracles or contrived setups and provides a simple, rigorous
example of genuine quantum advantage.

</details>


### [244] [Hydrodynamic Effects in Cryogenic Buffer Gas Cells: Design Insights from Hybrid Simulations](https://arxiv.org/abs/2508.04364)
*Nick Vogeley,Bernd Bauerhenne,Daqing Wang*

Main category: quant-ph

TL;DR: 本研究对低温缓冲气体束源进行了数值模拟，发现涡流的形成在特定条件下可以提高分子提取效率，并提出了实验验证的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管实验取得了进展，但由于描述致密缓冲气体和稀疏种子分子所需的大参数范围带来的挑战，数值研究仍然有限。本研究旨在数值评估低温缓冲气体束源，特别关注流体动力学提取模式下的球形电池内的涡流形成及其对分子冷却和提取性能的潜在增强作用。

Method: 本研究通过稳态滑流模拟氦缓冲气体，并采用直接模拟蒙特卡洛扩散程序追踪粒子轨迹，对在流体动力学提取模式下运行的低温缓冲气体束源进行了数值评估。

Result: 研究人员对不同缓冲气体通过量和注入角度下的源性能进行了比较，并确定了涡流形成能够增强分子提取效果的参数范围。

Conclusion: 研究结果表明，在某些特定参数范围内，涡流的形成能够增强分子的提取效果，并且模拟提取的实验可观测值可用于通过速度或飞行时间测量来验证这些效应。

Abstract: Cryogenic buffer gas beam sources have become an essential tool for
experiments requiring cold molecular beams with low forward velocities.
Although recent experimental advances have led to significant progress in
source optimization, numerical studies remain limited due to the challenges
posed by the large parameter ranges required to describe both the dense buffer
gas and the dilute seed molecules. In this work, we report a numerical
evaluation of cryogenic buffer gas beam cells operating in the hydrodynamic
extraction regime. While most prior studies focused on box-like or cylindrical
cells, we investigated hydrodynamic effects including vortex formation in a
spherical cell and assessed whether these could be utilized to enhance the
performance in molecule cooling and extraction. To achieve this, we performed
steady-state slip-flow simulations for helium buffer gas and employed a
direct-simulation Monte Carlo diffusion routine to track particle trajectories.
We compared the performance of the source across different buffer gas
throughputs and injection angles and identified parameter regimes where vortex
formation enhances molecule extraction. From the simulations, we extracted
experimental observables, which allow these effects to be verified through
velocity or time-of-flight measurements on the molecular beam.

</details>


### [245] [Universal Configuration for Optimizing Complexity in Variational Distributed Quantum Circuits](https://arxiv.org/abs/2508.04464)
*J. Montes,F. Borondo,Gabriel G. Carlo*

Main category: quant-ph

TL;DR: 分布式量子计算的关键是优化门分配以实现最大电路复杂度和最小深度。本研究发现了一个通用的最优配置，并使用基于马尔可夫矩阵的复杂度度量进行了验证。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算是扩展量子处理器最有前途的方法之一。确定最优配置（即以最小深度最大化电路复杂度的排列）是一个基本的设1计挑战。

Method: 本研究基于 Weinstein 等人提出的基于马尔可夫矩阵的复杂度度量，该度量量化了趋向 Haar 测度的收敛速率，并进行了分析和数值演示。

Result: 研究证明了存在一个通用的最优配置，该配置适用于各种核间通信拓扑。

Conclusion: 本研究证明存在一个通用的最优配置，用于在变分分布式电路中的任意核间通信拓扑中分配单比特和双比特门。

Abstract: Distributed quantum computing represents at present one of the most promising
approaches to scaling quantum processors. Current implementations typically
partition circuits into multiple cores, each composed of several qubits, with
inter-core connectivity playing a central role in ensuring scalability.
Identifying the optimal configuration -- defined as the arrangement that
maximizes circuit complexity with minimal depth -- thus constitutes a
fundamental design challenge. In this work, we demonstrate, both analytically
and numerically, the existence of a universal optimal configuration for
distributing single and two qubit gates across arbitrary intercore
communication topologies in variational distributed circuits. Our proof is
based on a complexity measure based on Markov matrices, which quantifies the
convergence rate toward the Haar measure, as introduced by Weinstein et al.
Finally, we validate our predictions through numerical comparisons with the
well established majorization criterion proposed in Ref 2.

</details>


### [246] [Simulation and Benchmarking of Real Quantum Hardware](https://arxiv.org/abs/2508.04483)
*T. Piskor,M. Schöndorf,M. Bauer,D. Smith,T. Ayral,S. Pogorzalek,A. Auer,M. Papič*

Main category: quant-ph

TL;DR: 提出了一种用于NISQ设备的噪声模型，该模型在超导硬件上进行了测试，并展示了优于现有方法的准确性，且适用于离子阱和中性原子设备。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，噪声是量子计算中的一个关键因素。为了抑制和减轻噪声的影响，并评估量子算法在给定硬件上的表现，需要准确描述真实硬件的噪声模型。

Method: 提出了一种能够准确描述真实硬件的噪声模型，并在超导硬件平台上进行了评估，随后通过模拟20量子比特的超导量子计算机来验证该模型，并与现有方法进行比较。

Result: 提出的噪声模型在预测准确性方面优于文献中的类似方法，并且可以应用于不同的量子计算硬件架构。

Conclusion: 所提出的噪声模型在超导硬件平台上进行了评估，并展示了可以适应其他常见架构（如离子阱或中性原子设备），并且通过模拟20量子比特的超导量子计算机对其模型进行了基准测试，与文献中类似方法相比，在预测准确性方面有所提高。

Abstract: The effects of noise are one of the most important factors to consider when
it comes to quantum computing in the noisy intermediate-scale quantum computing
(NISQ) era that we are currently in. Therefore, it is important not only to
gain more knowledge about the noise sources appearing in current quantum
computing hardware in order to suppress and mitigate their contributions, but
also to evaluate whether a given quantum algorithm can achieve reasonable
results on a given hardware. To accomplish this, we need noise models that can
describe the real hardware with sufficient accuracy. Here, we present a noise
model that has been evaluated on superconducting hardware platforms and could
be adapted to other common architectures such as trapped-ion or neutral atom
devices. We then benchmark our model by simulating a 20-qubit superconducting
quantum computer, and compare the accuracy of our model to similar approaches
from the literature and demonstrate an improvement in the overall prediction
accuracy.

</details>


### [247] [Quantum circuit complexity and unsupervised machine learning of topological order](https://arxiv.org/abs/2508.04486)
*Yanming Che,Clemens Gneiting,Xiaoguang Wang,Franco Nori*

Main category: quant-ph

TL;DR: This paper connects quantum circuit complexity with machine learning for topological order. It introduces new methods (kernels) based on fidelity and entanglement to cluster quantum phases, showing they work well in practice and have connections to existing machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the relationship between Kolmogorov complexity and unsupervised machine learning, and aims to explore quantum circuit complexity as a means to understand and build interpretable and efficient unsupervised machine learning for topological order in quantum many-body systems.

Method: The paper presents two theorems connecting Nielsen's quantum circuit complexity with fidelity change and entanglement generation. These connections are used to formulate fidelity-based and entanglement-based similarity measures (kernels). Numerical experiments are conducted using these kernels for unsupervised clustering of quantum phases.

Result: Two theorems are presented that link quantum circuit complexity to fidelity change and entanglement generation. Fidelity-based and entanglement-based kernels are formulated and numerically tested on clustering quantum phases of the bond-alternating XXZ spin chain, Kitaev's toric code ground state, and random product states, showing superior performance. Relations with classical shadow tomography and shadow kernel learning are also discussed.

Conclusion: The study establishes connections between quantum circuit computation, quantum complexity, and machine learning of topological quantum order, demonstrating the effectiveness of proposed kernels in unsupervised clustering tasks.

Abstract: Inspired by the close relationship between Kolmogorov complexity and
unsupervised machine learning, we explore quantum circuit complexity, an
important concept in quantum computation and quantum information science, as a
pivot to understand and to build interpretable and efficient unsupervised
machine learning for topological order in quantum many-body systems. To span a
bridge from conceptual power to practical applicability, we present two
theorems that connect Nielsen's quantum circuit complexity for the quantum path
planning between two arbitrary quantum many-body states with fidelity change
and entanglement generation, respectively. Leveraging these connections,
fidelity-based and entanglement-based similarity measures or kernels, which are
more practical for implementation, are formulated. Using the two proposed
kernels, numerical experiments targeting the unsupervised clustering of quantum
phases of the bond-alternating XXZ spin chain, the ground state of Kitaev's
toric code and random product states, are conducted, demonstrating their
superior performance. Relations with classical shadow tomography and shadow
kernel learning are also discussed, where the latter can be naturally derived
and understood from our approach. Our results establish connections between key
concepts and tools of quantum circuit computation, quantum complexity, and
machine learning of topological quantum order.

</details>


### [248] [Benchmarking Quantum and Classical Sequential Models for Urban Telecommunication Forecasting](https://arxiv.org/abs/2508.04488)
*Chi-Sheng Chen,Samuel Yen-Chi Chen,Yun-Cheng Tsai*

Main category: quant-ph

TL;DR: 本研究比较了LSTM、QLSTM、QASA、QRWKV和QFWP在预测短信活动方面的性能，发现量子增强的有效性取决于具体任务和模型设计，而非普遍优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 由于数据完整性限制，我们仅关注每个空间网格单元的SMS-in信号。

Method: 本研究评估了经典模型和量子启发式序列模型在预测米兰电信活动数据集中单变量时间序列（SMS-in）方面的性能。我们比较了LSTM、QLSTM、QASA、QRWKV和QFWP五种模型，在不同的输入序列长度（4、8、12、16、32和64）下进行评估。所有模型都基于给定的序列窗口内的历史值来预测下一个10分钟的SMS-in值。

Result: 研究结果表明，不同的模型对序列长度表现出不同的敏感性，这表明量子增强并非普遍有利。

Conclusion: 量子增强的有效性高度依赖于具体任务和架构设计，这反映了模型大小、参数化策略和时间建模能力之间固有的权衡。

Abstract: In this study, we evaluate the performance of classical and quantum-inspired
sequential models in forecasting univariate time series of incoming SMS
activity (SMS-in) using the Milan Telecommunication Activity Dataset. Due to
data completeness limitations, we focus exclusively on the SMS-in signal for
each spatial grid cell. We compare five models, LSTM (baseline), Quantum LSTM
(QLSTM), Quantum Adaptive Self-Attention (QASA), Quantum Receptance Weighted
Key-Value (QRWKV), and Quantum Fast Weight Programmers (QFWP), under varying
input sequence lengths (4, 8, 12, 16, 32 and 64). All models are trained to
predict the next 10-minute SMS-in value based solely on historical values
within a given sequence window. Our findings indicate that different models
exhibit varying sensitivities to sequence length, suggesting that quantum
enhancements are not universally advantageous. Rather, the effectiveness of
quantum modules is highly dependent on the specific task and architectural
design, reflecting inherent trade-offs among model size, parameterization
strategies, and temporal modeling capabilities.

</details>


### [249] [Efficient classical computation of the neural tangent kernel of quantum neural networks](https://arxiv.org/abs/2508.04498)
*Anderson Melchor Hernandez,Davide Pastorello,Giacomo De Palma*

Main category: quant-ph

TL;DR: 提出了一种高效的经典算法，用于估计一类量子神经网络的 NTK，并证明了这些网络无法实现量子优势。


<details>
  <summary>Details</summary>
Motivation: 为了有效估计与一类量子神经网络相关的 NTK，并进一步分析宽量子神经网络的预期输出，最终证明其无法实现量子优势。

Method: 提出了一种利用关键洞察（NTK 定义中的初始化参数分布的平均值可以用 Clifford 操作的平均值精确替换）来有效估计与一类量子神经网络相关的 NTK 的经典算法。

Result: 通过利用参数化门是 Clifford 操作这一洞察，该算法能够有效地进行电路的经典模拟，并计算宽量子神经网络的预期输出。

Conclusion: 该方法证明了宽量子神经网络无法实现量子优势。

Abstract: We propose an efficient classical algorithm to estimate the Neural Tangent
Kernel (NTK) associated with a broad class of quantum neural networks. These
networks consist of arbitrary unitary operators belonging to the Clifford group
interleaved with parametric gates given by the time evolution generated by an
arbitrary Hamiltonian belonging to the Pauli group. The proposed algorithm
leverages a key insight: the average over the distribution of initialization
parameters in the NTK definition can be exactly replaced by an average over
just four discrete values, chosen such that the corresponding parametric gates
are Clifford operations. This reduction enables an efficient classical
simulation of the circuit. Combined with recent results establishing the
equivalence between wide quantum neural networks and Gaussian processes
[Girardi \emph{et al.}, Comm. Math. Phys. 406, 92 (2025); Melchor Hernandez
\emph{et al.}, arXiv:2412.03182], our method enables efficient computation of
the expected output of wide, trained quantum neural networks, and therefore
shows that such networks cannot achieve quantum advantage.

</details>


### [250] [Efficient detection of spectrally multimode squeezed light through optical parametric amplification](https://arxiv.org/abs/2508.04502)
*Mahmoud Kalash,Ui-Nyung Han,Young-Sik Ra,Maria V. Chekhova*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multimode squeezed light is a key resource for high-dimensional photonic
quantum technologies, enabling applications in quantum-enhanced sensing,
quantum communication, and quantum computing. Efficient detection of such a
multimode squeezed state is essential for unlocking its full potential. Optical
parametric amplification (OPA) has recently gained attention as a powerful
technique offering loss-tolerant, direct broadband detection, and multimode
operation. While OPA has been used to characterize spatially multimode
squeezing, its application to spectrally multimode squeezing has not yet been
demonstrated. Here, we report on the first experimental demonstration of
spectrally multimode squeezing detection using OPA. We achieve simultaneous
detection of squeezing across more than 60 spectral modes of a broadband
squeezed vacuum state. The observed squeezing is nearly uniform, ranging from
-6.5 to -7 dB, which makes the source particularly suitable for constructing
continuous-variable cluster states, and indicates the multimode capability of
the OPA. The results extend the capabilities of OPA detection into the spectral
domain, advancing spectral-mode-based high-dimensional photonic quantum
technologies.

</details>


### [251] [Entanglement distribution in quantum networks via swapping of partially entangled states](https://arxiv.org/abs/2508.04536)
*Henrique Guerra,Tailan S. Sarubi,Rafael Chaves,Jonas Maziero*

Main category: quant-ph

TL;DR: 本研究将纠缠交换协议（ESP）应用于部分纠缠的量子网络，以在不同拓扑结构中分发量子纠缠，并为稳健的量子通信提供指导。


<details>
  <summary>Details</summary>
Motivation: 为了在量子网络中跨越远程节点分发量子相关性，即使涉及的量子比特对仅部分纠缠，也可以通过贝尔基测量来集中和传输纠缠。

Method: 通过分析将纠缠交换协议（ESP）应用于初始部分纠缠态，并检查初始状态的变换和评估生成最大纠缠态的成功概率来研究纠缠如何演化。

Result: 研究了ESP在具有线性、星形和混合配置的各种拓扑结构的量子网络中的应用，并评估了生成最大纠缠态的成功概率。

Conclusion: 该研究为在现实条件下设计稳健的量子通信策略提供了实际指导，并为量子网络中纠缠分布的动力学提供了新的见解。

Abstract: The entanglement swapping protocol (ESP) is a fundamental primitive for
distributing quantum correlations across distant nodes in a quantum network.
Recent studies have demonstrated that even when the involved qubit pairs are
only partially entangled, it is still possible to concentrate and transmit
entanglement via Bell-basis measurements. In this work, we extend these ideas
to quantum networks with various topologies - including linear, star, and
hybrid configurations - by analyzing the application of the ESP to initially
partially entangled states. We investigate how entanglement evolves under such
protocols by examining the transformations of the initial states and evaluating
the success probabilities for generating maximally entangled states at the
output. Our results offer new insights into the dynamics of the entanglement
distribution in quantum networks and provide practical guidelines for designing
robust quantum communication strategies under realistic conditions.

</details>


### [252] [Spectro-temporally tailored Non-Gaussian Quantum Operations in Thin-Film Waveguides](https://arxiv.org/abs/2508.04578)
*Peter Namdar,Patrick Folge,Carlos E. Lopetegui,Silia Babel,Benjamin Brecht,Christine Silberhorn,Valentina Parigi*

Main category: quant-ph

TL;DR: 该研究利用薄膜铌酸锂波导平台，通过逆向设计优化，实现了电信波长范围内高保真度的单光子减法和加法操作，为量子光子网络提供了关键技术。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展的量子信息系统，需要在电信波长范围内精确控制光的谱和时域自由度，并实现模式选择性的非高斯量子操作，如单光子减法（SPS）和加法（SPA）。

Method: 研究引入了一种逆向设计优化方案，通过联合光谱幅度（Joint Spectral Amplitude）和传输函数（Transfer Function）对量子光学响应进行建模，以优化波导和泵浦参数，最大化模式选择性和状态保真度。该方法首先在金属波导设计上进行测试，然后利用薄膜波导的色散工程能力来增强非线性相互作用。

Result: 研究表明，优化的薄膜铌酸锂波导平台能够支持高保真度的非高斯操作，特别是通过定制的非线性过程（如参量下转换和频率上转换），这对于下一代量子光子网络至关重要。

Conclusion: 该研究为在电信波长范围内实现光谱-时间模式选择的非高斯量子操作（特别是单光子减法和加法）提供了首个设计框架，利用了薄膜铌酸锂非线性波导平台。

Abstract: Advancements in photonic platforms have enabled the precise control of
light's spectral and temporal degrees of freedom, a capability crucial for the
development of scalable quantum information systems. In this work, we address
the challenge of implementing spectro-temporal mode-selective non-Gaussian
quantum operations, specifically single-photon subtraction (SPS) and addition
(SPA), in the telecom wavelength regime. Building on prior experimental
demonstrations of mode-selective near-infrared SPS, we present the first design
framework for achieving mode-selective SPA and SPS using thin-film lithium
niobate nonlinear waveguide platforms. We introduce an inverse-design
optimization scheme by modeling the quantum-optical response via the Joint
Spectral Amplitude and Transfer Function, in order to identify optimal
waveguide and pump parameters that maximize mode selectivity and state purity.
This approach is first tested on a metallic waveguide design. We then exploit
the dispersion engineering capabilities of thin-film waveguides, which offer
enhanced nonlinear interactions through tighter light confinement. Our findings
demonstrate that tailored nonlinear processes, particularly parametric
down-conversion and frequency up-conversion, can support high-fidelity
non-Gaussian operations essential for next-generation quantum photonic
networks.

</details>


### [253] [Optimizing quantum transport via the quantum Doob transform](https://arxiv.org/abs/2508.04622)
*Dolores Esteve,Carlos Pérez-Espigares,Ricardo Gutiérrez,Daniel Manzano*

Main category: quant-ph

TL;DR: 一种新的量子网络优化方法，通过修改相干和非相干动力学来提高传输效率，并发现优化传输与中心对称性相关。


<details>
  <summary>Details</summary>
Motivation: 在理解量子系统传输现象方面已取得重大进展，但优化传输特性的方法仍然有限，尤其是在复杂量子网络中。

Method: 提出了一种新的方法，将经典的广义杜布变换推广到量子网络，通过对系统生成器进行单次对角化来优化哈密顿量和耗散贡献，从而优化电流和活动等传输可观测值。

Result: 通过广泛的数值探索证明了该方法的有效性，表明最佳性能源于相干和非相干动力学的非平凡修改，并评估了在保持特定物理特性（如固定的耗散结构和输入-输出相互作用）的约束下的优化鲁棒性。

Conclusion: 优化的量子传输与中心对称性相关，中心对称性是提高量子系统传输效率相关性的相关性。

Abstract: Quantum transport plays a central role in both fundamental physics and the
development of quantum technologies. While significant progress has been made
in understanding transport phenomena in quantum systems, methods for optimizing
transport properties remain limited, particularly in complex quantum networks.
Building on recent advances in classical network optimization via the
generalized Doob transform, we introduce a novel method that extends this
approach to quantum networks. Our framework leverages a single diagonalization
of the system generator to efficiently tailor both the Hamiltonian and
dissipative contributions, optimizing transport observables such as currents
and activities. We demonstrate the method's effectiveness through extensive
numerical explorations, showing that optimal performance arises from
non-trivial modifications to both coherent and incoherent dynamics. We also
assess the robustness of the optimization under constraints that preserve
specific physical features, such as fixed dissipative structures and
input-output interactions. Finally, we discuss the connection between optimized
transport and centrosymmetry, highlighting the relevance of this property for
enhanced transport efficiency in quantum systems.

</details>


### [254] [Experimental device-independent certification of indefinite causal order](https://arxiv.org/abs/2508.04643)
*Dengke Qu,Quan Lin,Lei Xiao,Xiang Zhan,Peng Xue*

Main category: quant-ph

TL;DR: 本研究通过纠缠光子实验，在无需设备表征的情况下，仅凭测量结果就验证了量子因果顺序的不确定性，违反了因果不等式。


<details>
  <summary>Details</summary>
Motivation: 理解物理世界依赖于因果关系有时间顺序的假设，但量子力学允许因果顺序的叠加，这可以为量子信息任务提供新型量子资源。

Method: 通过类贝尔不等式检验的非局域性证明，仅从设备输出统计数据认证不确定因果顺序。实验上，使用类空间分隔的纠缠光子，其中一个光子作为量子开关中的控制量子比特，另一个作为额外的观测者，来验证因果不等式。

Result: 通过局部测量统计数据，我们观察到因果不等式被违反了24个标准差。

Conclusion: 本工作为不确定因果顺序提供了设备无关的认证证据，仅依赖于观测到的相关性，无需设备表征。我们的结果为不确定因果顺序的完整理解及其在量子信息处理中的潜在应用铺平了道路。

Abstract: Understanding the physical world fundamentally relies on the assumption that
events are temporally ordered, with past events serving as causes for future
ones. However, quantum mechanics permits events to occur in a superposition of
causal orders, providing new types of quantum resources for quantum information
tasks. Previous demonstrations of indefinite causal order have relied on a
process known as quantum switch and depended on specific assumptions about the
devices used in the laboratory. Recently, a theoretical scheme for the
certification of indefinite causal order in the quantum switch has been
obtained solely from the output statistics of the devices, analogous to the
device-independent proofs of nonlocality through violations of the Bell
inequality. Here, we report an experimental verification of the causal
inequality using spacelike-separated entangled photons, where one photon
functions as the control qubit in a quantum switch and the other serves as an
additional observer. Through local measurement statistics, we observe a
violation of the causal inequality by 24 standard deviations. This work
provides evidence for a device-independent certification of indefinite causal
order, relying solely on observed correlations without requiring device
characterization. Our results pave the way toward a complete understanding of
indefinite causal order and its potential applications in quantum information
processing.

</details>


### [255] [Cybersecurity of Quantum Key Distribution Implementations](https://arxiv.org/abs/2508.04669)
*Ittay Alfassi,Ran Gelles,Rotem Liss,Tal Mor*

Main category: quant-ph

TL;DR: 本研究将经典网络安全概念应用于QKD攻击分析，提出“量子模糊测试”等新工具，并证明“明亮照明”攻击可在信息有限情况下构建，旨在提升QKD产品的实际安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上量子密钥分发（QKD）协议被证明是安全的，但其实际应用往往偏离理论协议，使得这些实现容易受到各种攻击。因此，需要新的分析工具和方法学来解决QKD实现中的安全漏洞，以弥合理论安全与实际部署之间的差距。

Method: 本研究将经典网络安全中的漏洞、攻击面和利用等概念适配到QKD实现攻击分析中，并提出了三种新的概念和工具：“量子模糊测试”（首个用于QKD实现黑盒漏洞研究的工具）、“反向空间攻击”（一种利用不完美接收器攻击面的通用利用方法）以及“量子侧信道攻击”的明确量子力学定义。利用这些工具，研究分析了现有的QKD攻击，并证明了“明亮照明”攻击可以在仅了解很少设备实现信息的情况下被完全构建。

Result: 通过引入经典网络安全的概念和新的分析工具，本研究能够分析现有的QKD攻击，并证明“明亮照明”攻击可以在仅了解很少设备实现信息的情况下被完全构建。这表明，即使在信息有限的情况下，QKD实现也可能存在严重的漏洞。

Conclusion: 本研究通过引入经典网络安全的概念，如漏洞、攻击面和利用，并开发了诸如“量子模糊测试”、“反向空间攻击”和“量子侧信道攻击”等新颖的分析工具和方法学，为评估和增强量子密钥分发（QKD）实现的实际安全性提供了新的途径。研究表明，“明亮照明”攻击可以在对设备实现知之甚少的情况下被完全构建，这突显了对QKD实现进行彻底安全分析的必要性。这项工作旨在弥合当前QKD实验攻击分析方法与经典网络安全领域数十年研究之间的差距，从而提高QKD产品的实际安全性和在现实系统中的实用性。

Abstract: Practical implementations of Quantum Key Distribution (QKD) often deviate
from the theoretical protocols, exposing the implementations to various attacks
even when the underlying (ideal) protocol is proven secure. We present new
analysis tools and methodologies for quantum cybersecurity, adapting the
concepts of vulnerabilities, attack surfaces, and exploits from classical
cybersecurity to QKD implementation attacks. We present three additional
concepts, derived from the connection between classical and quantum
cybersecurity: "Quantum Fuzzing", which is the first tool for black-box
vulnerability research on QKD implementations; "Reversed-Space Attacks", which
are a generic exploit method using the attack surface of imperfect receivers;
and a concrete quantum-mechanical definition of "Quantum Side-Channel Attacks",
meaningfully distinguishing them from other types of attacks. Using our tools,
we analyze multiple existing QKD attacks and show that the "Bright
Illumination" attack could have been fully constructed even with minimal
knowledge of the device implementation. This work begins to bridge the gap
between current analysis methods for experimental attacks on QKD
implementations and the decades-long research in the field of classical
cybersecurity, improving the practical security of QKD products and enhancing
their usefulness in real-world systems.

</details>


### [256] [A probabilistic quantum algorithm for Lyapunov equations and matrix inversion](https://arxiv.org/abs/2508.04689)
*Marcello Benedetti,Ansis Rosmanis,Matthias Rosenkranz*

Main category: quant-ph

TL;DR: A probabilistic quantum algorithm prepares mixed states related to Lyapunov equations, offering efficiency and optimal query complexity for matrix inversion approximation.


<details>
  <summary>Details</summary>
Motivation: The paper presents a quantum algorithm for preparing mixed states proportional to solutions of Lyapunov equations, which are crucial for analyzing dynamical systems.

Method: The algorithm uses a probabilistic approach involving trace non-increasing completely positive maps and ancilla measurements, with a deterministic stopping rule to ensure efficiency and a bounded expected number of calls to block-encoding and state preparation circuits. It builds upon previous work by Zhang et al.

Result: The algorithm achieves an efficient process with a bounded expected number of calls to the relevant circuits. For approximating the normalized inverse of a positive definite matrix, it matches the optimal query complexity in terms of condition number and error.

Conclusion: This algorithm generates mixed states that approximate matrix-weighted sums and integrals, with a specific case matching optimal query complexity for approximating the inverse of a positive definite matrix.

Abstract: We present a probabilistic quantum algorithm for preparing mixed states
which, in expectation, are proportional to the solutions of Lyapunov equations
-- linear matrix equations ubiquitous in the analysis of classical and quantum
dynamical systems. Building on previous results by Zhang et al.,
arXiv:2304.04526, at each step the algorithm either returns the current state,
applies a trace non-increasing completely positive map, or restarts depending
on the outcomes of a biased coin flip and an ancilla measurement. We introduce
a deterministic stopping rule which leads to an efficient algorithm with a
bounded expected number of calls to a block-encoding and a state preparation
circuit representing the two input matrices of the Lyapunov equations. We also
consider approximating the normalized inverse of a positive definite matrix $A$
with condition number $\kappa$ up to trace distance error $\epsilon$. For this
special case the algorithm requires, in expectation, at most $\lceil
\kappa\ln(1/\epsilon) \rceil+1$ calls to a block-encoding of $\sqrt{A/\|A\|}$.
This matches the optimal query complexity in $\kappa$ and $\epsilon$ of the
related, but distinct, quantum linear system solvers. In its most general form,
the algorithm generates mixed states which approximate matrix-valued weighted
sums and integrals.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [257] [Control Closure Certificates](https://arxiv.org/abs/2508.03947)
*Vishnu Murali,Mohammed Adib Oumer,Majid Zamani*

Main category: cs.LO

TL;DR: 本研究提出了一种新的控制闭包证书方法，用于为离散时间控制系统合成满足 $\omega$- 规则规范的控制器。该方法通过结合析取良基性论证和平方和优化技术，实现了有效的控制器设计和验证。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是为离散时间控制系统合成控制器，以满足 $\omega$- 规则规范。现有的方法通常依赖于归纳不变量和良基性证明，但存在局限性。因此，研究人员提出了一种新的方法，即控制闭包证书，作为一种有效的、自动化的方法来验证离散时间动力学系统，并在此基础上合成控制器。

Method: 文章提出了一种使用控制闭包证书来合成离散时间控制系统的控制器的方法。该方法通过结合归纳不变量（如屏障证书）和良基性证明（如排序函数）来实现。作为一种替代方法，转移不变量允许使用析取良基性论证来确保良基性。文章进一步提出控制闭包证书是转移不变量的函数类似物，为验证离散时间动力学系统提供了自动化的方法。研究人员通过构造控制闭包证书来确保访问一个区域无限次（或有限次），并结合这些论证来处理奇偶校验规范。最后，通过对系统和指定期望 $\omega$- 规则规范的奇偶校验自动机的乘积进行控制闭包证书的搜索，来确保存在一个控制器 $\kappa$ 来强制执行该规范。此外，研究人员还提出了一种平方和优化方法来合成这些证书，并通过案例研究证明了其有效性。

Result: 研究结果表明，通过使用控制闭包证书和析取良基性论证，可以成功合成满足 $\omega$- 规则规范的控制器。平方和优化方法被证明是合成这些证书的有效途径，并在案例研究中得到了验证。

Conclusion: 该研究提出了一种结合控制闭包证书和析取适定性论证来合成离散时间控制系统的控制器，以满足 $\omega$- 规则规范。

Abstract: This paper introduces the notion of control closure certificates to
synthesize controllers for discrete-time control systems against
$\omega$-regular specifications. Typical functional approaches to synthesize
controllers against $\omega$-regular specifications rely on combining inductive
invariants (for example, via barrier certificates) with proofs of
well-foundedness (for example, via ranking functions). Transition invariants,
provide an alternative where instead of standard well-foundedness arguments one
may instead search for disjunctive well-foundedness arguments that together
ensure a well-foundedness argument. Closure certificates, functional analogs of
transition invariants, provide an effective, automated approach to verify
discrete-time dynamical systems against linear temporal logic and
$\omega$-regular specifications. We build on this notion to synthesize
controllers to ensure the satisfaction of $\omega$-regular specifications. To
do so, we first illustrate how one may construct control closure certificates
to visit a region infinitely often (or only finitely often) via disjunctive
well-founded arguments. We then combine these arguments to provide an argument
for parity specifications. Thus, finding an appropriate control closure
certificate over the product of the system and a parity automaton specifying a
desired $\omega$-regular specification ensures that there exists a controller
$\kappa$ to enforce the $\omega$-regular specification. We propose a
sum-of-squares optimization approach to synthesize such certificates and
demonstrate their efficacy in designing controllers over some case studies.

</details>


### [258] [GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning](https://arxiv.org/abs/2508.04438)
*Mark Chevallier,Filip Smola,Richard Schmoetten,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: GradSTL是首个全面的STL实现，可与神经符号学习集成，自动生成并保证正确性，使其能在任意采样信号上评估STL约束。


<details>
  <summary>Details</summary>
Motivation: 为将信号时序逻辑（STL）与神经符号学习相结合，并解决STL约束在任意采样信号上的评估问题。

Method: GradSTL是一个为张量提供平滑STL语义的形式化验证方法，其导数函数具有可证明的正确性，并且该实现是通过自动生成而非手动编码来保证正确性的。

Result: 通过一个案例研究表明，使用GradSTL的神经符号过程能够学习满足预先指定的STL约束，证明了其有效性。

Conclusion: GradSTL为信号时序逻辑（STL）与神经符号学习的结合提供了一个严谨的基础，并保证了其作为首个全面实现的STL的正确性。

Abstract: We present GradSTL, the first fully comprehensive implementation of signal
temporal logic (STL) suitable for integration with neurosymbolic learning. In
particular, GradSTL can successfully evaluate any STL constraint over any
signal, regardless of how it is sampled. Our formally verified approach
specifies smooth STL semantics over tensors, with formal proofs of soundness
and of correctness of its derivative function. Our implementation is generated
automatically from this formalisation, without manual coding, guaranteeing
correctness by construction. We show via a case study that using our
implementation, a neurosymbolic process learns to satisfy a pre-specified STL
constraint. Our approach offers a highly rigorous foundation for integrating
signal temporal logic and learning by gradient descent.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [259] [Think Before You Segment: An Object-aware Reasoning Agent for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2508.04418)
*Jinxing Zhou,Yanghao Zhou,Mingfei Han,Tong Wang,Xiaojun Chang,Hisham Cholakkal,Rao Muhammad Anwer*

Main category: cs.MM

TL;DR: TGS-Agent 通过模仿人类的“思考-定位-分割”过程，利用多模态语言模型（Ref-Thinker）进行显式引用理解，并结合 Grounding-DINO 和 SAM2 实现无需像素级监督的音频-视觉分割，并在新的基准测试 R	extsuperscript{2}-AVSBench 上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于通过多模态融合学习潜在嵌入，并使用可调的 SAM/SAM2 解码器进行分割，这种方法需要强像素级监督且缺乏可解释性。本研究旨在从显式引用理解的新颖视角出发，通过模仿人类推理过程来解决这些问题。

Method: 提出了一种新颖的 TGS-Agent 方法，将任务分解为 Think-Ground-Segment 过程。该方法首先使用 Ref-Thinker（一个多模态语言模型）通过文本、视觉和听觉线索进行推理，识别并生成目标对象的描述。然后，该描述作为显式提示，用于 Grounding-DINO 和 SAM2 模型，以进行无需像素级监督的粗粒度定位和精确分割。此外，还引入了一个名为 R	extsuperscript{2}-AVSBench 的新基准，用于评估模型的泛化能力。

Result: TGS-Agent 在标准 Ref-AVSBench 和提出的 R	extsuperscript{2}-AVSBench 上均取得了最先进的结果。

Conclusion: TGS-Agent 实现了 Referring Audio-Visual Segmentation (Ref-AVS) 任务的最先进结果，并在标准 Ref-AVSBench 和新的 R	extsuperscript{2}-AVSBench 上均表现优异。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment target objects
in audible videos based on given reference expressions. Prior works typically
rely on learning latent embeddings via multimodal fusion to prompt a tunable
SAM/SAM2 decoder for segmentation, which requires strong pixel-level
supervision and lacks interpretability. From a novel perspective of explicit
reference understanding, we propose TGS-Agent, which decomposes the task into a
Think-Ground-Segment process, mimicking the human reasoning procedure by first
identifying the referred object through multimodal analysis, followed by
coarse-grained grounding and precise segmentation. To this end, we first
propose Ref-Thinker, a multimodal language model capable of reasoning over
textual, visual, and auditory cues. We construct an instruction-tuning dataset
with explicit object-aware think-answer chains for Ref-Thinker fine-tuning. The
object description inferred by Ref-Thinker is used as an explicit prompt for
Grounding-DINO and SAM2, which perform grounding and segmentation without
relying on pixel-level supervision. Additionally, we introduce
R\textsuperscript{2}-AVSBench, a new benchmark with linguistically diverse and
reasoning-intensive references for better evaluating model generalization. Our
approach achieves state-of-the-art results on both standard Ref-AVSBench and
proposed R\textsuperscript{2}-AVSBench. Code will be available at
https://github.com/jasongief/TGS-Agent.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [260] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: FlashCommunication V2是一种新的通信模式，可以高效地跨GPU传输任意比特宽度的数据，通过位拆分和脉冲保留技术解决了低比特量化问题，并在NVLink和PCIe架构上实现了显著的通信加速。


<details>
  <summary>Details</summary>
Motivation: 当前分布式训练和部署大型语言模型面临通信瓶颈的挑战。

Method: 提出位拆分和脉冲保留技术，解决了低比特量化中的挑战。位拆分将不规则位宽分解为基本单元，确保了硬件兼容性；脉冲保留则保留了数值异常值，缩小了动态范围，将量化限制推至2比特。

Result: FlashCommunication V2在AllReduce通信中实现了最高3.2倍的加速，在All2All通信中实现了2倍的加速。

Conclusion: FlashCommunication V2通过位拆分和脉冲保留技术，实现了在任意位宽下的高效跨GPU通信，显著提升了通信系统的灵活性和资源利用率。该系统在NVLink和PCIe架构上均表现出色，分别实现了最高3.2倍和2倍的通信加速。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [261] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 针对 DLRM 训练中的嵌入表并行化挑战，提出了一种二维稀疏并行方法，结合数据并行和模型并行，并辅以动量缩放的逐行 AdaGrad 算法，显著提升了训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型（DLRM）的复杂性不断增加，需要能够高效训练海量数据的大规模分布式系统。嵌入表是 DLRM 的关键组成部分，但传统的并行策略面临可扩展性挑战，包括不平衡、数据查找通信密集以及嵌入激活内存开销大。

Method: 提出了一种新颖的二维稀疏并行方法，在模型并行之上引入数据并行，实现了高效的 all-to-all 通信并减少了峰值内存消耗。此外，开发了动量缩放的逐行 AdaGrad 算法来减轻与训练范式转变相关的性能损失。

Result: 实验证明，所提出的方法显著提高了训练效率，同时保持了模型性能不变，并在最多4K GPU上实现了近乎线性的训练速度扩展，设定了新的推荐模型训练最先进基准。

Conclusion: 所提出的方法显著提高了训练效率，同时保持了模型性能不变，并在最多4K GPU上实现了近乎线性的训练速度扩展，为推荐模型训练设定了新的最先进基准。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [262] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: 本文提出了一种名为 RSPC 的创新性分区方案，用于解决众包感知中的安全和可扩展性问题。该方案通过结合节点声誉值进行分区，并采用四阶段确认协议处理跨分区事务，实验证明 RSPC 能有效提高系统的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决众包感知平台安全和隐私保护的挑战，以及中心化管理带来的安全漏洞和可扩展性问题，本文提出了一种新的解决方案。

Method: 提出了一种基于声誉的有效分区方案（RSPC），该方案通过结合节点声誉值来计算最优分区大小，并根据节点声誉值将节点划分为几个不相交的分区。RSPC 通过选择合适的分区大小，只要遵循了对失败节点的允许最大阈值，就可以确保每个分区都有效。同时，RSPC 定期重组网络以避免分区攻击。此外，针对跨分区事务，提出了一种四阶段确认协议，以确保跨分区事务的高效和安全完成。

Result: RSPC 提高了可扩展性、降低了延迟并且提高了吞吐量。

Conclusion: 实验表明，RSPC 提高了可扩展性、降低了延迟并且提高了众包感知系统的吞吐量。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [263] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 提出了一种新的仿真方法，利用低精度矩阵引擎加速和提高单精度/双精度矩阵乘法的功耗效率，在GH200 Grace Hopper超级芯片上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 为了在深度学习至关重要的低精度矩阵乘法中，利用现有高性能低精度矩阵引擎的能力，实现单精度和双精度通用矩阵-矩阵乘法的仿真。

Method: 提出仿真单精度和双精度通用矩阵-矩阵乘法（SGEMM和DGEMM）的方法，利用高性能、低功耗的矩阵引擎。

Result: 在GH200 Grace Hopper超级芯片上，提出的DGEMM仿真比原生DGEMM有1.4倍的加速和43%的功耗效率提升；提出的SGEMM仿真比原生SGEMM有3.0倍的加速和154%的功耗效率提升。与传统仿真方法相比，提出的仿真方法性能提高2倍以上，功耗效率更优。

Conclusion: 该研究提出的低精度矩阵引擎仿真方法在GH200 Grace Hopper超级芯片上显著优于传统方法，在处理大型问题时，DGEMM仿真实现了1.4倍的速度提升和43%的功耗效率提升，SGEMM仿真实现了3.0倍的速度提升和154%的功耗效率提升，并且在性能和功耗效率方面均优于现有仿真方法。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [264] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: ADR协议通过基于DAG的结构和排名算法，解决了现有区块链的可扩展性和吞吐量问题，特别适用于物联网场景。


<details>
  <summary>Details</summary>
Motivation: 当前区块链系统存在吞吐量有限、可扩展性差和延迟高等问题，尤其是在节点身份管理方面，这使得区块链不适用于物联网等应用。本研究旨在提出ADR协议来增强区块链的可扩展性和吞吐量。

Method: ADR协议采用基于有向无环图（DAG）的结构，根据节点排名对其进行定位。与传统链不同，ADR允许诚实节点使用基于DAG的拓扑来写入区块和验证交易。该协议通过三个步骤来保护网络免遭双重支付并提高性能：1. 使用公钥和私钥验证节点。2. 构建高级DAG分类账，实现区块生产和交易验证。3. 通过排名算法剔除恶意节点，并根据性能对剩余节点进行排名和拓扑排序。

Result: 与IOTA和ByteBall等现有的基于DAG的区块链相比，ADR在交易吞吐量和网络活性方面表现出显著的提升。

Conclusion: ADR协议显著提高了交易吞吐量和网络活性，使其非常适合物联网应用。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [265] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 统计计算（SC）社区在高性能计算（HPC）领域参与度低。本文旨在弥合这一差距，提出融合SC和HPC社区的策略，以促进高性能统计计算（HPSC）的发展。


<details>
  <summary>Details</summary>
Motivation: 为了解决统计计算（SC）社区在高性能计算（HPC）领域存在差距的问题，本文旨在推动SC社区与HPC社区的融合，以应对大数据和大规模计算的挑战。

Method: 本文通过阐述SC社区的历史、HPSC的愿景、面临的挑战和机遇，以及未来的发展路线图，来论证SC和HPC社区融合的必要性和可行性。

Result: 本文探讨了SC社区在HPC领域的现状、挑战与机遇，并提出了融合两个社区的愿景和发展策略，以促进HPSC的发展。

Conclusion: HPSC社区的未来在于连接SC和HPC社区，克服技术和社区挑战，抓住机遇，并制定发展路线图。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [266] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: SelectiveShield是一种新的联邦学习混合防御框架，通过选择性地应用同态加密和差分隐私来平衡隐私和模型效用，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习防御机制（如差分隐私和同态加密）在隐私、模型效用和系统开销之间存在权衡，在非独立同分布数据和不同客户端能力的异构环境中这种挑战尤为严峻。本研究提出SelectiveShield以解决这些限制。

Method: SelectiveShield利用Fisher信息量来量化参数的敏感性，使客户端能够本地识别关键参数。通过协作协商协议，客户端就用于同态加密保护的最敏感参数共享集达成一致。对个体客户端独有重要性的参数被保留在本地以促进个性化，而非关键参数则通过自适应差分隐私噪声进行保护。

Result: 实验证明，SelectiveShield在显著降低梯度泄露风险的同时，保持了强大的模型效用，提供了一种实用的、可扩展的防御机制。

Conclusion: SelectiveShield是一个轻量级的混合防御框架，能够自适应地集成选择性的同态加密和差分隐私，在保持强大模型效用的同时，显著降低梯度泄露风险，为现实世界的联邦学习部署提供了一种实用且可扩展的防御机制。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [267] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: S2M3是一种用于边缘设备的分割和共享多模态架构，通过模块共享和智能放置来降低资源消耗并提高推理效率，解决了云端AI的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态AI服务过度依赖云端带来的带宽、延迟、隐私和可用性问题，以及边缘设备支持多任务的资源挑战。

Method: 提出了一种名为S2M3的分割和共享多模态架构，通过在功能级模块上分割多模态模型，并共享通用模块来实现跨任务的重用，从而减少资源消耗。

Result: 实验证明，S2M3在内存使用量、准确性和推理延迟方面均表现出色，在95个实例中的89个（93.7%）实现了最优放置，显著优于云端AI。

Conclusion: S2M3通过模块级贪心放置和请求级并行路由，成功解决了跨模型依赖问题，在资源受限设备上将内存使用量减少了高达50%（单任务）和62%（多任务），并将推理延迟减少了高达56.9%，同时保持了准确性。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [268] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 该研究提出了一个优化框架，通过结合Vessim和SAM模型，评估数据中心微电网的组成如何影响可持续性和可靠性，并考虑运行和构成排放。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估微电网组件的大小和组成如何影响长期可持续性和电力可靠性的工具，尽管数据中心越来越多地采用与微电网相结合的可再生能源和储能。

Method: 本研究提出了一种新颖的优化框架，该框架扩展了Vessim（一个计算和能源系统协同模拟器），并集成了美国国家可再生能源实验室（NREL）的系统顾问模型（SAM）的详细可再生能源发电模型。研究采用了多时段黑盒优化方法来探索高效的微电网组成。

Result: 该框架能够模拟计算工作负载、现场可再生能源生产和储能之间的相互作用，同时考虑运行和构成排放。通过多时段黑盒优化，可以探索高效的微电网组成，使运营商能在规划数据中心能源系统时做出更明智的决策。

Conclusion: 该研究提出了一种新颖的优化框架，用于模拟数据中心微电网中计算工作负载、可再生能源生产和储能之间的相互作用，并捕获运行和构成排放。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [269] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: A new hybrid scheduling algorithm using deep RL and ACO improves IoT data scheduling in the cloud by reducing response time and energy consumption while enhancing resource utilization and ensuring service agreements.


<details>
  <summary>Details</summary>
Motivation: The increasing number of IoT devices generates massive, heterogeneous data streams, requiring scalable and efficient cloud scheduling to meet latency, energy, and QoS demands. Existing methods struggle with the dynamic nature of IoT-cloud systems.

Method: A hybrid scheduling algorithm combining deep Reinforcement Learning (RL) and Ant Colony Optimization (ACO) is proposed. The deep RL agent uses a model-free policy-gradient approach for adaptive task allocation, while ACO performs a global search to optimize resource distribution and mitigate congestion.

Result: The proposed method achieved up to 18.4% reduction in average response time, 12.7% improvement in resource utilization, and 9.3% decrease in energy consumption compared to existing methods. It also ensures SLA compliance through deadline-aware scheduling and dynamic prioritization.

Conclusion: The study demonstrates that integrating model-free RL with swarm intelligence provides a scalable and energy-efficient solution for IoT data scheduling, presenting a promising approach for future IoT-cloud platforms.

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>


### [270] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: EPUS算法通过在边缘节点进行数据预处理，减少了传输到云的数据量和延迟，在高维和低维数据上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着万物互联（IoE）的普及，生成的数据量急剧增加，导致将所有数据传输到云进行分析的成本增加，并且需要更多的云资源。为了解决这个问题，需要一种更有效的数据处理方法。

Method: 提出了一种名为 EPUS（Edge-assisted Parallel Uncertain Skyline）的算法，该算法利用天空线候选集的概念在并行边缘计算节点上进行数据修剪，以减少传输到云的数据量。

Result: EPUS算法在处理二维数据时，与两种对比方法相比，处理延迟降低了50%以上。对于高维数据，EPUS方法也优于其他现有方法。

Conclusion: EPUS算法通过使用天空线候选集在并行边缘计算节点上修剪不太可能成为天空线数据的能力，从而在数据传输和云资源需求方面都取得了显著的改进。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [271] [GP and LLMs for Program Synthesis: No Clear Winners](https://arxiv.org/abs/2508.03966)
*Jose Guadalupe Hernandez,Anil Kumar Saini,Gabriel Ketron,Jason H. Moore*

Main category: cs.NE

TL;DR: 遗传编程（GP）和大型语言模型（LLM）在程序合成方法上有所不同。本研究比较了PushGP和GPT-4o在PSB2基准测试中的表现。结合使用PushGP和GPT-4o（数据-文本提示）效果最佳，解决了23/25项任务。不同提示方式和训练集大小影响了性能，且合成程序的相似性也存在差异，这表明优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 在程序合成领域，遗传编程（GP）使用输入-输出示例，而大型语言模型（LLM）使用文本描述。本研究旨在比较这两种方法的有效性。

Method: 比较了PushGP和GPT-4o在PSB2基准测试套件任务中的程序合成能力。对GPT-4o使用了三种提示变体：仅输入-输出示例（数据）、仅文本描述（文本）以及两者结合（数据-文本）。还改变了输入-输出示例的数量。比较了成功率和成功合成程序的相似性。

Result: PushGP和GPT-4o结合数据-文本提示解决了23/25项任务。仅数据和仅文本提示解决了更少任务，且在训练集减小时性能下降（对PushGP和GPT-4o而言）。不同提示策略下，GPT-4o合成程序的相似性存在显著差异。不同的优化技术很重要。

Conclusion: PushGP与GPT-4o结合数据-文本提示在PSB2基准测试集中解决了最多的任务（25项中的23项），尽管有些任务仅由其中一个合成器解决。PushGP和GPT-4o（仅数据提示）随着训练集大小的减小解决了更少任务，而其余合成器未受影响。GPT-4o（仅文本提示）和GPT-4o（仅数据提示）在成功合成的程序之间存在显著的相似性差异。没有占主导地位的程序合成器，这表明PushGP和LLM在程序合成中使用了不同的优化技术。

Abstract: Genetic programming (GP) and large language models (LLMs) differ in how
program specifications are provided: GP uses input-output examples, and LLMs
use text descriptions. In this work, we compared the ability of PushGP and
GPT-4o to synthesize computer programs for tasks from the PSB2 benchmark suite.
We used three prompt variants with GPT-4o: input-output examples (data-only),
textual description of the task (text-only), and a combination of both textual
descriptions and input-output examples (data-text). Additionally, we varied the
number of input-output examples available for building programs. For each
synthesizer and task combination, we compared success rates across all program
synthesizers, as well as the similarity between successful GPT-4o synthesized
programs. We found that the combination of PushGP and GPT-4o with data-text
prompting led to the greatest number of tasks solved (23 of the 25 tasks), even
though several tasks were solved exclusively by only one of the two
synthesizers. We also observed that PushGP and GPT-4o with data-only prompting
solved fewer tasks with the decrease in the training set size, while the
remaining synthesizers saw no decrease. We also detected significant
differences in similarity between the successful programs synthesized for
GPT-4o with text-only and data-only prompting. With there being no dominant
program synthesizer, this work highlights the importance of different
optimization techniques used by PushGP and LLMs to synthesize programs.

</details>


### [272] [Delving Deeper Into Astromorphic Transformers](https://arxiv.org/abs/2312.10925)
*Md Zesun Ahmed Mia,Malyaban Bal,Abhronil Sengupta*

Main category: cs.NE

TL;DR: This paper introduces Astromorphic Transformers, which use neuron-astrocyte interactions to mimic self-attention, achieving better performance in image classification, sentiment analysis, and language generation.


<details>
  <summary>Details</summary>
Motivation: To address the under-exploration of astrocytes' critical role in brain-inspired neuromorphic computing by delving into neuron-synapse-astrocyte interactions to mimic self-attention mechanisms.

Method: The paper explores neuron-synapse-astrocyte interactions to mimic Transformer self-attention mechanisms. It bioplausibly models Hebbian and presynaptic plasticities in neuron-astrocyte networks, incorporates non-linearities and feedback, and maps these computations to self-attention. The impact of bio-realistic effects is evaluated on machine learning tasks.

Result: Astromorphic Transformers demonstrate improved accuracy and learning speed on IMDB and CIFAR10 datasets, and better perplexity on WikiText-2, indicating enhanced generalization and stability.

Conclusion: Astromorphic Transformers (AST) leveraging neuron-astrocyte interactions show improved accuracy and learning speed on sentiment and image classification tasks, and better perplexity, generalization, and stability on natural language generation tasks compared to conventional models.

Abstract: Preliminary attempts at incorporating the critical role of astrocytes - cells
that constitute more than 50\% of human brain cells - in brain-inspired
neuromorphic computing remain in infancy. This paper seeks to delve deeper into
various key aspects of neuron-synapse-astrocyte interactions to mimic
self-attention mechanisms in Transformers. The cross-layer perspective explored
in this work involves bioplausible modeling of Hebbian and presynaptic
plasticities in neuron-astrocyte networks, incorporating effects of
non-linearities and feedback along with algorithmic formulations to map the
neuron-astrocyte computations to self-attention mechanism and evaluating the
impact of incorporating bio-realistic effects from the machine learning
application side. Our analysis on sentiment and image classification tasks
(IMDB and CIFAR10 datasets) highlights the advantages of Astromorphic
Transformers, offering improved accuracy and learning speed. Furthermore, the
model demonstrates strong natural language generation capabilities on the
WikiText-2 dataset, achieving better perplexity compared to conventional
models, thus showcasing enhanced generalization and stability across diverse
machine learning tasks.

</details>


### [273] [STARE: Predicting Decision Making Based on Spatio-Temporal Eye Movements](https://arxiv.org/abs/2508.04148)
*Moshe Unger,Alexander Tuzhilin,Michel Wedel*

Main category: cs.NE

TL;DR: STARE, a new DL model using eye-tracking and a T5-based architecture (Chronos), predicts consumer choices by analyzing eye movements mapped to regions of interest, outperforming existing methods in initial tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop foundational Deep Learning models for predicting consumer choice behaviors using raw gaze or eye fixation time series data, for which no such models currently exist. This research aims to bridge the gap by creating architectures that represent visual attention dynamics rooted in the neurophysiology of eye movements.

Method: The STARE (Spatio-Temporal Attention Representation for Eye Tracking) architecture utilizes a novel tokenization strategy to map eye-movement time series (raw gaze or eye fixations) onto predefined Regions of Interest. This spatio-temporal data is then processed by Chronos, a T5-based time-series foundation model enhanced with co-attention and/or cross-attention to capture directional and interocular influences.

Result: STARE was compared with several state-of-the-art alternatives on multiple datasets for predicting consumer choice behaviors from eye movements, showing competitive performance and establishing a first step towards developing and testing DL architectures for this purpose.

Conclusion: STARE is a novel Deep Learning architecture for predicting consumer choice behaviors from eye-tracking data, demonstrating promising results compared to state-of-the-art alternatives and paving the way for future research in visually-attention-driven predictive modeling.

Abstract: The present work proposes a Deep Learning architecture for the prediction of
various consumer choice behaviors from time series of raw gaze or eye fixations
on images of the decision environment, for which currently no foundational
models are available. The architecture, called STARE (Spatio-Temporal Attention
Representation for Eye Tracking), uses a new tokenization strategy, which
involves mapping the x- and y- pixel coordinates of eye-movement time series on
predefined, contiguous Regions of Interest. That tokenization makes the
spatio-temporal eye-movement data available to the Chronos, a time-series
foundation model based on the T5 architecture, to which co-attention and/or
cross-attention is added to capture directional and/or interocular influences
of eye movements. We compare STARE with several state-of-the art alternatives
on multiple datasets with the purpose of predicting consumer choice behaviors
from eye movements. We thus make a first step towards developing and testing DL
architectures that represent visual attention dynamics rooted in the
neurophysiology of eye movements.

</details>


### [274] [TDSNNs: Competitive Topographic Deep Spiking Neural Networks for Visual Cortex Modeling](https://arxiv.org/abs/2508.04270)
*Deming Zhou,Yuetong Fang,Zhaorui Wang,Renjing Xu*

Main category: cs.NE

TL;DR: 通过结合脉冲神经网络和时空约束，我们创造了一种具有高度生物相似性和卓越性能的拓扑深度学习模型，它能有效处理时间信息并保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统深度人工神经网络（ANN）在处理时间动态时存在的性能下降和生物保真度不足的问题，利用了脉冲神经网络（SNN）固有的时间动态和生物学上的合理性。

Method: 提出了一种新颖的时空约束（STC）损失函数，用于拓扑深度脉冲神经网络（TDSNN），以复制灵长类视觉皮层中的分层空间功能组织。

Result: STC有效地在模拟的视觉皮层区域生成了代表性的拓扑特征，并且这种脉冲架构在保持性能方面表现优异，与现有的拓扑ANN相比，性能下降极小（在ImageNet top-1准确性上无下降），并且在“大脑相似性”方面优于拓扑ANN。

Conclusion: 通过引入新颖的时空约束（STC）损失函数，成功地在模仿初级视觉皮层的分层空间功能组织方面，在深度学习模型（特别是脉冲神经网络SNN）中实现了拓扑结构。与现有的拓扑人工神经网络（ANN）相比，这种基于脉冲的方法在保持高水平性能的同时，表现出卓越的生物保真度和稳定性，特别是在ImageNet数据集的准确性方面没有下降。研究表明，拓扑结构通过脉冲机制促进了高效稳定的时间信息处理，增强了模型的鲁棒性，为设计更高效、更具生物学特征的深度学习模型提供了新的见解。

Abstract: The primate visual cortex exhibits topographic organization, where
functionally similar neurons are spatially clustered, a structure widely
believed to enhance neural processing efficiency. While prior works have
demonstrated that conventional deep ANNs can develop topographic
representations, these models largely neglect crucial temporal dynamics. This
oversight often leads to significant performance degradation in tasks like
object recognition and compromises their biological fidelity. To address this,
we leverage spiking neural networks (SNNs), which inherently capture
spike-based temporal dynamics and offer enhanced biological plausibility. We
propose a novel Spatio-Temporal Constraints (STC) loss function for topographic
deep spiking neural networks (TDSNNs), successfully replicating the
hierarchical spatial functional organization observed in the primate visual
cortex from low-level sensory input to high-level abstract representations. Our
results show that STC effectively generates representative topographic features
across simulated visual cortical areas. While introducing topography typically
leads to significant performance degradation in ANNs, our spiking architecture
exhibits a remarkably small performance drop (No drop in ImageNet top-1
accuracy, compared to a 3\% drop observed in TopoNet, which is the
best-performing topographic ANN so far) and outperforms topographic ANNs in
brain-likeness. We also reveal that topographic organization facilitates
efficient and stable temporal information processing via the spike mechanism in
TDSNNs, contributing to model robustness. These findings suggest that TDSNNs
offer a compelling balance between computational performance and brain-like
features, providing not only a framework for interpreting neural science
phenomena but also novel insights for designing more efficient and robust deep
learning models.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [275] [Bioinspired Synergistic Texture and Color Modulation Enabled by Surface Instability of Cholesteric Liquid Crystal Elastomers](https://arxiv.org/abs/2508.03870)
*Xiao Yang,Jay Sim,Wenbin Huang,Ruike Renee Zhao*

Main category: physics.app-ph

TL;DR: 受头足类动物伪装的启发，研究人员开发了一种结合表面纹理和颜色变化的CLCE-LCE双层膜，可用于动态热调节和应变响应显示。


<details>
  <summary>Details</summary>
Motivation: 受某些头足类动物动态伪装能力的启发，提出了一种能够通过可编程褶皱同时、可逆地调节表面纹理和结构色的CLCE-LCE双层膜。

Method: 通过调整双层膜的制造参数，实现了按需的褶皱形态和颜色组合。空间选择性紫外光固化实现了局部的表面纹理，而CLCE层的化学图案化实现了区域特定的颜色响应。

Result: CLCE-LCE双层膜通过协同调节表面形貌和颜色，实现了对光吸收的动态热调节，并实现了应变依赖的多尺度编码。

Conclusion: 这项工作建立了一个多功能的平台，将表面不稳定性与可调的结构色彩相结合，推动了具有可编程、应变响应表面和光学特性的智能材料。

Abstract: Certain cephalopods can dynamically camouflage by altering both skin texture
and color to match their surroundings. Inspired by this capability, we present
a cholesteric liquid crystal elastomer-liquid crystal elastomer (CLCE-LCE)
bilayer capable of simultaneous, reversible modulation of surface texture and
structural color through programmable wrinkling. By tuning the bilayer's
fabrication parameters, on-demand wrinkle morphologies and color combinations
are achieved. Spatially selective UV curing allows localized surface textures,
while chemical patterning of the CLCE layer enables region-specific color
responses, expanding the design space for multifunctional, spatially encoded
optical materials. The CLCE-LCE bilayer enables dynamic thermal regulation by
tuning light absorption through synergistically modulating surface morphology
and color. Notably, this system achieves strain-dependent multiscale encoding
via multistep selective UV curing, revealing distinct visual content under
different applied strains. This work establishes a versatile platform that
merges surface instabilities with tunable structural coloration, advancing
intelligent materials with programmable, strain-responsive surface and optical
properties.

</details>


### [276] [Towards terahertz nanomechanics](https://arxiv.org/abs/2508.03933)
*Jiacheng Xie,Weifeng Wu,Mohan Shen,Patrick Fay,Hong X. Tang*

Main category: physics.app-ph

TL;DR: 通过将铌酸锂薄膜厚度减小到67纳米，制造了220 GHz的谐振器，但声学损耗增加，需要解决表面缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 为了在太赫兹频率下推进机电谐振器，以实现声子信号处理的巨大带宽，并克服在这些高频下进行电气驱动和检测的挑战。

Method: 通过将铌酸锂薄膜厚度从300纳米减小到67纳米，并制造了悬浮的兰姆波谐振器。

Result: 制造了在220 GHz下工作的谐振器，比之前的记录高出一倍，并接近太赫兹频率阈值。然而，超薄薄膜会增加声学损耗。

Conclusion: 这项工作通过将铌酸锂薄膜厚度从300纳米减小到67纳米，并制造了悬浮的兰姆波谐振器，在220 GHz下实现了前所未有的谐振频率。未来的发展将依赖于减轻亚100纳米薄膜的表面缺陷。

Abstract: Advancing electromechanical resonators towards terahertz frequencies opens
vast bandwidths for phononic signal processing. In quantum phononics,
mechanical resonators at these frequencies can remain in their quantum ground
state even at kelvin temperatures, obviating the need for millikelvin cooling
typically required for GHz resonators. However, electrical actuation and
detection of mechanical motion at such high frequencies present significant
challenges, primarily due to the need for device miniaturization to support
acoustic waves with nanometer-scale wavelengths. One effective strategy is to
aggressively thin down piezoelectric thin films, ideally to a thickness on the
order of the acoustic wavelength, which is in the tens of nanometers. In this
work, we aggressively reduce the thickness of lithium niobate from 300 nm to 67
nm through several stages, and fabricate suspended Lamb-wave resonators at each
thickness level. These resonators achieve resonant frequencies as high as 220
GHz, doubling the previous record and approaching the terahertz frequency
threshold. While ultrathin films exhibit a clear advantage in frequency gains,
they also experience increased acoustic losses. Our results suggest that future
advances in terahertz nanomechanics will critically rely on mitigating surface
defects in sub-100 nm thin films.

</details>


### [277] [Eavesdropping Risk in Terahertz Channels by Covered Wavy Surfaces](https://arxiv.org/abs/2508.04114)
*Peian Li,Wenbo Liu,Jiabiao Zhao,Jiayuan Cui,Yapeng Ge,Qiang Niu,Yuping Yang,Xiangzhu Meng,Jianjun Ma*

Main category: physics.app-ph

TL;DR: 金属波浪表面（MWS）下的太赫兹通信覆盖材料会重新分配窃听威胁，而不是消除它们，并且仍然存在可行的拦截场景，这些场景无法通过传统的后向散射监测检测到。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信虽然提供前所未有的数据速率，但易受阻塞影响，限制了覆盖范围并引入了物理层安全漏洞。采用金属波浪表面（MWS）的非视距中继方案解决了覆盖范围的限制，但需要隐藏在室内材料下以实现实际部署。

Method: 研究了在113-170 GHz频率下，当MWS表面覆盖有墙纸、窗帘和墙灰时，太赫兹信道特性和安全漏洞。

Result: 研究结果表明，覆盖材料会重新分配窃听威胁，而不是消除它们，并且仍然存在可行的拦截场景，这些场景无法通过传统的后向散射监测检测到。

Conclusion: 覆盖材料会重新分配窃听威胁，而不是消除它们，并且仍然存在可行的拦截场景，这些场景无法通过传统的后向散射监测检测到。

Abstract: Terahertz communications offer unprecedented data rates for next-generation
wireless networks but suffer blockage susceptibility that restrict coverage and
introduce physical-layer security vulnerabilities. Non-line-of-sight relay
schemes using metallic wavy surfaces (MWS) address coverage limitations but
require concealment beneath indoor materials for practical deployment. This
work investigates THz channel characteristics and security vulnerabilities when
MWS surfaces are covered with wallpaper, curtain, and wall plaster across
113-170 GHz. Results reveal that covering materials redistribute rather than
eliminate eavesdropping threats, with persistent feasible interception
scenarios remaining undetectable through conventional backscattering
monitoring. These findings underscore the need for enhanced mechanisms designed
for covered reflecting elements.

</details>


### [278] [X-ray thermal diffuse scattering as a texture-robust temperature diagnostic for dynamically compressed solids](https://arxiv.org/abs/2508.04525)
*P. G. Heighway,D. J. Peake,T. Stevens,J. S. Wark,B. Albertazzi,S. J. Ali,L. Antonelli,M. R. Armstrong,C. Baehtz,O. B. Ball,S. Banerjee,A. B. Belonoshko,C. A. Bolme,V. Bouffetier,R. Briggs,K. Buakor,T. Butcher,S. Di Dio Cafiso,V. Cerantola,J. Chantel,A. Di Cicco,A. L. Coleman,J. Collier,G. Collins,A. J. Comley,F. Coppari,T. E. Cowan,G. Cristoforetti,H. Cynn,A. Descamps,F. Dorchies,M. J. Duff,A. Dwivedi,C. Edwards,J. H. Eggert,D. Errandonea,G. Fiquet,E. Galtier,A. Laso Garcia,H. Ginestet,L. Gizzi,A. Gleason,S. Goede,J. M. Gonzalez,M. G. Gorman,M. Harmand,N. Hartley,C. Hernandez-Gomez,A. Higginbotham,H. Höppner,O. S. Humphries,R. J. Husband,T. M. Hutchinson,H. Hwang,D. A. Keen,J. Kim,P. Koester,Z. Konopkova,D. Kraus,A. Krygier,L. Labate,A. E. Lazicki,Y. Lee,H-P. Liermann,P. Mason,M. Masruri,B. Massani,E. E. McBride,C. McGuire,J. D. McHardy,D. McGonegle,R. S. McWilliams,S. Merkel,G. Morard,B. Nagler,M. Nakatsutsumi,K. Nguyen-Cong,A-M. Norton,I. I. Oleynik,C. Otzen,N. Ozaki,S. Pandolfi,A. Pelka,K. A. Pereira,J. P. Phillips,C. Prescher,T. Preston,L. Randolph,D. Ranjan,A. Ravasio,J. Rips,D. Santamaria-Perez,D. J. Savage,M. Schoelmerich,J-P. Schwinkendorf,S. Singh,J. Smith,R. F. Smith,A. Sollier,J. Spear,C. Spindloe,M. Stevenson,C. Strohm,T-A. Suer,M. Tang,M. Toncian,T. Toncian,S. J. Tracy,A. Trapananti,T. Tschentscher,M. Tyldesley,C. E. Vennari,T. Vinci,S. C. Vogel,T. J. Volz,J. Vorberger,J. T. Willman,L. Wollenweber,U. Zastrau,E. Brambrink,K. Appel,M. I. McMahon*

Main category: physics.app-ph

TL;DR: 提出了一种包含织构的X射线热扩散散射（TDS）模型，该模型比传统模型更准确，并且对织构变化鲁棒，适用于多种材料作为温度诊断工具。


<details>
  <summary>Details</summary>
Motivation: 为了改进X射线衍射在分析材料特性时的精度，特别是对于具有任意晶体织构的多晶体材料。

Method: 基于Warren的经典方法，提出并验证了一个包含任意晶体织构的立方多晶体的X射线热扩散散射（TDS）模型。

Result: 所提出的包含织构的TDS模型比传统粉末模型能更准确地预测X射线衍射图谱。TDS信号受样品取向影响小，与完美随机粉末的信号相似。TDS信号的单次脉冲起伏在百分之几的水平，远小于布拉格峰强度的起伏。即使在压缩诱导的塑性变形导致织构演变后，TDS也基本保持不变。

Conclusion: X射线热扩散散射（TDS）是一种灵活的温度诊断方法，对织构变化具有鲁棒性，可用于商用箔材和理想粉末。

Abstract: We present a model of x-ray thermal diffuse scattering (TDS) from a cubic
polycrystal with an arbitrary crystallographic texture, based on the classic
approach of Warren. We compare the predictions of our model with femtosecond
x-ray diffraction patterns obtained from ambient and dynamically compressed
rolled copper foils obtained at the High Energy Density (HED) instrument of the
European X-Ray Free-Electron Laser (EuXFEL), and find that the texture-aware
TDS model yields more accurate results than does the conventional powder model
owed to Warren. Nevertheless, we further show that: with sufficient angular
detector coverage, the TDS signal is largely unchanged by sample orientation
and in all cases strongly resembles the signal from a perfectly random powder;
shot-to-shot fluctuations in the TDS signal resulting from grain-sampling
statistics are at the percent level, in stark contrast to the fluctuations in
the Bragg-peak intensities (which are over an order of magnitude greater); and
TDS is largely unchanged even following texture evolution caused by
compression-induced plastic deformation. We conclude that TDS is robust against
texture variation, making it a flexible temperature diagnostic applicable just
as well to off-the-shelf commercial foils as to ideal powders.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [279] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: ETRI Lifelog Dataset 2024, collected passively via smartphones and smartwatches, provides comprehensive data on daily behaviors and sleep, enabling research into human health and lifestyle patterns, with potential applications in predicting sleep quality and stress using machine learning.


<details>
  <summary>Details</summary>
Motivation: Improving human health and well-being requires an accurate and effective understanding of an individual's physical and mental state throughout daily life.

Method: Passive and continuous data collection for 24 hours a day using smartphones, smartwatches, and sleep sensors, with minimal interference to participants' usual behavior. Quantitative data on daily behaviors and sleep activities were gathered across multiple days. Subjective self-reports of fatigue, stress, and sleep quality were collected through surveys conducted immediately before and after sleep.

Result: The paper introduces the ETRI Lifelog Dataset 2024, detailing its structure and presenting potential applications, such as using machine learning models to predict sleep quality and stress.

Conclusion: ETRI Lifelog Dataset 2024 is a comprehensive dataset that can be a foundational resource for exploring meaningful insights into human daily life and lifestyle patterns. A portion of the data has been anonymized and made publicly available for further research.

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [280] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 本研究提出了一种利用可穿戴传感器和机器学习检测自主神经反射异常（AD）的新方法，HR和ECG数据表现出最佳性能，为AD的个性化实时监测提供了重要进展。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有AD监测方法侵入性强或依赖主观症状报告的局限性，本研究旨在开发一种可用于日常生活的、非侵入性的、可解释的机器学习框架，以实现AD的早期、准确检测，从而预防心血管并发症。

Method: 本研究提出了一种基于多模态可穿戴传感器（包括ECG、PPG、BioZ、温度、RR和HR）的非侵入性、可解释的机器学习框架，用于检测AD。通过BorutaSHAP进行特征选择和可解释性分析，并使用堆叠集成元模型聚合了特定模态和设备的弱学习器。研究通过分层交叉验证确保了模型的泛化能力。

Result: 该框架能够有效检测AD，其中HR和ECG衍生的特征（特别是捕获心律形态和变异性的特征）最为关键。Nearest Centroid集成模型取得了最佳性能（Macro F1 = 0.77+/-0.03），优于基线模型。在单一模态中，HR的AUC最高（0.93），其次是ECG（0.88）和PPG（0.86）。该模型对传感器缺失具有鲁棒性，并能与临床AD事件良好匹配。

Conclusion: 该研究成功开发了一种非侵入性、可解释的机器学习框架，利用多模态可穿戴传感器检测自主神经反射异常（AD）。该框架在预测AD方面表现出优于基线模型的性能，HR和ECG衍生的特征被证明是最具信息量的。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


### [281] [Zak-OTFS over CP-OFDM](https://arxiv.org/abs/2508.03906)
*Saif Khan Mohammed,Saurabh Prakash,Muhammad Ubadah,Imran Ali Khan,Ronny Hadani,Shlomo Rakib,Shachar Kons,Yoav Hebron,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS 调制可以通过低复杂度预编码和后处理在现有 CP-OFDM 系统上实现，从而获得性能提升，CP-OFDM 是其中的一个特例。


<details>
  <summary>Details</summary>
Motivation: 在高速延迟/多普勒扩展的通信场景下，Zak-OTFS 调制相比 CP-OFDM 具有更优越的性能。然而，在现有的 CP-OFDM 调制解调器中支持 Zak-OTFS 调制是一个实际的挑战。

Method: 通过 sinc 滤波和矩形时间窗，将 Zak-OTFS 调制实现为 CP-OFDM 的低复杂度预编码器，并将 Zak-OTFS 解调器实现为 CP-OFDM 解调器输出的低复杂度后处理。

Result: 该研究表明，通过特定的滤波和时间窗方法，可以在现有的 CP-OFDM 基础设施上实现 Zak-OTFS 调制，从而利用其性能优势。该方法将 Zak-OTFS 调制与 CP-OFDM 结合，并提出了一个包含 CP-OFDM 作为特例的调制族。

Conclusion: Zak-OTFS 调制可以作为一种低复杂度预编码器，在标准的 CP-OFDM 上实现，通过使用 sinc 滤波和矩形时间窗，可以利用现有的网络基础设施获得 Zak-OTFS 的优势。CP-OFDM 是该框架的一个特例，当延迟周期取最小值（等于带宽的倒数）时。

Abstract: Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to
achieve significantly better performance compared to the standardized
Cyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high
delay/Doppler spread scenarios envisaged in next generation communication
systems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)
domain, characterized by two parameters, (i) the pulse period along the delay
axis (``delay period") (Doppler period is related to the delay period), and
(ii) the pulse shaping filter. An important practical challenge is enabling
support for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper
we show that Zak-OTFS modulation with pulse shaping constrained to sinc
filtering (filter bandwidth equal to the communication bandwidth $B$) followed
by time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is
the symbol duration and $T_{cp}$ is the CP duration), can be implemented as a
low-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS
de-modulator with matched filtering constrained to sinc filtering (filter
bandwidth $B$) followed by rectangular time windowing over duration $T$ can be
implemented as a low-complexity post-processing of the CP-OFDM de-modulator
output. This proposed ``Zak-OTFS over CP-OFDM" architecture enables us to
harness the benefits of Zak-OTFS in existing network infrastructure. We also
show that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with
CP-OFDM being a special case when the delay period takes its minimum possible
value equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum
delay period.

</details>


### [282] [Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints](https://arxiv.org/abs/2508.04046)
*Xiao Tong,Lei Lei,Ang Li,A. Lee Swindlehurst,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 本论文提出了一种新的非线性波形优化框架，用于解决多用户 MIMO 通信系统中的构造性干扰 (CI) 波形设计问题。该方法通过引入额外的优化变量并利用 KKT 条件和二次规划 (QP) 来优化波形，并通过改进的 ADMM 算法提高计算效率。仿真结果表明，该方法在 とする高阶调制和长码块长度下性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的线性 CI-MIMO 波形设计方法，包括符号级预编码 (SLP) 和块级预编码 (BLP)，由于严格的符号级功率预算或块级自由度不足，存在性能限制。

Method: 提出了一种非线性波形优化框架，该框架引入了额外的优化变量，并最大化了传输块中的最小 CI 指标。使用函数和 KKT 条件以闭式形式推导出最优波形，并相对于对偶变量明确表达了解。此外，原始问题被等效地重新表述为可处理的二次规划 (QP) 问题。为有效求解导出的 QP 问题，通过集成线性时间投影技术，开发了一种改进的交替方向乘子法 (ADMM) 算法，从而显著提高了计算效率。

Result: 所提出的算法在 とする高阶调制和长码块长度下，性能显著优于传统的 CI-SLP 和 CI-BLP 方法。

Conclusion: 所提出的算法在 とする高阶调制和长码块长度下，性能显著优于传统的 CI-SLP 和 CI-BLP 方法。

Abstract: This paper investigates constructive interference (CI)-based waveform design
for phase shift keying and quadrature amplitude modulation symbols under
relaxed block-level power constraints in multi-user multiple-input
single-output (MU-MIMO) communication systems. Existing linear CI-based
precoding methods, including symbol-level precoding (SLP) and block-level
precoding (BLP), suffer from performance limitations due to strict symbol-level
power budgets or insufficient degrees of freedom over the block. To overcome
these challenges, we propose a nonlinear waveform optimization framework that
introduces additional optimization variables and maximizes the minimum CI
metric across the transmission block. The optimal waveform is derived in closed
form using the function and Karush Kuhn Tucker conditions, and the solution is
explicitly expressed with respect to the dual variables. Moreover, the original
problems are equivalently reformulated as tractable quadratic programming (QP)
problems. To efficiently solve the derived QP problems, we develop an improved
alternating direction method of multipliers (ADMM) algorithm by integrating a
linear-time projection technique, which significantly enhances the
computational efficiency. Simulation results demonstrate that the proposed
algorithms substantially outperform the conventional CI-SLP and CI-BLP
approaches, particularly under high-order modulations and large block lengths.

</details>


### [283] [WiFo-CF: Wireless Foundation Model for CSI Feedback](https://arxiv.org/abs/2508.04068)
*Liu Xuanyu,Gao Shijian,Liu Boxun,Cheng Xiang,Yang Liuqing*

Main category: eess.SP

TL;DR: WiFo-CF是一个针对CSI反馈的无线基础模型，通过创新的预训练策略和S-R MoE架构，能够处理各种异构系统配置，并在多种场景下实现高性能，同时具备良好的下游任务适应性和部署潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的CSI反馈方案虽然压缩能力强，但通常受限于固定的系统配置，泛化性和灵活性不足，因此需要一个能够处理异构配置（如不同的信道维度、反馈速率和数据分布）的统一框架。

Method: 提出了一种名为WiFo-CF的无线基础模型，采用多用户、多速率自监督预训练策略和混合共享与路由专家（S-R MoE）架构来处理CSI反馈中的异构配置问题。

Result: WiFo-CF在模拟和真实世界场景中，对分布内和分布外数据均表现出优越性能，并有效促进了CSI室内定位等下游任务的适应性。

Conclusion: WiFo-CF通过其创新的多用户、多速率自监督预训练策略和S-R MoE架构，能够处理异构配置，并在各种场景下实现卓越性能，同时其学习到的表征也能有效促进CSI室内定位等下游任务的适应性，证明了其可扩展性和部署潜力。

Abstract: Deep learning-based channel state information (CSI) feedback schemes
demonstrate strong compression capabilities but are typically constrained to
fixed system configurations, limiting their generalization and flexibility. To
address this challenge, WiFo-CF, a novel wireless foundation model tailored for
CSI feedback, is proposed, uniquely accommodating heterogeneous configurations
such as varying channel dimensions, feedback rates, and data distributions
within a unified framework through its key innovations: (1) a multi-user,
multi-rate self-supervised pre-training strategy; and (2) a Mixture of Shared
and Routed Expert (S-R MoE) architecture. Supporting the large-scale
pre-training of WiFo-CF is the first heterogeneous channel feedback dataset,
whose diverse patterns enable the model to achieve superior performance on both
in-distribution and out-of-distribution data across simulated and real-world
scenarios. Furthermore, the learned representations effectively facilitate
adaptation to downstream tasks such as CSI-based indoor localization,
validating WiFo-CF's scalability and deployment potential.

</details>


### [284] [DFT-s-OFDM with Chirp Modulation](https://arxiv.org/abs/2508.04075)
*Yujie Liu,Yong Liang Guan,David González G.,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, a new waveform called discrete Fourier transform spread
orthogonal frequency division multiplexing with chirp modulation
(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.
The information bits are conveyed by not only Q-ary constellation symbols but
also the starting frequency of chirp signal. It could maintain the benefits
provided by the chirped discrete Fourier transform spread orthogonal frequency
division multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio
(PAPR), full frequency diversity exploitation, etc. Simulation results confirm
that the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while
keeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In
addition, when maintaining the same spectral efficiency, the proposed
DFT-s-OFDM-CM with the splitting of information bits into two streams enables
the use of lower-order constellation modulation and offers greater resilience
to noise, resulting in a lower BER than the chirped DFT-s-OFDM.

</details>


### [285] [Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems](https://arxiv.org/abs/2508.04214)
*Yasaman Khorsandmanesh,Emil Björnson,Joakim Jaldén,Bengt Lindoff*

Main category: eess.SP

TL;DR: 提出了一种新颖的两阶段数字合并方案，用于减小数传设备（UE）中的基带样本量。


<details>
  <summary>Details</summary>
Motivation: 毫米波宽带点对点 MIMO 系统中的移动 UE 场景需要处理大量基带样本，这会带来计算和硬件复杂性。

Method: 提出了一种新颖的两阶段数字合并方案，用于减小数传设备（UE）中的基带样本量。第一阶段利用信道几何将 Nr 个接收信号减少到 Nc 路，并在波束相干时间内更新，这比小尺度衰落的信道相干时间长。第二阶段的合并根据衰落实现进行更新。开发了基于最大似然估计的导频辅助信道估计框架，并提出了数字预编码和合并设计。

Result: 所提出的方法优于混合波束成形。

Conclusion: 未来的系统具有使用两阶段全数字收发器的吸引力。

Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system
with fully digital transceivers at the base station and the user equipment
(UE), focusing on mobile UE scenarios. A main challenge when building a digital
UE combining is the large volume of baseband samples to handle. To mitigate
computational and hardware complexity, we propose a novel two-stage digital
combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received
signals to $N_{\text{c}}$ streams before baseband processing, leveraging
channel geometry for dimension reduction and updating at the beam coherence
time, which is longer than the channel coherence time of the small-scale
fading. By contrast, the second-stage combining is updated per fading
realization. We develop a pilot-based channel estimation framework for this
hardware setup based on maximum likelihoodestimation in both uplink and
downlink. Digital precoding and combining designs are proposed, and a spectral
efficiency expression that incorporates imperfect channel knowledge is derived.
The numerical results demonstrate that the proposed approach outperforms hybrid
beamforming, showcasing the attractiveness of using two-stage fully digital
transceivers in future systems.

</details>


### [286] [Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving](https://arxiv.org/abs/2508.04128)
*Di Wu,Yifei Jia,Siyuan Li,Shiqi Zhao,Jie Yang,Mohamad Sawan*

Main category: eess.SP

TL;DR: 本文提出了一种名为Neuro-MoBRE的新框架，用于解决脑机接口（BCI）中神经生理学数据的异质性问题。该框架通过结合混合专家模型和预训练策略，提高了解码的准确性和泛化能力，尤其在处理跨受试者和多任务数据时效果显著，实现了零样本解码。


<details>
  <summary>Details</summary>
Motivation: 现有神经生理解码方法主要局限于单任务和单受试者场景，泛化能力受限。尽管大型神经生理基础模型有潜力，但数据异质性带来了重大挑战。简单增加模型参数和数据集大小无法复制自然语言处理中的规模化成功，因此需要一种能够有效管理数据异质性的框架。

Method: 本文提出了一种名为Neuro-MoBRE的通用解码框架，该框架结合了基于脑区-时间嵌入的混合专家模型，为不同的脑区分配专门的专家处理，以解决结构和功能上的异质性。此外，还采用了区域掩码自编码预训练策略来增强跨受试者表示的一致性，并结合了任务解耦信息聚合方法来处理特定任务的神经变异。

Result: 在包含11名受试者和五项不同任务（包括复杂的语言解码和癫痫发作诊断）的颅内记录评估中，Neuro-MoBRE的表现优于现有技术，并在对未见过受试者的零样本解码任务上表现出稳健的泛化能力。

Conclusion: Neuro-MoBRE框架在处理神经生理学数据的异质性方面取得了显著成效，并且在各种任务和受试者上展现了强大的泛化能力，实现了零样本解码。

Abstract: Neurophysiological decoding, fundamental to advancing brain-computer
interface (BCI) technologies, has significantly benefited from recent advances
in deep learning. However, existing decoding approaches largely remain
constrained to single-task scenarios and individual subjects, limiting their
broader applicability and generalizability. Efforts towards creating
large-scale neurophysiological foundation models have shown promise, but
continue to struggle with significant challenges due to pervasive data
heterogeneity across subjects and decoding tasks. Simply increasing model
parameters and dataset size without explicitly addressing this heterogeneity
fails to replicate the scaling successes seen in natural language processing.
Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),
a general-purpose decoding framework explicitly designed to manage the
ubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE
incorporates a brain-regional-temporal embedding mechanism combined with a
mixture-of-experts approach, assigning neural signals from distinct brain
regions to specialized regional experts on a unified embedding basis, thus
explicitly resolving both structural and functional heterogeneity.
Additionally, our region-masked autoencoding pre-training strategy further
enhances representational consistency among subjects, complemented by a
task-disentangled information aggregation method tailored to effectively handle
task-specific neural variations. Evaluations conducted on intracranial
recordings from 11 subjects across five diverse tasks, including complex
language decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE
surpasses prior art and exhibits robust generalization for zero-shot decoding
on unseen subjects.

</details>


### [287] [Dual-Function Radar-Communication Beamforming with Outage Probability Metric](https://arxiv.org/abs/2508.04144)
*Hossein Maleki,Carles Diaz-Vilor,Ali Pezeshki,Vahid Tarokh,Hamid Jafarkhani*

Main category: eess.SP

TL;DR: 提出了用于双功能雷达通信系统的波束形成方法，通过优化雷达和通信性能，有效解决了频谱拥堵问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决频谱拥堵问题，探索通信与传感的集成设计，并开发用于双功能雷达通信系统的波束形成方法。

Method: 利用中心极限定理将随机优化问题转化为非凸确定性问题，并进一步通过半定规划（SDP）及其秩一约束进行松弛求解。

Result: 提出的波束形成方法在两种场景下均有效，并通过数值实验进行了验证。

Conclusion: 该研究提出了一种用于双功能雷达通信系统的波束形成方法，解决了频谱拥堵问题，并在考虑不完美信道状态信息的情况下，针对雷达中心和通信中心两种场景进行了优化。

Abstract: The integrated design of communication and sensing may offer a potential
solution to address spectrum congestion. In this work, we develop a beamforming
method for a dual-function radar-communication system, where the transmit
signal is used for both radar surveillance and communication with multiple
downlink users, despite imperfect channel state information (CSI). We focus on
two scenarios of interest: radar-centric and communication-centric. In the
radar-centric scenario, the primary goal is to optimize radar performance while
attaining acceptable communication performance. To this end, we minimize a
weighted sum of the mean-squared error in achieving a desired beampattern and a
mean-squared cross correlation of the radar returns from directions of interest
(DOI). We also seek to ensure that the probability of outage for the
communication users remains below a desired threshold. In the
communication-centric scenario, our main objective is to minimize the maximum
probability of outage among the communication users while keeping the
aforementioned radar metrics below a desired threshold. Both optimization
problems are stochastic and untractable. We first take advantage of central
limit theorem to obtain deterministic non-convex problems and then consider
relaxations of these problems in the form of semidefinite programs with rank-1
constraints. We provide numerical experiments demonstrating the effectiveness
of the proposed designs.

</details>


### [288] [Subspace Fitting Approach for Wideband Near-Field Localization](https://arxiv.org/abs/2508.04169)
*Ruiyun Zhang,Zhaolin Wang,Zhiqing Wei,Yuanwei Liu,Zehui Xiong,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出两种宽带近场定位的子空间拟合方法，联合估计距离和角度，并提出一种近似方法解耦参数。


<details>
  <summary>Details</summary>
Motivation: 与传统的远场系统不同，近场系统中的球面波传播会将距离和角度参数耦合起来，因此需要新的方法来处理这种情况。

Method: 提出了一种基于子空间拟合的MUSIC方法，用于联合估计距离和角度；同时还提出了一种基于Fresnel近似的MUSIC算法，用于解耦距离和角度参数。

Result: 数值结果验证了所提出的两种方法的有效性。

Conclusion: 提出的两种子空间拟合方法在宽带近场定位中是有效的。

Abstract: Two subspace fitting approaches are proposed for wideband near-field
localization. Unlike in conventional far-field systems, where distance and
angle can be estimated separately, spherical wave propagation in near-field
systems couples these parameters. We therefore derive a frequency-domain
near-field signal model for multi-target wideband systems and develop a
subspace fitting-based MUSIC method that jointly estimates distance and angle.
To reduce complexity, a Fresnel approximation MUSIC algorithm is further
introduced to decouple the distance and angle parameters. Numerical results
verify the effectiveness of both proposed approaches.

</details>


### [289] [Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems](https://arxiv.org/abs/2508.04185)
*Evangelos Koutsonas,Xiaonan Mu,Nan Qi,Stylianos Trevlakis,Theodoros A. Tsiftsis,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: SICS协议通过在NOMA方案中叠加信息信号与控制信号，解决了RIS信令延迟高于信道相干时间的问题，并能在优化用户数据率的同时保证控制信号的解码。


<details>
  <summary>Details</summary>
Motivation: 在将RIS集成到无线接入网络时，在边缘单元和RIS微控制器（MC）之间需要进行信令。然而，在许多实际场景中，信令延迟高于通信信道相干时间，导致RIS上的信令过时。

Method: 提出了一种同时信息和控制信令（SICS）协议，该协议允许通过无线控制信号传输进行操作适应。SICS假设MC配备单天线，并与RIS在同一频率运行。RIS以同时传输和反射（STAR）模式运行，并且源采用非正交多址（NOMA）将信息信号叠加到控制信号上。为了在确保MC能够解码控制信号的同时最大化可实现的用户数据速率，我们提出了优化问题，并得到了RIS的反射和传输系数以及NOMA方案的叠加系数。

Result: 我们的结果揭示了SICS方法的鲁棒性。

Conclusion: SICS协议具有鲁棒性

Abstract: Integration of RIS in radio access networks requires signaling between edge
units and the RIS microcontroller (MC). Unfortunately, in several practical
scenarios, the signaling latency is higher than the communication channel
coherence time, which causes outdated signaling at the RIS. To counterbalance
this, we introduce a simultaneous information and control signaling (SICS)
protocol that enables operation adaptation through wireless control signal
transmission. SICS assumes that the MC is equipped with a single antenna that
operates at the same frequency as the RIS. RIS operates in simultaneous
transmission and reflection (STAR) mode, and the source employs non-orthogonal
multiple access (NOMA) to superposition the information signal to the control
signal. To maximize the achievable user data rate while ensuring the MC's
ability to decode the control signal, we formulate and solve the corresponding
optimization problem that returns RIS's reflection and transmission
coefficients as well as the superposition coefficients of the NOMA scheme. Our
results reveal the robustness of the SICS approach.

</details>


### [290] [Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP](https://arxiv.org/abs/2508.04222)
*Thibaut Ceulemans,Cel Thys,Robbert Beerten,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 针对ELAA系统信道估计的挑战，提出了一种结合HMM和极域联合稀疏恢复的新算法（VR-HMM-P-SOMP），在低信噪比和稀疏场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 针对极大规模阵列（ELAA）系统中近场传播和空间非平稳性带来的信道估计复杂性问题，传统的估计技术难以有效应对。

Method: 提出了一种新颖的算法：基于可见域-隐马尔可夫模型辅助的极域联合正交匹配追踪（VR-HMM-P-SOMP）。该方法扩展了贪婪稀疏恢复框架，通过结合隐马尔可夫模型（HMM）进行可见域（VR）估计，并采用新颖的发射形式和Viterbi解码，实现了天线层面的自适应掩码和空间非平稳性处理。

Result: 仿真结果表明，所提出的VR-HMM-P-SOMP算法相比现有技术，在低信噪比和稀疏场景下提高了估计精度，并保持了低计算复杂度。

Conclusion: 该算法在低信噪比和稀疏场景下提高了估计精度，同时保持了低计算复杂度，并且在多种设计参数和信道条件下表现出鲁棒性，为ELAA系统提供了实用的解决方案。

Abstract: This work focuses on channel estimation in extremely large aperture array
(ELAA) systems, where near-field propagation and spatial non-stationarity
introduce complexities that hinder the effectiveness of traditional estimation
techniques. A physics-based hybrid channel model is developed, incorporating
non-binary visibility region (VR) masks to simulate diffraction-induced power
variations across the antenna array. To address the estimation challenges posed
by these channel conditions, a novel algorithm is proposed:
Visibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching
Pursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework
by integrating VR estimation through a hidden Markov model (HMM), using a novel
emission formulation and Viterbi decoding. This allows the algorithm to
adaptively mask steering vectors and account for spatial non-stationarity at
the antenna level. Simulation results demonstrate that the proposed method
enhances estimation accuracy compared to existing techniques, particularly in
low-SNR and sparse scenarios, while maintaining a low computational complexity.
The algorithm presents robustness across a range of design parameters and
channel conditions, offering a practical solution for ELAA systems.

</details>


### [291] [Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications](https://arxiv.org/abs/2508.04223)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Shuai Liu,Hongyang Du,Geyong Min*

Main category: eess.SP

TL;DR: 本研究提出了一种名为 WS-DC 的新颖方法，通过引入频谱效率感知码本设计框架和基于 Wasserstein 距离的正则化，解决了现有语义通信方法中码本激活稀疏和频谱效率低下的问题。WS-DC 在提高推理准确性和码本效率方面表现出色，有望实现接近信道容量的语义通信。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向型语义通信 (ToSC) 方法依赖于学习到的码本，但这些码本激活稀疏，导致频谱效率低下和信道容量利用不足。亟需设计一种能够支持任务特定推理并接近信道容量理论极限的码本。

Method: 提出了一种频谱效率感知的码本设计框架，将码本激活概率纳入优化过程，并引入 Wasserstein (WS) 距离作为正则化度量，以缩小学习到的激活分布与最优信道输入分布之间的差距。从生成角度重新诠释了 WS 理论，并提出了 WS-DC 方案，以学习紧凑、面向任务且面向信道的潜在表示。

Result: 实验结果表明，WS-DC 在推理准确性方面优于现有方法，并显著提高了码本效率。

Conclusion: WS-DC 作为一种基于 Wasserstein 距离的自适应混合分布方案，在推理准确性和码本效率方面均优于现有方法，为实现接近容量的语义通信系统提供了有前景的方向。

Abstract: Digital task-oriented semantic communication (ToSC) aims to transmit only
task-relevant information, significantly reducing communication overhead.
Existing ToSC methods typically rely on learned codebooks to encode semantic
features and map them to constellation symbols. However, these codebooks are
often sparsely activated, resulting in low spectral efficiency and
underutilization of channel capacity. This highlights a key challenge: how to
design a codebook that not only supports task-specific inference but also
approaches the theoretical limits of channel capacity. To address this
challenge, we construct a spectral efficiency-aware codebook design framework
that explicitly incorporates the codebook activation probability into the
optimization process. Beyond maximizing task performance, we introduce the
Wasserstein (WS) distance as a regularization metric to minimize the gap
between the learned activation distribution and the optimal channel input
distribution. Furthermore, we reinterpret WS theory from a generative
perspective to align with the semantic nature of ToSC. Combining the above two
aspects, we propose a WS-based adaptive hybrid distribution scheme, termed
WS-DC, which learns compact, task-driven and channel-aware latent
representations. Experimental results demonstrate that WS-DC not only
outperforms existing approaches in inference accuracy but also significantly
improves codebook efficiency, offering a promising direction toward
capacity-approaching semantic communication systems.

</details>


### [292] [ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening](https://arxiv.org/abs/2508.04240)
*Sitong Chen,Beiqianyi Li,Cuilin He,Dongyang Li,Mingyang Wu,Xinke Shen,Song Wang,Xuetao Wei,Xindi Wang,Haiyan Wu,Quanying Liu*

Main category: eess.SP

TL;DR: 中文脑电-语言数据集ChineseEEG-2包含朗读和被动听模态，旨在为脑-LLM对齐提供基准。


<details>
  <summary>Details</summary>
Motivation: 为了实现EEG（脑电图）信号与LLM（大语言模型）的语义表征对齐，需要大规模的、跨多种语言模态（说话、听、读）的成对脑语言数据。然而，目前这类数据集，尤其是非英语语言的数据，仍然稀缺。

Method: 本研究提出了ChineseEEG-2数据集，该数据集包含两种新的主动模态：朗读（RA）和被动听（PL）。在朗读任务中，同时记录了4名参与者约10.7小时的脑电图（EEG）和音频；在被动听任务中，将朗读的音频播放给另外8名参与者，记录了约21.6小时的EEG。该数据集整合了EEG信号、精确音频、预训练语言模型的语义嵌入以及任务标签。

Result: ChineseEEG-2数据集包含高密度EEG信号、精确的音频记录、来自预训练语言模型的对齐语义嵌入以及任务标签。该数据集支持跨说话、听和读三种模态的联合语义对齐学习，并能为神经解码算法提供基准测试，特别是在中文多模态语言任务下促进脑-LLM的对齐。

Conclusion: ChineseEEG-2是一个高质量的中文脑电-语言数据集，旨在为神经解码模型提供基准，促进多模态语言任务下的脑-语言模型（LLM）对齐，尤其是在中文场景下。

Abstract: EEG-based neural decoding requires large-scale benchmark datasets. Paired
brain-language data across speaking, listening, and reading modalities are
essential for aligning neural activity with the semantic representation of
large language models (LLMs). However, such datasets are rare, especially for
non-English languages. Here, we present ChineseEEG-2, a high-density EEG
dataset designed for benchmarking neural decoding models under real-world
language tasks. Building on our previous ChineseEEG dataset, which focused on
silent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and
Passive Listening (PL), using the same Chinese corpus. EEG and audio were
simultaneously recorded from four participants during ~10.7 hours of reading
aloud. These recordings were then played to eight other participants,
collecting ~21.6 hours of EEG during listening. This setup enables speech
temporal and semantic alignment across the RA and PL modalities. ChineseEEG-2
includes EEG signals, precise audio, aligned semantic embeddings from
pre-trained language models, and task labels. Together with ChineseEEG, this
dataset supports joint semantic alignment learning across speaking, listening,
and reading. It enables benchmarking of neural decoding algorithms and promotes
brain-LLM alignment under multimodal language tasks, especially in Chinese.
ChineseEEG-2 provides a benchmark dataset for next-generation neural semantic
decoding.

</details>


### [293] [Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond](https://arxiv.org/abs/2508.04253)
*Yiyan Ma,Bo Ai,Jinhong Yuan,Shuangyang Li,Qingqing Cheng,Zhenguo Shi,Weijie Yuan,Zhiqiang Wei,Akram Shafie,Guoyu Ma,Yunlong Lu,Mi Yang,Zhangdui Zhong*

Main category: eess.SP

TL;DR: DD-a-OFDM 通过在传统OFDM中加入延迟-多普勒域处理，在高移动场景下性能优于传统OFDM，且在信道估计精度和导频开销上优于OTFS。


<details>
  <summary>Details</summary>
Motivation: 为了解决正交频分复用（OFDM）系统在高速移动场景下因严重多普勒频偏导致的子载波正交性损失问题，以及延迟-多普勒域多载波（DDMC）调制（如OTFS）虽然能利用时频（TF）域信道分集但存在接收端复杂度高和TF资源分配不灵活等挑战，文章旨在提出一种能够增强OFDM性能的方案。

Method: 文章提出了一种DD域信号处理辅助的OFDM（DD-a-OFDM）方案，其核心在于在经典OFDM收发信机框架内，融入了DD域信道估计和TF域均衡技术。具体来说，文章设计了DD-a-OFDM系统结构，利用离散TF导频进行DD域信道估计，并将TF域的载波间干扰（ICI）转化为DD域的高斯干扰。在此基础上，文章推导了DD域信道估计的Cramér-Rao下界（CRLBs），并开发了基于最大似然（ML）和峰值检测的信道估计器，以及相应的TF域均衡器。

Result: 数值结果验证了所提出的DD-a-OFDM方案。与经典OFDM相比，该方案降低了误比特率（BER）。与OTFS相比，DD-a-OFDM在信道估计精度方面表现更优，且导频开销更低。

Conclusion: DD-a-OFDM通过引入延迟-多普勒(DD)域信号处理，在保留OFDM经典收发信机结构的同时，提升了OFDM在高移动场景下的性能。文章提出的DD域信道估计和TF域均衡方法，有效解决了OFDM在存在严重多普勒频偏时的子载波正交性损失问题，与OTFS相比，在信道估计精度和导频开销方面表现更优。

Abstract: High-mobility scenarios will be a critical part of 6G systems. Since the
widely deployed orthogonal frequency division multiplexing (OFDM) waveform
suffers from subcarrier orthogonality loss under severe Doppler spread,
delay-Doppler domain multi-carrier (DDMC) modulation systems, such as
orthogonal time frequency space (OTFS), have been extensively studied. While
OTFS can exploit time-frequency (TF) domain channel diversity, it faces
challenges including high receiver complexity and inflexible TF resource
allocation, making OFDM still the most promising waveform for 6G. In this
article, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme
to enhance OFDM performance based on DDMC research insights. First, we design a
DD-a-OFDM system structure, retaining the classical OFDM transceiver while
incorporating DD domain channel estimation and TF domain equalization. Second,
we detail DD domain channel estimation using discrete TF pilots and prove that
TF domain inter-carrier interference (ICI) could be transformed into DD domain
Gaussian interference. Third, we derive closed-form Cram\'{e}r-Rao lower bounds
(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood
(ML) and peak detection-based channel estimators, along with a corresponding TF
domain equalizer. Numerical results verify the proposed design, showing that
DD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and
outperforms OTFS in channel estimation accuracy with lower pilot overhead.

</details>


### [294] [Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication](https://arxiv.org/abs/2508.04291)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Hongyang Du,Haojin Li,Chen Sun,Haijun Zhang*

Main category: eess.SP

TL;DR: 本论文提出了一种信道感知的离散语义编码框架，通过Wasserstein正则化目标使离散表示与信道特性和任务需求相匹配，从而提高了通信性能和任务准确性，特别是在低功耗边缘网络和多变无线条件下。


<details>
  <summary>Details</summary>
Motivation: 当前面向任务的语义通信（ToSC）框架常常将语义感知的离散映射与底层信道特性及任务需求分离开来，导致通信性能不佳、任务效用下降以及在多变无线条件下的泛化能力受限。此外，传统的码本构建方法忽视了信道感知，限制了在资源约束下语义符号选择的有效性。

Method: 提出一个面向低功耗边缘网络的信道感知离散语义编码框架，并利用Wasserstein正则化目标使离散码激活与最优输入分布对齐。

Result: 实验结果表明，该方法在不同信噪比（SNR）下的推理任务中，在准确性和通信效率方面均取得了显著的提升。

Conclusion: 该研究为离散语义与信道优化提供新的见解，有望促进语义通信在未来数字基础设施中的广泛应用。

Abstract: Discrete representation has emerged as a powerful tool in task-oriented
semantic communication (ToSC), offering compact, interpretable, and efficient
representations well-suited for low-power edge intelligence scenarios. Its
inherent digital nature aligns seamlessly with hardware-friendly deployment and
robust storage/transmission protocols. However, despite its strengths, current
ToSC frameworks often decouple semantic-aware discrete mapping from the
underlying channel characteristics and task demands. This mismatch leads to
suboptimal communication performance, degraded task utility, and limited
generalization under variable wireless conditions. Moreover, conventional
designs frequently overlook channel-awareness in codebook construction,
restricting the effectiveness of semantic symbol selection under constrained
resources. To address these limitations, this paper proposes a channel-aware
discrete semantic coding framework tailored for low-power edge networks.
Leveraging a Wasserstein-regularized objective, our approach aligns discrete
code activations with optimal input distributions, thereby improving semantic
fidelity, robustness, and task accuracy. Extensive experiments on the inference
tasks across diverse signal-to-noise ratio (SNR) regimes show that our method
achieves notable gains in accuracy and communication efficiency. This work
provides new insights into integrating discrete semantics and channel
optimization, paving the way for the widespread adoption of semantic
communication in future digital infrastructures.

</details>


### [295] [Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications](https://arxiv.org/abs/2508.04322)
*Ruopeng Xu,Zhaohui Yang,Zhaoyang Zhang,Mohammad Shikh-Bahaei,Kaibin Huang,Dusit Niyato*

Main category: eess.SP

TL;DR: 本论文提出了一种基于流体天线中继（FAR）的节能无线通信系统，解决了由阻塞引起的非视距（NLoS）链路问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决非视距（NLoS）通信中的问题，现有的流体天线系统（FAS）研究主要集中在视距（LoS）通信场景，忽略了仅存在NLoS链路的情况。

Method: 我们设计了一个结合了放大转发（AF）协议的FAR辅助通信系统。通过优化流体天线（FA）在FAR两侧的位置，我们旨在最大化能量效率（EE），并提出了一种迭代算法来解决优化问题。

Result: 仿真结果表明，与传统的RIS方案和AF中继方案相比，所提出的算法在EE方面表现更优，分别实现了高达23.39%和39.94%的EE提升。

Conclusion: 通过优化流体天线（FA）在双方中继器的位置，可以实现对穿过阻塞导致非视距（NLoS）链路的信号进行可控的相移。此外，我们建立了同时考虑阻塞穿透矩阵、大尺度衰落和小尺度衰落的信道模型。通过联合优化中继器的位置、FA 位置、功率控制和波束形成设计，并提出迭代算法来解决所提出的优化问题，以最大化系统的能量效率（EE）。仿真结果表明，与传统的RIS方案和AF中继方案相比，该算法在EE方面表现更优。

Abstract: In this paper, we propose an energy efficient wireless communication system
based on fluid antenna relay (FAR) to solve the problem of non-line-of-sight
(NLoS) links caused by blockages with considering the physical properties.
Driven by the demand for the sixth generation (6G) communication, fluid antenna
systems (FASs) have become a key technology due to their flexibility in
dynamically adjusting antenna positions. Existing research on FAS primarily
focuses on line-of-sight (LoS) communication scenarios, and neglects the
situations where only NLoS links exist. To address the issues posted by NLoS
communication, we design an FAR-assisted communication system combined with
amplify-and-forward (AF) protocol. In order to alleviate the high energy
consumption introduced by AF protocol while ensuring communication quality, we
formulate an energy efficiency (EE) maximization problem. By optimizing the
positions of the fluid antennas (FAs) on both sides of the FAR, we achieve
controllable phase shifts of the signals transmitting through the blockage
which causes the NLoS link. Besides, we establish a channel model that jointly
considers the blockage-through matrix, large-scale fading, and small-scale
fading. To maximize the EE of the system, we jointly optimize the FAR position,
FA positions, power control, and beamforming design under given constraints,
and propose an iterative algorithm to solve this formulated optimization
problem. Simulation results show that the proposed algorithm outperforms the
traditional schemes in terms of EE, achieving up to $23.39\%$ and $39.94\%$
higher EE than the conventional reconfigurable intelligent surface (RIS) scheme
and traditional AF relay scheme, respectively.

</details>


### [296] [Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination](https://arxiv.org/abs/2508.04331)
*Mohamadreza Delbari,Qikai Zhou,Robin Neuder,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: LC-based RIS is frequency-dependent, causing performance degradation and information leakage in secure systems. This paper designs an RIS for wideband OFDM systems to enhance secrecy rate by avoiding leakage to eavesdroppers, outperforming methods that ignore frequency effects.


<details>
  <summary>Details</summary>
Motivation: Liquid crystal (LC) based RIS's phase-shift response is inherently frequency-dependent, leading to performance degradation and considerable information leakage in secure communication systems.

Method: Design RIS for a wideband orthogonal frequency division multiplexing (OFDM) system to illuminate a desired area containing legitimate users while avoiding leakage to regions where potential eavesdroppers may be located, to avoid the need for full channel state information (CSI) acquisition and frequent RIS reconfiguration.

Result: Simulation results demonstrate that the proposed algorithm improves the secrecy rate compared to methods that neglect frequency-dependent effects.

Conclusion: The proposed algorithm improves the secrecy rate compared to methods that neglect frequency-dependent effects, achieving a secrecy rate of about 2 bits/symbol over an 8 GHz bandwidth when the center frequency is 60 GHz.

Abstract: Liquid crystal (LC) technology provides a low-power and scalable approach to
implement a reconfigurable intelligent surface (RIS). However, the LC-based
RIS's phase-shift response is inherently frequency-dependent, which can lead to
performance degradation if not properly addressed. This issue becomes
especially critical in secure communication systems, where such variations may
result in considerable information leakage. To avoid the need for full channel
state information (CSI) acquisition and frequent RIS reconfiguration, we design
RIS for a wideband orthogonal frequency division multiplexing (OFDM) system to
illuminate a desired area containing legitimate users while avoiding leakage to
regions where potential eavesdroppers may be located. Our simulation results
demonstrate that the proposed algorithm improves the secrecy rate compared to
methods that neglect frequency-dependent effects. In the considered setup, the
proposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz
bandwidth when the center frequency is 60 GHz.

</details>


### [297] [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](https://arxiv.org/abs/2508.04570)
*A. Tarik Leblebici,Sumeyra Hassan,Erdal Panayirci,H. Vincent Poor*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes a joint communication and indoor positioning (JCP) system
based on visible light communication (VLC) designed for high-precision indoor
environments. The framework supports 2D and 3D positioning using received
signal strength (RSS) from pilot transmissions, enhanced by the radical axis
theorem to improve accuracy under measurement uncertainties. Communication is
achieved using spatial modulation (SM) with M-ary pulse amplitude modulation
(PAM), where data is conveyed through the modulation symbol and the active
light-emitting diode (LED) index, improving spectral efficiency while
maintaining low complexity. A pilot-aided least squares (LS) estimator is
employed for joint channel and dimming coefficient estimation, enabling robust
symbol detection in multipath environments characterized by both line-of-sight
(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician
fading. The proposed system incorporates a dimming control mechanism to meet
lighting requirements while maintaining reliable communication and positioning
performance. Simulation results demonstrate sub-centimeter localization
accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below
10^{-6} for low-order PAM schemes. Additionally, comparative analysis across
user locations reveals that positioning and communication performance improve
significantly near the geometric center of the LED layout. These findings
validate the effectiveness of the proposed system for future 6G indoor networks
requiring integrated localization and communication under practical channel
conditions.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [298] [Topological domain-wall states from Umklapp scattering in twisted bilayer graphene](https://arxiv.org/abs/2508.03761)
*Juncheng Li,Cong Chen,Wang Yao*

Main category: cond-mat.mes-hall

TL;DR: 大角度扭转双层石墨烯中的面间Umklapp散射和结构手性是低能物理和拓扑性质的关键。手性反转产生的拓扑畴壁态，在不同对称构型下表现出不同的电子行为。


<details>
  <summary>Details</summary>
Motivation: 探索小角度扭转的材料之外，研究大角度扭转双层石墨烯中面间Umklapp散射的作用，揭示其在低能物理和能带拓扑中的关键作用。

Method: 利用对称性约束的有效k·p模型来分析大角度扭转双层石墨烯的电子特性，并通过原子模拟来验证拓扑态的鲁棒性。

Result: D6对称构型表现出门控频谱和手性面间耦合；D3对称构型表现出半金属行为。手性反转产生拓扑畴壁态，表现为在相反扭转区域界面处的反向传播赝自旋模式。

Conclusion: 本文揭示了在大角度扭转双层石墨烯中，面间Umklapp散射在低能物理和非常规能带拓扑中起着关键作用。通过构建对称性约束的有效k·p模型，我们展示了结构手性如何产生不同的电子响应。

Abstract: Twistronics, harnessing interlayer rotation to tailor electronic states in
van der Waals materials, has predominantly focused on small-angle regime. Here,
we unveil the pivotal role of intervalley Umklapp scattering in large-angle
twisted bilayer graphene, which governs low-energy physics and drives
unconventional band topology. By constructing symmetry-constrained effective
$k\cdot p$ models for $\pm 21.8^{\circ}$-twisted bilayers, we demonstrate how
structural chirality imprints distinct electronic responses. The $D_6$
configuration exhibits a gapped spectrum with chiral interlayer coupling, while
$D_3$ symmetric stacking configuration displays semimetallic behavior.
Crucially, chirality inversion creates topological domain-wall states, which
manifest as counterpropagating pseudospin modes at interfaces between
oppositely twisted regions. These states, absent in untwisted bilayers, emerge
from a Jackiw-Rebbi-like mechanism tied to chirality reversal. Atomistic
simulations confirm these topological states and demonstrate their robustness
against symmetry-breaking perturbations. The interplay between twist-induced
chirality and topology opens new pathways for engineering domain-wall states in
twisted materials.

</details>


### [299] [Absence of dissipation-free topological edge states in quadratic open fermions](https://arxiv.org/abs/2508.03821)
*Liang Mao*

Main category: cond-mat.mes-hall

TL;DR: 开放费米子系统中的拓扑边缘态不能是无耗散的。


<details>
  <summary>Details</summary>
Motivation: 研究开放量子系统中的拓扑现象，特别是与封闭系统中的拓扑绝缘体和超导体相关的无耗散边缘态。

Method: 将Lindbladian映射到编码能带结构的自伴算子，并证明该算子总是与拓扑平庸的算子绝热连接，从而表明不存在由体拓扑保护的鲁棒无耗散边缘态。

Result: 证明了在由Lindblad主方程描述的通用二次开放费米子系统中，不存在由体拓扑保护的鲁棒无耗散边缘态，该结果适用于任何酉或反酉对称性。

Conclusion: 不能在由Lindblad主方程控制的通用二次开放费米子系统中托管无耗散拓扑边缘态。

Abstract: We prove a no-go theorem that generic quadratic open fermionic systems,
governed by Lindblad master equations, cannot host dissipation-free topological
edge states. Drawing an analogy to topological insulators and superconductors,
we map the Lindbladian to a first-quantized matrix representation that encodes
the band structure, whose zero-energy topological edge modes are exactly
dissipation-free. This matrix, however, is always adiabatically connected to a
topologically trivial matrix, even under symmetry constraints. We formulate s
rigorous adiabatic path to demonstrate this property. Thus, there is no robust
dissipation-free edge modes protected by the bulk topology in quadratic open
fermions, under any unitary or anti-unitary symmetries. Our result applies to
generic quadratic fermionic Lindbladians, requiring only gapped bulk and
bounded spectrum for technical convenience. Our result establish a definitive
boundary for the existence of robust topological phenomena in open fermionic
systems.

</details>


### [300] [Hybrid metal-semiconductor quantum dots in InAs as a platform for quantum simulation](https://arxiv.org/abs/2508.03928)
*Praveen Sriram,Connie L. Hsueh,Karna A. Morey,Tiantian Wang,Candice Thomas,Geoffrey C. Gardner,Marc A. Kastner,Michael J. Manfra,David Goldhaber-Gordon*

Main category: cond-mat.mes-hall

TL;DR: 混合金属-半导体岛阵列可用于量子模拟，优于传统量子点，具有电子一致性和可调耦合。实验制备了具有可调耦合的混合岛，并在不同耦合极限下进行了表征，观察到库仑峰和动力学库仑块。


<details>
  <summary>Details</summary>
Motivation: To develop a new approach to quantum simulation using hybrid metal-semiconductor islands that overcome the limitations of conventional quantum dots, specifically their lack of electronic identity and less tunable coupling.

Method: Fabrication and characterization of hybrid metal-semiconductor islands with a submicron metallic component contacting a gate-confined InAs quantum well, featuring tunable couplings to macroscopic leads.

Result: The study demonstrated highly uniform Coulomb peaks in single-electron transistors formed in the weak-coupling limit, with no resolvable excitation spectrum in the Coulomb diamonds. Increasing transmission towards the ballistic regime showed an evolution to dynamical Coulomb blockade.

Conclusion: Arrays of hybrid metal-semiconductor islands provide a scalable platform for quantum simulation, offering advantages over conventional quantum dots due to electronically identical sites and tunable intersite coupling.

Abstract: Arrays of hybrid metal-semiconductor islands offer a new approach to quantum
simulation, with key advantages over arrays of conventional quantum dots.
Because the metallic component of these hybrid islands has a quasi-continuous
level spectrum, each site in an array can be effectively electronically
identical; in contrast, each conventional semiconductor quantum dot has its own
spectral fingerprint. Meanwhile, the semiconductor component retains
gate-tunability of intersite coupling. This combination creates a scalable
platform for simulating correlated ground states driven by Coulomb
interactions. We report the fabrication and characterization of hybrid
metal-semiconductor islands, featuring a submicron metallic component
transparently contacting a gate-confined region of an InAs quantum well with
tunable couplings to macroscopic leads. Tuning to the weak-coupling limit forms
a single-electron transistor with highly-uniform Coulomb peaks, with no
resolvable excitation spectrum in the Coulomb diamonds. Upon increasing the
transmissions toward the ballistic regime we observe an evolution to dynamical
Coulomb blockade.

</details>


### [301] [Transmon qubit using Sn as a junction superconductor](https://arxiv.org/abs/2508.04007)
*Amrita Purkayastha,Amritesh Sharma,Param J. Patel,An-Hsi Chen,Connor P. Dempsey,Shreyas Asodekar,Subhayan Sinha,Maxime Tomasian,Mihir Pendharkar,Christopher J. Palmstrøm,Moïra Hocevar,Kun Zuo,Michael Hatridge,Sergey M. Frolov*

Main category: cond-mat.mes-hall

TL;DR: 使用InAs纳米线和β-Sn超导壳实现的超浸子量子比特，实现了3 GHz的频率调谐范围，并讨论了提高相干时间的潜在因素和方法。


<details>
  <summary>Details</summary>
Motivation: 探索超越铝的超导材料，使用具有半导体势垒的结来提供非线性电感，从而实现可调谐的量子比特。

Method: 采用InAs半导体纳米线和β-Sn超导壳构建超浸子量子比特。通过栅极电压调谐约瑟夫森能量，实现了3 GHz的量子比特频率调谐范围。

Result: 在最低量子比特频率下获得了最长的能量弛豫时间T1 = 27微秒，在较高频率下实现了最长的回波退相干时间T2 = 1.8微秒。

Conclusion: InAs纳米线与β-Sn超导壳相结合的器件实现了超导量子比特，并展示了通过栅极电压调谐量子比特频率的潜力。

Abstract: Superconductor qubits typically use aluminum-aluminum oxide tunnel junctions
to provide the non-linear inductance. Junctions with semiconductor barriers
make it possible to vary the superconductor material and explore beyond
aluminum. We use InAs semiconductor nanowires coated with thin superconducting
shells of beta-Sn to realize transmon qubits. By tuning the Josephson energy
with a gate voltage, we adjust the qubit frequency over a range of 3 GHz. The
longest energy relaxation time, T1 = 27 microseconds, is obtained at the lowest
qubit frequencies, while the longest echo dephasing time, T2 = 1.8
microseconds, is achieved at higher frequencies. We assess the possible factors
limiting coherence times in these devices and discuss steps to enhance
performance through improvements in materials fabrication and circuit design.

</details>


### [302] [Straightforward Method to Orient Black Phosphorus from Bulk to Thin Layers using a Standard Green Laser](https://arxiv.org/abs/2508.04142)
*Etienne Carré,Frédéric Fossard,Jean-Sébastien Mérot,Denis Boivin,Nicolas Horezan,Victor Zatko,Florian Godel,Bruno Dlubak,Marie-Blandine Martin,Pierre Seneor,Etienne Gaufres,Julien Barjon,Annick Loiseau,Ingrid Stenger*

Main category: cond-mat.mes-hall

TL;DR: 使用角度分辨偏振拉曼光谱（ARPRS）结合干涉效应和光学指数，可以简单有效地测定黑磷（BP）的取向，无需复杂的表征技术。


<details>
  <summary>Details</summary>
Motivation: 现有的黑磷（BP）取向技术（如TEM或XRD）复杂且不易于常规表征，需要更简单易行的方法。

Method: 使用单波长（514 nm）拉曼设置，通过角度分辨偏振拉曼光谱（ARPRS）结合厚度依赖的干涉效应和各向异性光学指数来测定黑磷（BP）的取向。

Result: 所提出的ARPRS方法可用于测定从块体晶体到薄层等不同厚度BP的取向，并通过TEM和EBSD验证了其对厚样品和超薄样品均适用。

Conclusion: 该方法通过结合角度分辨偏振拉曼光谱、依赖于厚度的干涉效应和各向异性光学指数，为不同厚度的黑磷（BP）提供了可靠的取向测定框架，并且已被TEM和EBSD验证，可应用于厚样品和超薄样品。

Abstract: The crystallographic orientation of anisotropic 2D materials plays a crucial
role in their physical properties and device performance. However, standard
orientation techniques such as transmission electron microscopy (TEM) or X-ray
diffraction (XRD) can be complex and less accessible for routine
characterization. In this study, we investigate the orientation of black
phosphorus (BP) from bulk crystals to thin layers using angle-resolved
polarized Raman spectroscopy (ARPRS) with a single-wavelength (514 nm) Raman
setup. By incorporating thickness-dependent interference effects and
anisotropic optical indices, this approach provides a reliable framework for
orientation determination across different BP thicknesses. The method is
validated through direct orientation measurements using TEM and Electron
Backscattering Diffraction (EBSD), confirming its applicability to both thick
and ultrathin samples. Given its simplicity and compatibility with widely
available Raman setups, this approach offers a practical solution for
characterizing BP orientation without requiring advanced structural
characterization techniques.

</details>


### [303] [Effect of screening on Seebeck coefficient in bilayer graphene/AlGaAs electron gas](https://arxiv.org/abs/2508.04184)
*Vo Van Tai,Nguyen Duy Vy,Truong Van Tuan,Nguyen Quoc Khanh*

Main category: cond-mat.mes-hall

TL;DR: 本研究在低温（低于50K）下，分析了双层石墨烯(BLG)体系中由声子拖曳引起的塞曼系数(S^g)受二维电子气(q2DEG)筛选效应的影响。研究发现筛选效应在低温下显著，并与载流子密度和层间距有关。该研究为理解和优化热电材料在特定条件下的性能提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 为了优化热电材料和寻找合适的应用，了解塞曼系数至关重要。塞曼系数对结构变化的敏感性使其成为研究材料电荷输运特性的有效手段。在低温下，声子拖曳项在塞曼系数中占主导地位。本研究旨在深入理解在双层石墨烯(BLG)-准二维电子气(q2DEG)体系中，低温筛选效应对声子拖曳引起的塞曼系数的具体影响及其调控机制。

Method: 本研究通过理论计算，考察了在低于50K的温度范围内，双层石墨烯(BLG)-准二维电子气(q2DEG)体系中，由声子拖曳引起的塞曼系数(S^g)的温度依赖性筛选效应。研究对比了BLG在有无q2DEG筛选条件下的电子-声子相互作用，并分析了筛选效应对S^g的影响，同时探讨了载流子密度和量子阱宽度等因素对筛选效应的调节作用。

Result: 在低于50K的温度下，双层石墨烯(BLG)-准二维电子气(q2DEG)体系中的声子拖曳塞曼系数(S^g)受筛选效应影响。筛选效应在低温下尤为显著，并随BLG载流子密度变化。双层筛选函数随层间距增大而增大，与单层筛选函数行为相似。量子阱宽度影响筛选效应，在100 Å以下增强，以上则保持不变或减小。BLG载流子密度低于q2DEG时，两种筛选函数均增强。

Conclusion: 研究表明，在低于50K的温度下，双层石墨烯(BLG)中由声子拖曳引起的塞曼系数(S^g)会受到二维电子气(q2DEG)的筛选效应影响。筛选效应在低温下尤为显著，并与BLG中的载流子密度密切相关。双层筛选函数随层间距d的增大而增大，与单层筛选函数在大d时的行为相似。此外，量子阱宽度也会影响筛选效应，但其影响在100 Å后趋于饱和。当BLG的载流子密度低于q2DEG时，两种筛选函数均增强，但增强幅度相差不大。

Abstract: The knowledge of Seebeck coefficient is a key factor in optimization of
thermoelectric materials and finding right applications for it. A high
sensitivity to structural change makes thermopower measurements an excellent
technique for the study on the charge transport properties of a given material.
The phonondrag term dominates at low temperature in the Seebeck coefficient
This study examines the temperaturedependent screening effect on the
phonondraginduced Seebeck coefficient S^g in a bilayer graphene-
BLG-AlGaAs-quasi-twodimensional electron gas (q2DEG) system at the temperature
below 50 K. The BLG layer interacts with both deformation potential acoustic
phonons and stronger piezoelectric field acoustic phonons from AlGaAs/GaAs. We
compare the electronphonon interactions in BLG with and without screening by
q2DEG. The screening effect reduces particularly at low temperatures and shows
a strong dependence on the carrier density in the BLG layer. The doublelayer
screening function increases with layer separation d paralleling the monolayer
screening at large d. Additionally varying the GaAs quantum well width reveals
that increases with width less than 100 \AA under doublelayer screening but
remains unchanged beyond this threshold while monolayer screening decreases as
the width increases. Both screening functions enhance when the BLG carrier
density is lower than that of q2DEG though the magnitude difference between
them is minimal

</details>


### [304] [Inelastic electron tunneling through adatoms and molecular nanomagnets](https://arxiv.org/abs/2508.04449)
*Daria Kyvala,Jindrich Kolorenc*

Main category: cond-mat.mes-hall

TL;DR: This paper theoretically analyzes inelastic electron tunneling spectra (IETS) in magnetic nanosystems on surfaces. It finds that the observed spectral transitions depend on whether the magnetic moment is dominated by electron spin or orbital angular momentum, leading to different selection rules.


<details>
  <summary>Details</summary>
Motivation: To theoretically describe the IETS of magnetic nanosystems on solid surfaces, accounting for complex interactions like sequential tunneling and strong spin-orbit coupling.

Method: The paper uses a cluster Hubbard model to theoretically describe the inelastic electron tunneling spectra (IETS) of magnetic nanosystems adsorbed on solid surfaces, considering scenarios with sequential tunneling through multiple magnetic centers or strong spin-orbit coupling.

Result: The study finds that IETS transitions are governed by different selection rules depending on the nature of the magnetic moment. Specifically, for atoms with large orbital moments, transitions follow $\Delta J_z\leq 2\ell+1$, while for spin-dominated moments, $\Delta J_z\leq 1$ is observed. An example of sequential tunneling through a nickelocene molecule is also illustrated.

Conclusion: IETS spectra of magnetic nanosystems are governed by selection rules that depend on whether spin or orbital momentum dominates the magnetic moment. For orbital-momentum dominated moments, transitions follow $\Delta J_z\leq 2\ell+1$. For spin-dominated moments, the traditional $\Delta J_z\leq 1$ rule applies.

Abstract: We discuss a theoretical description of the inelastic electron tunneling
spectra (IETS) of a magnetic nanosystem (an atom or a molecule) adsorbed on a
solid surface measured in a scanning tunneling microscope (STM). We represent
the nanosystem by means of a cluster Hubbard model, which allows us to study
scenarios when the tunneling electrons sequentially interact with several
magnetic centers inside the nanosystem or when the magnetic centers are made
out of heavy atoms with a strong spin-orbit coupling and large orbital moments.
The sequential tunneling through multiple centers is illustrated on an adatom
probed by an STM tip with a nickelocene molecule attached to it. For atoms with
a large orbital moment, we find the transitions accessible by IETS to be
governed by the selection rule $\Delta J_z\leq 2\ell+1$, where $J_z$ is the
projection of the total angular momentum of the atom to the quantization axis
and $\ell$ is the orbital momentum quantum number of the partially filled
atomic shell carrying the magnetic moment. For atoms with magnetic moments
dominated by spin, the spectra are naturally dominated by transitions
fulfilling the traditional selection rule $\Delta J_z\leq 1$.

</details>


### [305] [Structural and helix reversal defects of carbon nanosprings](https://arxiv.org/abs/2508.04490)
*Alexander V. Savin,Elena A. Korznikova,Sergey V. Dmitriev*

Main category: cond-mat.mes-hall

TL;DR: 本研究使用分子动力学模拟分析了碳纳米弹簧的弯曲和扭转变形，发现其轴向热膨胀系数高于许多金属，适用于宽温度范围的纳米传感器。


<details>
  <summary>Details</summary>
Motivation: 研究碳纳米弹簧的结构转变，特别是弯曲和扭转模式，以及它们在不同温度下的性能，以用于纳米技术和纳米传感器设计。

Method: 使用分子动力学模拟分析了碳纳米弹簧的结构转变，包括弯曲和扭转变形模式。

Result: 碳纳米弹簧的轴向热膨胀系数显著高于许多金属和合金，这对于设计在宽温度范围内运行的纳米传感器很有用。

Conclusion: 碳纳米弹簧由于其手性结构，具有独特且有潜力用于纳米技术的特性。本研究通过分子动力学模拟分析了由平面并环[18]冠[6]和 Kekulene 分子（石墨烯螺旋体和螺旋纳米带）形式的螺旋大分子的碳纳米弹簧的结构转变。虽然碳纳米弹簧的拉伸/压缩性质已被文献分析，但本研究探讨了包括弯曲和扭转在内的其他变形模式。根据碳纳米弹簧的几何特性，描述了结构和螺旋反转缺陷的形成。研究发现，碳纳米弹簧的轴向热膨胀系数显著高于许多金属和合金。这些结果对于设计在宽温度范围内运行的纳米传感器非常有用。

Abstract: Due to their chiral structure, carbon nanosprings possess unique properties
that are promising for nanotechnology applications. The structural
transformations of carbon nanosprings in the form of spiral macromolecules
derived from planar coronene and kekulene molecules (graphene helicoids and
spiral nanoribbons) are analyzed using molecular dynamics simulations. While
the tension/compression of such nanosprings has been analyzed in the
literature, this study investigates other modes of deformation, including
bending and twisting. Depending on the geometric characteristics of the carbon
nanosprings, the formation of structural and helix reversal defects is
described. It is found that nanosprings demonstrate a significantly higher
coefficient of axial thermal expansion than many metals and alloys. These
results are useful for designing nanosensors that operate over a wide
temperature range.

</details>


### [306] [Density of States (Gate) - Controlled Andreev Molecule and Sensor](https://arxiv.org/abs/2508.04519)
*Xiaofan Shi,Ziwei Dou,Guoan Li,Dong Pan,Yuxiao Song,Anqi Wang,Zhiyuan Zhang,Xingchen Guo,Xiao Deng,Ruixuan Zhang,Liangqian Xu,Xiao Chen,Yupeng Li,Bingbing Tong,Xiaohui Song,Zhaozheng Lyu,Peiling Li,Fanming Qu,Guangtong Liu,Jianhua Zhao,Li Lu,Jie Shen*

Main category: cond-mat.mes-hall

TL;DR: 通过门控安德森分子实现了无需高磁场和超导环路即可精确控制拓扑量子比特，并提高了传感器的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 现有安德森分子（拓扑量子计算和近藤链的关键组成部分）的磁通量控制方式限制了其可扩展性。本研究旨在提供一种更优越的控制方式。

Method: 提出了一种门控安德森分子，利用静电调控实现对安德森分子的控制，并将其扩展到多位点近藤链和奇偶校验读数传感器。

Result: 成功引入了一种门控安德森分子，实现了对安德森分子的精确控制，并扩展到多位点近藤链和高灵敏度量子传感器，为拓扑量子计算和量子传感器的发展奠定了基础。

Conclusion: 该平台通过静电调控单个位点的状态密度，非局域地增强了另一个位点的临界电流，从而消除了超导环路，实现了无需高磁场的门控安德森分子，这在可扩展性、可调性和灵敏度方面具有优势。此外，该研究将安德森分子扩展到多位点近藤链，并实现了能够分辨单库珀对电荷以进行奇偶校验读数的非侵入式传感器，为拓扑超导安德森束和长近藤链的构建以及拓扑量子比特的实现提供了新的途径。

Abstract: Topological quantum computing typically relies on topological Andreev bound
states (ABSs) engineered in hybrid superconductor-semiconductor devices, where
gate control offers key advantages. While strong Zeeman fields can induce such
states, an alternative approach emerges through Andreev molecules -- closely
spaced, coupled ABSs, also key building-block for Kitaev chain -- that enable
topological behavior without high magnetic fields. However, existing Andreev
molecules are controlled via magnetic flux in superconducting loops, limiting
scalability. Here, we introduce a gate-controlled Andreev molecule, where
electrostatic tuning of the density of states in one site nonlocally enhances
the critical current of another. This eliminates superconducting loops,
offering superior tunability, scalability, and sensitivity. We further extend
such an Andreev molecule to a multi-site Kitaev chain, and a noninvasive sensor
resolving single-Cooper-pair charge for parity readout. This platform bridges
the gap between scalable ABS engineering and high-sensitivity quantum sensing,
advancing the development for constructing and parity-readout in topological
ABSs and long Kitaev chains towards topological qubits.

</details>


### [307] [Localization structure of electronic states in the quantum Hall effect](https://arxiv.org/abs/2508.04528)
*Alioune Seye,Marcel Filoche*

Main category: cond-mat.mes-hall

TL;DR: A magnetic localization landscape (MLL) approach effectively models electronic state localization in the Integer Quantum Hall Effect, offering insights beyond traditional methods.


<details>
  <summary>Details</summary>
Motivation: The study investigates the localization of electronic states in the Integer Quantum Hall Effect (IQHE) using a novel magnetic localization landscape (MLL) approach.

Method: We utilized a magnetic localization landscape (MLL) approach, defining a modified landscape function that incorporates magnetic effects, to study a continuum Schr"odinger model with a disordered electrostatic potential.

Result: The MLL effective potential successfully captures key localization features, predicts eigenstate energies, and reveals spatial confinement regions. Numerical simulations show energy-dependent localization patterns around potential minima/maxima and significant edge effects near boundaries.

Conclusion: The MLL framework provides a robust method for understanding transport and localization in disordered quantum Hall systems, extending landscape theory to magnetic systems and bridging semiclassical intuition with full quantum models.

Abstract: We investigate the localization of electronic states in the Integer Quantum
Hall Effect (IQHE) using a magnetic localization landscape (MLL) approach. By
studying a continuum Schr\"odinger model with disordered electrostatic
potential, we demonstrate that the MLL, defined via a modified landscape
function incorporating magnetic effects, captures key features of quantum state
localization. The MLL effective potential reveals the spatial confinement
regions and provides predictions of eigenstate energies, particularly in
regimes where traditional semiclassical approximations break down. Numerical
simulations show that below a critical energy, states localize around minima of
the effective potential, while above it, they cluster around maxima-with edge
effects becoming significant near boundaries. Bridging the gap between
semiclassical intuition and full quantum models, the MLL offers a robust
framework to understand transport and localization in disordered quantum Hall
systems, and extends the applicability of landscape theory to magnetic systems.

</details>


### [308] [Light induced transitions of valley Chern numbers and flat bands in a non-twisted moire graphene-hexagonal boron nitride superlattice](https://arxiv.org/abs/2508.04620)
*Saud Alabdulal,Miftah Hadi Syahputra Anfa,Hocine Bahlouli,Michael Vogl*

Main category: cond-mat.mes-hall

TL;DR: We study non-twisted moire materials under light and find rich topology and band flattening effects similar to twisted moire materials, suggesting these phenomena are present in more accessible platforms.


<details>
  <summary>Details</summary>
Motivation: Motivated by the rich topology and interesting quasi-band structure of twisted moire materials subjected to light, we study a non-twisted moire material under the influence of light. Our work is in part motivated by a desire to find an easier-to-synthesize platform that can help experimentally elucidate the interesting physics of moire materials coupled to light.

Method: We study a non-twisted moire material under the influence of light.

Result: Similar to twisted moire materials, we uncover rich topology and interesting band flattening effects, which we summarize in relevant plots such as a topological phase diagram.

Conclusion: Much of the interesting phenomenology of twisted moire materials under the influence of electromagnetic waves seems to be generically present even in more experimentally accessible untwisted moire platforms, which remain highly tunable by light.

Abstract: Motivated by the rich topology and interesting quasi-band structure of
twisted moire materials subjected to light, we study a non-twisted moire
material under the influence of light. Our work is in part motivated by a
desire to find an easier-to-synthesize platform that can help experimentally
elucidate the interesting physics of moir\'e materials coupled to light.
Similar to twisted moire materials, we uncover rich topology and interesting
band flattening effects, which we summarize in relevant plots such as a
topological phase diagram. Our work demonstrates that much of the interesting
phenomenology of twisted moire materials under the influence of electromagnetic
waves seems to be generically present even in more experimentally accessible
untwisted moire platforms, which remain highly tunable by light.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [309] [A 60-Addition, Rank-23 Scheme for Exact 3x3 Matrix Multiplication](https://arxiv.org/abs/2508.03857)
*Joshua Stapleton*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We reduce the additive cost of general (non-commutative) 3x3 matrix
multiplication from the previous records of 61 (Schwartz-Vaknin, 2023) and 62
(Martensson-Wagner, 2025) to 60 without a change of basis. To our knowledge,
this represents a new state-of-the-art.

</details>


### [310] [Counting Distinct Square Substrings in Sublinear Time](https://arxiv.org/abs/2508.03930)
*Panagiotis Charalampopoulos,Manal Mohamed,Jakub Radoszewski,Wojciech Rytter,Tomasz Waleń,Wiktor Zuba*

Main category: cs.DS

TL;DR: 本文提出了一种在压缩字符串中计算不同平方数的亚线性时间算法（O(n/logσn)），解决了长周期运行和层运行的计数难题，并引入了稀疏-Lyndon根来处理压缩字符串中的Lyndon根计算问题。


<details>
  <summary>Details</summary>
Motivation: 在压缩字符串的表示中，高效地计算不同平方数的数量，特别是在亚线性时间内。

Method: 本文利用了 Crochemore 等人 [TCS 2014] 描述的从运行中提取平方的技术，并针对压缩模型开发了新的方法，包括对长周期运行（周期为 Ω(logσn)）的 O(n/logσn) 大小的表示，以及利用金字塔状的层运行组的组合性质来计算层运行中的平方数。

Result: 在压缩字符串设置中，使用字-RAM模型，在 O(n/logσn) 时间内计算出不同平方的数量。

Conclusion: 本文首次提出了在压缩字符串设置中计算不同平方数的亚线性时间算法，时间复杂度为 O(n/logσn)。

Abstract: We show that the number of distinct squares in a packed string of length $n$
over an alphabet of size $\sigma$ can be computed in $O(n/\log_\sigma n)$ time
in the word-RAM model. This paper is the first to introduce a sublinear-time
algorithm for counting squares in the packed setting. The packed representation
of a string of length $n$ over an alphabet of size $\sigma$ is given as a
sequence of $O(n/\log_\sigma n)$ machine words in the word-RAM model (a machine
word consists of $\omega \ge \log_2 n$ bits). Previously, it was known how to
count distinct squares in $O(n)$ time [Gusfield and Stoye, JCSS 2004], even for
a string over an integer alphabet [Crochemore et al., TCS 2014; Bannai et al.,
CPM 2017; Charalampopoulos et al., SPIRE 2020]. We use the techniques for
extracting squares from runs described by Crochemore et al. [TCS 2014].
However, the packed model requires novel approaches.
  We need an $O(n/\log_\sigma n)$-sized representation of all long-period runs
(runs with period $\Omega(\log_\sigma n)$) which allows for a sublinear-time
counting of the -- potentially linearly-many -- implied squares. The
long-period runs with a string period that is periodic itself (called layer
runs) are an obstacle, since their number can be $\Omega(n)$. The number of all
other long-period runs is $O(n/\log_\sigma n)$ and we can construct an implicit
representation of all long-period runs in $O(n/\log_\sigma n)$ time by
leveraging the insights of Amir et al. [ESA 2019]. We count squares in layer
runs by exploiting combinatorial properties of pyramidally-shaped groups of
layer runs. Another difficulty lies in computing the locations of Lyndon roots
of runs in packed strings, which is needed for grouping runs that may generate
equal squares. To overcome this difficulty, we introduce sparse-Lyndon roots
which are based on string synchronizers [Kempa and Kociumaka, STOC 2019].

</details>


### [311] [Exactly simulating stochastic chemical reaction networks in sub-constant time per reaction](https://arxiv.org/abs/2508.04079)
*Joshua Petrack,David Doty*

Main category: cs.DS

TL;DR: 本研究提出了一种新的化学反应网络模拟算法，比Gillespie算法更快，运行时间与模拟的反应数量成亚线性关系。


<details>
  <summary>Details</summary>
Motivation: 传统的Gillespie算法模拟l个反应需要O(l)的时间。本研究旨在提出一种更快的算法，以亚线性时间模拟化学反应网络。

Method: 该算法改编自Berenbrink等人提出的用于模拟分布式计算模型“种群协议”的算法，并将其扩展到更通用的化学反应网络模型。

Result: 在合理假设下，该算法能以亚线性时间（O(l/sqrt(n))或O(l/n^(2/5))）模拟l个化学反应，其性能优于传统的Gillespie算法。研究还提供了基于Rust实现的Python包。

Conclusion: 该研究首次提出了化学反应网络随机模拟算法，该算法能够模拟l个反应，并精确地保持随机动力学，同时运行时间与l成亚线性关系。在合理假设下，该算法模拟n个分子间l个反应的时间复杂度为：当l >= n^(5/4)时为O(l/sqrt(n))，当n <= l <= n^(5/4)时为O(l/n^(2/5))。

Abstract: The model of chemical reaction networks is among the oldest and most widely
studied and used in natural science. The model describes reactions among
abstract chemical species, for instance $A + B \to C$, which indicates that if
a molecule of type $A$ interacts with a molecule of type $B$ (the reactants),
they may stick together to form a molecule of type $C$ (the product). The
standard algorithm for simulating (discrete, stochastic) chemical reaction
networks is the Gillespie algorithm [JPC 1977], which stochastically simulates
one reaction at a time, so to simulate $\ell$ consecutive reactions, it
requires total running time $\Omega(\ell)$.
  We give the first chemical reaction network stochastic simulation algorithm
that can simulate $\ell$ reactions, provably preserving the exact stochastic
dynamics (sampling from precisely the same distribution as the Gillespie
algorithm), yet using time provably sublinear in $\ell$. Under reasonable
assumptions, our algorithm can simulate $\ell$ reactions among $n$ total
molecules in time $O(\ell/\sqrt n)$ when $\ell \ge n^{5/4}$, and in time
$O(\ell/n^{2/5})$ when $n \le \ell \le n^{5/4}$. Our work adapts an algorithm
of Berenbrink, Hammer, Kaaser, Meyer, Penschuck, and Tran [ESA 2020] for
simulating the distributed computing model known as population protocols,
extending it (in a very nontrivial way) to the more general chemical reaction
network setting.
  We provide an implementation of our algorithm as a Python package, with the
core logic implemented in Rust, with remarkably fast performance in practice.

</details>


### [312] [Exact Matching in Matrix Multiplication Time](https://arxiv.org/abs/2508.04081)
*Ryotaro Sato,Yutaro Yamaguchi*

Main category: cs.DS

TL;DR: 通过快速计算特征多项式，改进了匹配算法，实现了与矩阵乘法相当的效率，并扩展到线性秩相关问题。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在改进已有的代数匹配算法，并探索使用快速计算特征多项式的方法来提高算法效率。

Method: 通过快速计算矩阵的特征多项式来改进代数匹配算法，并将其应用于精确匹配和线性秩相关问题。

Result: 精确匹配问题可以在与矩阵乘法相同的时间复杂度内以高概率解决，并讨论了该方法在解决线性秩相关问题上的扩展性。

Conclusion: 该研究展示了如何通过快速计算矩阵的特征多项式来改进代数匹配算法，并将精确匹配问题在极高概率下在与矩阵乘法相同的时间复杂度内解决，同时讨论了该方法在解决线性秩相关问题上的扩展性。

Abstract: Initiated by Mulmuley, Vazirani, and Vazirani (1987), many algebraic
algorithms have been developed for matching and related problems. In this
paper, we review basic facts and discuss possible improvements with the aid of
fast computation of the characteristic polynomial of a matrix. In particular,
we show that the so-called exact matching problem can be solved with high
probability in asymptotically the same time order as matrix multiplication. We
also discuss its extension to the linear matroid parity problem.

</details>


### [313] [Approximation Algorithms for Scheduling Crowdsourcing Tasks in Mobile Social Networks](https://arxiv.org/abs/2508.04159)
*Chi-Yeh Chen*

Main category: cs.DS

TL;DR: 本文修正了移动社交网络调度问题的近似比分析，并提出了两种新的近似算法，一种是随机算法，另一种是确定性算法，旨在最小化总加权完成时间。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决移动社交网络中的调度问题，特别是最小化总加权完成时间。

Method: 本文首先证明了Zhang等人（IEEE Transactions on Mobile Computing, 2025）提出的近似比分析是不正确的，并给出了正确的分析结果。接着，提出了一种随机近似算法，将总加权完成时间的近似比期望值控制在 $1.5+\epsilon$。

Result: 本文纠正了先前研究的近似比分析，并提出了两种新的近似算法。随机算法的期望近似比为 $1.5+\epsilon$，确定性算法的近似比为 $\max\left\{2.5,1+\epsilon\right\}$，在特定条件下可达 $1.5+\epsilon$。

Conclusion: 文章最后提出了一个确定性近似算法，用于最小化移动社交网络中的总加权完成时间。该算法实现了 $\max\left\{2.5,1+\epsilon\right\}$ 的近似比，并且在特定条件下（任务所需服务时间或总接触时间足够大）可以达到 $1.5+\epsilon$ 的近似比。

Abstract: This paper addresses the scheduling problem in mobile social networks. We
begin by proving that the approximation ratio analysis presented in the paper
by Zhang \textit{et al.} (IEEE Transactions on Mobile Computing, 2025) is
incorrect, and we provide the correct analysis results. Furthermore, when the
required service time for a task exceeds the total contact time between the
requester and the crowd worker, we demonstrate that the approximation ratio of
the Largest-Ratio-First task scheduling algorithm can reach $2 - \frac{1}{m}$.
Next, we introduce a randomized approximation algorithm to minimize mobile
social networks' total weighted completion time. This algorithm achieves an
expected approximation ratio of $1.5 + \epsilon$ for $\epsilon>0$. Finally, we
present a deterministic approximation algorithm that minimizes mobile social
networks' total weighted completion time. This deterministic algorithm achieves
an approximation ratio of $\max\left\{2.5,1+\epsilon\right\}$ for $\epsilon>0$.
Additionally, when the task's required service time or the total contact time
between the requester and the crowd worker is sufficiently large, this
algorithm can reach an approximation ratio of $1.5+\epsilon$ for $\epsilon>0$.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [314] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: RLGS 是一个强化学习框架，可以自动调整 3DGS 的超参数，以获得更好的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS 中的超参数调整过程劳动密集且依赖专家，这常常导致不一致的重建和次优化的结果。

Method: 提出 RLGS，一个即插即用的强化学习框架，通过轻量级策略模块自适应调整 3DGS 中的超参数，例如学习率和稠密化阈值。

Result: RLGS 持续提升渲染质量，例如，在固定的高斯预算下，将 Taming-3DGS 在 Tanks and Temple (TNT) 数据集上的 PSNR 提高了 0.7dB，并且在基线性能饱和时仍能产生改进。

Conclusion: RLGS 为 3DGS 训练提供了有效且通用的自动化超参数调整解决方案，弥合了强化学习在 3DGS 应用中的差距。

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


### [315] [Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research](https://arxiv.org/abs/2508.04326)
*Ke Li,Mana Masuda,Susanne Schmidt,Shohei Mori*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

</details>


### [316] [Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds](https://arxiv.org/abs/2508.04508)
*Haodong Zhu,Changbai Li,Yangyang Ren,Zichao Feng,Xuhui Liu,Hanlin Chen,Xiantong Zhen,Baochang Zhang*

Main category: cs.GR

TL;DR: Surf3R是一种创新的端到端3D重建方法，无需相机校准即可从稀疏视图快速重建高质量3D表面，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建方法依赖于准确的相机校准和姿态估计，预处理复杂且耗时，阻碍了实际应用。Surf3R旨在解决此问题，无需姿态估计即可从稀疏视图重建3D表面，并在10秒内完成整个场景的重建。

Method: Surf3R是一种端到端的前馈方法，采用多分支、多视图解码架构，通过分支处理、跨视图注意力和分支间融合来引导重建过程，并引入基于显式3D高斯表示的D-Normal正则化器来联合优化3D几何。

Result: Surf3R能够从稀疏视图中重建3D表面，并在10秒内完成整个场景的重建，同时还能有效捕捉互补的几何线索，无需相机校准，显著提高了3D一致性和表面细节的准确性。

Conclusion: Surf3R方法在ScanNet++和Replica数据集上实现了最先进的表面重建指标，并表现出出色的泛化能力和效率。

Abstract: Current multi-view 3D reconstruction methods rely on accurate camera
calibration and pose estimation, requiring complex and time-intensive
pre-processing that hinders their practical deployment. To address this
challenge, we introduce Surf3R, an end-to-end feedforward approach that
reconstructs 3D surfaces from sparse views without estimating camera poses and
completes an entire scene in under 10 seconds. Our method employs a
multi-branch and multi-view decoding architecture in which multiple reference
views jointly guide the reconstruction process. Through the proposed
branch-wise processing, cross-view attention, and inter-branch fusion, the
model effectively captures complementary geometric cues without requiring
camera calibration. Moreover, we introduce a D-Normal regularizer based on an
explicit 3D Gaussian representation for surface reconstruction. It couples
surface normals with other geometric parameters to jointly optimize the 3D
geometry, significantly improving 3D consistency and surface detail accuracy.
Experimental results demonstrate that Surf3R achieves state-of-the-art
performance on multiple surface reconstruction metrics on ScanNet++ and Replica
datasets, exhibiting excellent generalization and efficiency.

</details>


### [317] [MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics](https://arxiv.org/abs/2508.04687)
*Ye Pan,Ruisi Zhang,Jingying Wang,Nengfu Chen,Yilin Qiu,Yu Ding,Kenny Mitchell*

Main category: cs.GR

TL;DR: 通过结合传统动画技术和机器学习，提出了一种新的动画系统，用于驱动3D风格化角色，并取得了优于商业产品的效果。


<details>
  <summary>Details</summary>
Motivation: 目的是改进基于性能的动画，通过驱动可信的3D风格化角色，实现真正的感知动画。

Method: 提出了一种结合传统blendshape动画技术和多种机器学习模型的非实时和实时系统。非实时系统利用2D人脸图像生成3D模型参数的3D情绪迁移网络；实时系统提出blendshape自适应网络，生成具有几何一致性和时间稳定性的角色参数运动。

Result: 提出的系统在表情识别度、强度和吸引力方面优于商业产品Faceware。

Conclusion: 研究结果显示，通过本系统驱动的动画角色在表情识别度、强度和吸引力方面均优于商业产品Faceware，可用于动画制作流程，帮助动画师更快、更准确地创作角色表情。

Abstract: Our purpose is to improve performance-based animation which can drive
believable 3D stylized characters that are truly perceptual. By combining
traditional blendshape animation techniques with multiple machine learning
models, we present both non-real time and real time solutions which drive
character expressions in a geometrically consistent and perceptually valid way.
For the non-real time system, we propose a 3D emotion transfer network makes
use of a 2D human image to generate a stylized 3D rig parameters. For the real
time system, we propose a blendshape adaption network which generates the
character rig parameter motions with geometric consistency and temporally
stability. We demonstrate the effectiveness of our system by comparing to a
commercial product Faceware. Results reveal that ratings of the recognition,
intensity, and attractiveness of expressions depicted for animated characters
via our systems are statistically higher than Faceware. Our results may be
implemented into the animation pipeline, and provide animators with a system
for creating the expressions they wish to use more quickly and accurately.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [318] [Simulations of dielectric permittivity of water by Machine Learned Potentials with long-range Coulombic interactions](https://arxiv.org/abs/2508.04628)
*Kehan Cai,Chunyi Zhang,Xifan Wu*

Main category: cond-mat.soft

TL;DR: 使用机器学习方法，结合深度势能和神经网络，在不同电边界条件下（金属、绝缘、Kirkwood-Frohlich）成功计算了水的介电常数及其相关性质，为模拟极性液体在不同静电环境下的介电性质提供了稳健通用的框架。


<details>
  <summary>Details</summary>
Motivation: 为了在机器学习框架下统一计算水的介电常数，并系统地结合各种电边界条件，以解决其在物理、生物和化学过程中的基本作用。

Method: 本研究采用机器学习框架，结合长期包含的深度势能和预测最大局域Wannier函数中心的辅助深度神经网络，并考察了金属、绝缘和Kirkwood-Frohlich三种电边界条件。

Result: 在三种不同的电边界条件下，成功计算了水的介电常数、Kirkwood相关因子和相关长度，并验证了该机器学习框架的稳健性和可推广性。ra.

Conclusion: 本文提出了一种统一的机器学习框架，用于在各种电边界条件下计算水的介电常数，并评估了不同边界条件对偶极子涨落和介电弛豫动力学的影响，同时展示了计算Kirkwood相关因子、相关长度和介电常数的一致方法，证明了长程静电在其中起着关键作用。

Abstract: The dielectric permittivity of liquid water is a fundamental property that
underlies its distinctive behaviors in numerious physical, biological, and
chemical processes. Within a machine learning framework, we present a unified
approach to compute the dielectric permittivity of water, systematically
incorporating various electric boundary conditions. Our method employs a
long-range-inclusive deep potential trained on data from hybrid density
functional theory calculations. Dielectric response is evaluated using an
auxiliary deep neural network that predicts the centers of maximally localized
Wannier functions. We investigate three types of electric boundary
conditions--metallic, insulating, and Kirkwood-Frohlich--to assess their
influence on correlated dipole fluctuations and dielectric relaxation dynamics.
In particular, we demonstrate a consistent methodology for computing the
Kirkwood correlation factor, correlation length, and dielectric permittivity
under each boundary condition, where long-range electrostatics play a critical
role. This work establishes a robust and generalizable machine-learning
framework for modeling the dielectric properties of polar liquids under diverse
electrostatic environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [319] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 本篇论文提出了一种名为LRTuckerRep的新模型，用于解决计算科学中关键的多维数据补全问题。该模型通过结合低秩性和平滑性，并采用创新的Tucker分解方法，有效克服了现有技术的不足。通过开发专门的算法，LRTuckerRep在图像修复和交通数据插补等任务中展现出优越的性能，尤其在高缺失率的情况下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的多维数据补全方法，如全局低秩近似或局部平滑正则化，存在计算成本高、可能破坏数据结构、需要手动调整参数和泛化能力差等局限性。本研究旨在克服这些局限性。

Method: 提出了一种新的低秩Tucker表示（LRTuckerRep）模型，该模型在Tucker分解中统一了全局和局部先验建模。LRTuckerRep通过因子矩阵上的自适应加权核范数和稀疏Tucker核来编码低秩性，并通过因子空间上的无参数拉普拉斯正则化来捕获平滑性。为了有效解决由此产生的非凸优化问题，开发了两种具有可证明收敛保证的迭代算法。

Result: LRTuckerRep模型在多维图像修复和交通数据插补实验中，与基线方法相比，在处理高缺失率时，实现了卓越的补全精度和鲁棒性。

Conclusion: LRTuckerRep模型在多维数据补全方面取得了优于基线方法的性能，特别是在高缺失率的情况下，表现出更高的准确性和鲁棒性。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [320] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon通过利用特权信息和监督对比学习，提高了情感模型从实验室到现实世界迁移的鲁棒性，并在基准测试中取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 情感计算（AC）在深度学习的推动下取得了显著进展，但仍然存在一个持续的挑战：情感模型从受控的实验室环境（体外）到不受控制的真实世界环境（体内）的可靠迁移。

Method: 提出了一种名为特权对比预训练（PriCon）的框架，该框架首先通过监督对比学习（SCL）进行预训练，然后充当特权信息学习（LUPI）框架内的教师模型。PriCon在训练过程中利用特权信息，并通过SCL增强了导出情感模型的鲁棒性。

Result: 在RECOLA和AGAIN两个情感数据集上进行的实验表明，使用PriCon训练的模型始终优于LUPI和端到端模型。值得注意的是，在许多情况下，PriCon模型的性能与在训练和测试期间均可访问所有模态的模型相当。

Conclusion: PriCon框架有潜力弥合体外和体内情感建模之间的差距，为现实世界应用提供了一种可扩展且实用的解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [321] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种新的轨迹压缩框架，通过频域物理建模和有界误差优化，能够高效压缩任意维度（包括三维）的轨迹数据，并在压缩率和保真度上均优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 针对位置感知设备产生的大量轨迹数据，需要一种能克服现有方法在处理二维轨迹、时间同步和运动连续性方面的不足的压缩方案。

Method: PILOT-C框架整合了频域物理建模和有界误差优化，通过独立压缩每个空间轴来支持任意维度轨迹。

Result: PILOT-C在压缩率方面平均比CISED-W高19.2%，在轨迹保真度方面平均比CISED-W低32.6%。在三维数据集上，PILOT-C相比SQUISH-E提高了49%的压缩率。

Conclusion: PILOT-C框架在压缩率和轨迹保真度方面均优于现有方法，并能有效扩展至三维轨迹，同时保持相同的计算复杂度。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [322] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 该研究提出了一种“嵌入->在潜空间学习->映射回环境空间”的框架，利用流形学习和LSTM/MVAR等机器学习技术，从大规模行人模拟中学习宏观密度演化的有效算子，实现了行人动态的快速准确模拟。


<details>
  <summary>Details</summary>
Motivation: 为了解决在众包动力学中连接微观和宏观建模尺度这一重要且开放的挑战，以便进行系统数值分析、优化和控制。

Method: 文章提出了一种结合流形和机器学习的方法，通过四个阶段来学习演化算子：1. 从离散的微观数据中导出连续的宏观场（密度）；2. 基于流形学习，构建从宏观空间到潜空间的映射；3. 在潜空间中学习降阶代理模型（使用LSTM和MVAR）；4. 将模型从潜空间重构回宏观空间。该方法在潜空间中使用奇异值分解（SVD）进行主成分分析（POD），以保持质量守恒。

Result: 该方法在走廊障碍物场景下，使用社会力模型生成数据，并施加周期性边界条件。数值结果表明，该方法具有高度准确性、稳健性和泛化能力，能够快速准确地对基于代理的行人动态模拟进行建模。

Conclusion: 该研究提出的结合流形和机器学习的方法能够为基于代理的模拟生成准确、稳健和可泛化的行人动态模型，从而实现快速模拟。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [323] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种创新的CXR诊断模型，通过“思考-回答”推理和可验证的奖励机制，解决了现有方法的局限性，并在多项评估中取得了显著优于其他模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型主要依赖于“一次性”诊断方法，缺乏对推理过程的可验证监督，这导致了多任务CXR诊断中的推理时间长、奖励稀疏和幻觉频繁等挑战。

Method: 提出了一种名为CX-Mind的生成模型，该模型通过基于课程的强化学习和可验证的过程奖励（CuRL-VPR）实现用于CXR任务的交错“思考-回答”推理。具体来说，构建了一个包含708,473张图像和2,619,148个样本的指令调优数据集CX-Set，并生成了42,828个由临床报告监督的高质量交错推理数据点。在Group Relative Policy Optimization框架下进行了两个阶段的优化：首先利用闭域任务稳定基本推理，然后转移到开放域诊断，并结合基于规则的条件过程奖励，以绕过对预训练奖励模型的需求。

Result: CX-Mind在视觉理解、文本生成和时空对齐方面显著优于现有的医学和通用领域MLLMs，平均性能比同类CXR特定模型提高了25.1%。在真实世界临床数据集（Rui-CXR）上，CX-Mind在14种疾病上的平均recall@1大幅超过了第二名的结果，并且多中心专家评估证实了其临床实用性。

Conclusion: CX-Mind在视觉理解、文本生成和时空对齐方面显著优于现有的医学和通用领域MLLMs，在与CXR相关的特定模型上平均性能提高了25.1%。在真实世界临床数据集（Rui-CXR）上，CX-Mind在14种疾病上的平均recall@1大幅超过了第二名的结果，多中心专家评估也进一步证实了其在多个维度上的临床效用。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [324] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: LLMs可以通过LKS进行大规模知识编辑，同时保留其通用能力。


<details>
  <summary>Details</summary>
Motivation: 现有的模型编辑方法难以同时编辑大量事实信息，并且可能会损害模型的通用能力。

Method: 提出了一种名为潜在知识手术刀（LKS）的LLM编辑器，该编辑器通过轻量级超网络操纵特定实体的潜在知识，以实现精确和大规模的编辑。

Result: 在Llama-2和Mistral上进行的实验表明，即使同时编辑的数量达到10,000个，LKS也能有效地执行知识编辑，同时保留了被编辑LLM的通用能力。

Conclusion: LLMs可以通过编辑其内部表示来大规模地进行知识编辑，并且可以保留其通用能力。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [325] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 该研究提出了一种新的随机算法，用于在矩阵自由设置中估计矩阵范数，并展示了其在深度神经网络和推荐系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 提出新的随机算法用于估计矩阵的二比无穷和一比二范数，并展示其在深度神经网络训练和推荐系统中的应用。

Method: 基于Hutchinson对角线估计器及其Hutch++版本的修改，并提供了相应的Oracle复杂度界限。

Result: 提供了两种方法的Oracle复杂度界限，并展示了其在图像分类任务中的实际应用以及在推荐系统中缓解对抗性攻击的效果。

Conclusion: 所提出的新随机算法可用于矩阵自由设置中的二比无穷和一比二范数估计，并可应用于深度神经网络训练中的基于雅可比的正则化以及推荐系统中对抗性攻击的缓解。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [326] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: MMSFM是一种新的方法，可以对高维系统进行建模，即使只有稀疏的、不规则时间点的观测值，并且在各种数据集上都显示出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 从稀疏的、不规则时间点的快照观测值中模拟高维系统的演变，这在高维空间中是一个重大挑战，因为传统的降维技术可能会过度简化动力学并忽略非平衡系统中的瞬态行为。

Method: 提出了一种新颖的模拟免费分数和流匹配方法到多边际设置的扩展，称为多边际随机流匹配（MMSFM），它使在高维空间中进行分数匹配，以避免过拟合，并使用测量值样条来处理不规则快照时间。

Result: MMSFM在高维空间中实现了数据对齐，而无需降维，并且对不规则快照时间具有鲁棒性。

Conclusion: MMSFM在多个合成和基准数据集上得到了验证，包括在不均匀时间点收集的基因表达数据和图像渐进任务，证明了该方法的通用性。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [327] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一个多模态框架，整合了眼底图像、临床特征和文本描述，通过增强的XGBoost模型提高了青光眼检测的准确性和可解释性，准确率达98.71%。


<details>
  <summary>Details</summary>
Motivation: 早期、准确地检测青光眼对于防止不可逆的视力丧失至关重要。然而，现有方法通常依赖于单一模态数据且缺乏可解释性，这限制了它们的临床效用。

Method: GlaBoost是一个多模态梯度提升框架，它整合了结构化临床特征、眼底图像嵌入和专家定义的文本描述，以预测青光眼风险。该框架使用预训练的卷积编码器从视网膜眼底照片中提取高级视觉表示，并使用基于Transformer的语言模型对自由文本神经视网膜边缘评估进行编码。这些异构信号与手动评估的风险评分和量化眼科指标一起，被融合到一个统一的特征空间中，并通过增强的XGBoost模型进行分类。

Result: 实验表明，GlaBoost显著优于基线模型，验证准确率为98.71%。特征重要性分析揭示了与临床一致的模式，其中杯盘比、视杯苍白和特定的文本嵌入对模型决策的贡献最大。

Conclusion: GlaBoost提供了一种可解释且可扩展的青光眼诊断解决方案，并可扩展应用于其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [328] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: This paper uses deep learning, specifically cGANs, to improve emotion detection by combining text, audio, and facial data, showing better results than older methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional unimodal techniques in emotion detection, this paper explores a multimodal framework that integrates various data types.

Method: This paper proposes a multimodal framework integrating text, audio, and facial expressions, utilizing Conditional Generative Adversarial Networks (cGANs) trained to generate synthetic emotion-rich data and improve classification accuracy across multiple modalities.

Result: Experimental results demonstrate significant improvements in emotion recognition performance compared to baseline models.

Conclusion: The study highlights the potential of cGANs in enhancing human-computer interaction systems by enabling more nuanced emotional understanding.

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [329] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 受大脑启发，提出了一种结合静态和动态 SNN 的终身 NIDS 架构，使用 GWR 和 Ad-STDP 进行增量学习，在 UNSW-NB15 数据集上达到 85.3% 的准确率，并有望实现低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑层级处理和能源效率的启发，旨在开发一种用于终身网络入侵检测系统（NIDS）的尖峰神经网络（SNN）架构。

Method: 提出了一种受大脑启发的尖峰神经网络（SNN）架构，用于网络入侵检测系统（NIDS）。该系统采用静态 SNN 识别潜在入侵，然后激活动态 SNN 对攻击类型进行分类。动态分类器采用“按需生长”（GWR）结构可塑性和新颖的自适应尖峰时间依赖可塑性（Ad-STDP）学习规则，以实现增量学习和知识保留。

Result: 在 UNSW-NB15 基准测试中，该架构在持续学习设置下表现出强大的适应性，减少了灾难性遗忘，并实现了 85.3% 的总体准确率。使用 Intel Lava 框架的模拟证实了高运行稀疏性，证明了其在神经形态硬件上低功耗部署的潜力。

Conclusion: 该架构展示了强大的适应性，减少了灾难性遗忘，并在持续学习环境中实现了 85.3% 的总体准确率，这表明它在持续学习网络入侵检测方面具有潜力。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [330] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: LLMPrior利用LLM将非结构化数据转换为贝叶斯先验，Fed-LLMPrior在此基础上聚合分布式先验。


<details>
  <summary>Details</summary>
Motivation: 先验分布的规范是贝叶斯推理的基础，但它仍然是一个重大的瓶颈。先验提取过程通常是手动、主观且不可扩展的任务。

Method: 提出了一种利用大型语言模型（LLM）自动化和扩展先验提取过程的新颖框架。引入了LLMPrior，一个将非结构化上下文（如自然语言描述、数据或图形）转换为有效、可处理的概率分布的算子。通过将LLM与显式、可处理的生成模型（如高斯混合模型）进行架构耦合，确保了结果先验满足基本的数学属性。进一步将该框架扩展到多智能体系统，采用对数意见池聚合分散知识引起的先验分布。提出了Fed-LLMPrior算法，用于聚合分布式、上下文相关的先验，并能抵抗智能体异质性。

Result: LLMPrior能够将丰富的非结构化上下文转换为有效的概率分布，并且Fed-LLMPrior能够聚合分布式、上下文相关的先验，并能抵抗智能体异质性。

Conclusion: 该工作为一类新的工具奠定了基础，这些工具可能降低复杂贝叶斯建模的进入门槛。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [331] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种在线分布鲁棒强化学习算法，在未知环境中也能优化最坏情况性能，并证明了其接近最优。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在真实世界部署中由于仿真到现实差距（sim-to-real gap）而导致的性能下降问题。现有的分布鲁棒强化学习方法通常需要生成模型或覆盖范围广泛的离线数据集，这在未知环境中不实用。因此，研究更现实和更具挑战性的在线分布鲁棒强化学习设置，即在单一未知训练环境中进行交互，同时优化最坏情况性能。

Method: 提出了一种计算高效的算法，用于在线分布鲁棒强化学习，该算法针对一般的f散度不确定性集合（包括Chi-Square和KL散度球），并具有次线性遗憾界。该算法在最小假设下工作，并且实验验证了其鲁棒性和效率。

Result: 在理论上，证明了该算法的次线性遗憾界，并建立了在线学习的最小遗憾界，证明了所提出方法的接近最优性。在实验上，通过在多种环境中的广泛实验，证实了该算法的鲁棒性和效率，验证了理论发现。

Conclusion: 该研究提出了一个在线分布鲁棒强化学习算法，能够优化在未知单一训练环境下的最坏情况性能，并具有次线性遗憾界。该算法在理论和实验上都证明了其鲁棒性和效率，并且接近最优。未来工作可以探索更广泛的f散度以及更复杂的场景。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [332] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: GTPO是一种新的策略优化方法，解决了GRPO的局限性，通过处理冲突代币和过滤序列来提高稳定性和性能，并且不需要KL散度正则化。


<details>
  <summary>Details</summary>
Motivation: GRPO存在两个主要局限性：1.代币经常在具有正负奖励的完成中出现，导致冲突的梯度更新，即使它们对于维持正确结构至关重要，也会降低其输出概率。2.带有负面奖励的完成可能会惩罚自信的响应，并将模型决策转向不太可能的代币，从而逐渐压平输出分布并降低学习效果。

Method: GTPO（Group-relative Trajectory-based Policy Optimization）通过识别同一位置的冲突代币（在具有相反奖励的完成中出现），跳过负面更新，同时加强正面更新。为了防止策略崩溃，GTPO还过滤掉熵超过可证明阈值的完成。

Result: GTPO在GSM8K、MATH和AIME 2024基准测试中显示出更高的训练稳定性和改进的性能。

Conclusion: GTPO通过识别冲突代币、跳过负面更新并加强正面更新来解决GRPO的局限性，从而提供更稳定、更有效的策略优化。它还通过过滤掉熵超过可证明阈值的序列来防止策略崩溃。与GRPO不同，GTPO在没有参考模型的情况下，不需要KL散度正则化，同时在GSM8K、MATH和AIME 2024基准测试中都显示出更高的训练稳定性和改进的性能。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [333] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出U-PINet，一种结合物理信息和深度学习的框架，用于高效、高精度的电磁散射建模。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统数值求解器在可扩展性和计算成本方面的问题，以及纯数据驱动深度学习方法在物理约束嵌入和标注数据方面的限制，提出了U-PINet。

Method: 提出了一种名为U-PINet的U型物理信息网络，该网络是首个全深度学习的、物理信息层级框架，用于计算电磁学。它通过多尺度处理神经网络架构来模拟近场和远场相互作用的分解和耦合，并利用物理启发的稀疏图表示来高效地模拟复杂三维物体网格元素之间的自耦合和互耦合。

Result: 实验结果表明，U-PINet能够准确预测表面电流分布，与传统求解器高度一致，同时显著减少了计算时间，并在准确性和鲁棒性方面优于传统的深度学习基线方法。

Conclusion: U-PINet框架在电磁散射建模方面实现了高精度、高效率、良好的泛化能力和物理一致性，并且在雷达散射截面预测等下游应用中展现了可行性。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [334] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 本研究提出了一种基于物理信息神经网络（PINN）的新方法，用于估算核聚变装置EAST中的热通量，相比传统方法，该方法在保持高精度的同时，计算效率大幅提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统有限元方法（FEM）在核聚变装置EAST中进行热通量估算时计算效率低下且难以进行实时模拟的挑战。

Method: 提出了一种新颖的物理信息神经网络（PINN），通过输入空间坐标和时间戳，并计算边界损失、初始条件损失和基于热传导方程的物理损失。此外，还以数据驱动的方式采样少量数据点以适应特定的热传导场景，从而提高模型的预测能力。

Result: 实验结果表明，所提出的热传导物理信息神经网络在计算效率方面实现了40倍的加速，同时保持了与有限元方法相当的准确性。

Conclusion: 该研究提出的热传导物理信息神经网络在加热条件均匀和不均匀的情况下，实现了与有限元方法相当的准确性，同时计算效率提高了40倍。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [335] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: SoilNet是一个多模态、多任务模型，通过结合图像和时空元数据，并利用层级标签表示，有效解决了土壤层分类的挑战，提高了土壤健康监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在许多领域取得了进展，但在土壤层分类等经验科学领域尚未能有效应用。土壤层分类因其多模态、多任务的特性以及复杂的层级标签分类体系而面临挑战。准确的土壤层分类对于监测土壤健康至关重要，而土壤健康直接影响农业生产力、粮食安全、生态系统稳定性和气候适应性。

Method: 提出了一种名为SoilNet的多模态、多任务模型，采用结构化的模块化流程。该模型首先预测深度标记，将土壤剖面分割成候选层。接着，对每个分割出的层进行特定层分的形态特征提取。最后，基于多模态的连接特征向量和考虑土壤层之间复杂层级关系的图结构标签表示，来预测土壤层的标签。

Result: 在真实土壤剖面数据集上证明了该方法的有效性。

Conclusion: 该方法通过结合图像和时空元数据，并利用基于图的标签表示来处理复杂的分层分类问题，在真实土壤剖面数据集上展示了有效性。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [336] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE是一个用于高效学习的稀疏智能抽取框架，采用最大熵采样方法，可扩展训练并进行能耗基准测试。与随机和相空间采样相比，SICKLE在大型湍流数据集上展示了提高模型准确性和大幅降低能耗的潜力。


<details>
  <summary>Details</summary>
Motivation: 在摩尔定律和Dennard缩放定律失效的背景下，为了提高训练效率，需要重新思考数据量的问题。通过智能子采样，能否用更少的数据训练出更好的模型？

Method: 提出了一种名为SICKLE的稀疏智能数据抽取框架，该框架包含一种新的最大熵（MaxEnt）采样方法，并支持可扩展训练和能耗基准测试。

Result: 将MaxEnt采样与随机采样和相空间采样在湍流的直接数值模拟（DNS）数据集上进行了比较。在Frontier超级计算机上进行的规模化评估表明，子采样作为预处理步骤可以提高模型准确性并显著降低能耗。

Conclusion: SICKLE框架在训练过程中通过智能子采样显著提高了模型准确性并降低了能耗，在某些情况下能耗降低高达38倍。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [337] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: Bernoulli-LoRA是一种新的理论框架，通过概率性地选择更新矩阵来改进LoRA方法，并在理论和实验上都显示出有效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法（如LoRA）在实践中有效，但理论理解有限。本研究旨在为PEFT方法提供更坚实的理论基础。

Method: 提出了一种名为Bernoulli-LoRA的新型理论框架，引入了概率伯努利机制来选择更新哪个矩阵。分析了Bernoulli-LoRA-GD、Bernoulli-LoRA-SGD、Bernoulli-LoRA-PAGE、Bernoulli-LoRA-MVR、Bernoulli-LoRA-QGD、Bernoulli-LoRA-MARINA和Bernoulli-LoRA-EF21等变体，并在非凸优化和凸非光滑函数下建立了收敛性保证和收敛率。

Result: 通过理论分析和广泛的实验，验证了Bernoulli-LoRA框架及其变体的有效性，并证明了其收敛性。

Conclusion: 该研究提出了Bernoulli-LoRA，一个包含并推广了现有LoRA方法的理论框架，并通过概率伯努利机制选择更新哪个矩阵。研究在非凸优化和凸非光滑函数下分析了该框架的多种变体，并提供了收敛性保证和收敛率。实验结果验证了理论发现和方法的有效性。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [338] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种新颖的、可扩展的、基于神经网络的黑盒优化方法，它通过单独的探索和利用标准以及自适应采样区域来提高效率，无需不确定性估计。SNBO在多维问题上表现优于现有方法，显著减少了函数评估次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 标准的贝叶斯优化（BO）在高维和大量函数评估时面临计算复杂性挑战。基于神经网络（NN）的方法虽然可扩展性更好，但通常需要计算密集且复杂的不确定性估计。SNBO旨在解决这些限制。

Method: SNBO是一种新颖的可扩展的基于神经网络的黑盒优化方法，它不依赖于模型不确定性估计。SNBO使用单独的探索和利用标准来添加新样本，并自适应地控制采样区域以确保有效的优化。

Result: SNBO在10到102维的优化问题上进行了评估，并与四个最先进的基线算法进行了比较。SNBO在大多数测试问题上获得了比表现最佳的基线算法更好的函数值，同时所需的函数评估次数减少了40-60%，运行时间减少了至少一个数量级。

Conclusion: SNBO在大多数测试问题上优于最先进的基线算法，所需的函数评估次数减少了40-60%，运行时间减少了一个数量级以上。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [339] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本研究提出了DP-NCB算法框架，首次实现了Bandit算法的差分隐私和公平性（纳什遗憾最优）的统一，并提供了理论和模拟支持，适用于高风险、社会影响广泛的应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决在多臂老虎机（Bandit）算法应用于临床试验和个性化决策等敏感领域时，保护用户数据隐私和确保跨决策轮次公平性的需求，并弥合现有算法在隐私和公平性之间存在的割裂问题。

Method: 提出了一种名为差分隐私纳什置信边界（DP-NCB）的新颖统一算法框架，该框架能够同时满足ε-差分隐私和纳什遗憾的最优性。

Result: DP-NCB 在理论上保证了ε-差分隐私，并实现了与已知下界相当的纳什遗憾（相差对数因子），且适用于全局和局部差分隐私模型，并且是随时可用的。模拟结果表明，DP-NCB 相比现有最优基线算法，在纳什遗憾方面表现更优。

Conclusion: 该研究提出了差分隐私纳什置信边界（DP-NCB），一个能够同时保证ε-差分隐私并实现最优纳什遗憾的统一框架，为设计兼具隐私保护和公平性的 Bandit 算法提供了原则性基础，适用于高风险、社会影响广泛的应用。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [340] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出了一种名为 VAE-DNN 的新型模型，用于求解参数化非线性偏微分方程。该模型通过可分训练的方式，显著降低了训练时间和能源消耗，并在精度上优于 FNO 和 DeepONet 等现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决参数化非线性偏微分方程的正向和反向问题，并寻求比现有模型（如 FNO 和 DeepONet）更高效、更准确的替代方案。

Method: 提出了一种可分训练的替代模型（VAE-DNN），该模型包含一个编码器将输入降至潜在空间，一个全连接神经网络将潜在空间映射到 PDE 解的潜在空间，以及一个解码器来重建 PDE 解。其创新之处在于三个组成部分可以独立训练，并通过将编码器和解码器分别作为 $y(m{x})$ 和 $h(m{x},t)$ 的变分自编码器（VAE）的一部分来实现。

Result: VAE-DNN 模型在求解非线性扩散方程（控制非承压含水层地下水流）的正向和反向问题时，相比 FNO 和 DeepONet 模型，不仅训练时间和能源消耗更低，而且精度也更高。

Conclusion: VAE-DNN 模型在求解参数化非线性偏微分方程的正向和反向问题时，相比 FNO 和 DeepONet 模型，展现出更高的效率和更优的精度。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [341] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 一种利用众包KPI和监管数据进行频谱需求预测的时空框架，优于ITU模型，为政策制定者提供更准确、更具操作性的见解。


<details>
  <summary>Details</summary>
Motivation: 准确的频谱需求预测对于知情的频谱分配、有效的监管规划以及促进现代无线通信网络的可持续增长至关重要。它支持政府的努力，特别是国际电信联盟（ITU）的努力，以建立公平的频谱分配政策、改进拍卖机制，并满足新兴技术（如先进的5G、即将到来的6G和物联网）的需求。

Method: 提出一个时空预测框架，该框架利用众包用户端关键绩效指标（KPI）和监管数据集来建模和预测频谱需求。该方法通过结合先进的特征工程、全面的相关性分析和迁移学习技术。

Result: 该框架实现了卓越的预测精度和跨区域泛化能力，与作为基准的ITU估计值进行的比较评估，突显了该框架提供更现实和可操作的预测的能力。实验结果验证了该方法论的有效性。

Conclusion: 该研究提出的时空预测框架利用众包用户端关键绩效指标（KPI）和监管数据集来建模和预测频谱需求。该方法通过结合先进的特征工程、全面的相关性分析和迁移学习技术，实现了卓越的预测精度和跨区域泛化能力。与传统上受任意输入和不切实际假设限制的ITU模型不同，该方法利用了细粒度、数据驱动的见解，考虑了频谱利用率的空间和时间变化。与作为基准的ITU估计值进行的比较评估，突显了该框架提供更现实和可操作的预测的能力。实验结果验证了该方法论的有效性，并强调了其作为政策制定者和监管机构加强频谱管理和规划的稳健方法的潜力。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [342] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 该研究提出了一种以预测为导向的信息论数据子采样方法，以在管理计算成本的同时从数据流中学习，并在实际应用中取得了优于先前方法的性能，但强调了仔细的模型设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 在数据流中捕获相关信息的同时，将计算成本保持在可管理的范围内。

Method: 提出了一种以预测为导向的信息论方法，通过减少下游预测的不确定性来进行智能数据子采样，用于离线学习。

Result: 在两个广泛研究的问题上，以预测为导向的方法比先前提出的信息论技术表现更好。

Conclusion: 该方法在两个广泛研究的问题上表现优于先前提出的信息论技术，但在实践中可靠地实现强劲性能需要仔细的模型设计。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [343] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种结合脉冲控制和切换控制的强化学习框架，用于1型糖尿病的个性化治疗，通过在约束马尔可夫决策过程中加入生理状态特征，并在T1DM控制任务中显著降低了血糖水平违规的发生率。


<details>
  <summary>Details</summary>
Motivation: 为了解决强化学习（RL）在处理干预措施的延迟和异质性效应方面存在的挑战，本研究旨在开发一种用于T1DM技术（如自动化胰岛素输送）的个性化治疗的RL框架，以管理生理变量在临床安全靶区内。

Method: 本研究提出了一种新颖的强化学习框架，该框架结合了脉冲控制（用于胰岛素推注等离散干预）和切换控制（用于长期治疗和方案调整），以捕捉治疗过程复杂的时序动态。该方法的核心是约束马尔可夫决策过程，并加入了生理状态特征，从而在临床和资源约束下实现安全的策略学习，并考虑了胰岛素衰减等生物学因素。

Result: 该研究在风格化的T1DM控制任务中，将血糖水平违规的发生率从现有技术的22.4%降低至10.8%，并提供了收敛性的理论保证。

Conclusion: 本研究为在医疗保健领域，特别是在1型糖尿病（T1DM）等慢性病管理中，开发安全且具有时间感知能力的强化学习（RL）奠定了基础。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [344] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 一种结合多任务学习和RNN的混合模型，用于预测葡萄物候，提高了精度和鲁棒性，优于传统模型和纯深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 为了满足葡萄园精细化管理对精确物候预测的需求，弥补传统生物物理模型精度不足和深度学习方法因数据稀疏（尤其是在品种层面）而受限的缺点。

Method: 提出了一种结合多任务学习和循环神经网络（RNN）的混合建模方法，用于参数化可微分的生物物理模型。该方法利用多任务学习来预测生物物理模型的参数，从而在不同葡萄品种之间实现共享学习，同时保留生物结构。

Result: 实验评估表明，该混合模型在预测物候阶段、抗寒性和小麦产量等作物状态变量方面，显著优于传统的生物物理模型和基线深度学习方法。

Conclusion: 该混合模型通过结合多任务学习和循环神经网络来参数化可微分的生物物理模型，提高了葡萄物候学预测的鲁棒性和准确性，并且在预测物候阶段和其他作物状态变量（如抗寒性和小麦产量）方面显著优于传统的生物物理模型和基线深度学习方法。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [345] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 揭示了基于距离的分类器（如kNN和SVM）的隐藏神经网络结构，使得LRP等XAI技术可用于解释其预测，并提供了优于基线方法的解释。


<details>
  <summary>Details</summary>
Motivation: 为了确保基于距离的分类器（如kNN和SVM）的预测具有可解释性，并弥合现有XAI方法与这些模型之间的差距。

Method: 揭示了基于距离的分类器中隐藏的神经网络结构，该结构由线性检测单元和非线性池化层组成，并应用了层级相关传播（LRP）等可解释人工智能（XAI）技术。

Result: 通过定量评估，证明了所提出的新颖解释方法优于若干基线方法，并通过两个实际用例展示了解释基于距离的模型整体的实用性。

Conclusion: 解释基于距离的分类器（如kNN和SVM）的预测，通过揭示其潜在的神经网络结构（线性检测单元和非线性池化层），并应用LRP等XAI技术，从而提供比基线方法更优越的解释。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [346] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 主动学习结合迁移学习可用于跨域时间序列异常检测，但性能提升缓慢且存在上限。不使用聚类时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索主动学习和迁移学习在跨域时间序列异常检测任务中的有效性，并深入理解主动学习中样本选择机制对模型性能的影响，特别是与现有研究的对比和性能提升的上限。

Method: 本研究采用主动学习和迁移学习相结合的方法，并结合聚类分析来检测跨域时间序列数据中的异常。通过实验评估了不同策略（如是否应用聚类、不同样本选择策略）对模型性能的影响，并分析了主动学习带来的性能提升速率。

Result: 结果显示，主动学习能提升模型性能，但提升速率较文献报道的慢，这与改进的实验设计有关。聚类与主动学习存在交互作用，不使用聚类时性能最佳。迁移学习与主动学习结合的性能存在上限，当选择更多目标点用于训练时，性能会先提升然后趋于平缓，这表明主动学习选择样本的效率以及样本选择的顺序对性能有重要影响。

Conclusion: 该研究表明，在跨域时间序列异常检测中，主动学习与迁移学习的结合是有效的，但性能提升与所选样本数量的关系呈线性平坦函数。研究还发现，聚类与主动学习之间存在相互作用，通常在不应用聚类的情况下（即使用单个簇）能达到最佳性能。另外，与文献报道相比，主动学习带来的性能提升速率较慢，这归因于改进的实验设计，即在采样池和测试池中使用不同的数据样本。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [347] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: Transformers can learn to predict Markov chain transitions (estimating probabilities instead of memorizing) if they are large enough and trained on enough data, especially with better state encoding for varied structures.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand the capacity of transformers to learn algorithms that rely on context, particularly when trained only through next token prediction, using Markov chains as a testbed.

Method: Transformers were trained using next token prediction on Markov chains with random transition matrices. The matrices used during training and testing differed. The impact of transformer size and training set size was investigated, along with the effect of more involved state encoding.

Result: A threshold in transformer and training set size was identified, above which models learn to estimate transition probabilities rather than just memorizing training patterns. Enhanced state encoding was shown to lead to more robust predictions for Markov chains with structures not encountered during training.

Conclusion: The study demonstrates that transformers can learn algorithms involving context, specifically estimating transition probabilities of Markov chains, by solely using next token prediction. A threshold in transformer size and training set size exists above which this learning occurs. More complex state encoding improves prediction robustness for unseen Markov chain structures.

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [348] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT通过调整高风险人群的评分来提高公平性，同时尽量减少对AUC性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健、金融和刑事司法等高风险领域，利用接受者操作特征曲线（AUC）下的公平性度量受到了越来越多的关注。在这些领域，公平性通常在风险评分而非二元结果上进行评估，并且一个普遍的挑战是，执行严格的公平性可能会显著降低AUC性能。

Method: 提出了一种新颖的、模型无关的后处理框架FairPOT，利用最优输运策略性地对齐不同群体的风险评分分布，但选择性地通过转换可控比例（即顶 lambda 分位数）的不利群体内的分数。将FairPOT扩展到部分AUC设置。

Result: FairPOT在全局和部分AUC场景中始终优于现有的后处理技术，通常在改善公平性的同时仅有轻微的AUC下降，甚至在效用方面有所提升。

Conclusion: FairPOT通过有选择地调整不利群体的高风险评分来解决公平性与AUC性能之间的权衡问题，并在全局和部分AUC场景中优于现有方法，具有计算效率和实际适应性，是现实部署的有希望的解决方案。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [349] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet, a physics-informed deep operator network with adaptive activation, shows promise as an efficient surrogate model for bubble dynamics simulation.


<details>
  <summary>Details</summary>
Motivation: To address the spectral bias in deep learning and provide a computationally efficient surrogate model for simulating bubble dynamics.

Method: BubbleONet, built upon the PI-DeepONet framework and integrating the Rowdy adaptive activation function, is designed to map pressure profiles to bubble radius responses. The model's performance is evaluated across various scenarios, including Rayleigh-Plesset and Keller-Miksis equations with single and multiple initial radii, and its single-step versus two-step training techniques are investigated.

Result: The results demonstrate that BubbleONet is effective in simulating bubble dynamics across different scenarios.

Conclusion: BubbleONet is a promising surrogate model for simulating bubble dynamics, offering a computationally efficient alternative to traditional numerical solvers.

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [350] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP is a new framework that lets users control privacy in sensing devices like smartphones. It uses AI to understand user activities from sensor data and language, allowing users to label activities as private or not. It can then change the data to protect privacy while keeping it useful.


<details>
  <summary>Details</summary>
Motivation: Existing privacy-preserving methods for IMU data are not adaptable or user-controllable, often relying on static labels or large training datasets. This work addresses the need for user-controllable privacy that can adapt to evolving preferences.

Method: PrivCLIP uses multimodal contrastive learning to map IMU data and natural language activity descriptions into a shared embedding space for few-shot detection of sensitive activities. A language-guided sanitizer and IMU-GPT transform sensitive data into privacy-compliant, non-sensitive data.

Result: PrivCLIP significantly outperforms baseline methods in both privacy protection and data utility across multiple human activity recognition datasets.

Conclusion: PrivCLIP enables dynamic, user-controllable, few-shot privacy preservation for IMU sensor data by aligning sensor data with natural language descriptions using contrastive learning. It outperforms baselines in privacy protection and data utility.

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [351] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA通过文本聚类和联合正则分解技术，解决了多任务学习中LoRA适配器的任务干扰问题，显著提升了LLM在各项下游任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA适配器在高效微调LLM方面取得了成功，但在多任务场景下，来自异构源的LoRA适配器合并常会导致任务干扰，从而降低下游性能。TC-LoRA旨在解决这一问题。

Method: TC-LoRA通过两个关键技术来解决LoRA在多任务学习中的任务干扰问题：1. 文本层面：将训练样本在嵌入空间中进行聚类，识别输入格式的相似性，并为每个聚类训练专门的LoRA适配器。2. 参数层面：引入联合正则分解（CP分解），以解耦LoRA适配器之间特定任务和共享的因素，从而保留关键知识并减少跨任务干扰。

Result: TC-LoRA在Phi-3和Mistral-7B模型上的实验结果显示，在零样本和技能组合任务（包括推理、问答和编码）上，其准确率分别提高了1.4%和2.3%，证明了TC-LoRA在LLM适应性方面的有效性，并且优于基于SVD的基线方法。

Conclusion: TC-LoRA通过在文本和参数层面解决任务干扰问题，有效提高了LoRA在多任务场景下的性能，在推理、问答和代码等多个任务上均优于现有基线方法。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [352] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: DCFL 框架通过解耦对比损失解决了联邦学习中对比学习的有限样本问题，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在联邦学习的有限样本场景中，其渐近假设（无限负样本）被违反，导致性能下降。

Method: 提出了一种名为 DCFL（Decoupled Contrastive Learning for Federated Learning）的新框架，该框架将现有的对比损失分解为对齐和一致性两个目标。

Result: DCFL 在正样本对齐和负样本一致性方面优于现有对比学习方法，并在 CIFAR-10、CIFAR-100 和 Tiny-ImageNet 数据集上取得了比现有最先进的联邦学习方法更好的性能。

Conclusion: DCFL 框架通过解耦对比损失为对齐和一致性两个目标，能够独立校准吸引力和排斥力，无需依赖渐近假设，为联邦学习环境和少量数据提供了合适的对比学习方法。实验结果表明，DCFL 在正样本对齐和负样本一致性方面优于现有对比学习方法，并在 CIFAR-10、CIFAR-100 和 Tiny-ImageNet 等标准基准上持续优于最先进的联邦学习方法。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [353] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: A comparative survey of TensorFlow and PyTorch highlights PyTorch's research advantages and TensorFlow's production strengths, aiding tool selection.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive comparison of TensorFlow and PyTorch, the two leading deep learning frameworks, to help practitioners choose the appropriate tool.

Method: Comparative survey analyzing usability, performance, and deployment trade-offs of TensorFlow and PyTorch, drawing on benchmarks, studies, and ecosystem analysis.

Result: PyTorch is favored in research for its simplicity and flexibility, while TensorFlow offers a more robust production ecosystem. Both frameworks have distinct strengths and weaknesses.

Conclusion: TensorFlow and PyTorch are both capable frameworks, but PyTorch excels in research due to its simplicity and flexibility, while TensorFlow offers a more mature production ecosystem. The choice depends on the specific needs of the practitioner.

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [354] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: FeDaL 是一种新的联邦学习方法，通过消除域偏倚来提高时间序列基础模型在异构数据集上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数据集异质性会引入显著的域偏倚，从而根本上降低时间序列基础模型（TSFM）的泛化能力，而这一挑战仍然是研究较少的领域。本研究重新思考了使用联邦学习范式来开发 TSFM。

Method: 提出了一种新颖的联邦数据集学习（FeDaL）方法，该方法通过学习数据集无关的时间表示来解决异构时间序列问题。FeDaL 的分布式架构通过将异构时间序列数据集分解为共享的泛化知识和保留的个性化知识，为解决此问题提供了一个自然的解决方案。此外，基于 TSFM 架构，FeDaL 通过增加域偏倚消除（DBE）和全局偏倚消除（GBE）这两个互补机制，显式地减轻了局部和全局偏倚。

Result: FeDaL 成功地解决了异构时间序列数据带来的域偏倚问题，并在跨数据集泛化方面取得了优于现有方法的性能。

Conclusion: FeDaL 在现实世界的数据集上进行了广泛评估，涵盖了八个任务（包括表示学习和下游时间序列分析），并与 54 个基线进行了比较，证明了其跨数据集泛化能力。此外，还分析了 FeDaL 的联邦扩展行为，说明了数据量、客户端数量和加入率在去中心化下对模型性能的影响。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [355] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: A Quantum Temporal Fusion Transformer (QTFT) is proposed, enhancing the classical TFT with quantum computing. QTFT shows promising results, matching or exceeding the classical TFT's performance on forecasting tasks and is compatible with current quantum hardware.


<details>
  <summary>Details</summary>
Motivation: To extend the capabilities of the classical Temporal Fusion Transformer (TFT) for multi-horizon time series forecasting by incorporating quantum computing.

Method: Proposed a Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid quantum-classical architecture extending the classical TFT framework using a variational quantum algorithm.

Result: QTFT demonstrates successful training and accurate prediction on forecasting datasets. Experimental results show it outperforms the classical TFT in training and test loss in certain cases, and achieves comparable performance in others.

Conclusion: QTFT is successfully trained and capable of accurately predicting future values, outperforming or matching the classical TFT in certain test cases. Its foundation on a variational quantum algorithm allows for implementation on NISQ devices.

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [356] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 通过微调和少样本提示来改进自动短答案分级（ASAG）。评估了 OpenAI 的闭源模型和 Llama 开源模型。结果表明，对于 Llama 模型，微调效果有限，但对于 OpenAI 模型则优于少样本方法。合成数据可以显著提高 Llama 3.1 8B-Instruct 模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在自动短答案分级（ASAG）领域，提高 LLM 的性能，特别是对比微调和少样本提示方法。

Method: 评估了 OpenAI 的微调服务和QLORA等微调方法，以及它们与少样本提示的交互，用于自动短答案分级（ASAG）并生成结构化（JSON）输出。

Result: 对于 Llama 开源模型，少量的微调数据效果有限；但对于 OpenAI 的闭源模型，微调方法优于少样本基线指令微调 LLM。实验证据表明，微调的优势可能受领域主题的影响。使用大量合成数据可以显著提升 Llama 3.1 8B-Instruct 模型的性能。

Conclusion: 微调方法可以优于 OpenAI 的闭源模型的少样本基线指令微调 LLM，但对于 Llama 开源模型来说，用少量数据进行微调的实用性有限。此外，使用大量廉价生成的合成训练数据来启动 Llama 3.1 8B-Instruct 开源模型的初始训练示例，可以极大地提高其性能。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [357] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT 是一种新的联邦学习后门攻击方法，利用潜在驱动的自编码器生成多样化、目标特定的触发器，能够规避检测并实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习后门攻击方法受限于固定模式或单一目标触发器，不够灵活且易于被检测。FLAT 旨在提供一种更灵活、更复杂的后门攻击方法。

Method: 提出了一种名为 FLAT (FL Arbitrary-Target Attack) 的新型后门攻击方法，该方法利用潜在驱动的条件自编码器来按需生成多样化、目标特定的触发器。通过引入潜在码，FLAT 能够创建视觉自适应且高度可变的触发器，使攻击者无需重新训练即可选择任意目标，并能规避传统的检测机制。

Result: FLAT 能够生成多样化的、目标特定的触发器，实现攻击成功率和隐蔽性的统一，为联邦后门攻击引入了新的灵活性和复杂性。

Conclusion: FLAT 实现了高攻击成功率，并能抵御先进的联邦防御机制。实验结果凸显了在联邦环境中应对潜在驱动、多目标后门威胁的新防御策略的紧迫性。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [358] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: AFMVC通过对抗性学习去除敏感属性信息，实现公平多视图聚类，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有得多视图聚类方法主要关注聚类性能，忽略了在以人为中心的应用中至关重要的公平性问题。虽然一些研究探索了多视图聚类中的群体公平性，但它们通常依赖于敏感属性和潜在聚类结构之间对齐的假设，而这种假设在实践中常常失败，并可能降低聚类性能。

Method: 提出了一种名为AFMVC的对抗性公平多视图聚类框架，该框架利用对抗训练将公平性学习整合到表示学习过程中，旨在从学习到的特征中去除敏感属性信息，确保聚类分配不受敏感属性的影响。

Result: 在具有公平性约束的数据集上的大量实验表明，AFMVC框架在公平性和聚类性能方面均优于现有的多视图聚类和公平感知聚类方法。

Conclusion: AFMVC框架通过将公平性学习整合到表示学习过程中，并利用对抗训练从学习到的特征中去除敏感属性信息，从而实现公平性。该方法通过KL散度使特定视图的聚类分配与公平性不变的共识分布保持一致，以保证聚类一致性且不显著损害公平性。实验结果表明，AFMVC在公平性和聚类性能方面优于现有的多视图聚类和公平感知聚类方法。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [359] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: Vision-language models (VLMs) are vulnerable to privacy attacks that can reconstruct private training data. Novel methods like SMI-AW show high accuracy in these attacks, posing a significant risk.


<details>
  <summary>Details</summary>
Motivation: Prior works on model inversion attacks have focused on conventional unimodal DNNs, leaving the vulnerability of vision-language models (VLMs) underexplored. This paper aims to be the first to study VLMs' susceptibility to leaking private visual training data.

Method: The paper proposes novel token-based and sequence-based model inversion strategies tailored for VLMs, including Token-based Model Inversion (TMI), Convergent Token-based Model Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based Model Inversion with Adaptive Token Weighting (SMI-AW). The study also utilizes a logit-maximization loss based on vocabulary representation.

Result: Experiments on three state-of-the-art VLMs and multiple datasets demonstrate that VLMs are indeed vulnerable to training data leakage. The proposed sequence-based methods, especially SMI-AW, achieve competitive reconstruction and outperform token-based methods in attack accuracy and visual similarity. Human evaluation of reconstructed images yielded a 75.31% attack accuracy.

Conclusion: VLMs are vulnerable to training data leakage through model inversion attacks, with proposed sequence-based methods like SMI-AW achieving competitive reconstruction accuracy and outperforming token-based methods. Human evaluation shows a 75.31% attack accuracy, highlighting the severity of these threats.

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [360] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 为了解决在策略优化中，当多个样本响应在同一提示下收敛到相同结果时（无论正确与否）会导致基于组的优势退化为零，从而引发梯度消失，使样本对学习无效的问题，提出了一致性感知策略优化框架，通过结构化全局奖励和熵基软融合机制来解决这一问题，并在数学推理基准上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决在策略优化中，当多个样本响应在同一提示下收敛到相同结果时（无论正确与否）会导致基于组的优势退化为零，从而引发梯度消失，使样本对学习无效的问题。

Method: 提出了一致性感知策略优化框架，通过结构化全局奖励和熵基软融合机制来解决多样本响应一致时梯度消失的问题。

Result: 在多个数学推理基准上取得了显著的性能提升，验证了该框架的有效性、鲁棒性和通用性。

Conclusion: 该框架通过引入基于结果一致性的结构化全局奖励和基于熵的软融合机制，解决了多样本响应一致时梯度消失的问题，并在数学推理基准上取得了显著的性能提升，证明了其鲁棒性和通用性。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [361] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 针对太阳能发电预测中的域偏移问题，提出一种半监督深度域适应框架，仅用 20% 目标数据即可提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决由于不同气象区域不断变化的天气条件而引起的域偏移问题，以实现对不同地理位置的太阳能发电进行准确预测。

Method: 提出一个半监督深度域适应框架，使用源地点数据训练深度卷积神经网络，并采用无源教师-学生模型配置进行适应，利用一致性和交叉熵损失进行半监督学习。

Result: 在目标域仅标记 20% 数据的情况下，与非适应性方法相比，在加利福尼亚州、佛罗里达州和纽约州分别将预测准确性提高了 11.36%、6.65% 和 4.92%。

Conclusion: 该研究提出了一个半监督深度域适应框架，能够使用目标地点极少量的标记数据进行准确预测。通过在源地点数据上训练深度卷积神经网络，并利用无源教师-学生模型配置进行适应，该框架在加利福尼亚州、佛罗里达州和纽约州等地取得了显著的预测准确性提升。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [362] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 该研究提出了一种新的从质谱图生成分子结构的流水线，通过使用预训练的MIST和MolForge模型，并结合概率阈值化技术，显著提高了生成分子的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决从质谱图从头生成分子结构的问题，并提供一个强大的基线用于未来的研究。

Method: 采用MIST作为编码器，MolForge作为解码器，并对MolForge进行预训练。在解码过程中，将MolForge预测指纹的概率进行阈值化处理，作为阶跃函数，以关注子结构的存在。

Result: 与之前的最先进方法相比，生成分子的准确性有了十倍的提高，top-1准确率为28%，top-10准确率为36%。

Conclusion: 该方法通过结合MIST和MolForge，并对MolForge进行预训练，以及采用概率阈值化处理，实现了从质谱图生成分子结构的任务，并且在准确性上比先前最先进的方法有了十倍的提升，生成了28%的正确单分子结构（top-1）和36%的正确十分子结构（top-10）。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [363] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: SAMT is a new method for training deep neural networks that updates parameters block-wise with adaptive step sizes, improving stability and reducing updates needed for better generalization.


<details>
  <summary>Details</summary>
Motivation: Standard SGD for deep neural networks requires simultaneous updates to all parameters, leading to unstable convergence and high computational cost in non-convex optimization problems. SAMT aims to address these issues by reducing per-step computational overhead and enhancing training stability.

Method: SAMT updates network parameters in an alternating manner by treating the weights of each layer as a block. It incorporates a novel adaptive step size strategy inspired by meta-learning, supporting various trainable step sizes (scalar, element-wise, row-wise, column-wise) for adaptive step size selection tailored to each block.

Result: Extensive experiments on multiple benchmarks demonstrate that SAMT achieves better generalization performance with fewer parameter updates compared to state-of-the-art methods.

Conclusion: SAMT achieves better generalization performance with fewer parameter updates compared to state-of-the-art methods, highlighting its effectiveness and potential in neural network optimization.

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [364] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: CRA 是一种新方法，可以解决外部推理系统中的奖励攻击问题，该问题源于 PRM 的混淆语义特征。通过使用因果推断技术，CRA 可以估计推理路径的真实奖励，从而提高准确性。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统容易出现奖励攻击，其中 PRM 会为高分但逻辑上不正确的路径分配高分，从而导致答案不正确。从因果推断的角度来看，这种现象主要归因于混淆性语义特征的存在。

Method: 提出了一种名为因果奖励调整 (CRA) 的方法，该方法通过估计推理路径的真实奖励来缓解奖励攻击。CRA 训练 PRM 的内部激活上的稀疏自编码器以恢复可解释的特征，然后使用后门调整来纠正混淆。

Result: 实验表明，CRA 缓解了奖励攻击并提高了最终准确性。

Conclusion: CRA 缓解了奖励攻击并提高了最终准确性，而无需修改策略模型或重新训练 PRM。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [365] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [366] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS 是一种用于时间序列数据的数据驱动 AutoML 方法，通过利用元数据清理数据，可将预测误差平均降低 6%。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是探索改进数据质量作为 AutoML 在时间序列数据上的一种有益方向，因为近期研究表明轻量级模型在时间序列预测方面可能表现出色。

Method: 提出了一种名为 DCATS（Data-Centric Agent for Time Series）的数据驱动代理，该代理利用元数据来清理时间序列数据，以优化预测性能。

Result: 在交通流量预测的大规模数据集上，DCATS 与四种时间序列预测模型结合使用，平均可将误差减少 6%，证明了其有效性。

Conclusion: 该研究表明，在 AutoML 中采用数据驱动的方法，特别是通过 DCATS 优化时间序列数据的质量，可以显著提高预测性能，平均可减少 6% 的误差。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [367] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的自动多普勒角度估计方法，使用颈动脉超声图像进行训练和评估，其准确性可媲美甚至优于人工估计，有望应用于临床。


<details>
  <summary>Details</summary>
Motivation: 角度估计是多普勒超声临床工作流程中测量血流速度的重要步骤，不正确的角度估计是导致多普勒血流速度测量误差的主要原因。

Method: 提出一种基于深度学习的方法，使用2100个人类颈动脉超声图像（包含数据增强）进行训练。提取图像特征的五个预训练模型和一个自定义的浅层网络被用于多普勒角度估计。

Result: 所提出的自动多普勒角度估计方法的平均绝对误差（MAE）在3.9到9.4度之间，其中表现最好的模型的MAE低于可接受的临床多普勒角度误差阈值，从而避免了将正常速度值误分类为狭窄。

Conclusion: 深度学习技术有潜力应用于自动超声多普勒角度估计，并可能集成到商用超声扫描仪的成像软件中。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [368] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time 是一种新颖的三模态框架，通过时间、频谱和提示分支来改进多元时间序列预测。它通过频率编码和自适应的跨模态融合来解决现有方法的局限性，并在各种数据集和少样本学习场景中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Transformer 和 LLM 的多元时间序列预测模型存在一些局限性，例如：1. 依赖于固定的归纳偏置；2. 忽略变量之间的相互作用；3. 采用静态融合策略，适应性差，难以捕捉特定预测范围内的细微关系。这些限制阻碍了模型在时间序列数据中捕捉细微且与预测范围相关的关系。T3Time 旨在解决这些问题。

Method: T3Time 是一种新颖的三模态框架，由时间、频谱和提示分支组成。该框架包含一个专门的频率编码分支，用于捕获周期性结构，还有一个门控机制，可以根据预测范围学习时间特征和频谱特征之间的优先级。此外，还提出了一种机制，通过动态加权每个模态对齐头的权重来聚合多个跨模态对齐头。

Result: T3Time 在各种数据集上的实验表明，其性能持续优于最先进的基线模型，在 MSE 和 MAE 方面平均分别降低了 3.28% 和 2.29%。此外，该模型在少样本学习设置中表现出强大的泛化能力，在仅使用 5% 的训练数据时，MSE 和 MAE 平均分别降低了 4.13% 和 1.91%；在使用 10% 的数据时，则分别降低了 3.62% 和 1.98%。

Conclusion: T3Time 在各种数据集上的实验表明，其性能持续优于最先进的基线模型，在 MSE 和 MAE 方面平均分别降低了 3.28% 和 2.29%。此外，该模型在少样本学习设置中表现出强大的泛化能力，在仅使用 5% 的训练数据时，MSE 和 MAE 平均分别降低了 4.13% 和 1.91%；在使用 10% 的数据时，则分别降低了 3.62% 和 1.98%。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [369] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个无需编程的Python工具，通过敏感性分析帮助用户理解和优化ML模型。


<details>
  <summary>Details</summary>
Motivation: 该研究提出SAInT工具，旨在帮助AI研究人员和领域专家通过可视化和交互式界面，在无需编程的情况下，探索和理解机器学习模型的行为，以指导特征选择和数据优化。

Method: SAInT工具自动化模型训练和选择，使用基于方差的敏感性分析提供全局特征归因，并通过LIME和SHAP提供每个实例的解释。

Result: 在泰坦尼克号数据集的分类任务上演示了SAInT系统的有效性，证明了敏感性信息如何指导特征选择和数据优化。

Conclusion: SAInT是一个Python工具，通过集成局部和全局敏感性分析来可视化地探索和理解机器学习模型行为。该系统支持人机协同（HITL）工作流程，允许用户（AI研究人员和领域专家）通过交互式图形界面进行模型配置、训练、评估和解释，无需编程。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [370] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 提出Mockingbird框架，利用LLM的角色扮演和自我反思能力适应通用机器学习任务，实验表明其效果可接受但不及领域特定文档和人类专家反馈。


<details>
  <summary>Details</summary>
Motivation: LLM在推理能力和推断速度方面的快速发展，揭示了其在通用机器学习任务方面的潜力，作者出于对此潜在可能性的好奇而进行此项工作。

Method: 提出一个名为Mockingbird的框架，通过指示LLM扮演函数角色并反思自身错误来适应通用的机器学习任务，并评估其在多项通用机器学习任务上的性能和可扩展性。

Result: 在常见的机器学习任务上，LLM驱动的机器学习方法（如Mockingbird）可以取得可接受的结果，但仅靠自我反思无法超越领域特定文档和人类专家反馈的效果。

Conclusion: LLM驱动的机器学习方法（如Mockingbird）可以在常见的机器学习任务上取得可接受的结果，但仅靠自我反思目前无法超越领域特定文档和人类专家反馈的效果。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [371] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 我们提出了一种名为VL-DAC的强化学习算法，用于训练视觉语言模型。该算法无需调参，并且能让模型在模拟环境中训练后，在真实环境中表现出色，提高了在导航、空间规划等任务上的能力。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉语言模型（VLMs）在将原始视觉观察转化为连贯的、由语言条件驱动的动作序列方面仍存在不足。虽然先前的强化学习（RL）方法有潜力解决这个问题，但它们在泛化能力、超参数调整和环境依赖性方面存在局限性。本研究旨在开发一种更轻量级、无需调参且泛化能力强的RL算法，以提升VLMs在多模态交互任务中的表现。

Method: VL-DAC算法将策略更新应用于动作令牌，同时仅在环境步骤级别进行价值学习。这种解耦的设计避免了不稳定的权重项，实现了更快、更可靠的收敛。与之前的强化学习方法不同，VL-DAC不需要进行复杂的超参数调整，并且在低状态变异性的环境中表现优异。

Result: 在MiniWorld、Gym-Cards、ALFWorld或WebShop等模拟器上使用VL-DAC训练单个VLM，即可在BALROG（+50%）、VSI-Bench（+5%）和VisualWebBench（+2%）等真实图像基准测试中实现广泛的泛化，且不会损害图像理解能力。

Conclusion: VL-DAC（Vision-Language Decoupled Actor-Critic）是一种新的、无需调参的强化学习算法，它通过将策略更新与价值学习解耦，能够有效地训练视觉语言模型（VLMs）。该算法在多个模拟环境中训练，并在真实世界的基准测试中表现出良好的泛化能力，包括在BALROG、VSI-Bench和VisualWebBench等任务上取得了显著的性能提升，同时保持了图像理解的准确性。这证明了仅在廉价的合成环境中训练的简单强化学习算法，能够赋予VLMs在特定任务上的强大能力。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [372] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出了一种名为WSS-CL的新型机器学习遗忘方法，利用权重显著性，通过两阶段（遗忘和对抗性微调）对模型参数进行优化，实现了高效且精确的遗忘，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习遗忘（高效删除特定数据对训练模型的影响）方法在实现精确遗忘、保持稳定性和跨域适用性方面存在挑战。现有方法主要集中在数据中心或基于权重的策略。

Method: 该研究提出了一种名为“权重显著性软指导对比学习用于高效机器学习遗忘图像分类”（WSS-CL）的新型两阶段高效机器学习遗忘方法。该方法利用权重显著性来专注于遗忘过程中关键的模型参数。第一阶段（遗忘阶段）通过最大化输出对数与聚合伪标签之间的KL散度，在对数空间中实现高效遗忘。第二阶段（对抗性微调阶段）以一种自监督的方式引入对比学习，通过使用缩放后的特征表示，在特征空间中最大化被遗忘数据样本与保留数据样本之间的距离，其中被遗忘样本和配对的增强样本充当正样本对，而保留样本充当对比损失计算中的负样本对。

Result: 实验评估表明，所提出的WSS-CL方法在遗忘效果方面得到了显著改进，与最先进的方法相比，性能损失可以忽略不计。

Conclusion: 该方法在监督和自监督设置下都表现出了改进的遗忘效果，并且与现有方法相比，损失可以忽略不计。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [373] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 提出一种基于词元分类和遗忘的SFT新方法，以减少对高质量数据的依赖，并提升模型性能和响应多样性。


<details>
  <summary>Details</summary>
Motivation: 为了减轻监督微调（SFT）对数据质量和数量的过度依赖，避免性能提升有限甚至下降。

Method: 通过将语料库中的词元分为有益（正）和有害（负）两类，并对负词元进行显式遗忘，来指导模型学习。

Result: 实验表明，该遗忘机制在提高模型整体性能和促进模型响应多样性方面是有效的。

Conclusion: 该遗忘机制不仅提高了模型的整体性能，还促进了模型产生更多样化的响应。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [374] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的隐私推理范式，通过将输入特征分散到多个服务器进行共享，解决了现有PI方法的隐私-效率权衡问题，并提供了增强隐私的扩展版本。


<details>
  <summary>Details</summary>
Motivation: 现有的隐私推理（PI）方法在隐私和效率之间存在根本性的权衡：加密方法虽然提供强大的保护，但计算开销高昂；而像拆分推理这样的高效方法会将中间特征暴露给逆向攻击。

Method: PrivDFS将输入特征分散到多个非串通、非通信的服务器上进行独立的部分推理，客户端安全地聚合服务器的输出来重建最终预测。PrivDFS-AT使用基于扩散的代理攻击者进行对抗性训练，以强制执行抗逆向攻击的特征分割。PrivDFS-KD利用用户特定的密钥来多样化分割策略，以防止基于查询的逆向攻击泛化。

Result: 实验表明，PrivDFS实现了与深度拆分推理相当的隐私保护，同时将客户端计算量减少了多达100倍，且没有精度损失。其扩展版本PrivDFS-AT和PrivDFS-KD能够有效抵御基于扩散的分布内攻击和自适应攻击。

Conclusion: PrivDFS通过将输入特征分散到多个服务器进行独立部分推理，实现了隐私和效率的平衡，其扩展版本PrivDFS-AT和PrivDFS-KD通过对抗性训练和用户特定密钥进一步增强了隐私保护。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [375] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 我们提出了首个针对多实例学习（MIL）的持续学习方法，通过巧妙的样本选择策略，在白血病诊断等应用中解决了灾难性遗忘问题，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床环境的动态性需要定期更新机器学习模型以保持性能，而持续学习旨在实现在不发生灾难性遗忘的情况下训练模型。然而，现有的最先进方法在多实例学习（MIL）方面效果不佳，而MIL常用于单细胞分析的血液疾病诊断。

Method: 本文提出了一种基于重述（rehearsal-based）的持续学习方法，该方法通过实例注意分数和实例到包均值及类别均值向量的距离来选择要存储在先前任务的示例集中的样本和实例，以保持数据的多样性。

Result: 实验结果表明，本文提出的方法在真实世界的白血病实验室数据上，在类别增量场景下，显著优于现有的持续学习方法，是首个实现MIL的持续学习方法。

Conclusion: 本文提出了首个针对多实例学习（MIL）的持续学习方法，并使用了一个真实世界的白血病实验室数据集进行了评估，结果表明该方法在类别增量场景下显著优于现有的持续学习方法，实现了MIL的持续学习，能够适应数据分布随时间的变化。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [376] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种创新的INT6量化框架，通过优化量化策略和GPU核，在保持高精度的同时，显著提升了LLMs的推理速度和内存效率，解决了INT6量化在现有硬件上的部署难题。


<details>
  <summary>Details</summary>
Motivation: 现有INT4/INT8量化方法在降低大型语言模型（LLMs）成本时，常以牺牲准确性或效率为代价。INT6量化提供了更好的平衡点，但缺乏现代GPU的硬件支持，导致其加速受限。

Method: FlexQ框架采用统一的6位权重量化，并通过层级敏感性分析自适应地保留8位激活。开发了支持W6A6和W6A8表示的GPU核，利用二元张量核（BTC）等效技术绕过原生INT6张量核的缺失，以最大化硬件效率。

Result: 在LLaMA模型上的评估显示，FlexQ保持了接近FP16的准确率，困惑度增加不超过0.05。所提出的GPU核在LLaMA-2-70B线性层上实现了比ABQ-LLM平均1.39倍的加速。FlexQ端到端实现了比SmoothQuant高1.33倍的推理加速和1.21倍的内存节省。

Conclusion: FlexQ框架通过结合算法创新和系统优化，在LLaMA模型上实现了接近FP16的准确率，同时带来了显著的推理加速和内存节省。通过自适应的6位/8位量化和专门的GPU核，有效解决了现有INT6量化缺乏硬件支持的问题。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [377] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 本研究系统性地回顾了2020-2024年间关于多模态模型可解释性的研究，发现基于注意力的方法是主流，但未能完全捕捉模态间交互。评估方法普遍不足，缺乏标准化。研究者提出了一系列建议，旨在改进多模态XAI的评估和报告实践，以促进更可靠、更透明的多模态AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和注意力机制的进步，对能够解释这些复杂模型决策过程的可解释人工智能（XAI）的需求日益增长。本研究旨在系统性地回顾和分析多模态模型的可解释性研究，以应对模型解释的挑战。

Method: 这是一项系统性的文献综述，分析了2020年1月至2024年初发表的关于多模态模型可解释性的研究。通过对模型架构、涉及的模态、解释算法和评估方法等多个维度进行考察，总结了当前的研究现状和挑战。

Result: 大多数研究集中在视觉-语言和仅语言模型上，最常用的解释技术是基于注意力的方法。然而，这些方法在捕捉模态间交互的全部范围方面存在不足，并且模型架构的多样性加剧了这一挑战。此外，多模态XAI的评估方法普遍缺乏系统性、一致性和鲁棒性。

Conclusion: 评估方法缺乏系统性、一致性和鲁棒性，未能充分考虑模态特定的认知和情境因素。建议加强评估方法的严谨性、透明度和标准化，以促进更具可解释性、责任感和可靠性的多模态人工智能系统研究。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [378] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: CMCFAE是一种新的生成模型，它将云模型整合到WAE框架中，通过使用云模型特征函数来规范化潜在空间，从而实现更精确的数据分布建模和更好的重建质量、潜在空间结构和样本多样性。


<details>
  <summary>Details</summary>
Motivation: 为了更精确地模拟复杂的数据分布，并解决传统自编码器在重建样本中出现的均质化问题，CMCFAE模型被提出。该模型旨在提供比依赖标准高斯先验和传统散度度量更灵活、更真实的潜在空间表示。

Method: 本研究将云模型特征函数整合到Wasserstein自编码器（WAE）框架中，提出了一种新颖的生成模型CMCFAE。通过利用云模型的特征函数来规范化潜在空间，该方法能够更精确地模拟复杂的数据分布。与依赖标准高斯先验和传统散度度量的传统方法不同，CMCFAE采用云模型先验，为潜在空间提供了更灵活、更真实的表示，从而减轻了重建样本的均质化现象。研究推导了云模型的特征函数，并在WAE框架内提出了相应的正则化器。

Result: 在MNIST、FashionMNIST、CIFAR-10和CelebA上的大量定量和定性评估表明，CMCFAE在重建质量、潜在空间结构和样本多样性方面优于现有模型。

Conclusion: CMCFAE通过将云模型理论与MMD正则化相结合，为增强基于自编码器的生成模型提供了一个有前景的新视角。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [379] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 我们提出了一种新范式：训练一个AI来策略性地“破坏”另一个AI。


<details>
  <summary>Details</summary>
Motivation: 目前的自动化方法依赖于脆弱的提示模板或单轮攻击，未能捕捉现实世界中对抗性对话的复杂、交互式性质。

Method: 通过将红队测试形式化为马尔可夫决策过程（MDP）并采用分层强化学习（RL）框架，我们有效地解决了固有的稀疏奖励和长时程挑战。我们的生成代理通过细粒度的token级伤害奖励来学习连贯的多轮攻击策略。

Result: 我们的方法为LLM红队测试设定了新的最先进水平，从根本上将LLM红队测试重新定义为一个动态的、基于轨迹的过程。

Conclusion: LLM红队测试应被视为一个动态的、基于轨迹的过程，而不是一个单步测试，这对于健壮的AI部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [380] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: Attention mechanisms, while powerful for large-scale AI, underperform in small-scale task switching scenarios compared to traditional models. Transformers fail at a basic task switching benchmark. Only a novel combination of cisformer and extensive attention achieves high accuracy, suggesting attention can be improved by studying different formulations in task-switching contexts.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to investigate why large-scale generative AI, which heavily relies on attention mechanisms, does not easily translate to superior performance in small-scale applications compared to traditional models like MLPs or recurrent networks, particularly in the context of 'task switching'.

Method: The study examines task switching in models by interspersing control tokens in token sequences. It compares the performance of standard transformers, LSTMs, and MLPs on a task switching reference model based on finite domain arithmetics (including increment, addition, reverse copy, and context operations). The study also includes an extended transformer (cisformer) and an alternative attention mechanism (extensive attention) in the comparative analysis.

Result: Standard transformers, LSTMs, and MLPs achieve similar, modest prediction accuracies in the task switching reference model. A combination of cisformer and extensive attention is the only model that achieves considerable performance levels, reaching approximately 95% accuracy.

Conclusion: The study reveals that standard transformers struggle with basic task switching problems, achieving only modest prediction accuracies comparable to LSTMs and MLPs. However, a combination of cisformer and extensive attention mechanisms is the only model that achieves considerable performance levels (around 95%). The findings suggest that comparing different formulations of attention in task-switching settings can lead to a better understanding and improvement of attention mechanisms.

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [381] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出了一种名为 CARD 的新推测解码框架，采用“查询-校正”范式，通过解耦草稿和验证过程，提高了 LLM 推理效率，最高可达 4.83 倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法需要遵循“先草稿后验证”的范式，导致草稿和验证过程顺序执行，推理效率低下，并限制了草稿模型的大小。此外，一旦候选序列中的单个令牌在草稿过程中被拒绝，所有后续候选令牌都必须被丢弃，导致草稿效率低下。

Method: 提出了一种基于缓存的并行推测解码框架，采用“查询-校正”范式，其中草稿模型生成候选令牌以填充共享缓存，而目标模型同时校正草稿模型的生成方向。

Result: 该方法实现了高达 4.83 倍于标准解码的速度提升，且无需对草稿或目标模型进行微调。

Conclusion: CARD 框架通过解耦草稿和验证过程，实现了接近草稿模型的推理速度，并将速度提升了 4.83 倍，且无需对草稿或目标模型进行微调。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [382] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种新的Transformer-based neural operator方法，通过同时学习和融合全局与局部特征来解决偏微分方程。它在处理多尺度问题、保持物理一致性和数值稳定性方面优于现有方法，并在多个基准测试和工业模拟中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer-based neural operators方法忽略了局部物理细节和全局特征之间的协同学习，而这对于解决多尺度问题、保持物理一致性和长期模拟的数值稳定性以及准确捕捉过渡动力学至关重要。

Method: 提出了一种名为GFocal的基于Transformer的神经算子方法，该方法强制实现全局和局部特征的同时学习与融合。利用基于Nyström注意力的全局块和基于切片的核心块来利用全局相关性和局部特征，生成物理感知标记。随后通过基于卷积的门控块进行调制和集成，以实现多尺度信息的动态融合。

Result: GFocal实现了准确的建模和预测，在五个基准测试中的四个取得了最先进的性能，平均相对增益为15.2%，并在工业规模模拟中表现出色。

Conclusion: GFocal在物理特征建模和预测方面取得了准确的性能，能够处理任意几何形状和初始条件。实验表明，GFocal在五个基准测试中的四个取得了最先进的性能，平均相对增益为15.2%，并且在汽车和翼型空气动力学模拟等工业规模模拟中表现出色。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [383] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: FedHiP是一种新颖的个性化联邦学习方法，通过使用闭式解来避免基于梯度的更新，从而解决了数据异质性问题，并提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化联邦学习（PFL）方法由于客户端之间普遍存在的数据异质性（非IID数据）而面临严峻的挑战，这严重阻碍了收敛和降低了性能。主要原因是依赖于基于梯度的更新，而这种更新本质上对非IID数据敏感。

Method: FedHiP方案利用预训练的迁移学习模型作为固定的骨干，通过无梯度特征提取，并开发了用于无梯度训练的解析分类器。它包括三个阶段：解析本地训练、解析全局聚合和解析本地个性化。

Result: FedHiP方案具有异质性不变的理想特性，无论其他客户端的数据分布如何非IID，每个个性化模型都保持相同。实验证明，FedHiP方案在准确性方面优于最先进的基线。

Conclusion: FedHiP方案通过解析（即闭式）解来避免基于梯度的更新，从而解决了异质性问题，在准确性方面超越了最先进的基线5.79%-20.97%。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [384] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 隔壁很冷。


<details>
  <summary>Details</summary>
Motivation: 减少国内能源需求是气候缓解和燃料贫困策略的核心，但能效干预措施的影响高度异质化。

Method: 利用在英格兰具有代表性的住房存量数据训练的因果机器学习模型，估计了墙体隔热对天然气消耗的平均和条件处理效应，并关注了对不同能源负担细分群体的分布效应。

Result: 虽然干预措施平均而言能减少天然气需求（高达19%），但低能源负担群体获得了可观的节省，而高能源负担群体几乎没有或根本没有减少。这种模式反映了一种由行为驱动的机制：高成本收入比（例如，大于0.1）限制的家庭将节省的资金重新分配用于改善热舒适度，而不是降低消耗。

Conclusion: 该研究强调了在评估国内能源政策时，需要一个更广泛的框架，该框架既要考虑气候影响，也要考虑公平性问题。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [385] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 通过引入可调的评分树，对机器学习模型进行层次化评估，以区分不同错误类型的严重性。


<details>
  <summary>Details</summary>
Motivation: 现有的分类和目标检测评估方法通常采用简单的“通过/失败”评分机制，未能区分不同错误之间的严重程度差异，尤其是在类别间存在层次关系时。

Method: 提出了一种利用评分树（scoring trees）的层次化评分方法，该方法能够根据预测类别与真实类别在类别层次结构中的距离给予部分分数。

Result: 所提出的层次化评分指标能够更精细地捕捉模型错误，并且可以通过调整评分树来优化模型评估，以适应不同的错误权衡策略。

Conclusion: 该研究提出了一种评估机器学习模型性能的新方法，该方法不仅考虑了错误的数量，还考虑了错误的类型和影响，并通过可调的评分树来实现。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [386] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: A new framework called Causal Reflection helps agents improve causal reasoning by modeling causality dynamically and using LLMs for explanations, allowing them to adapt and self-correct in changing environments.


<details>
  <summary>Details</summary>
Motivation: Traditional RL agents and LLMs struggle with robust causal reasoning, relying on spurious correlations and lacking causal understanding.

Method: The framework introduces Causal Reflection, which models causality as a dynamic function and includes a Reflect mechanism to identify outcome mismatches and generate causal hypotheses. LLMs are used as inference engines for explanations and counterfactuals.

Result: The framework enables agents to reason about delayed and nonlinear effects, adapt, self-correct, and communicate causal understanding.

Conclusion: Causal Reflection provides a framework for agents to adapt, self-correct, and communicate causal understanding in evolving environments by explicitly modeling causality and using LLMs as structured inference engines.

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [387] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM 是一种新颖的卷积方法，通过在多个时间尺度上独立处理每个通道的 FIR 滤波器，显著减少了模型复杂性，同时在多元时间序列分类任务中取得了优于或媲美现有最先进模型的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 和 CNN 模型计算量大、频率多样性有限且参数量大，而 PRISM 旨在通过其多分辨率、每通道的设计来解决这些问题。

Method: PRISM（Per-channel Resolution-Informed Symmetric Module）是一种基于卷积的特征提取器，它在多个时间尺度上独立地对每个通道应用对称有限脉冲响应（FIR）滤波器。

Result: PRISM 在人体活动、睡眠分期和生物医学基准测试中，在参数量和计算量仅为领先 CNN 和 Transformer 模型约十分之一的情况下，匹配或优于它们。

Conclusion: PRISM 是一种准确且资源高效的多元时间序列分类解决方案，通过结合经典信号处理和深度学习。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [388] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: Fed-CI是一个创新的联邦交通预测框架，通过通道独立范式（CIP）消除了客户端间的通信需求，实现了高效、准确且符合隐私的预测。该框架显著降低了通信开销，加速了训练过程，并在真实数据集上取得了最先进的性能，提高了预测精度并降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦交通预测方法虽然能提高预测精度，但通信开销大，传输延迟显著减慢了训练过程。随着交通数据量的增长，当前方法的资源消耗变得不可持续。因此，需要一种能够减少通信开销并提高训练效率的方法。

Method: 提出了一种新颖的变量关系建模范式，称为通道独立范式（CIP），使每个节点能够仅使用本地信息执行高效准确的预测。在此基础上，开发了Fed-CI联邦学习框架，使每个客户端能够独立处理自己的数据，同时有效缓解了由于缺乏直接数据共享而造成的信息丢失。

Result: Fed-CI框架在RMSE、MAE和MAPE方面分别提高了8%、14%和16%，并且通信成本大大降低。实验表明，Fed-CI在所有数据集和联邦设置下都优于现有方法。

Conclusion: Fed-CI框架在多个真实世界数据集上进行了广泛的实验，在所有数据集和联邦设置下始终优于现有方法。它在RMSE、MAE和MAPE方面分别提高了8%、14%和16%，同时显著降低了通信成本。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [389] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 分析了5000多个身份盗窃案例，发现数据泄露模式，构建了身份生态系统图，并利用图神经网络开发了隐私风险预测框架，以预测进一步的数据泄露。


<details>
  <summary>Details</summary>
Motivation: 个人和组织在缺乏对其相对隐私风险的基本了解的情况下，难以保护个人信息。

Method: 通过分析超过5000个身份盗窃和欺诈案例，构建了一个身份生态系统图，该图以节点表示PII属性，以边表示它们之间的经验性披露关系。利用图论和图神经网络开发了一个隐私风险预测框架。

Result: 研究结果表明，该方法能够有效地回答核心问题：给定身份属性的披露是否可能导致另一个属性的披露。

Conclusion: 该研究构建了一个身份生态系统图，并开发了一个隐私风险预测框架，以估计当某些个人身份信息（PII）属性受到侵害时，进一步泄露的可能性。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [390] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: GraphProp通过预测图不变性来增强图基础模型（Gfm）的结构泛化能力，从而在图级任务中取得更好的性能，尤其是在节点属性缺失的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统的GFM主要关注将不同领域的节点特征转移到统一的表示空间，但缺乏结构上的跨域泛化能力。然而，图结构比节点特征和图标签能提供更一致的跨域信息，因此需要一个能有效捕捉结构信息并实现结构泛化的模型。

Method: GraphProp的训练过程包括两个阶段：首先，通过预测图不变性来训练一个结构Gfm，以捕捉抽象的结构信息并提供具有区分性的跨域图表示；其次，利用结构Gfm提供的表示作为位置编码，训练一个全面的Gfm，并结合特定领域的节点属性和图标签来进一步提高跨域节点特征的泛化能力。

Result: 实验证明，GraphProp在有监督学习和少样本学习方面显著优于现有方法，特别是在处理缺乏节点属性的图方面。

Conclusion: GraphProp通过强调结构泛化，在图级任务（如图分类）中表现出强大的泛化能力，尤其是在处理没有节点属性的图时，其表现显著优于现有方法。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [391] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 使用新的神经网络训练方法，可以非侵入式地测量铝点焊质量，结果准确并有工业应用潜力。


<details>
  <summary>Details</summary>
Motivation: 点焊是汽车白车身连接的主流工艺，熔核直径是关键质量指标，但其测量通常需要破坏性测试。本研究旨在利用物理信息神经网络，通过非侵入式方法从实验数据中重建内部过程状态，实现对铝点焊的有效质量评估。

Method: 本研究提出并评估了两种新的训练策略：1. 使用渐进式加入实验损失（位移和熔核直径）和自定义学习率/早停策略来处理多目标优化冲突；2. 通过查找表条件更新温度相关材料参数以确保物理合理性。研究还采用了轴对称二维模型，并在进行一维简化模型评估后，在二维模型中进行了验证。

Result: 该二维网络能够预测动态位移和熔核增长，且结果在实验置信区间内。此外，该方法支持将焊接阶段从钢转移到铝，并显示出在工业应用中实现快速、基于模型的质量控制的强大潜力。

Conclusion: 该研究展示了物理信息神经网络在铝点焊中的应用潜力，通过新颖的训练策略实现了对动态位移和熔核直径的准确预测，为实现快速、基于模型的工业质量控制提供了有效途径。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [392] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种新的生成模型框架，该框架使用算子进行插值，可以处理多种任务，无需进行特定任务的训练。


<details>
  <summary>Details</summary>
Motivation: 为了广泛推广流和扩散模型的时序动力学，并构建能够执行多项任务而无需特定任务训练的多功能生成模型。

Method: 提出了一种学习概率分布之间映射的框架，通过将标量时间变量替换为向量、矩阵或线性算子来推广随机插值，从而跨越多个维度空间来桥接概率分布。

Result: 通过数值实验证明了该方法在各种任务上的零样本有效性，展示了其作为通用替代品的潜力。

Conclusion: 该方法为条件生成、修复、微调、后验采样和多尺度建模提供了零样本的有效性，并可能成为专业模型的通用任务无关替代品。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [393] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: CaPulse is a new causality-based framework for time series anomaly detection that addresses common data challenges and improves upon existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing time series anomaly detection methods often fail to capture the underlying mechanisms behind anomaly generation and face challenges like label scarcity, data imbalance, and complex multi-periodicity.

Method: The paper proposes a causality-based framework called CaPulse, which uses a structural causal model to understand anomaly generation and employs Periodical Normalizing Flows with a novel mask mechanism and periodical learners for anomaly detection.

Result: Extensive experiments on seven real-world datasets show that CaPulse outperforms existing methods.

Conclusion: CaPulse consistently outperforms existing methods, achieving AUROC improvements of 3% to 17%, with enhanced interpretability.

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [394] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 提出了一种创新的GNN预训练方法，通过结合节点和边缘信息、使用MoE框架和参数高效微调，显著提高了链接预测任务的性能，尤其在低资源场景下效果显著，并大幅降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决图神经网络（GNNs）在链接预测（LP）任务中面临的监督信号有限、对初始化敏感和在分布变化下泛化能力差等挑战。

Method: 提出了一种新颖的预训练框架，集成了节点和边缘信息，并采用late fusion策略有效结合两者的输出。同时，引入了Mixture-of-Experts（MoE）框架来处理预训练数据的多样性并避免负迁移，以及一种参数高效的微调策略以适应新数据集。

Result: 在16个数据集上进行了广泛的实验，证明了该方法的有效性。

Conclusion: 该方法在低资源链接预测任务上达到了最先进的性能，并且在计算开销降低10,000倍以上的情况下，与端到端训练的方法相比取得了具有竞争力的结果。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [395] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一个用于生物声学的预训练模型，已扩展到支持多种物种，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展Perch模型，使其不仅限于禽类，还能处理包括海洋物种在内的大型多分类群数据集，以提高生物声学分析的性能。

Method: Perch 2.0 使用原型学习分类器和新的源预测训练标准，通过自蒸馏进行训练。

Result: Perch 2.0在BirdSet和BEANS基准测试中取得了最先进的性能，并在海洋迁移学习任务中表现优于专门的海洋模型。

Conclusion: Perch 2.0在BirdSet和BEANS基准测试中获得了最先进的性能，并且在海洋迁移学习任务中优于专门的海洋模型，尽管其海洋训练数据很少。文章提出了细粒度物种分类作为生物声学稳健预训练任务的假设。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [396] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种创新的优化方法，能够高效地学习具有扰动标签噪声的单索引模型，适用于更广泛的单调激活函数，并实现了常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 在高斯分布和扰动标签噪声存在的情况下，学习单索引模型是一个基本问题。

Method: 本文开发了一种新的优化框架，该框架不局限于常规梯度方法，而是通过直接利用问题结构、高斯空间性质以及单调函数的正则性，识别出一个有用的向量场来指导算法更新。

Result: 实现了针对所有单调激活函数（包括单调Lipschitz函数和半空间等不连续函数）的常数因子近似，优于现有方法在近似因子或激活函数族上的局限性。

Conclusion: 本文提出了首个计算上有效的算法，能够为高斯分布下的带扰动标签噪声的单索引模型（Single-Index Models）提供常数因子近似，该算法适用于所有单调激活函数，且其矩不超过 $2+\zeta$, $\zeta > 0$。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [397] [A comparative study of the high-pressure structural stability of zirconolite materials for nuclear waste immobilisation](https://arxiv.org/abs/2508.03786)
*Daniel Errandonea,Robin Turnbull,Josu Sanchez-Martin,Robert Oliva,Alfonso Munoz,Silvana Radescu,Andres Mujica,Lewis Blackburn,Neil C. Hyatt,Catalin Popescu,Jordi Ibanez-Insa*

Main category: cond-mat.mtrl-sci

TL;DR: High-pressure study of zirconolite materials reveals a new structure for zirconolite-2M and a phase transition at 14.7 GPa, while other zirconolite polymorphs remain stable under pressure.


<details>
  <summary>Details</summary>
Motivation: To comparatively study the high-pressure behaviors of nuclear waste immobilisation materials, specifically different polymorphs of zirconolite.

Method: Synchrotron powder X-ray diffraction and density-functional theory calculations were used to study the high-pressure behaviors of zirconolite-2M, -4M, -3O, and -3T.

Result: A new triclinic crystal structure (space group P-1), named zirconolite-2TR, is proposed for zirconolite-2M. Zirconolite-2TR transitions to a monoclinic structure (space group C2/c) at 14.7 GPa. No pressure-induced phase transitions were observed in zirconolite-4M, -3O, and -3T.

Conclusion: Zirconolite-2M undergoes a phase transition at 14.7 GPa to a monoclinic structure (space group C2/c), which is different from the previously proposed high-pressure structure. Zirconolite-4M, -3O, and -3T show no evidence of pressure-induced phase transitions. Linear compressibility and a room-temperature pressure-volume equation of state are presented and discussed.

Abstract: We present a comparative study of the high-pressure behaviours of the nuclear
waste immobilisation materials zirconolite-2M, -4M, -3O, and -3T. The materials
are studied under high-pressure conditions using synchrotron powder X-ray
diffraction. For zirconolite-2M we also performed density-functional theory
calculations. A new triclinic crystal structure (space group P-1), instead of
the previously assigned monoclinic structure (space group C2/c) is proposed for
zirconolite-2M. We named the triclinic structure as zirconolite-2TR. We also
found that zirconolite-2TR undergoes a phase transition at 14.7 GPa to a
monoclinic structure described by space group C2/c, which is different than the
high-pressure structure previously proposed in the literature. These results
are discussed in comparison with previous studies on zirconolite-2M and the
related compound calzirtite. For the other three zirconolite structures (4M,
3O, and 3T) this is the first high-pressure study, and we find no evidence for
pressure induced phase transitions in any of them. The linear compressibility
of the studied compounds, as well as a room-temperature pressure-volume
equation of state, are also presented and discussed.

</details>


### [398] [Orientational Disorder of NH$_3$ in Hexammine Magnesium Borohydride](https://arxiv.org/abs/2508.03874)
*Liam A. V. Nagle-Cocco,Andreas Schneemann,Kevin H. Stone,Vitalie Stavila,Thomas Gennett,Nicholas A. Strange*

Main category: cond-mat.mtrl-sci

TL;DR: 研究人员利用同步X射线衍射技术，解决了六氨合镁硼氢化物Mg(NH$_3$)$_6$(BH$_4$)$_2$的晶体结构问题，发现其室温结构模型需要考虑氨分子的取向无序性，并在低温下氨分子的取向自由度被冻结。该研究为理解该材料的氢存储和作为电池电解质的性质提供了结构基础。


<details>
  <summary>Details</summary>
Motivation: 由于氨分子位置和行为的不确定性，该材料的晶体结构一直未能得到正确的解析。

Method: 使用同步X射线衍射技术。

Result: 提出了一种考虑氨分子取向无序性的模型来解析室温结构，并解释了低温下晶胞参数的变化是由于氨分子取向自由度冻结造成的。结合结构解析结果，对室温下的红外光谱进行了完整的振动模式归属。

Conclusion: 该材料的室温结构需要考虑氨分子在晶体结构中的取向无序性，而在120 K下，氨分子的取向自由度被冻结，导致晶胞参数发生变化。

Abstract: Hexammine magnesium borohydride, Mg(NH$_3$)$_6$(BH$_4$)$_2$, consists of
adducted NH$_3$ molecules locked in a matrix of Mg cations and borohydride
anions. It is a candidate material for hydrogen storage, with 16.8wt\% hydrogen
stored in both the NH$_3$ and borohydride anions. It also may be of interest as
a Mg$^{2+}$ conducting electrolyte in solid state batteries. Its crystal
structure has, until now, eluded a proper structural solution due to ambiguity
regarding the NH$_3$ position and behaviour. In this work, we show using
synchrotron X-ray diffraction that the room-temperature structure can be solved
only with a model assuming orientational disorder of ammonia molecules within
the crystal structure. Cooling the sample to 120\,K yields additional Bragg
peaks, which can only be solved with a unit cell expansion consistent with a
freezing of the orientational freedom of ammonia molecules. Using this insight
from the structure solution, we perform a full assignment of the vibrational
modes in the room-temperature IR spectrum.

</details>


### [399] [The role of orbital polarization and spin-dependent electron-phonon scatterings in chiral-induced spin selectivity](https://arxiv.org/abs/2508.03886)
*Mayank Gupta,Andrew Grieder,Mayada Fadel,Jacopo Simoni,Junting Yu,Ravishankar Sundararaman,Yuan Ping*

Main category: cond-mat.mtrl-sci

TL;DR: 通过第一性原理研究，阐明了手性硒（Se）中手性诱导自旋选择性（CISS）效应的微观机制，发现了CISS与CEE的关键区别，并解释了CISS中自旋极化随器件长度增加的特性。


<details>
  <summary>Details</summary>
Motivation: 研究手性材料中的CISS效应，特别是其在非磁材料中无需外部磁场即可在室温下产生自旋极化电流的特性，旨在阐明其微观机制。

Method: 采用包含电子-声子散射和自旋-轨道耦合（SOC）的第一性原理时空分辨密度矩阵动力学方法，研究了手性硒（Se）中的CISS微观机制。

Result: 研究表明，沿着手性轴的电荷传输会引起显著的手性依赖的自旋和轨道极化，且该极化随手性增加而单调增加。此外，研究还发现CISS产生的轨道极化对SOC的依赖性较弱，与自旋不同。

Conclusion: 本研究揭示了CISS和CEE的关键区别，源于自旋相关的电子-声子散射，这解释了CISS中自旋极化随器件长度增加的现象。

Abstract: Chiral materials exhibit unique spin and charge transport properties, notably
through the chiral induced spin selectivity (CISS) effect, enabling
spin-polarized currents in nonmagnetic materials without external magnetic
fields at room temperature. In this study, we investigate the microscopic
mechanisms underlying CISS in a prototypical chiral solid, trigonal selenium
(Se), based on a first principles spatial-temporal resolved density-matrix
dynamics approach, including electron-phonon scattering with self-consistent
spin-orbit couplings (SOC). Our approach elucidates the interplay of SOC,
structural chirality, and spin-dependent electron-phonon interactions in
driving the generation and transport of spin and orbital angular momentum. We
demonstrate that charge transport along the chiral axis induces significant
chirality-dependent spin and orbital polarization, which shows a monotonic
increase with higher chirality. Meanwhile, we show the orbital polarization
generated in CISS has a weak dependence on SOC, unlike spin. Most importantly,
we reveal the key difference between the CISS and colinear Edelstein effect
(CEE) originating from spin-dependent electron-phonon scatterings, which
explains the spin polarization increase with device lengths, a unique feature
in CISS.

</details>


### [400] [EAC-Net: Real-space charge density via equivariant atomic contributions](https://arxiv.org/abs/2508.04052)
*Qin Xuejian,Lv Taoyuze,Zhong Zhicheng*

Main category: cond-mat.mtrl-sci

TL;DR: EAC-Net是一个深度学习框架，可以高效准确地预测电荷密度，预测误差通常低于1%，并能泛化到训练分布之外。


<details>
  <summary>Details</summary>
Motivation: 电荷密度是量子模拟中的一个基本量，但其精确计算仍然是一个主要的瓶颈。

Method: 提出了一种名为Equivariant Atomic Contribution Network (EAC-Net)的深度学习框架，该框架通过引入原子-网格耦合机制，结合了基于网格和基于基函数模型的优点，实现了准确性和效率的同时提高。

Result: EAC-Net在多种系统（包括非晶态固体、分子液体、表面结构和金属合金）上进行了评估，发现其始终保持高精度，预测误差通常低于1%。EAC-mp在Material Project的CHGCAR数据集上进行训练，达到了与现有大型电荷密度模型相当的最新准确度，同时提供了原子分解的电荷密度。该模型在各种材料系统上表现出强大的零样本预测能力，并能很好地泛化到训练分布之外，支持结构扰动下的非自洽能带结构计算等下游应用。

Conclusion: EAC-Net是一个可扩展且通用的框架，用于加速电子结构预测，并有可能应用于高通量材料筛选和机器学习驱动的模拟工作流。

Abstract: Charge density is a fundamental quantity in quantum simulations, yet its
accurate computation remains a major bottleneck. We present the Equivariant
Atomic Contribution Network (EAC-Net), a deep learning framework for efficient
and accurate charge density prediction. By introducing an atom-grid coupling
mechanism, EAC-Net integrates the strengths of grid-based and
basis-function-based models, achieving simultaneous improvements in accuracy
and efficiency. We evaluated EAC-Net on a wide variety of systems, including
amorphous solids, molecular liquids, surface structures, and metallic alloys,
and found that it consistently achieves high accuracy with prediction errors
typically below 1%. We further develop EAC-mp by training on Material Project's
CHGCAR datasets, which achieves state-of-the-art accuracy comparable to
existing large charge density models while providing atomic-decomposed charge
densities. The model demonstrates strong zero-shot prediction capabilities
across diverse material systems. Moreover, EAC-Net generalizes well beyond the
training distribution, supporting downstream applications such as
non-self-consistent band structure calculations under structural perturbations.
By bridging local chemical environments and global charge distributions,
EAC-Net provides a scalable and general framework for accelerating electronic
structure prediction, with potential applications in high-throughput materials
screening and machine-learning-driven simulation workflows.

</details>


### [401] [Competing Magnetic Phases in Li-Fe-Ge Kagome Systems](https://arxiv.org/abs/2508.04095)
*Zhen Zhang,Kirill D. Belashchenko,Xiaoyi Su,Atreyee Das,Sergey L. Bud'ko,Paul C. Canfield,Vladimir Antropov*

Main category: cond-mat.mtrl-sci

TL;DR: 通过理论计算和实验，研究了LiFe$_6$Ge$_6$等铜火成岩体化合物的磁性质。发现LiFe$_6$Ge$_6$存在不寻常的磁序，如旋涡状自旋螺旋和A型反铁磁态，并伴随自旋重取向转变。这些结果为探索新磁材料提供了基础。


<details>
  <summary>Details</summary>
Motivation: 为了理解和探索铜火成岩体磁性材料中竞争的层间磁相互作用所导致的各种磁相，以及这些磁相所带来的潜在拓扑或量子材料特性。

Method: 利用第一性原理计算研究了LiFe$_6$Ge$_6$、LiFe$_6$Ge$_4$和LiFe$_6$Ge$_5$的电子结构和磁性质，并结合磁RKKY交换耦合分析来解释和确认计算结果。同时，通过实验观测了LiFe$_6$Ge$_6$单晶的磁有序行为，包括反铁磁排序温度和低于特定温度下的自旋重取向转变。

Result: 理论计算预测LiFe$_6$Ge$_4$和LiFe$_6$Ge$_5$的磁基态为涉及铁磁和反铁磁层间取向混合的共线反铁磁态。而LiFe$_6$Ge$_6$的磁基态则是一种不重叠的旋涡状自旋螺旋，接近于共线A型反铁磁态。RKKY交换耦合分析支持了电子结构计算结果，且计算得到的原子磁矩值与实验估值吻合良好。实验观测到LiFe$_6$Ge$_6$在约540K时出现反铁磁有序，在约270K以下发生自旋重取向转变，并伴有小的铁磁分量（可能存在自旋倾斜）。

Conclusion: 该研究揭示了LiFe$_6$Ge$_6$、LiFe$_6$Ge$_4$和LiFe$_6$Ge$_5$三种化合物中存在的复杂磁相互作用和多样的磁相，特别是LiFe$_6$Ge$_6$中的层间磁相互作用导致了不寻常的磁序行为，如旋涡状自旋螺旋和A型反铁磁态。理论计算和实验观测结果相互印证，确认了非共线磁态和共线磁态的存在与转换，为探索新颖磁相和拓扑磁性材料提供了平台。

Abstract: Competing interlayer magnetic interactions in kagome magnets can lead to
diverse magnetic phases, which enable various promising topological or quantum
material properties. Here, the electronic structure and magnetic properties
have been studied using first-principles calculations for the LiFe$_6$Ge$_6$,
LiFe$_6$Ge$_4$, and LiFe$_6$Ge$_5$ compounds sharing the kagome Fe$_3$Ge layer
motif but with different interlayer arrangements. For LiFe$_6$Ge$_4$ and
LiFe$_6$Ge$_5$, the predicted magnetic ground states are collinear
antiferromagnetic (AFM) states involving a mix of ferromagnetic (FM) and AFM
interlayer orientations. Whereas for LiFe$_6$Ge$_6$, an incommensurate
cycloidal spin spiral is stabilized as a ground state, being close to a
collinear A-type AFM state. The analysis of magnetic RKKY exchange coupling
confirms the results of electronic structure calculations. The values of atomic
magnetic moments are in good agreement with existing experimental estimations.
Our experiments on LiFe$_6$Ge$_6$ single crystals have observed AFM ordering at
~540 K and spin-reorientation transition with a small FM component (possibly
with spin canting) below ~270 K. Thus, both theory and experiment independently
suggest the existence and sequence of non-collinear and collinear magnetic
states in kagome LiFe$_6$Ge$_6$. Our findings provide a platform for exploring
various novel magnetic phases and associated unconventional or topological
magnetism.

</details>


### [402] [Accelerating Discovery of Ternary Chiral Materials via Large-Scale Random Crystal Structure Prediction](https://arxiv.org/abs/2508.04110)
*Jiexi Song,Diwei Shi,Fengyuan Xuan,Chongde Cao*

Main category: cond-mat.mtrl-sci

TL;DR: 通过机器学习和随机结构搜索，发现了120多种具有拓扑特性等新颖功能的新手性晶体材料。


<details>
  <summary>Details</summary>
Motivation: 手性无机晶体数据库中现有材料稀少，但其具有新颖的拓扑特性和基本科学意义，因此需要一种有效的方法来发现更多此类材料。

Method: 结合通用机器学习内禀势（uMLIPs）进行高通量结构优化和随机结构搜索（RSS）进行广泛探索，对三元体系进行大规模可变成分晶体结构预测，重点关注手性空间群。

Result: 成功发现了超过120种新的手性无机晶体，其中一些材料展现了非线性霍尔效应、量子度量和对称性强制六重拓扑点、长费米弧和大的磁阻等量子现象，以及在非线性光学和超导性方面的应用潜力。

Conclusion: 该工作通过结合通用机器学习内禀势（uMLIPs）和随机结构搜索（RSS），为大规模发现具有拓扑特性的手性无机晶体提供了可行途径。高通量筛选和第一性原理验证发现了超过120种新的手性无机晶体，并展现出在拓扑特性、非线性光学和超导性等方面的应用潜力，其中包含具有非线性霍尔效应、量子度量和对称性强制六重拓扑点、长费米弧和大的磁阻等量子现象的材料。该研究显著扩大了手性功能材料的范围，并展示了一种可扩展、高效的复杂材料预测发现策略。

Abstract: Chiral inorganic crystals with topological characteristics, prized for their
exotic properties and fundamental interest, remain scarce in existing database.
This work establishes a viable route for their large-scale discovery by
integrating universal machine learning interatomic potentials (uMLIPs) for
high-throughput structure optimization with the broad exploration capability of
Random Structure Search (RSS). We implemented this combined uMLIP-RSS workflow
to perform massive variable-composition crystal structure prediction across
ternary systems, specifically targeting chiral space groups. High-throughput
uMLIP-based optimization and stability screening of over 20 million randomly
generated chiral structures identified numerous potentially stable phases out
of existing database. Subsequent validation by first-principles confirmed over
120 new chiral inorganic crystals with promising functional applications,
including topological characteristics, nonlinear optics, and superconductivity.
Notably, this set includes materials exhibiting remarkable quantum phenomena,
such as the nonlinear Hall effect driven by berry curvature dipole, quantum
metric and symmetry-enforced six-fold topological points, long Fermi arcs and
large magnetoresistance. This work substantially expands the pool of chiral
functional materials and demonstrates a scalable, efficient strategy for
predictive discovery in complex materials.

</details>


### [403] [Open Gas-Cell Transmission Electron Microscopy at 50 pm Resolution](https://arxiv.org/abs/2508.04294)
*Idan Biran,Frederik Dam,Sophie Kargo Kaptain,Ruben Bueno Villoro,Maarten Wirix,Christian Kisielowski,Peter C. K. Vesborg,Jakob Kibsgaard,Thomas Bligaard,Christian D. Damsgaard,Joerg R. Jinschek,Stig Helveg*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究介绍了一种能在高达 1 mbar 压力下实现 50 pm 分辨率的新型透射电子显微镜系统，可用于在气相环境中进行纳米材料的原位和操作成像，推动了对气-固相互作用的研究。


<details>
  <summary>Details</summary>
Motivation: 将单原子灵敏成像能力从高真空扩展到气相环境，从而在化学反应条件下实现对纳米材料动力学的可视化。

Method: 该研究使用具有开放式气体池、四级差分泵系统、五阶象差校正器、单色化电子束、超稳定显微镜平台、Nelsonian 低电子剂量率照明和直接电子检测功能的新型透射电子显微镜系统，在高达 1 mbar 的压力下实现 50 pm 的分辨率，并使用纳米晶金在氮气中进行演示。

Result: 在高达 1 mbar 的压力下保持 50 pm 分辨率，并通过杨氏条纹实验和出射波相成像证实了原子分辨率，并指出了表面末端位置相关的振动模糊。

Conclusion: 该系统利用开放式气体池、四级差分泵系统、五阶象差校正器、单色化电子束、超稳定显微镜平台、Nelsonian 低电子剂量率照明和直接电子检测，在高达 1 mbar 的压力下保持 50 pm 分辨率，证明了其在高达 1 mbar 的压力下保持 50 pm 分辨率的能力，并通过杨氏条纹实验和出射波相成像证实了原子分辨率，并指出了表面末端位置相关的振动模糊。因此，该平台推动了包括催化、腐蚀和晶体生长在内的各个领域中原位和操作 TEM 对气-固相互作用的研究。

Abstract: Transmission electron microscopy (TEM) has reached ~ 50 picometer resolution
in a high vacuum, enabling single-atom sensitive imaging of nanomaterials.
Extending this capability to gaseous environments would allow for similar
visualizations of nanomaterial dynamics under chemically reactive conditions.
Here, we examine a new TEM system that maintains 50 pm resolution at pressures
up to 1 mbar, demonstrated using nanocrystalline Au immersed in N2. The system
features an open gas-cell with a four-stage differential pumping system, a 5th
order aberration corrector for broad-beam TEM, a monochromatized electron beam,
an ultra-stable microscope platform, Nelsonian low electron dose-rate
illumination, and direct electron detection. Young fringe experiments and exit
wave phase imaging confirm the atomic resolution and indicate
location-dependent vibrational blur at surface terminations. Thus, this
platform advances in situ and operando TEM studies of gas-surface interactions
in diverse fields, including catalysis, corrosion, and crystal growth.

</details>


### [404] [Theory of circular dichroism in resonant inelastic x-ray scattering](https://arxiv.org/abs/2508.04388)
*M. Furo,A. Hariki,J. Kuneš*

Main category: cond-mat.mtrl-sci

TL;DR: RIXS-CD是磁性材料中一种新的光谱技术，它利用右旋和左旋圆偏振入射光子的散射振幅之差来探测材料的磁性。与现有的光谱技术相比，RIXS-CD对时间反演对称性破缺不敏感，这使得它能够在更广泛的材料中应用。


<details>
  <summary>Details</summary>
Motivation: 分析磁性材料中resonat ineltic x-ray scattering (RIXS) 中的圆二色性 (CD)。

Method: 采用杂质近似，忽略了不同原子上散射事件之间的干涉。通过对几种常见的反铁磁体和交替磁体结构进行对称性分析，并概述了通用方法。利用第一性原理获得的真实晶体场的原子模型进行数值计算。

Result: RIXS-CD与X射线磁圆二色性等一阶光谱区分开来，表现出对时间反演对称性破缺的不敏感性。发现RIXS-CD存在于低对称性的正常（无序）态材料中。在反铁磁体中，RIXS-CD在Néel矢量反转下不变。在交替磁体和铁磁体中，时间反转态的RIXS-CD谱通常是独立的，除非存在连接的哈密顿量幺正对称性的特殊情况。

Conclusion: RIXS-CD对时间反演对称性破缺不敏感，在低对称性的正常（无序）态材料中存在。在反铁磁体中，RIXS-CD在Néel矢量反转下不变。在交替磁体和铁磁体中，时间反转态的RIXS-CD谱通常是独立的，除非存在连接的哈密顿量幺正对称性的特殊情况。

Abstract: We analyze circular dichroism (CD) in resonant inelastic x-ray scattering
(RIXS) in magnetic materials. We define RIXS-CD as the difference between
scattering amplitudes for the right- and left-circularly polarized incoming
photons and unpolarized (total) outgoing photons. We employ the impurity
approximation, in which the interference between scattering events on different
atoms is neglected. We perform the symmetry analysis of several common
antiferromagnetic and altermagnetic structures and outline the general
approach. The analysis is supported by numerical calculations using atomic
model with realistic crystal fields obtained from first principles. We show
that RIXS-CD is distinguished from first-order spectroscopies such as x-ray
magnetic circular dichroism by insensitivity to the time-reversal symmetry
breaking. As a result we find that RIXS-CD is present in the normal
(disordered) state of materials with lower symmetry. In antiferromagnets the
RIXS-CD is invariant under N\'eel vector reversal. In altermagnets and
ferromagnets the RIXS-CD spectra for time-reversed states are, in general,
independent except for the special case when there is a unitary symmetry of the
Hamiltonian connecting the

</details>


### [405] [Nature of field-induced transitions and hysteretic magnetoresistance in non-collinear antiferromagnet EuIn2As2](https://arxiv.org/abs/2508.04477)
*Karan Singh,Jan Skolimowski,Giuseppe Cuono,Raghottam M. Sattigeri,Andrzej Ptok,Orest Pavlosiuk,Tetiana Romanova,Tomasz Tolinski,Piotr Wisniewski,Carmine Autieri,Dariusz Kaczorowski*

Main category: cond-mat.mtrl-sci

TL;DR: EuIn2As2 是一种具有复杂磁结构的材料，其磁性和电输运性质在外磁场作用下会发生变化，这为理解磁结构与电输运之间的关系提供了新的视角。


<details>
  <summary>Details</summary>
Motivation: 研究 EuIn2As2 的磁性和电输运性质，以拓宽对复杂磁结构及其对电输运影响的理解。

Method: 通过实验和理论计算相结合的方式，研究了 EuIn2As2 的磁性和电输运性质。

Result: 实验和理论结果表明，EuIn2As2 处于一种断裂的螺旋反铁磁态，并且在外磁场作用下会发生磁跃变跃迁，导致磁电阻发生显著变化。

Conclusion: EuIn2As2 是一种新型的材料，它具有复杂的磁结构，并对电输运性质产生影响。

Abstract: We examine the magnetic and electrical transport properties of the hexagonal
EuIn2As2 compound, combining experimental and theoretical results. This
compound is predicted to be an axion-insulator from an electronic point of view
and an altermagnet while in the collinear magnetic phase. However, experiments
indicate that the Fermi level lies within the valence band rather than in the
topological gap, potentially leading to the dominance of magnetic properties.
Our detailed studies on magnetization and electrical transport support the
presence of a broken-helix antiferromagnetic state, which was previously
identified by X-ray and neutron diffraction experiments. Notably, we observed
within that state a field-induced metamagnetic transition marked by a large
hysteresis in magnetoresistance, which turns into a sharp upturn for the
magnetic field tilted by 15 degree from the c-axis of the crystal. Combined
with theoretical calculations, it is explained that the application of a
magnetic field changes the low-resistivity antiferromagnetic domain walls to
the high-resistivity domain walls due to the reduction in the Fermi surface
sheets interaction area in the domain walls, originating from p-orbitals of As.
EuIn2As2, therefore, presents a new case study that broadens the understanding
of complex magnetic structures and their influence on electrical transport.

</details>


### [406] [$β$-Irida-Graphene: A New 2D Carbon Allotrope for Sodium-Ion Battery Anodes](https://arxiv.org/abs/2508.04506)
*José A. S. Laranjeira,Kleuton A. L. Lima,Nicolas F. Martins,Luiz A. Ribeiro Junior,Douglas S. Galvão,Luis A. Cabral,Julio R. Sambrano*

Main category: cond-mat.mtrl-sci

TL;DR: β-Irida-graphene (β-IG) 是一种新型二维碳材料，具有良好的稳定性和高钠离子容量，有望用于下一代钠离子电池。


<details>
  <summary>Details</summary>
Motivation: 为了应对钠离子电池（SIBs）中钠离子扩散慢和结构应变等挑战，研究提出了一种名为 β-Irida-graphene (β-IG) 的新型二维碳材料。

Method: 通过密度泛函理论和从头分子动力学模拟，研究了 β-Irida-graphene (β-IG) 的性能。

Result: β-IG 表现出优异的热、动力学和机械稳定性，具有良好的导电性和钠离子迁移能力（能垒 < 0.30 eV），并预测了高达 554.5 mAh/g 的比容量。

Conclusion: 研究发现 β-Irida-graphene (β-IG) 是一种有潜力的新型二维碳材料，可作为下一代钠离子电池的有希望的负极候选材料，具有高倍率性能和结构稳健性。

Abstract: The quest for sustainable and efficient energy storage has driven the
exploration of sodium-ion batteries (SIBs) as promising alternatives to
lithium-ion systems. However, the larger ionic radius of sodium poses intrinsic
challenges such as slow diffusion and structural strain in conventional
electrode materials. As a contribution to addressing these limitations, the
\b{eta}-Irida-graphene ($\beta$-IG) is herein introduced, a novel
two-dimensional (2D) carbon allotrope derived from Irida-graphene, featuring a
diverse polygonal lattice of 3-, 4-, 6-, 8-, and 9-membered carbon rings.
Through density functional theory and ab initio molecular dynamics simulations,
$\beta$-IG demonstrated remarkable thermal, dynamical, and mechanical
stability, coupled with intrinsic conductive character and efficient sodium-ion
mobility (energy barriers < 0.30 eV). Furthermore, the adsorption of sodium
ions was energetically favorable, delivering an impressive predicted specific
capacity of 554.5 mAh/g. The reported findings highlight $\beta$-IG as a good
potential anode candidate for next-generation SIBs, offering high-rate
performance and structural robustness, and expanding the functional design
space for advanced carbon-based electrode materials.

</details>


### [407] [Using Topology to Predict Electrides in the Solid State](https://arxiv.org/abs/2508.04548)
*Stefano Racioppi,Eva Zurek*

Main category: cond-mat.mtrl-sci

TL;DR: 使用演化算法和 NNAs 拓扑标准来发现电子固体。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂相空间中加速发现具有电子固体特性的材料。

Method: 利用拓扑标准和晶体结构预测方法（XtalOpt 演化算法）来加速发现和筛选具有非核吸引子（NNAs）的电子固体。

Result: 通过对 Ca5Pb3 在 20 GPa 下进行晶体结构预测，验证了该方法的可靠性，并发现了一个新的 P4/mmm 结构，该结构在间隙位置具有 NNAs。

Conclusion: 该方法展示了如何利用演化算法和拓扑描述符有效地搜索复杂相空间以发现新的电子固体候选物。

Abstract: Electrides are characterized by electron density highly localized in
interstitial sites, which do not coincide with the interatomic contacts. The
rigorous quantum mechanical definition of electrides is based upon topological
criteria derived from the electron density, and in particular the presence of
non-nuclear attractors (NNAs). We employ these topological criteria in
combination with crystal structure prediction methods (the XtalOpt evolutionary
algorithm), to accelerate the discovery of crystalline electrides at ambient
and non-ambient pressures. The localization and quantification of NNAs is used
as the primary discriminator for the electride character of a solid within a
multi-objective evolutionary structure search. We demonstrate the reliability
of this approach through a comprehensive crystal structure prediction study of
Ca5Pb3 at 20 GPa, a system previously theorized to exhibit electride character
under compression. Our strategy could predict, and sort on-the-fly, several
unknown low-enthalpy phases that possess NNAs in interstitial loci, such as the
newly discovered P4/mmm structure. These results demonstrate how evolutionary
algorithms, guided by rigorous topological descriptors, can be relied upon to
effectively survey complex phases to find new electride candidates.

</details>


### [408] [Growth of few-layer molecular crystals of PTCDI on hexagonal boron nitride by microspacing air-gap sublimation](https://arxiv.org/abs/2508.04591)
*Nils LeCoutre,Tolibjon Abdurakhmonov,Paul Weinbrenner,Kenji Watanabe,Takashi Taniguchi,Tobias Korn,Franziska Fennel,Oliver Kühn,Friedemann Reinhard*

Main category: cond-mat.mtrl-sci

TL;DR: A simplified in-air sublimation technique can produce high-quality crystalline films of dye molecules on 2D materials, with potential applications in quantum technology and optoelectronics.


<details>
  <summary>Details</summary>
Motivation: Extended two-dimensional (2D) crystals of dye molecules adsorbed on 2D material substrates like boron nitride have recently become a subject of intense study, with potential applications ranging from quantum technology to optoelectronics.

Method: We demonstrate that few-layer crystalline films of the organic dye molecule PTCDI on boron nitride can be produced by microspacing in-air sublimation, a radically simplified technique, not requiring complicated vacuum systems.

Result: The resulting layers display clearly resolved atomic step terraces in atomic force microscopy, and a clear polarization anisotropy in their fluorescence, confirming molecular alignment and long-range order. Using density functional theory and classical molecular dynamics simulations, the canted motive is identified as the most likely building block for the morphology of a PTDCI monolayer on the hBN substrate.

Conclusion: 

Abstract: Extended two-dimensional (2D) crystals of dye molecules adsorbed on 2D
material substrates like boron nitride have recently become a subject of
intense study, with potential applications ranging from quantum technology to
optoelectronics. The most established technique for the production of these
films is physical vapor transport in vacuum. We demonstrate that few-layer
crystalline films of the organic dye molecule PTCDI on boron nitride can be
produced by microspacing in-air sublimation, a radically simplified technique,
not requiring complicated vacuum systems. The resulting layers display clearly
resolved atomic step terraces in atomic force microscopy, and a clear
polarization anisotropy in their fluorescence, confirming molecular alignment
and long-range order. Using density functional theory and classical molecular
dynamics simulations, the canted motive is identified as the most likely
building block for the morphology of a PTDCI monolayer on the hBN substrate.

</details>


### [409] [A colossal dielectric response of HfxZr1-xO2 nanoparticles](https://arxiv.org/abs/2508.04697)
*Oleksandr S. Pylypchuk,Victor V. Vainberg,Vladimir N. Poroshin,Oksana V. Leshchenko,Victor N. Pavlikov,Irina V. Kondakova,Serhii E. Ivanchenko,Lesya P. Yurchenko,Lesya Demchenko,Anna O. Diachenko,Myroslav V. Karpets,Mikhail P. Trubitsyn,Eugene A. Eliseev,Anna N. Morozovska*

Main category: cond-mat.mtrl-sci

TL;DR: 通过合成缺氧HfₓZr₁₋ₓO₂纳米粒子，在特定温度下观察到巨大的介电响应，并证实了其铁电特性，该材料有望用于开发与硅兼容的铁电纳米材料。


<details>
  <summary>Details</summary>
Motivation: 揭示小尺寸（5-10 nm）缺氧HfₓZr₁₋ₓO₂纳米粒子（x = 1 - 0.4）的巨大介电响应，并探索其铁电特性及潜在应用。

Method: 通过固态有机硝酸盐合成法制备了小尺寸（5-10 nm）的缺氧HfₓZr₁₋ₓO₂纳米粒子（x = 1 - 0.4）。利用Curie-Weiss类型依赖关系拟合了扩散铁电-顺电相变，并结合Heywang势垒模型和可变程跳跃导电模型解释了介电常数和电阻率之间的相关性。使用Landau-Ginzburg-Devonshire方法和密度泛函理论计算解释了铁电行为。

Result: 制备的HfₓZr₁₋ₓO₂纳米粉体在38-88°C时表现出明显的介电常数最大值，最大值随x从1增加到0.4而从1.5*10³增加到1.5*10⁵（低频）。介电常数和电阻率的温度依赖性呈现镜像对称关系，这可以通过Heywang势垒模型和可变程跳跃导电模型得到很好的描述。研究表明，在氧空位存在的情况下，小尺寸HfₓZr₁₋ₓO₂纳米粒子可以表现出铁电性。

Conclusion: 研究结果可能有助于开发基于HfₓZr₁₋ₓO₂纳米粒子的、与硅兼容的铁电纳米材料。

Abstract: We reveal a colossal dielectric response of small (5 - 10 nm)
oxygen-deficient HfxZr1-xO2 nanoparticles (x = 1 - 0.4), prepared by the
solid-state organonitrate synthesis. The effective dielectric permittivity of
the pressed HfxZr1-xO2 nanopowders has a pronounced maximum at 38 - 88 C, which
shape can be fitted by the Curie-Weiss type dependence modified for the diffuse
ferroelectric-paraelectric phase transition. The maximal value of the
dielectric permittivity increases from 1.5*10^3 (for x = 1) to 1.5*10^5 (for x=
0.4) at low frequencies (~4 Hz); being much smaller, namely changing from 7
(for x = 1) to 20 (for x = 0.4) at high frequencies (~500 kHz). The frequency
dispersion of the dielectric permittivity maximum position is almost absent,
meanwhile the shape and width of the maximum changes in a complex way with
increase in frequency. The temperature dependencies of the dielectric
permittivity and resistivity are almost mirror-like turned over in respect to
each other, which means that all their features, such as position and shape of
maxima, plateau, minima and inflexions, almost coincide after the mirror
reflection in respect to the temperature axis. These correlations of
resistivity and dielectric permittivity are well-described in the Heywang
barrier model applied together with the variable range hopping conduction model
in semiconducting ferroelectrics. The ferroelectric-like behavior of the
dielectric permittivity is explained by the Landau-Ginzburg-Devonshire approach
and density functional theory calculations, which reveal that small HfxZr1-xO2
nanoparticles can become ferroelectric in the presence of oxygen vacancies.
Obtained results may be useful for developing silicon-compatible ferroelectric
nanomaterials based on HfxZr1-xO2 nanoparticles.

</details>


### [410] [Tailored Thermal and Mechanical Performance of Biodegradable PLA-P(VDF-TrFE) Polymer Blends](https://arxiv.org/abs/2508.04662)
*G Suresh,B. Satyanarayana,C. Thirmal,Kaushal Jagarlamudi,T Komala,Jimlee Patowary,Ashutosh Kumar*

Main category: cond-mat.mtrl-sci

TL;DR: P(VDF-TrFE)和PLA的共混薄膜可以通过调整比例来优化其机械和电学性能，适用于多种功能性应用，如传感器、3D打印和柔性电子。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索聚偏氟乙烯-三氟乙烯（P(VDF-TrFE)）与聚乳酸（PLA）共混薄膜的结构-性能关系，以评估其在功能性应用中的适用性。

Method: 制备了不同P(VDF-TrFE):PLA比例的自由态共混薄膜，并通过热分析、傅里叶变换红外光谱、拉伸强度测量和形貌分析来评估其结构-性能关系。

Result: 研究发现，25% P(VDF-TrFE)含量的薄膜具有最高的PLA结晶度，而50:50的共混物则具有最高的电活性β相含量。25:75的共混物表现出优异的机械强度，而50:50的共混物在拉伸模量和电活性相含量之间取得了平衡，适用于传感器和3D打印应用。较高P(VDF-TrFE)含量的薄膜更柔软，适用于柔性电子应用。

Conclusion: 通过简单的成分控制，可以调节半结晶聚合物混合物的机械和功能特性，为开发功能性材料提供了途径。

Abstract: The development of polymer blends has emerged as a strategic approach for
designing multifunctional materials with enhanced tailored characteristics.
Current work investigates and reports for the first time, the
structure-property relationships in free-standing blend films of
poly(vinylidene fluoride-trifluoroethylene) (P(VDF-TrFE)) and polylactic acid
(PLA), prepared to evaluate their suitability for functional applications. For
this investigation, films of approximately 40 $\mu$m thick were fabricated by
systematically varying the P(VDF-TrFE):PLA ratio. Thermal analysis revealed a
higher PLA crystallinity at 25\% P(VDF-TrFE) content, while Fourier-transform
infrared spectroscopy showed the electroactive $\beta$-phase fraction to be
highest in the 50:50 composition. These findings correlated with tensile
strength measurements and morphology, demonstrating that molecular ordering and
phase distribution significantly influence the mechanical performance. The
25:75 blend exhibited superior mechanical strength due to enhanced PLA
crystallization and polymer chain alignment. In contrast, the 50:50 blend
achieved a balance between tensile modulus and electroactive phase development,
marking it a promising candidate for sensors and 3D printing applications. At
higher P(VDF-TrFE) content, reduced crystallinity in PLA resulted in softer,
more compliant films which would be suitable for flexible electronic
applications. These results establish a pathway to tune mechanical and
functional properties in semicrystalline polymer blends through facile
compositional control.

</details>


### [411] [Diffusion in a $d$-dimensional rough potential](https://arxiv.org/abs/2508.04674)
*Jacob Jeffries,Emilio Mendoza Reyes,Fadi Abdeljawad,Murray Daw,Enrique Martinez*

Main category: cond-mat.mtrl-sci

TL;DR: 开发了一种多维扩散解析模型，与KMC模拟对比发现低噪声极限下吻合良好，差异归因于模型未考虑的渗流通路。


<details>
  <summary>Details</summary>
Motivation: 为了在非平衡条件下理解材料的微观结构演化，需要预测材料中的扩散行为。然而，已有的原子模拟方法虽然能预测扩散传输系数，但计算成本高昂。

Method: 使用平均首次通过时间分析开发了一个多维有噪声固溶体扩散的解析模型，并与KMC模拟进行了比较。

Result: 所提出的解析模型在低噪声极限下与KMC模拟结果吻合良好，但存在差异，这归因于KMC模拟考虑了模型未包含的渗流通路。

Conclusion: 该分析表明，在低噪声极限下，所提出的解析模型与KMC模拟结果具有良好的一致性。模拟结果中观察到的扩散系数增加是由于模型未捕获的渗流通路所致。

Abstract: The prediction of diffusion in solids is necessary to understand the
microstructure evolution in materials out of equilibrium. Although one can
reasonably predict diffusive transport coefficients using atomistic methods,
these approaches can be very computationally expensive. In this work, we
develop an analytical model for the diffusivity in a noisy solid solution in an
arbitrary number of dimensions using a mean first passage time analysis. These
analytical results are then compared with kinetic Monte Carlo (KMC)
simulations, which are in good agreement with the simulation data in the
low-noise limit. We argue that the difference is expected from percolation
pathways that increase the diffusivity in the KMC analysis but are not captured
by the model. This generalization to arbitrary dimensions has been elusive to
the community since Zwanzig [PNAS, 85, 2029 (1988)] published his seminal work
on 1-dimensional systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [412] [Mechanism Design for Facility Location using Predictions](https://arxiv.org/abs/2508.03818)
*Toby Walsh*

Main category: cs.GT

TL;DR: 在设备选址问题中，我们提出了一种新的机制，该机制使用关于最优设施位置的预测，并考虑了公平性（最大距离和最小效用）。新机制比现有机制更鲁棒，并且可以通过调整参数来权衡鲁棒性和一致性。此外，我们还为具有两个设施的选址问题设计了新的策略证明机制。


<details>
  <summary>Details</summary>
Motivation: 我们研究了在预测最优设施位置的增强下，设备选址问题的机制。

Method: 我们考虑了具有预测的机制如何表现不佳，从而设计了新的更鲁棒的机制。我们通过考虑最大距离和最小效用，提供了比仅考虑最大距离更重要的见解。

Result: 我们证明了一种考虑了任何代理与设施的最大距离和任何代理的最小效用的公平观点，与仅考虑最大距离的观点相比，提供了重要的新的见解。我们考虑了在准确性方面的一致性（预测准确时的最坏情况）和鲁棒性（无论预测准确性如何的最坏情况）。

Conclusion: 通过调整参数，可以权衡鲁棒性与一致性。我们超越了单设施问题，为定位两个具有有界一致性和鲁棒性的设施设计了新颖的策略证明机制，该机制使用两个关于两个设施位置的预测。

Abstract: We study mechanisms for the facility location problem augmented with
predictions of the optimal facility location. We demonstrate that an
egalitarian viewpoint which considers both the maximum distance of any agent
from the facility and the minimum utility of any agent provides important new
insights compared to a viewpoint that just considers the maximum distance. As
in previous studies, we consider performance in terms of consistency (worst
case when predictions are accurate) and robustness (worst case irrespective of
the accuracy of predictions). By considering how mechanisms with predictions
can perform poorly, we design new mechanisms that are more robust. Indeed, by
adjusting parameters, we demonstrate how to trade robustness for consistency.
We go beyond the single facility problem by designing novel strategy proof
mechanisms for locating two facilities with bounded consistency and robustness
that use two predictions for where to locate the two facilities.

</details>


### [413] [What Do Agents Think Others Would Do? Level-2 Inverse Games for Inferring Agents' Estimates of Others' Objectives](https://arxiv.org/abs/2508.03824)
*Hamzah I. Khan,Jingqi Li,David Fridovich-Keil*

Main category: cs.GT

TL;DR: 智能体间的战略互动需要推断其目标，但现实世界的复杂性（如城市驾驶）使得一阶逆博弈论的假设失效。本研究提出二阶推理框架，考虑智能体对彼此目标的异构估计，并开发了基于梯度的求解方法，有效解决了现实场景中的目标推断问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的分散决策场景，如城市驾驶和讨价还价，智能体可能基于对彼此目标的冲突看法进行行动，而现有的一阶逆博弈论方法假设智能体拥有彼此目标的完整知识，这一假设在这种情况下不再成立。

Method: 提出了一种用于二阶推理的框架，并开发了一种有效的基于梯度的局部解识别方法。

Result: 通过理论分析和在城市驾驶场景中的实验，证明了推断智能体对彼此目标的异构估计的必要性，并表明所提出的二阶推理方法能够发现一阶方法所忽略的细微对齐问题。

Conclusion: 该研究提出了一个用于解决“什么是每个智能体对所有智能体的目标？”这一问题的二阶推理框架，并证明了二阶推理问题即使在良性设置下也是非凸的，同时开发了一种有效的基于梯度的局部解识别方法。实验结果表明，该方法能够揭示一阶方法所忽略的细微对齐问题。

Abstract: Effectively interpreting strategic interactions among multiple agents
requires us to infer each agent's objective from limited information. Existing
inverse game-theoretic approaches frame this challenge in terms of a "level-1"
inference problem, in which we take the perspective of a third-party observer
and assume that individual agents share complete knowledge of one another's
objectives. However, this assumption breaks down in decentralized, real-world
decision scenarios like urban driving and bargaining, in which agents may act
based on conflicting views of one another's objectives. We demonstrate the
necessity of inferring agents' heterogeneous estimates of each other's
objectives through empirical examples, and by theoretically characterizing the
prediction error of level-1 inference on fictitious gameplay data from
linear-quadratic games. To address this fundamental issue, we propose a
framework for level-2 inference to address the question: "What does each agent
believe about all agents' objectives?" We prove that the level-2 inference
problem is non-convex even in benign settings like linear-quadratic games, and
we develop an efficient gradient-based approach for identifying local
solutions. Experiments on a synthetic urban driving example show that our
approach uncovers nuanced misalignments that level-1 methods miss.

</details>


### [414] [Inequality in the Age of Pseudonymity](https://arxiv.org/abs/2508.04668)
*Aviv Yaish,Nir Chemaya,Lin William Cong,Dahlia Malkhi*

Main category: cs.GT

TL;DR: Sybil（虚假身份）的存在会扭曲伪匿名环境中的不平等度量，如Gini系数。虽然存在一些Sybil-proof度量，但它们在细粒度评估方面能力有限。


<details>
  <summary>Details</summary>
Motivation: 分析在伪匿名环境中（如互联网或基于区块链的平台）不平等度量的应用情况，以及Sybil（即虚假身份）对度量结果的扭曲。

Method: 分析了伪匿名环境中不平等度量的表现，并提出了几种满足放松属性的Sybil-proof度量，以及它们对细粒度评估不平等的能力的限制。

Result: 证明了包括Gini系数在内的流行度量容易受到Sybil操纵，并探讨了产生Sybil的动态。

Conclusion: 当存在Sybil时，使用满足经典期望属性的不平等度量是不可能正确度量经济体的不平等度的。

Abstract: Inequality measures such as the Gini coefficient are used to inform and
motivate policymaking, and are increasingly applied to digital platforms. We
analyze how measures fare in pseudonymous settings, as common to internet-based
or blockchain-based platforms. One key challenge that arises is the ability of
actors to create multiple fake identities under fictitious false names, also
known as ``Sybils.'' While some actors may do so to preserve their privacy, we
show that this can inadvertently distort inequality metrics. As we show, when
using inequality measures that satisfy literature's canonical set of desired
properties, the presence of Sybils in an economy implies that it is impossible
to properly measure the economy's inequality. Then, we present several classes
of Sybil-proof measures that satisfy relaxed versions of the aforementioned
desired properties, and, by fully characterizing them, we prove that the
structure imposed restricts their ability to assess inequality at a
fine-grained level. In addition, we prove that popular inequality metrics,
including the famous Gini coefficient, are vulnerable to Sybil manipulations,
and examine the dynamics that result in the creation of Sybils, whether in
pseudonymous settings or traditional ones.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [415] [Data-Driven Discovery of Mobility Periodicity for Understanding Urban Transportation Systems](https://arxiv.org/abs/2508.03747)
*Xinyu Chen,Qi Wang,Yunhan Zheng,Nina Cao,HanQin Cai,Jinhua Zhao*

Main category: cs.SI

TL;DR: 该研究提出了一种基于稀疏自回归的机器学习方法，用于分析人类出行数据中的时间规律性，如每周周期性。通过对杭州、纽约和芝加哥的数据分析，揭示了 COVID-19 对出行模式的影响，并发现纽约的出行恢复比芝加哥更快。


<details>
  <summary>Details</summary>
Motivation: 揭示人类出行的时间规律性对于发现城市动态至关重要，并对各种决策过程和城市系统应用具有重要意义。

Method: 本研究将复杂的多维度人类出行数据中的周期性量化问题，表述为时间序列自回归中的稀疏正自相关性识别。该方法允许从数据驱动和可解释的机器学习视角发现和量化显著的周期性模式，如每周周期性。

Result: 将该框架应用于杭州地铁客流和纽约、芝加哥的网约车出行数据，揭示了跨越多年和不同空间位置的可解释的每周周期性。分析了 2019 年至 2024 年的网约车数据，展示了 COVID-19 大流行对出行规律性的颠覆性影响以及随后的恢复趋势，突出了纽约和芝加哥之间恢复模式百分比和速度的差异。研究发现，纽约和芝加哥在 2020 年每周周期性均显著下降，纽约的出行规律性恢复速度快于芝加哥。稀疏自回归的可解释性为人类出行的潜在时间模式提供了见解，为理解城市系统提供了有价值的工具。

Conclusion: 该研究提出了一种数据驱动且可解释的机器学习方法，用于识别和量化人类出行数据中的时间规律性，例如每周规律性。该方法将周期性量化问题表述为时间序列自回归中的稀疏正自相关性识别，并应用于杭州地铁客流、纽约和芝加哥的网约车出行数据。研究结果揭示了跨空间和多年的可解释的每周规律性，并特别关注了 COVID-19 大流行对出行规律性的影响以及纽约和芝加哥的恢复趋势差异，表明纽约的恢复速度快于芝加哥。该研究强调了解释性稀疏自回归在理解城市系统和人类出行模式中的潜力，并展示了解释性机器学习在挖掘真实出行数据见解方面的价值。

Abstract: Uncovering the temporal regularity of human mobility is crucial for
discovering urban dynamics and has implications for various decision-making
processes and urban system applications. This study formulates the periodicity
quantification problem in complex and multidimensional human mobility data as a
sparse identification of dominant positive auto-correlations in time series
autoregression, allowing one to discover and quantify significant periodic
patterns such as weekly periodicity from a data-driven and interpretable
machine learning perspective. We apply our framework to real-world human
mobility data, including metro passenger flow in Hangzhou, China and
ridesharing trips in New York City (NYC) and Chicago, USA, revealing the
interpretable weekly periodicity across different spatial locations over past
several years. In particular, our analysis of ridesharing data from 2019 to
2024 demonstrates the disruptive impact of the COVID-19 pandemic on mobility
regularity and the subsequent recovery trends, highlighting differences in the
recovery pattern percentages and speeds between NYC and Chicago. We explore
that both NYC and Chicago experienced a remarkable reduction of weekly
periodicity in 2020, and the recovery of mobility regularity in NYC is faster
than Chicago. The interpretability of sparse autoregression provides insights
into the underlying temporal patterns of human mobility, offering a valuable
tool for understanding urban systems. Our findings highlight the potential of
interpretable machine learning to unlock crucial insights from real-world
mobility data.

</details>


### [416] [Using Stochastic Block Models for Community Detection: The issue of edge-connectivity](https://arxiv.org/abs/2508.03843)
*The-Anh Vu-Le,Minhyuk Park,Ian Chen,George Chacko,Tandy Warnow*

Main category: cs.SI

TL;DR: WCC技术可解决SBM模型产生的社区连通性差的问题，并提高了社区检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决社区检测方法产生的社区连接性差的问题，特别是针对SBM模型。

Method: 检查了其他SBM软件或graph-tool中的嵌套SBM计算出的聚类连通性问题。通过检查描述长度公式来剖析graph-tool度校正SBM聚类产生非连通聚类的原因，并探索修改描述长度公式的影响。

Result: 所有经过测试的SBM聚类方法都会产生非连通社区，并且graph-tool在PySBM的基础上有所改进。WCC技术对平坦和嵌套SBM都有改进，并且具有良好的可扩展性。

Conclusion: 研究表明，WCC技术能提高平坦SBM和嵌套SBM的准确性，并且能够扩展到拥有数百万节点的网络。

Abstract: A relevant, sometimes overlooked, quality criterion for communities in graphs
is that they should be well-connected in addition to being edge-dense. Prior
work has shown that leading community detection methods can produce
poorly-connected communities, and some even produce internally disconnected
communities. A recent study by Park et al. in Complex Networks and their
Applications 2024 showed that this problem is evident in clusterings from three
Stochastic Block Models (SBMs) in graph-tool, a popular software package. To
address this issue, Park et al. presented a simple technique, Well-Connected
Clusters (WCC), that repeatedly finds and removes small edge cuts of size at
most $\log_{10}n$ in clusters, where $n$ is the number of nodes in the cluster,
and showed that treatment of graph-tool SBM clusterings with WCC improves
accuracy. Here we examine the question of cluster connectivity for clusterings
computed using other SBM software or nested SBMs within graph-tool. Our study,
using a wide range of real-world and synthetic networks, shows that all tested
SBM clustering methods produce communities that are disconnected, and that
graph-tool improves on PySBM. We provide insight into why graph-tool
degree-corrected SBM clustering produces disconnected clusters by examining the
description length formula it uses, and explore the impact of modifications to
the description length formula. Finally, we show that WCC provides an
improvement in accuracy for both flat and nested SBMs and establish that it
scales to networks with millions of nodes.

</details>


### [417] [Hierarchical community detection via maximum entropy partitions and the renormalization group](https://arxiv.org/abs/2508.04034)
*Jorge Martinez Armas*

Main category: cs.SI

TL;DR: HCE 是一种新的分层社区检测框架，可以直接在树状图上操作，并能跨不同尺度的网络识别有意义的结构。


<details>
  <summary>Details</summary>
Motivation: 跨多个尺度的有意义的结构识别仍然是网络科学中的一个核心挑战。

Method: HCE 直接在树状图上操作，不依赖于边级统计信息。它选择分辨率级别，以最大化社区大小分布的熵与社区数量之间的原则性权衡，对应于结构高度异质性的尺度。

Result: HCE 在具有不同层次、大小不平衡和噪声的合成基准上进行了评估，并显示出与地面实况高度一致的分区。在社交和神经科学系统的真实网络中，HCE 揭示了可解释的模块化层次结构，与已知的结构和功能组织一致。

Conclusion: HCE 是一种通用、与模型无关的框架，用于检测分层社区结构中的信息层。它直接在树状图上操作，不依赖于边级统计信息。该标准适用于由广泛的聚类算法和距离度量产生的树状图。HCE 在具有不同层次、大小不平衡和噪声的合成基准上进行了评估，并显示出与地面实况高度一致的分区。在社交和神经科学系统的真实网络中，HCE 揭示了可解释的模块化层次结构，与已知的结构和功能组织一致。

Abstract: Identifying meaningful structure across multiple scales remains a central
challenge in network science. We introduce Hierarchical Clustering Entropy
(HCE), a general and model-agnostic framework for detecting informative levels
in hierarchical community structures. Unlike existing approaches, HCE operates
directly on dendrograms without relying on edge-level statistics. It selects
resolution levels that maximize a principled trade-off between the entropy of
the community size distribution and the number of communities, corresponding to
scales of high structural heterogeneity. This criterion applies to dendrograms
produced by a wide range of clustering algorithms and distance metrics,
including modularity-based and correlation-based methods. We evaluate HCE on
synthetic benchmarks with varying degrees of hierarchy, size imbalance, and
noise, including LFR and both symmetric and asymmetric multiscale models, and
show that it consistently identifies partitions closely aligned with ground
truth. Applied to real-world networks in social and neuroscience systems, HCE
reveals interpretable modular hierarchies that align with known structural and
functional organizations. As a scalable and principled method, HCE offers a
general, domain-independent approach to hierarchical community detection with
potential applications across biological, social, and technological systems.

</details>


### [418] [Quasi-Clique Discovery via Energy Diffusion](https://arxiv.org/abs/2508.04174)
*Yu Zhang,Yilong Luo,Mingyuan Ma,Yao Chen,Enqiang Zhu,Jin Xu,Chanjuan Liu*

Main category: cs.SI

TL;DR: EDQC 是一种基于能量扩散的新算法，用于在图中发现拟二次团。它通过模拟能量扩散来识别高密度子图，相比现有方法更有效且结果更稳定。


<details>
  <summary>Details</summary>
Motivation: 图挖掘中的拟二次团发现（子图的边密度不小于给定阈值）是基础任务，在社交网络、生物信息学和电子商务等领域有广泛应用。现有启发式方法在效率和跨图解的一致性方面存在不足。

Method: EDQC 算法通过从源顶点进行随机能量扩散，将能量集中在结构内聚的区域，从而实现高效的稠密子图发现，无需穷举搜索或特定数据集调优。

Result: 实验结果表明，EDQC 在 30 个真实世界数据集上，与现有最先进方法相比，在大多数数据集上能持续发现更大的拟二次团，并且解的质量方差更低。

Conclusion: EDQC 是一种基于能量扩散的新型拟二次团发现算法，相比现有方法在大多数数据集上能发现更大的拟二次团，并且解的质量方差更低。

Abstract: Discovering quasi-cliques -- subgraphs with edge density no less than a given
threshold -- is a fundamental task in graph mining, with broad applications in
social networks, bioinformatics, and e-commerce. Existing heuristics often rely
on greedy rules, similarity measures, or metaheuristic search, but struggle to
maintain both efficiency and solution consistency across diverse graphs. This
paper introduces EDQC, a novel quasi-clique discovery algorithm inspired by
energy diffusion. Instead of explicitly enumerating candidate subgraphs, EDQC
performs stochastic energy diffusion from source vertices, naturally
concentrating energy within structurally cohesive regions. The approach enables
efficient dense subgraph discovery without exhaustive search or
dataset-specific tuning. Experimental results on 30 real-world datasets
demonstrate that EDQC consistently discovers larger quasi-cliques than
state-of-the-art baselines on the majority of datasets, while also yielding
lower variance in solution quality. To the best of our knowledge, EDQC is the
first method to incorporate energy diffusion into quasi-clique discovery.

</details>


### [419] [Tweets vs Pathogen Spread: A Case Study of COVID-19 in American States](https://arxiv.org/abs/2508.04187)
*Sara Shabani,Sahar Jafarbegloo,Sadegh Raeisi,Fakhteh Ghanbarnejad*

Main category: cs.SI

TL;DR: 本文提出一个耦合SIR动力学的模型，研究认知和疾病的相互影响，并通过Twitter数据进行实证分析，发现提高认知有助于抑制疫情，并且Twitter活跃度与模型参数存在相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨了认知和疾病相互影响的概念，以及个体为预防疾病而采取的行动和认知水平如何深刻影响疾病传播动力学。同时，疾病爆发也会影响人们的认知方式。

Method: 本文提出一个耦合了两个SIR动力学的零模型，并采用均值场方法进行分析。通过探索参数空间来量化相互影响对各种可观测量的影响。此外，基于零模型，对与COVID-19相关的Twitter数据和美国各州的确认病例进行了实证分析。

Result: 研究结果表明，在特定的参数空间区域内，通过提高认知水平可以抑制疫情。此外，模型能够通过调整参数来改变疫情期间占主导地位的人口群体。实证分析还发现，Twitter活跃度排名与模型分配的免疫参数之间存在稳健的相关性。

Conclusion: 模型表明，通过调整参数，可以改变疫情期间占主导地位的人口群体。通过将模型参数分配给每个州，发现这些参数在不同的疫情高峰期会发生变化。此外，实证数据显示，各州在Twitter上的活跃度排名与模型分配的免疫参数之间存在显著相关性，这表明在疫情从最初高峰到后续高峰的整个过程中，持续的认知起着关键作用。

Abstract: The concept of the mutual influence that awareness and disease may exert on
each other has recently presented significant challenges. The actions
individuals take to prevent contracting a disease and their level of awareness
can profoundly affect the dynamics of its spread. Simultaneously, disease
outbreaks impact how people become aware. In response, we initially propose a
null model that couples two Susceptible-Infectious-Recovered (SIR) dynamics and
analyze it using a mean-field approach. Subsequently, we explore the parameter
space to quantify the effects of this mutual influence on various observables.
Finally, based on this null model, we conduct an empirical analysis of Twitter
data related to COVID-19 and confirmed cases within American states. Our
findings indicate that in specific regions of the parameter space, it is
possible to suppress the epidemic by increasing awareness, and we investigate
phase transitions. Furthermore, our model demonstrates the ability to alter the
dominant population group by adjusting parameters throughout the course of the
outbreak. Additionally, using the model, we assign a set of parameters to each
state, revealing that these parameters change at different pandemic peaks.
Notably, a robust correlation emerges between the ranking of states' Twitter
activity, as gathered from empirical data, and the immunity parameters assigned
to each state using our model. This observation underscores the pivotal role of
sustained awareness transitioning from the initial to the subsequent peaks in
the disease progression.

</details>


### [420] [Graph Representation Learning with Massive Unlabeled Data for Rumor Detection](https://arxiv.org/abs/2508.04252)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 本研究提出一种利用大规模无标签话题数据和图自监督学习方法来提升谣言检测模型泛化能力的方法，实验证明该方法在少样本条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有谣言检测方法在处理大规模标注数据困难、泛化能力不足以及在新事件上性能退化等问题，本研究旨在利用大规模无标签话题数据提升图表示学习模型在各种话题上的语义学习能力。

Method: 本研究采用了三种典型的图自监督学习方法（InfoGraph、JOAO和GraphMAE）以及两种常用的训练策略，并使用从微博和推特爬取的大规模无标签话题数据集进行训练，同时结合了包含10年数据的微博谣言数据集以缓解时间与话题的差异。

Result: 实验结果表明，本研究使用的方法在谣言检测任务上取得了优于先前方法的性能，并且在少样本条件下表现出色，验证了所提出的方法的有效性和泛化能力。

Conclusion: 现有的基于图表示学习的谣言检测方法，在结合了我们的大规模无标签推文数据集后，展现出了优于先前特定于谣言检测任务的方法的性能，并且在少样本条件下表现良好，证明了其具有更强的泛化能力。

Abstract: With the development of social media, rumors spread quickly, cause great harm
to society and economy. Thereby, many effective rumor detection methods have
been developed, among which the rumor propagation structure learning based
methods are particularly effective compared to other methods. However, the
existing methods still suffer from many issues including the difficulty to
obtain large-scale labeled rumor datasets, which leads to the low
generalization ability and the performance degeneration on new events since
rumors are time-critical and usually appear with hot topics or newly emergent
events. In order to solve the above problems, in this study, we used
large-scale unlabeled topic datasets crawled from the social media platform
Weibo and Twitter with claim propagation structure to improve the semantic
learning ability of a graph reprentation learing model on various topics. We
use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE
in two commonly used training strategies, to verify the performance of general
graph semi-supervised methods in rumor detection tasks. In addition, for
alleviating the time and topic difference between unlabeled topic data and
rumor data, we also collected a rumor dataset covering a variety of topics over
a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our
experiments show that these general graph self-supervised learning methods
outperform previous methods specifically designed for rumor detection tasks and
achieve good performance under few-shot conditions, demonstrating the better
generalization ability with the help of our massive unlabeled topic dataset.

</details>


### [421] [Assortativity in geometric and scale-free networks](https://arxiv.org/abs/2508.04608)
*Marc Kaufmann,Ulysse Schaller,Thomas Bläsius,Johannes Lengler*

Main category: cs.SI

TL;DR: 研究了网络中的度关联性，发现 Pearson 关联系数不适用于重尾网络，并提出了一个新的可调关联性模型。


<details>
  <summary>Details</summary>
Motivation: 研究网络中的度关联性，并指出现有的度关联性度量方法的局限性，特别是 Pearson 关联系数在重尾度分布网络中的不适用性。

Method: 对 Chung-Lu 图和几何不均匀随机图（GIRGs）进行了数学分析，并通过数值分析了真实世界的网络。

Result: Pearson 关联系数不能有效衡量重尾分布网络的关联性；现有的生成模型是关联性中性的；提出了一个可调关联性的 GIRG 模型。

Conclusion: 许多真实世界的网络表现出非零的度关联性，而我们提出的模型可以进行可调的度关联性。

Abstract: The assortative behavior of a network is the tendency of similar (or
dissimilar) nodes to connect to each other. This tendency can have an influence
on various properties of the network, such as its robustness or the dynamics of
spreading processes. In this paper, we study degree assortativity both in
real-world networks and in several generative models for networks with
heavy-tailed degree distribution based on latent spaces. In particular, we
study Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs).
  Previous research on assortativity has primarily focused on measuring the
degree assortativity in real-world networks using the Pearson assortativity
coefficient, despite reservations against this coefficient. We rigorously
confirm these reservations by mathematically proving that the Pearson
assortativity coefficient does not measure assortativity in any network with
sufficiently heavy-tailed degree distributions, which is typical for real-world
networks. Moreover, we find that other single-valued assortativity coefficients
also do not sufficiently capture the wiring preferences of nodes, which often
vary greatly by node degree. We therefore take a more fine-grained approach,
analyzing a wide range of conditional and joint weight and degree distributions
of connected nodes, both numerically in real-world networks and mathematically
in the generative graph models. We provide several methods of visualizing the
results.
  We show that the generative models are assortativity-neutral, while many
real-world networks are not. Therefore, we also propose an extension of the
GIRG model which retains the manifold desirable properties induced by the
degree distribution and the latent space, but also exhibits tunable
assortativity. We analyze the resulting model mathematically, and give a
fine-grained quantification of its assortativity.

</details>


### [422] [Layers of a City: Network-Based Insights into San Diego's Transportation Ecosystem](https://arxiv.org/abs/2508.04694)
*Matthew Chan,Steve Sharp,Jiajian Zhu,Raman Ebrahimi*

Main category: cs.SI

TL;DR: 本研究利用网络科学分析圣地亚哥的交通系统，发现其存在显著的公平差距和韧性问题，并指出圣地亚哥并非一个普遍适宜步行的城市。


<details>
  <summary>Details</summary>
Motivation: 分析城市交通网络的结构和功能对于提高流动性、公平性和韧性至关重要。

Method: 本研究利用网络科学对圣地亚哥的交通系统进行了多模式分析。使用来自OpenStreetMap (OSM) 和圣地亚哥都市交通系统 (MTS) 的数据构建了一个多层图，其中包含驾驶、步行和公共交通层。通过整合数千个兴趣点 (POIs)，分析了网络的可达性、结构和韧性，采用了中心性测量、社区检测和步行性指标。

Result: 分析显示，圣地亚哥的交通系统呈现出明显的“核心-边缘”二元分化。30.3%的兴趣点远离步行距离内的公共交通，表明郊区和农村地区的可达性存在显著的公平差距。中心性分析突显了驾驶网络过度依赖关键高速公路作为瓶颈，表明网络韧性较低，并证实圣地亚哥并非一个普遍适宜步行的城市。社区检测表明，交通方式决定了出行的规模，步行产生的是紧凑的本地出行集群，而驾驶产生的是广泛的区域出行集群。

Conclusion: 本研究提供了一个全面的框架来诊断城市交通系统，并提供量化的见解，为改善圣地亚哥的交通公平性和基础设施韧性提供有针对性的干预措施。

Abstract: Analyzing the structure and function of urban transportation networks is
critical for enhancing mobility, equity, and resilience. This paper leverages
network science to conduct a multi-modal analysis of San Diego's transportation
system. We construct a multi-layer graph using data from OpenStreetMap (OSM)
and the San Diego Metropolitan Transit System (MTS), representing driving,
walking, and public transit layers. By integrating thousands of Points of
Interest (POIs), we analyze network accessibility, structure, and resilience
through centrality measures, community detection, and a proposed metric for
walkability.
  Our analysis reveals a system defined by a stark core-periphery divide. We
find that while the urban core is well-integrated, 30.3% of POIs are isolated
from public transit within a walkable distance, indicating significant equity
gaps in suburban and rural access. Centrality analysis highlights the driving
network's over-reliance on critical freeways as bottlenecks, suggesting low
network resilience, while confirming that San Diego is not a broadly walkable
city. Furthermore, community detection demonstrates that transportation mode
dictates the scale of mobility, producing compact, local clusters for walking
and broad, regional clusters for driving. Collectively, this work provides a
comprehensive framework for diagnosing urban mobility systems, offering
quantitative insights that can inform targeted interventions to improve
transportation equity and infrastructure resilience in San Diego.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [423] [Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems](https://arxiv.org/abs/2508.03837)
*Davide Zoni,Andrea Galimberti,Adriano Guarisco*

Main category: cs.AR

TL;DR: Rhea是一个用于设计和验证缓存一致性内存子系统的统一框架，能够生成可配置RTL并结合gem5和Verilator进行仿真，在多核系统中表现出良好的有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 设计和验证高效的缓存一致性内存子系统是现代多核片上系统架构开发中的关键任务，但它非常复杂。

Method: Rhea框架集成了Verilator的周期精确RTL仿真和gem5的全系统仿真，可以与实际的RTL一起运行真实世界的基准程序和操作系统。Rhea可以生成支持各种架构参数的可综合、高度可配置的RTL。

Result: 使用Rhea设计的MSI缓存一致性内存子系统在22个基准应用程序上进行了评估，其性能在MI和MOESI模型中属于中等水平。与gem5 MI相比，gem5-Verilator的混合仿真开销适中（最高为2.7倍），但通过模拟真实RTL硬件实现了更高的仿真保真度。这种开销随着规模的增大而减小，在十六核场景中降至1.6倍。

Conclusion: Rhea框架能够有效地支持从单核到十六核系统的多核片上系统缓存一致性内存子系统的设计和验证，并且易于扩展，能够加快缓存一致性内存子系统设计的研发。

Abstract: Designing and validating efficient cache-coherent memory subsystems is a
critical yet complex task in the development of modern multi-core
system-on-chip architectures. Rhea is a unified framework that streamlines the
design and system-level validation of RTL cache-coherent memory subsystems. On
the design side, Rhea generates synthesizable, highly configurable RTL
supporting various architectural parameters. On the validation side, Rhea
integrates Verilator's cycle-accurate RTL simulation with gem5's full-system
simulation, allowing realistic workloads and operating systems to run alongside
the actual RTL under test. We apply Rhea to design MSI-based RTL memory
subsystems with one and two levels of private caches and scaling up to sixteen
cores. Their evaluation with 22 applications from state-of-the-art benchmark
suites shows intermediate performance relative to gem5 Ruby's MI and MOESI
models. The hybrid gem5-Verilator co-simulation flow incurs a moderate
simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher
fidelity by simulating real RTL hardware. This overhead decreases with scale,
down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's
effectiveness and scalability in enabling fast development of RTL
cache-coherent memory subsystem designs.

</details>


### [424] [FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead](https://arxiv.org/abs/2508.03866)
*Seock-Hwan Noh,Hoyeon Lee,Junkyum Kim,Junsu Im,Jay H. Park,Sungjin Lee,Sam H. Noh,Yeseong Kim,Jaeha Kung*

Main category: cs.AR

TL;DR: FlashVault 是一种创新的 NAND 闪存片上自加密架构，在不增加面积开销的情况下，支持多种加密算法，并在性能上超越了现有方案。


<details>
  <summary>Details</summary>
Motivation: 为了在 NAND 闪存芯片内实现自加密，无需增加额外面积即可支持多种加密算法，满足日益增长的安全需求和法规要求。

Method: FlashVault 被设计为一个嵌入到 4D V-NAND 结构中的片上自加密架构，集成了可重构的加密引擎，支持块密码、公钥和后量子算法。通过 RTL 实现和 P&R 进行评估。

Result: FlashVault 在性能上优于基于 CPU 的加密（1.46~3.45 倍）和近核心处理架构（1.02~2.01 倍），证明了其作为安全 SSD 架构的有效性。

Conclusion: FlashVault 是一种安全的 SSD 架构，能够满足各种加密要求，并且优于基于 CPU 和近核心处理器的加密方法。

Abstract: We present FlashVault, an in-NAND self-encryption architecture that embeds a
reconfigurable cryptographic engine into the unused silicon area of a
state-of-the-art 4D V-NAND structure. FlashVault supports not only block
ciphers for data encryption but also public-key and post-quantum algorithms for
digital signatures, all within the NAND flash chip. This design enables each
NAND chip to operate as a self-contained enclave without incurring area
overhead, while eliminating the need for off-chip encryption. We implement
FlashVault at the register-transfer level (RTL) and perform place-and-route
(P&R) for accurate power/area evaluation. Our analysis shows that the power
budget determines the number of cryptographic engines per NAND chip. We
integrate this architectural choice into a full-system simulation and evaluate
its performance on a wide range of cryptographic algorithms. Our results show
that FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)
and near-core processing architecture (1.02~2.01x), demonstrating its
effectiveness as a secure SSD architecture that meets diverse cryptographic
requirements imposed by regulatory standards and enterprise policies.

</details>


### [425] [TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads](https://arxiv.org/abs/2508.03900)
*Navaneeth Kunhi Purayil,Diyou Shen,Matteo Perotti,Luca Benini*

Main category: cs.AR

TL;DR: TROOP通过硬件优化提高了向量处理器的内存带宽利用率和能效，尤其是在GEMV等任务上，实现了显著的性能提升和高能效，同时面积开销很小。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型快速发展，需要灵活高效的硬件解决方案，以避免硬连线加速器过时。向量处理器因其可编程性和利用数据并行性实现高能效的潜力而备受关注。然而，当前最先进的VPEs在L1内存带宽和能效方面存在局限，尤其是在通用矩阵向量乘法（GEMV）等数据重用性较低的工作负载上表现不佳。为了充分利用L1内存接口的带宽，需要优化VPE微架构以达到“在-屋顶线”的利用率。

Method: 该研究提出了一系列硬件优化，包括解耦的加载-存储接口、改进的向量链式操作、影子缓冲区以解决向量寄存器文件（VRF）冲突，以及地址扰码技术。这些优化旨在提高向量处理元素（VPEs）的微架构效率，使其能够充分利用L1内存接口的可用带宽，实现“在-屋顶线”（at-the-roofline）性能。研究在12nm FinFET工艺下，使用开源的流线型向量处理器进行了实现和评估。

Result: TROOP在12nm FinFET技术下的流线型向量处理器上实现了显著的性能提升。对于GEMV、DOTP和AXPY等内存密集型核心计算任务，分别实现了1.5倍、2.2倍和2.6倍的加速，达到了“在-屋顶线”性能。在能效方面，TROOP最多提升了45%，DOTP核的能效达到了38 DP-GFLOPs/W（1 GHz, TT, 0.8V），同时保持了GEMM核61 DP-GFLOPs/W的高能效。该方案仅增加了不到7%的面积开销。

Conclusion: 该研究提出了TROOP，一套针对向量处理元素（VPEs）的硬件优化方案，通过解耦的加载-存储接口、改进的向量链式操作、用于隐藏VRF冲突的影子缓冲区以及地址扰码技术，实现了“在-屋顶线”（at-the-roofline）性能，同时不牺牲面积和能效。在12nm FinFET工艺下实现并验证的TROOP，在GEMV、DOTP和AXPY等内存密集型核心计算任务上分别实现了1.5倍、2.2倍和2.6倍的显著加速，实现了近乎理想的L1内存带宽利用率。此外，TROOP将能效最多提升了45%，DOTP核的能效达到了38 DP-GFLOPs/W（1 GHz, TT, 0.8V），同时保持了GEMM核61 DP-GFLOPs/W的高能效，而面积开销仅增加了不到7%。

Abstract: The fast evolution of Machine Learning (ML) models requires flexible and
efficient hardware solutions as hardwired accelerators face rapid obsolescence.
Vector processors are fully programmable and achieve high energy efficiencies
by exploiting data parallelism, amortizing instruction fetch and decoding
costs. Hence, a promising design choice is to build accelerators based on
shared L1-memory clusters of streamlined Vector Processing Elements (VPEs).
However, current state-of-the-art VPEs are limited in L1 memory bandwidth and
achieve high efficiency only for computational kernels with high data reuse in
the Vector Register File (VRF), such as General Matrix Multiplication (GEMM).
Performance is suboptimal for workloads with lower data reuse like General
Matrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at
the L1 memory interface, the VPE micro-architecture must be optimized to
achieve near-ideal utilization, i.e., to be as close as possible to the L1
memory roofline (at-the-roofline). In this work, we propose TROOP, a set of
hardware optimizations that include decoupled load-store interfaces, improved
vector chaining, shadow buffers to hide VRF conflicts, and address scrambling
techniques to achieve at-the-roofline performance for VPEs without compromising
their area and energy efficiency. We implement TROOP on an open-source
streamlined vector processor in a 12nm FinFET technology. TROOP achieves
significant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key
memory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline
performance. Additionally, TROOP enhances the energy efficiency by up to 45%,
reaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high
energy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area
overhead of less than 7%.

</details>


### [426] [OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite](https://arxiv.org/abs/2508.04106)
*Shan Shen,Xingyang Li,Zhuohua Liu,Yikai Wang,Yiheng Wu,Junhao Ma,Yuquan Sun,Wei W. Xing*

Main category: cs.AR

TL;DR: OpenYield是一个开源的SRAM良率分析平台，通过提供更真实的电路模型和标准化的评估、优化工具，弥合了学术研究与工业实践之间的差距，促进了存储器设计的创新。


<details>
  <summary>Details</summary>
Motivation: 学术界在SRAM良率分析方面的研究与工业界复杂现实之间存在显著差距，缺乏开放、真实的基准测试导致了可复现性危机，使得学术界的技术难以在工业界应用。

Method: 提出并实现了一个名为OpenYield的开源生态系统，包含三个核心部分：1. 能够纳入二阶效应寄生参数、单元间漏电耦合和外围电路变异的SRAM电路生成器；2. 包含基线 ज्यामुळे分析算法的标准化工评估平台；3. 用于展示OpenYield在增强SRAM设计鲁棒性和效率方面应用的标准化SRAM优化平台。

Result: OpenYield提供了一个基础，能够进行有意义的学术界与工业界合作，加速存储器设计的创新。该框架可用于优化算法的基准测试，并已成功用于增强SRAM设计的鲁棒性和效率。

Conclusion: OpenYield通过提供包含二阶效应寄生参数、单元间漏电耦合和外围电路变异等工业界关键因素的SRAM电路生成器、标准化的评估平台和优化平台，解决了学术界模型与工业界现实之间脱节的问题，促进了学术界与工业界的合作，加速了存储器设计的创新。

Abstract: Static Random-Access Memory (SRAM) yield analysis is essential for
semiconductor innovation, yet research progress faces a critical challenge: the
significant disconnect between simplified academic models and complex
industrial realities. The absence of open, realistic benchmarks has created a
reproducibility crisis, where promising academic techniques often fail to
translate to industrial practice. We present \textit{OpenYield}, a
comprehensive open-source ecosystem designed to address this critical gap
through three core contributions: (1) A realistic SRAM circuit generator that
uniquely incorporates critical second-order-effect parasitics, inter-cell
leakage coupling, and peripheral circuit variations, which are typically
omitted in academic studies but decisive in industrial designs. (2) A
standardized evaluation platform with a simple interface and implemented
baseline yield analysis algorithms, enabling fair comparisons and reproducible
research. (3) A standardized SRAM optimization platform, demonstrating
OpenYield's utility in enhancing SRAM design robustness and efficiency,
providing a comprehensive benchmark for optimization algorithms. OpenYield
creates a foundation for meaningful academia-industry collaboration,
accelerating innovation in memory design. The framework is publicly available
on \href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}

</details>


### [427] [ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs](https://arxiv.org/abs/2508.04516)
*Ishraq Tashdid,Dewan Saiham,Nafisa Anjum,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.AR

TL;DR: ECOLogic 是一种结合 ASIC 和 FPGA 优点的混合设计，通过 ECOScore 框架优化 IP 划分，实现高性能、安全、可更新和环保的可重构系统。


<details>
  <summary>Details</summary>
Motivation: ASIC 的灵活性低且有知识产权风险，而 FPGA 存在面积、功耗和性能的开销，导致碳足迹较高。因此，需要一种兼具高性能、灵活性和可持续性的解决方案。

Method: ECOLogic 是一种混合设计范例，通过在 ASIC 中嵌入轻量级 eFPGA 结构来实现安全、可更新和资源感知的计算。ECOScore 是一个量化评分框架，用于评估 IP 的适应性、盗版威胁、性能容忍度和资源匹配度，以指导 RTL 划分。

Result: ECOLogic 保留了平均 90% 的 ASIC 级性能（最高 2 GHz），实现了 9.8 ns 的时序松弛度，并将功耗平均降低了 480 倍。在可持续性方面，部署碳足迹减少了 99.7%，与纯 FPGA 相比，排放量降低了 300 到 500 倍。

Conclusion: ECOLogic 是一种高性能、安全且环保的可持续解决方案，适用于下一代可重构系统。

Abstract: Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs
among performance, flexibility, and sustainability. ASICs provide high
efficiency but are inflexible post-fabrication, require costly re-spins for
updates, and expose IPs to piracy risks. FPGAs offer reconfigurability and
reuse, yet suffer from substantial area, power, and performance overheads,
resulting in higher carbon footprints. We present ECOLogic, a hybrid design
paradigm that embeds lightweight eFPGA fabric within ASICs to enable secure,
updatable, and resource-aware computation. Central to this architecture is
ECOScore, a quantitative scoring framework that evaluates IPs based on
adaptability, piracy threat, performance tolerance, and resource fit to guide
RTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an
average of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns
timing slack (versus 5.1 ns in FPGA), and reduces power by 480 times on
average. Moreover, sustainability analysis shows a 99.7 percent reduction in
deployment carbon footprint and 300 to 500 times lower emissions relative to
FPGA-only implementations. These results position ECOLogic as a
high-performance, secure, and environmentally sustainable solution for
next-generation reconfigurable systems.

</details>


### [428] [Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems](https://arxiv.org/abs/2508.04609)
*Osama Abdelaleim,Arun Prakash,Ayhan Irfanoglu,Veljko Milutinovic*

Main category: cs.AR

TL;DR: 该研究提出了一种模拟电路，可以快速求解对称正定线性方程组，其求解速度与矩阵大小无关。


<details>
  <summary>Details</summary>
Motivation: 加速线性方程组的求解对于科学模拟、数据分析和机器学习等众多应用至关重要。

Method: 提出了一种通用的模拟直接求解器电路，利用非反相运算放大器配置构建负阻电路，以模拟对称系统，并对系统架构进行了优化。

Result: 对于对角占优对称矩阵，该系统以O(1)的复杂度求解，达到了理论上的最大速度。对于非对角占优的对称正定系统，求解速度取决于特征值和最大非对角线项等矩阵属性，但与矩阵大小无关。

Conclusion: 该设计通过利用运算放大器和电阻器来解决线性方程组，实现了O(1)的复杂度，并展示了其在解决对称正定线性方程组方面的鲁棒性。

Abstract: Accelerating the solution of linear systems of equations is critical due to
their central role in numerous applications, such as scientific simulations,
data analytics, and machine learning. This paper presents a general-purpose
analog direct solver circuit designed to accelerate the solution of positive
definite symmetric linear systems of equations. The proposed design leverages
non-inverting operational amplifier configurations to create a negative
resistance circuit, effectively modeling any symmetric system. The paper
details the principles behind the design, optimizations of the system
architecture, and numerical results that demonstrate the robustness of the
design. The findings reveal that the proposed system solves diagonally dominant
symmetric matrices with O(1) complexity, achieving the theoretical maximum
speed as the circuit relies solely on resistors. For non-diagonally dominant
symmetric positive-definite systems, the solution speed depends on matrix
properties such as eigenvalues and the maximum off-diagonal term, but remains
independent of matrix size.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [429] [Agentic-AI based Mathematical Framework for Commercialization of Energy Resilience in Electrical Distribution System Planning and Operation](https://arxiv.org/abs/2508.04170)
*Aniket Johri,Divyanshi Dwivedi,Mayukha Pal*

Main category: eess.SY

TL;DR: 该研究提出了一种结合AI（双智能体PPO）和市场机制的新方法来增强电网韧性，实现了技术和经济上的双重效益。


<details>
  <summary>Details</summary>
Motivation: 现有电网韧性增强方法主要关注技术层面，缺乏有效的市场机制来商业化韧性功能并优化其部署。此外，传统的网络重构方法难以适应正常和紧急情况。因此，有必要开发经济可行的框架来增强电网韧性。

Method: 提出了一种结合双智能体PPO和市场机制的新框架。其中，战略智能体选择最优的DER驱动的切换配置，战术智能体则在预算和天气限制下调整个体开关状态和电网偏好。该框架在一个模拟动态灾难事件、预算限制和韧性成本权衡的仿真环境中进行训练和测试。

Result: 该框架在10次测试中平均韧性得分为0.85±0.08，效益成本比为0.12±0.01。在灾难步骤中，85%的动作选择了具有4个DER的配置，表明了其在经济可行性和韧性增强方面的有效性。

Conclusion: 该框架通过结合双智能体近端策略优化（PPO）和基于市场的机制，实现了韧性增强和市场盈利能力之间的平衡，为电网韧性投资创造了可持续的市场激励。

Abstract: The increasing vulnerability of electrical distribution systems to extreme
weather events and cyber threats necessitates the development of economically
viable frameworks for resilience enhancement. While existing approaches focus
primarily on technical resilience metrics and enhancement strategies, there
remains a significant gap in establishing market-driven mechanisms that can
effectively commercialize resilience features while optimizing their deployment
through intelligent decision-making. Moreover, traditional optimization
approaches for distribution network reconfiguration often fail to dynamically
adapt to both normal and emergency conditions. This paper introduces a novel
framework integrating dual-agent Proximal Policy Optimization (PPO) with
market-based mechanisms, achieving an average resilience score of 0.85 0.08
over 10 test episodes. The proposed architecture leverages a dual-agent PPO
scheme, where a strategic agent selects optimal DER-driven switching
configurations, while a tactical agent fine-tunes individual switch states and
grid preferences under budget and weather constraints. These agents interact
within a custom-built dynamic simulation environment that models stochastic
calamity events, budget limits, and resilience-cost trade-offs. A comprehensive
reward function is designed that balances resilience enhancement objectives
with market profitability (with up to 200x reward incentives, resulting in 85%
of actions during calamity steps selecting configurations with 4 DERs),
incorporating factors such as load recovery speed, system robustness, and
customer satisfaction. Over 10 test episodes, the framework achieved a
benefit-cost ratio of 0.12 0.01, demonstrating sustainable market incentives
for resilience investment. This framework creates sustainable market incentives

</details>


### [430] [Information Bulletin Strategy in Impatient Queuing](https://arxiv.org/abs/2508.04241)
*Anthony Kiggundu,Bin Han,Hans D. Schotten*

Main category: eess.SY

TL;DR: 6G networks need autonomous operations, requiring tenants to make rational decisions based on status information. This paper proposes an information bulletin strategy to determine what, how much, and how often information should be shared between tenants and queues, aiming to minimize delay and impatience by learning tenant behavior and adapting queue processing rates.


<details>
  <summary>Details</summary>
Motivation: In 6G networks, decentralized control in multi-tenant systems is suggested for autonomous network operations. Autonomy requires independent rationale decisions by tenants, which in turn requires timely and continuous access to status information. However, what information to share, how much to communicate, and how frequently updates should be dispatched remain open research challenges.

Method: This manuscript proposes an information bulletin strategy defined around two models of the system descriptor states. Queues periodically broadcast these information models to tenants at different time intervals. The impatience is formulated as an optimization problem.

Result: The manuscript aims to minimize overall delay and impatience by adapting queue processing rates based on learned tenant behavior. Numerical experiments will be performed to evaluate the performance of the learned queue policy and assess how closely it approaches optimal conditions.

Conclusion: The proposed information bulletin strategy, through two models of system descriptor states and periodic broadcasts, aims to minimize overall delay and impatience by adapting queue processing rates based on tenant behavior. Numerical experiments are conducted to evaluate performance and assess proximity to optimal conditions.

Abstract: In Sixth Generation (6G) networks, decentralized control in multi-tenant
systems is a suggested enabler for autonomous network operations. However,
autonomy requires independent rationale decisions be taken by tenants. This
rationality can only be underpinned by timely and continuous access to status
information. Despite its importance, the questions of what information should
be shared, how much should be communicated, and how frequently updates should
be dispatched remain open research challenges.
  This manuscript proposes an information bulletin strategy defined around two
models of the system descriptor states to address these fundamental questions.
The strategy is that queues periodically broadcast these information models to
tenants at different time intervals, who may respond by reneging from the queue
or jockeying to a more favorable one. The expectation is that over time, the
queues adapt their processing rates based on what they learn from the tenant
behavior. The objective is to minimize overall delay and the impatience. We
formulate for this impatience as an optimization problem, whose analytical
solution is intractable. We perform numerical experiments to evaluate the
performance of the learned queue policy and to assess how closely it approaches
optimal conditions.

</details>


### [431] [A virtual sensor fusion approach for state of charge estimation of lithium-ion cells](https://arxiv.org/abs/2508.04268)
*Davide Previtali,Daniele Masti,Mirko Mazzoleni,Fabio Previdi*

Main category: eess.SY

TL;DR: 本研究提出了一种新的SOC估算方法，结合了虚拟传感器（VS）和扩展卡尔曼滤波器（EKF）。VS通过学习APV模型和训练ML模型来预测SOC，然后将预测的SOC与电池端子电压一起输入EKF。此外，还提出了一种数据驱动的噪声协方差矩阵校准方法。实验结果表明，该方法提高了SOC估算的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决锂离子电池SOC估算不准确和不稳定的问题，通过结合物理模型（ECM）和数据驱动模型（ML）的优势，以及利用先进的观测器设计和机器学习技术，来提高SOC估算的精度和性能。

Method: 本研究提出了一种结合了等效电路模型（ECM）和机器学习（ML）的卡尔曼滤波器（KF）的锂离子电池荷电状态（SOC）估算方法。具体而言，该方法首先学习一个数据驱动的仿射参数变动（APV）模型，然后基于APV模型推导出一组线性观测器，并结合观测器提取的特征、输入和输出数据训练一个机器学习模型以预测SOC。接着，将机器学习模型预测的SOC与电池端子电压一同作为扩展卡尔曼滤波器（EKF）的输出量测。此外，研究还提出了一种数据驱动的噪声协方差矩阵校准策略，并将其应用于EKF。

Result: 结合了VS预测SOC和电池端子电压作为EKF的量测，并采用数据驱动的校准策略来优化EKF的噪声协方差矩阵。

Conclusion: 实验结果表明，所设计的EAKF-VS方法在SOC估算精度和稳定性方面均优于现有方法。

Abstract: This paper addresses the estimation of the State Of Charge (SOC) of
lithium-ion cells via the combination of two widely used paradigms: Kalman
Filters (KFs) equipped with Equivalent Circuit Models (ECMs) and
machine-learning approaches. In particular, a recent Virtual Sensor (VS)
synthesis technique is considered, which operates as follows: (i) learn an
Affine Parameter-Varying (APV) model of the cell directly from data, (ii)
derive a bank of linear observers from the APV model, (iii) train a
machine-learning technique from features extracted from the observers together
with input and output data to predict the SOC. The SOC predictions returned by
the VS are supplied to an Extended KF (EKF) as output measurements along with
the cell terminal voltage, combining the two paradigms. A data-driven
calibration strategy for the noise covariance matrices of the EKF is proposed.
Experimental results show that the designed approach is beneficial w.r.t. SOC
estimation accuracy and smoothness.

</details>


### [432] [Design of Adaptive Hybrid Downlink NOMA-TDMA for Visible Light Communications Networks](https://arxiv.org/abs/2508.04380)
*Tuan A. Hoang,Chuyen T. Nguyen,Thanh V. Pham*

Main category: eess.SY

TL;DR: 提出了一种自适应混合NOMA-TDMA方案，通过SCA优化用户配对，提高了VLC网络的用户总速率和性能。


<details>
  <summary>Details</summary>
Motivation: 为了在多用户可见光通信（VLC）网络中提高用户的总速率性能并保持低复杂度。

Method: 提出了一种自适应混合非正交多址（NOMA）-时分多址（TDMA）方案，并通过连续凸近似（SCA）方法解决了用户配对的优化问题。

Result: 仿真结果表明，该方案在不同用户数和发射功率下优于传统方法。

Conclusion: 所提出的自适应混合NOMA-TDMA方案在不同数量的用户和发射LED功率下，优于传统的混合NOMA-TDMA方法。

Abstract: This paper proposes an adaptive hybrid non-orthogonal multiple access
(NOMA)-time division multiple access (TDMA) scheme for multi-user visible light
communication (VLC) networks, aiming to enhance users' sum-rate performance
while maintaining low complexity. In the proposed scheme, users are divided
into groups where each group is served in a different time slot using TDMA.
Within each group, up to two users can be served simultaneously using NOMA. A
central challenge lies in determining which users should be paired together for
NOMA, as the effectiveness of successive interference cancellation (SIC)
employed by NOMA depends on the difference between users' channel gains. To
address this, for a pair of users, we determine the range of their channel gain
ratio within which the pair benefits more from NOMA or TDMA. Identifying the
lower and upper bounds of this range is formulated as two optimization problems
which are solved efficiently using the Successive Convex Approximation (SCA)
method. Simulation results demonstrate that the proposed scheme outperforms the
conventional hybrid NOMA-TDMA method under different numbers of users and
transmit LED powers.

</details>


### [433] [Error Accumulation using Linearized Models for Aggregating Flexibility in Distribution Systems](https://arxiv.org/abs/2508.04382)
*Yanlin Jiang,Xinliang Dai,Frederik Zahn,Yi Guo,Veit Hagenmeyer*

Main category: eess.SY

TL;DR: 该研究分析了线性潮流模型在灵活性聚合中的应用，发现它们低估了线路损耗，且误差会随时间和地点累积。


<details>
  <summary>Details</summary>
Motivation: 研究基于线性模型的灵活性聚合方法。

Method: 通过研究线性AC潮流、DC潮流和LinDistFlow模型的理论基础及其假设，并考虑网络拓扑、电压约束和线路损耗等关键系统细节来分析灵活性聚合方法。在KIT Campus Nord网络上使用真实负荷和太阳能数据进行了模拟。

Result: 线性模型（在无负损耗情况下）普遍低估线路损耗，并且损耗误差会在PCC和长时域上累积。

Conclusion: 线性潮流模型（包括AC潮流、DC潮流和LinDistFlow）低估了线路损耗，并且损耗误差会在公共连接点（PCC）和长时域上累积。

Abstract: This paper investigates flexibility aggregation approaches based on linear
models. We begin by examining the theoretical foundations of linear AC power
flow, two variants of so-called DC power flow, and the LinDistFlow model, along
with their underlying assumptions. The discussion covers key system details,
including network topology, voltage constraints, and line losses. Simulations
are conducted on the KIT Campus Nord network with real demand and solar data.
Results show that, in the absence of negative losses, line losses are generally
underestimated by linear models. Furthermore, line losses errors tend to
accumulate both at the point of common coupling (PCC) and over extended time
horizons.

</details>


### [434] [Case Studies of Generative Machine Learning Models for Dynamical Systems](https://arxiv.org/abs/2508.04459)
*Nachiket U. Bapat,Randy C. Paffenroth,Raghvendra V. Cowlagi*

Main category: eess.SY

TL;DR: 本文研究了用于航空航天工程的生成式人工智能模型（GAIM），重点关注了如何通过GAIM减少仿真数据与实际运行数据之间的模型不匹配问题。文中提出了基于VAE的模型，能够在数据量较少的情况下生成满足控制方程且与训练数据统计上相似的数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决航空航天工程中仿真数据与实际运行数据之间因建模误差、简化和不确定性而产生的模型不匹配问题，本文探讨了使用生成式人工智能模型（GAIM）来显著减少这种模型不匹配的可能性。

Method: 本文针对最优控制系统，特别是飞机导航中的最小时间导航和最小暴露导航问题，对生成式人工智能模型（GAIM）进行了研究。研究了生成式对抗网络（GAN）和两种变分自编码器（VAE）模型，并基于哈密顿函数不变性原理设计了训练损失函数。

Result: 研究表明，所提出的GAIM模型，特别是VAE模型，在训练数据量较少的情况下，能够生成满足控制方程且与训练数据统计上相似的数据。

Conclusion: 文中提出的生成式人工智能模型（GAIM），特别是基于变分自编码器（VAE）的模型，能够合成满足控制方程且在数据量较小的情况下与训练数据在统计上相似的数据。

Abstract: Systems like aircraft and spacecraft are expensive to operate in the real
world. The design, validation, and testing for such systems therefore relies on
a combination of mathematical modeling, abundant numerical simulations, and a
relatively small set of real-world experiments. Due to modeling errors,
simplifications, and uncertainties, the data synthesized by simulation models
often does not match data from the system's real-world operation. We consider
the broad research question of whether this model mismatch can be significantly
reduced by generative artificial intelligence models (GAIMs). Unlike text- or
image-processing, where generative models have attained recent successes, GAIM
development for aerospace engineering applications must not only train with
scarce operational data, but their outputs must also satisfy governing
equations based on natural laws, e.g., conservation laws. The scope of this
paper primarily focuses on two case studies of optimally controlled systems
that are commonly understood and employed in aircraft guidance, namely:
minimum-time navigation in a wind field and minimum-exposure navigation in a
threat field. We report GAIMs that are trained with a relatively small set, of
the order of a few hundred, of examples and with underlying governing
equations. By focusing on optimally controlled systems, we formulate training
loss functions based on invariance of the Hamiltonian function along system
trajectories. We investigate three GAIM architectures, namely: the generative
adversarial network (GAN) and two variants of the variational autoencoder
(VAE). We provide architectural details and thorough performance analyses of
these models. The main finding is that our new models, especially the VAE-based
models, are able to synthesize data that satisfy the governing equations and
are statistically similar to the training data despite small volumes of
training data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [435] [Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork](https://arxiv.org/abs/2508.04163)
*Hasra Dodampegama,Mohan Sridharan*

Main category: cs.AI

TL;DR: This paper proposes a hybrid approach (knowledge-based + data-driven) for AI agents to collaborate effectively in ad hoc teamwork, overcoming limitations of purely data-driven methods and showing positive results in a realistic simulation.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art methods for ad hoc teamwork are often data-driven, requiring large labeled datasets, lacking transparency, and making rapid knowledge revision difficult. Additionally, increasing agent numbers complicate decision-making for effective collaboration.

Method: The proposed architecture enables ad hoc agents to determine actions through non-monotonic logical reasoning using prior commonsense domain-specific knowledge, rapidly learned and revised models of other agents' behavior, and anticipated abstract future goals from a foundation model.

Result: Experimental evaluation in the VirtualHome environment demonstrates the architecture's capabilities.

Conclusion: The paper advocates for leveraging the complementary strengths of knowledge-based and data-driven methods for reasoning and learning in ad hoc teamwork, showing promise in realistic simulations.

Abstract: AI agents deployed in assistive roles often have to collaborate with other
agents (humans, AI systems) without prior coordination. Methods considered
state of the art for such ad hoc teamwork often pursue a data-driven approach
that needs a large labeled dataset of prior observations, lacks transparency,
and makes it difficult to rapidly revise existing knowledge in response to
changes. As the number of agents increases, the complexity of decision-making
makes it difficult to collaborate effectively. This paper advocates leveraging
the complementary strengths of knowledge-based and data-driven methods for
reasoning and learning for ad hoc teamwork. For any given goal, our
architecture enables each ad hoc agent to determine its actions through
non-monotonic logical reasoning with: (a) prior commonsense domain-specific
knowledge; (b) models learned and revised rapidly to predict the behavior of
other agents; and (c) anticipated abstract future goals based on generic
knowledge of similar situations in an existing foundation model. We
experimentally evaluate our architecture's capabilities in VirtualHome, a
realistic physics-based 3D simulation environment.

</details>


### [436] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: MI9是首个为代理AI设计的运行时治理框架，通过实时控制解决其独特的治理挑战，从而实现安全、负责任的生产部署。


<details>
  <summary>Details</summary>
Motivation: 传统的AI治理方法无法应对具有涌现和意外行为的代理AI系统引入的新型代理相关风险，因此需要一个专门为代理AI设计的运行时治理框架。

Method: MI9是一个完全集成的运行时治理框架，包含六个组件：代理风险指数、代理语义遥测捕获、持续授权监控、基于有限状态机（FSM）的符合性引擎、目标条件漂移检测和渐进式遏制策略。

Result: MI9能够系统地解决治理挑战，并为安全地大规模部署代理AI系统奠定技术基础。

Conclusion: MI9为可代理AI系统的安全和对齐提供了基础性基础设施，实现了系统化、安全和负责任的部署。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [437] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: Evo-MARL是一种新的MARL框架，通过训练任务智能体共同获得防御能力来提高多智能体系统的安全性。与依赖外部安全模块不同，Evo-MARL将安全机制内部化，并通过对抗性训练不断提高性能，实验证明其在降低攻击成功率和提高准确性方面均有显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施依赖外部安全代理，存在保护有限和单点故障的风险，并且增加安全代理会增加成本和复杂性。

Method: 提出了一种名为Evo-MARL的新型多智能体强化学习（MARL）框架，该框架使所有任务智能体能够共同获得防御能力。通过将进化搜索与参数共享强化学习相结合，共同进化攻击者和防御者，从而将安全机制内部化，并在共同进化的威胁下持续增强MAS性能。

Result: 实验结果表明，Evo-MARL能够将攻击成功率降低高达22%，同时将推理任务的准确性提高高达5%。

Conclusion: Evo-MARL框架能够提高多智能体系统的鲁棒性，同时提升其在推理任务上的准确性，证明了安全性和实用性可以协同改进。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [438] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 本研究提出了一种名为 MOTIF 的新框架，利用两个 LLM 智能体之间的回合制交互来共同优化组合优化问题求解器的多个组件，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前在处理 NP-hard 组合优化问题（COPs）时，设计有效的算法组件仍然是一个基本障碍。尽管最近在使用大型语言模型（LLMs）合成高质量组件方面取得了进展，但大多数方法将搜索限制在单个元素上，错失了更广泛的创新机会。本研究旨在将求解器设计扩展为一个多策略优化问题，以期在统一的目标下联合改进一组相互依赖的组件。

Method: 提出了一种名为 MOTIF（Multi-strategy Optimization via Turn-based Interactive Framework）的新框架。该框架基于蒙特卡洛树搜索，促进两个大型语言模型（LLM）智能体之间的回合制优化。在每个回合中，一个智能体通过利用自身和对手先前更新的历史来改进一个组件，从而促进竞争压力和合作。

Result: MOTIF 框架能够发现多样化的高性能解决方案，并且在多个组合优化问题域上的实验结果一致优于当前最先进的方法。

Conclusion: MOTIF 框架在多个组合优化问题域上展现出持续优于最先进方法的性能，凸显了基于回合的多智能体提示在完全自动化求解器设计方面的潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [439] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: SymbolBench基准测试和结合LLM与GP的框架，用于评估和提升LLMs在时间序列数据中发现符号定律的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs从时间序列数据中推断可解释的、与上下文对齐的符号结构的能力，以应对科学发现中的核心挑战。

Method: 提出SymbolBench基准测试，包含多元符号回归、布尔网络推断和因果发现三个任务，并提出一个结合LLM和遗传编程（GP）的闭环符号推理框架。

Result: LLMs在SymbolBench上的表现揭示了其在时间序列符号推理方面的优势和局限性，强调了结合领域知识、上下文对齐和推理结构的重要性。

Conclusion: LLMs结合GP进行符号回归、布尔网络推断和因果发现，可以提升在科学发现中的能力，但仍需结合领域知识、上下文对齐和推理结构。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [440] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: MLRMs容易被情感操纵，即使它们识别到视觉风险。EmoAgent框架利用这一点，通过情感提示来操纵模型，导致不安全输出。研究提出了新的风险评估指标。


<details>
  <summary>Details</summary>
Motivation: 观察到面向人机服务的MLRMs在深度思考阶段极易受到用户情感线索的影响，即使在有安全协议或检查的情况下，高情感强度也可能导致其绕过安全限制，产生有害输出。

Method: 提出了一种名为EmoAgent的自主对抗性情感代理框架，该框架通过精心设计的夸张情感提示来劫持MLRMs的推理过程，特别是利用用户在深度思考阶段对情感线索的敏感性。

Result: 证明了EmoAgent的有效性，并揭示了MLRMs在安全行为方面更深层次的情感认知失调。通过引入的三个新指标（RRSS、RVNR、RAIC）对MLRMs进行了量化评估。

Conclusion: EmoAgent框架能够有效地利用用户的情感线索来操纵MLRMs，即使在模型识别到视觉风险的情况下，也可能导致不安全或有害的输出。该研究揭示了模型安全行为中更深层次的情感认知失调，并提出了RRSS、RVNR和RAIC三个新指标来量化这些风险。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [441] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 本文介绍了Cognition Forest和Galaxy框架，用于构建更智能、更主动、更安全的个人助理IPA，并在实验中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前智能个人助理（IPA）在主动性、隐私保护和自我进化方面存在挑战。为了解决这些问题，需要设计新的IPA，其核心在于LLM代理的认知架构。

Method: 本文提出了一种名为Cognition Forest的语义结构，用于统一认知建模与系统设计，并基于此设计了Galaxy框架。该框架支持多维度交互和个性化能力生成，并实现了两个基于Galaxy的代理：KoRa（支持响应和主动技能）和Kernel（支持自我进化和隐私保护）。

Result: Galaxy框架在实验中表现优于多个最先进的基准，并且通过消融研究和实际交互案例验证了其有效性。

Conclusion: 本文提出的Cognition Forest和Galaxy框架，通过将认知建模与系统设计相结合，实现了具备响应性、主动性、隐私保护和自我进化能力的智能个人助理（IPA）。实验结果表明，Galaxy在性能上超越了现有基准，并验证了其有效性。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [442] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent是一种不确定性感知代理，通过自适应感知（组件推荐和用户反馈）解决了GUI任务的输入冗余和决策模糊问题，并在ComplexAction数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解决GUI代理在输入冗余和决策模糊方面存在的不足。

Method: RecAgent采用自适应感知，区分感知不确定性和决策不确定性。通过组件推荐机制减少感知不确定性，通过交互式模块请求用户反馈来处理决策不确定性。

Result: 实验证明了RecAgent的有效性，并提出了ComplexAction数据集用于评估GUI代理。

Conclusion: RecAgent通过组件推荐和用户反馈解决了GUI代理的输入冗余和决策模糊问题，并在ComplexAction数据集上验证了其有效性。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [443] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: A new Self-Evolution Agent (SEA) for computer use is proposed, featuring innovative methods for data generation, reinforcement learning, and model enhancement, leading to improved performance with a 7B parameter model.


<details>
  <summary>Details</summary>
Motivation: To address the performance limitations of current computer use agents and develop a more capable agent for user tasks.

Method: The paper proposes an automatic pipeline for verifiable trajectory generation, step-wise reinforcement learning for efficient long-horizon training, and an enhancement method to merge grounding and planning abilities into a single model without extra training.

Result: The Self-Evolution Agent (SEA) with 7B parameters achieves state-of-the-art performance compared to models of similar size and comparable performance to larger models.

Conclusion: SEA outperforms models with the same parameter count and matches larger models, demonstrating the effectiveness of the proposed innovations in data generation, training strategy, and enhancement.

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [444] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: Generative AI can personalize learning content to career goals, boosting engagement, satisfaction, and efficiency, bridging the gap between academia and the workplace.


<details>
  <summary>Details</summary>
Motivation: Investigate how career goal-based content adaptation in generative AI learning systems influences learner engagement, satisfaction, and study efficiency.

Method: A mixed-methods experiment involving over 4,000 learners, comparing a group receiving career goal-tailored learning scenarios with a control group.

Result: Quantitative results showed increased session duration, higher satisfaction ratings, and a modest reduction in study duration compared to standard content. Qualitative analysis indicated learners found personalized material motivating, practical, and conducive to deep cognitive engagement and strong identification with the content.

Conclusion: The study underscores the value of aligning educational content with learners' career goals and suggests that scalable AI personalization can bridge academic knowledge and workplace applicability.

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [445] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT通过知识图谱和可执行代码增强了代码生成和数学推理能力，在数学推理任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在数学推理和代码生成等复杂推理任务中的局限性，提出KGA-ECoT框架。

Method: KGA-ECoT框架通过构建结构化任务图，利用GraphRAG从数学库中检索知识，并生成可验证的代码以确保计算精度。

Result: 在多个数学推理基准的评估中，KGA-ECoT显著优于现有的提示方法，准确率提升了几个百分点到十几个百分点。

Conclusion: KGA-ECoT是一个稳健且高度可泛化的框架，能够有效处理复杂的数学推理任务。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [446] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: GeoSR是一个创新的框架，通过整合地理学原理和多代理协同，显著提升了大型语言模型在地理空间预测任务中的表现，解决了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在处理地理空间问题时面临的空间一致性、多步推理和地理偏见等挑战。

Method: 提出了一种名为GeoSR的自细化代理推理框架，该框架将地理学核心原理（特别是托布勒第一定律）嵌入到迭代预测循环中。该框架包含三个协同工作的代理：变量选择代理、点选择代理和细化代理，通过利用空间依赖性和变量间关系来逐步改进预测质量。

Result: 实验结果表明，GeoSR在物理世界属性估计和社会经济预测等任务上，相比标准提示策略，能够持续地提高预测性能。

Conclusion: GeoSR框架通过整合地理统计先验和空间结构化推理，能够显著提高LLM在地理空间预测任务中的准确性和公平性，实验结果表明其在各种任务上均优于标准提示策略。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


### [447] [Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement](https://arxiv.org/abs/2508.04105)
*Karrtik Iyer,Manikandan Ravikiran,Prasanna Pendse,Shayan Mohanty*

Main category: cs.AI

TL;DR: 本研究提出“语义熵”来衡量 GPT-4 对学生回答的解释多样性，以此作为评分不确定性的信号。实验证明该指标与人类评分者的一致性相关，并能在不同学科和任务类型中有效应用，有助于提升 AI 评分的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评分系统在处理简短回答时效率很高，但缺乏表明评分不确定性或可能存在争议的机制。本研究旨在解决这一问题。

Method: 通过分析同一学生对同一题目的多种 GPT-4 生成的解释，引入“语义熵”作为衡量指标，量化理由的多样性。具体方法是：基于蕴含相似性对理由进行聚类，并计算这些聚类上的熵。

Result: 实验结果表明，语义熵与人类评分者的一致性相关，在不同学科间具有普适性，并且在需要解释性推理的任务中会增加。

Conclusion: 研究结果表明，语义熵是一种可解释的不确定性信号，能够增强 AI 辅助评分工作流的透明度和可信度。

Abstract: Automated grading systems can efficiently score short-answer responses, yet
they often fail to indicate when a grading decision is uncertain or potentially
contentious. We introduce semantic entropy, a measure of variability across
multiple GPT-4-generated explanations for the same student response, as a proxy
for human grader disagreement. By clustering rationales via entailment-based
similarity and computing entropy over these clusters, we quantify the diversity
of justifications without relying on final output scores. We address three
research questions: (1) Does semantic entropy align with human grader
disagreement? (2) Does it generalize across academic subjects? (3) Is it
sensitive to structural task features such as source dependency? Experiments on
the ASAP-SAS dataset show that semantic entropy correlates with rater
disagreement, varies meaningfully across subjects, and increases in tasks
requiring interpretive reasoning. Our findings position semantic entropy as an
interpretable uncertainty signal that supports more transparent and trustworthy
AI-assisted grading workflows.

</details>


### [448] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: SEAgent是一个能够自主学习和适应新软件的计算机使用代理（CUA）框架。通过经验学习、任务生成和分层训练策略，显著提高了代理在未知软件上的表现，并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机使用代理（CUAs）主要依赖人工标注数据，在面对新颖和专业软件时表现不佳，尤其是在缺乏人工标注的情况下。SEAgent旨在解决这一挑战，使CUAs能够自主进化并适应不熟悉的软件环境。

Method: SEAgent框架通过经验学习自主掌握新软件。具体包括：设计了用于分步轨迹评估的世界状态模型和用于生成任务的课程生成器。通过模仿失败操作和优化成功策略来更新代理策略。采用从专才到通才的训练策略，整合专才代理的经验洞察，以发展更强大的通才CUA。

Result: SEAgent在五个新软件环境中，成功率从11.3%提升至34.5%，相比竞争性开源CUA UI-TARS有23.2%的显著提升。最终的统一代理在专门软件上的性能超越了单个专家代理的集合。

Conclusion: SEAgent通过在OS-World的五个新软件环境中进行验证，在成功率上相比UI-TARS有显著提升（从11.3%提升到34.5%），证明了其在自主学习和适应新软件方面的有效性，并超越了单个专家代理的性能。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


### [449] [A Compositional Framework for On-the-Fly LTLf Synthesis](https://arxiv.org/abs/2508.04116)
*Yongkang Li,Shengping Xiao,Shufang Zhu,Jianwen Li,Geguang Pu*

Main category: cs.AI

TL;DR: 提出了一种组合式动态综合框架，用于LTLf公式的反应式综合，通过在游戏求解期间组合小公式来克服DFA构造的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决LTLf公式的DFA构造的挑战，DFA构造是2EXPTIME-complete的。现有的方法要么在求解游戏前组合DFA，要么在求解游戏期间增量地构建DFA，但没有一种方法占主导地位。

Method: 提出了一种组合式动态综合框架，该框架在游戏求解期间应用组合，而不是在自动机（游戏场景）构造期间应用组合。框架支持两种组合变体：在组合前进行剪枝，或在组合期间进行剪枝。

Result: 该框架能够解决许多其他求解器无法处理的实例。

Conclusion: 该框架整合了两种方法的优点，能够解决其他求解器无法处理的实例，并且两种组合变体都有其独特的优点。

Abstract: Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can
be reduced to a two-player game over a Deterministic Finite Automaton (DFA) of
the LTLf specification. The primary challenge here is DFA construction, which
is 2EXPTIME-complete in the worst case. Existing techniques either construct
the DFA compositionally before solving the game, leveraging automata
minimization to mitigate state-space explosion, or build the DFA incrementally
during game solving to avoid full DFA construction. However, neither is
dominant. In this paper, we introduce a compositional on-the-fly synthesis
framework that integrates the strengths of both approaches, focusing on large
conjunctions of smaller LTLf formulas common in practice. This framework
applies composition during game solving instead of automata (game arena)
construction. While composing all intermediate results may be necessary in the
worst case, pruning these results simplifies subsequent compositions and
enables early detection of unrealizability. Specifically, the framework allows
two composition variants: pruning before composition to take full advantage of
minimization or pruning during composition to guide on-the-fly synthesis.
Compared to state-of-the-art synthesis solvers, our framework is able to solve
a notable number of instances that other solvers cannot handle. A detailed
analysis shows that both composition variants have unique merits.

</details>


### [450] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: AgREE是一个新颖的基于Agent的框架，通过迭代检索和多步推理来构建知识图谱三元组，即使没有训练也能显著优于现有方法，尤其擅长处理新出现的实体。


<details>
  <summary>Details</summary>
Motivation: 解决开放域知识图谱补全（KGC）在不断变化的世界中面临的重大挑战，特别是考虑到日常新闻中新实体的持续出现。现有方法通常需要大量的监督和训练数据，并且难以捕捉关于不受欢迎和/或新出现的实体的全面和最新的信息。

Method: AgREE框架结合了迭代检索动作和多步推理，以动态地构建丰富的知识图谱三元组。

Result: AgREE在构建知识图谱三元组方面显著优于现有方法，尤其是在针对语言模型训练过程中未出现的实体方面，表现高出9.8%。此外，还提出了一种新的评估方法和一个针对新出现实体的KGC新基准。

Conclusion: AgREE框架通过结合基于Agent的推理和战略信息检索，在动态信息环境中有效地维护了最新的知识图谱。

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [451] [Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities](https://arxiv.org/abs/2508.04235)
*Jiaying Zhu,Ziyang Zheng,Zhengyuan Shi,Yalun Cai,Qiang Xu*

Main category: cs.AI

TL;DR: CASCAD是一个创新的框架，它利用GNN来分析电路的条件概率，并通过指导CDCL启发式方法来提高SAT求解器的性能，在LEC基准测试中实现了显著的时间节省。


<details>
  <summary>Details</summary>
Motivation: 标准的SAT工作流程将电路转换为CNF，这会丢弃丰富的结构和功能信息，导致求解器性能不佳。

Method: 提出了一种名为CASCAD的新型电路感知SAT求解框架，该框架利用图神经网络（GNN）计算的电路级条件概率，并利用这些概率来指导CDCL启发式方法（变量相位选择和子句管理），以提高求解器效率。

Result: 在具有挑战性的真实世界逻辑等效性检查（LEC）基准测试上的广泛评估表明，与基于CNF的方法相比，CASCAD将求解时间缩短了高达10倍，并且通过概率引导的子句过滤策略将运行时间又减少了23.5%。

Conclusion: 该研究强调了在SAT求解器中保留电路级结构洞察的重要性，为未来提高SAT求解效率和EDA工具设计奠定了坚实的基础。

Abstract: Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design
Automation. The standard workflow for solving CSAT problems converts circuits
into Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by
Conflict-Driven Clause Learning (CDCL). However, this process inherently
discards rich structural and functional information, leading to suboptimal
solver performance. To address this limitation, we introduce CASCAD, a novel
circuit-aware SAT solving framework that directly leverages circuit-level
conditional probabilities computed via Graph Neural Networks (GNNs). By
explicitly modeling gate-level conditional probabilities, CASCAD dynamically
guides two critical CDCL heuristics -- variable phase selection and clause
managementto significantly enhance solver efficiency. Extensive evaluations on
challenging real-world Logical Equivalence Checking (LEC) benchmarks
demonstrate that CASCAD reduces solving times by up to 10x compared to
state-of-the-art CNF-based approaches, achieving an additional 23.5% runtime
reduction via our probability-guided clause filtering strategy. Our results
underscore the importance of preserving circuit-level structural insights
within SAT solvers, providing a robust foundation for future improvements in
SAT-solving efficiency and EDA tool design.

</details>


### [452] [Large Language Model's Multi-Capability Alignment in Biomedical Domain](https://arxiv.org/abs/2508.04278)
*Wentao Wu,Linqing Chen,Hanmeng Zhong,Weilei Wang*

Main category: cs.AI

TL;DR: BalancedBio 是一个参数高效的生物医学推理框架，通过 MKGSG 和能力感知组相对策略优化，解决了 AI 对齐中的多能力集成和安全问题，并在多个生物医学任务上取得了最先进的成果，同时降低了成本并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决特定领域 AI 对齐中的多能力集成问题，特别是在生物医学领域，需要参数高效且安全可靠的推理能力。

Method: BalancedBio 框架通过以下两个创新点实现参数高效的生物医学推理：1. 结合临床工作流约束和医学本体验证的“医学知识接地合成生成”（MKGSG），以提高事实准确性和安全性。2. “能力感知组相对策略优化”，通过一个结合了基于规则和基于模型的奖励分数的奖励模型，在强化学习中推导出最优的混合奖励权重，以保持能力的 تعلم.

Result: BalancedBio 在参数高效模型类别中取得了最先进的成果，在领域专业知识（BIOMED-MMLU 80.95%，提升 15.32%）、推理（61.94%，提升 7.75%）、指令遵循（67.95%，提升 6.44%）和集成（86.7%，提升 18.5%）方面均有显著提高。实际部署方面，成本降低了 78%，诊断准确性提高了 23%，临床医生接受度达到了 89%。

Conclusion: BalancedBio 通过提供一个有理论依据的框架，实现了参数高效的生物医学推理，并解决了特定领域 AI 对齐中的多能力集成问题。它通过建立的“生物医学多能力收敛定理”证明了正交梯度空间对于防止能力干扰和安全部署至关重要。该框架在三个关键能力上取得了最先进的结果，并实现了显著的成本降低、诊断准确性提高和高临床医生接受度，为生物医学 AI 对齐提供了一种原则性的方法。

Abstract: BalancedBio is a theoretically grounded framework for parameter-efficient
biomedical reasoning, addressing multi-capability integration in
domain-specific AI alignment. It establishes the Biomedical Multi-Capability
Convergence Theorem, proving orthogonal gradient spaces are essential to
prevent capability interference for safe deployment. Key innovations include:
(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending
Source2Synth with clinical workflow constraints and medical ontology validation
for factual accuracy and safety; and (2) Capability Aware Group Relative Policy
Optimization, deriving optimal hybrid reward weighting to maintain
orthogonality in RL, using a reward model with rule-based and model-based
scores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal
convergence, preserving performance across capabilities. It achieves
state-of-the-art results in its parameter class: domain expertise (80.95%
BIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction
following (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety
guarantees include bounds on capability preservation and clinical accuracy.
Real-world deployment yields 78% cost reduction, 23% improved diagnostic
accuracy, and 89% clinician acceptance. This work provides a principled
methodology for biomedical AI alignment, enabling efficient reasoning with
essential safety and reliability, with the 0.5B model version to be released.

</details>


### [453] [Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling](https://arxiv.org/abs/2508.04282)
*Yongyi Wang,Lingfeng Li,Bozhou Chen,Ang Li,Hanyu Liu,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本研究提出了一种新的POMDP合成方法，能够控制环境的难度，以更好地评估和设计内存增强的强化学习算法。


<details>
  <summary>Details</summary>
Motivation: 现有的内存增强RL基准测试缺乏对挑战程度的可控性，而合成环境对此至关重要，因此本研究旨在解决这一问题。

Method: 本研究利用线性过程动力学、状态聚合和奖励再分配的方法来构建具有预定义属性的定制化POMDP。

Result: 本研究创建了一系列具有不同难度级别的POMDP环境，并通过实证验证了其有效性，为内存增强RL提供了经验支持。

Conclusion: 本研究提出了一个用于分析和设计具有可控挑战的POMDP环境的理论框架和方法，为内存增强RL提供了有力的支持。

Abstract: Recent research has developed benchmarks for memory-augmented reinforcement
learning (RL) algorithms, providing Partially Observable Markov Decision
Process (POMDP) environments where agents depend on past observations to make
decisions. While many benchmarks incorporate sufficiently complex real-world
problems, they lack controllability over the degree of challenges posed to
memory models. In contrast, synthetic environments enable fine-grained
manipulation of dynamics, making them critical for detailed and rigorous
evaluation of memory-augmented RL. Our study focuses on POMDP synthesis with
three key contributions:
  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand
Structure (MDS), transition invariance, and related concepts; 2. A methodology
leveraging linear process dynamics, state aggregation, and reward
redistribution to construct customized POMDPs with predefined properties; 3.
Empirically validated series of POMDP environments with increasing difficulty
levels, designed based on our theoretical insights. Our work clarifies the
challenges of memory-augmented RL in solving POMDPs, provides guidelines for
analyzing and designing POMDP environments, and offers empirical support for
selecting memory models in RL tasks.

</details>


### [454] [Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models](https://arxiv.org/abs/2508.04339)
*Anran Xu,Jincheng Wang,Baigen Cai,Tao Wen*

Main category: cs.AI

TL;DR: DRN 是一种新的逻辑推理范式，通过不确定性最小化来解决大型语言模型的认知陷阱问题，并在多个基准测试中取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型在逻辑推理中，当语义启发式与决定性证据发生冲突时（即认知陷阱）的失败问题。

Method: DRN 将逻辑推理重新定义为不确定性最小化，而不是概率最大化。它通过显式跟踪信念状态和量化竞争性假设的认知不确定性，实现内在的可解释性。

Result: 在 LCR-1000 上的定制 DRN 模型比标准基线模型提高了 15.2% 的准确率。与 Mistral-7B 集成的混合系统将最具挑战性问题的准确率从 20% 提高到 80%。DRN 在 TruthfulQA 上的性能提高了 23.6%，展示了良好的零样本泛化能力。

Conclusion: DRN 是一种基础的、可验证的系统 2 推理组件，可用于构建更可信赖的 AI 系统。

Abstract: Large language models often fail at logical reasoning when semantic
heuristics conflict with decisive evidence - a phenomenon we term cognitive
traps. To address this fundamental limitation, we introduce the Deliberative
Reasoning Network (DRN), a novel paradigm that reframes logical reasoning from
probability maximization to uncertainty minimization. Instead of asking "Which
answer is most likely?", DRN asks "Which hypothesis has the most internally
consistent evidence?". DRN achieves intrinsic interpretability by explicitly
tracking belief states and quantifying epistemic uncertainty for competing
hypotheses through an iterative evidence synthesis process. We validate our
approach through two complementary architectures - a bespoke discriminative
model that embodies the core uncertainty minimization principle, and a
lightweight verification module that enhances existing generative LLMs.
Evaluated on LCR-1000, our new adversarial reasoning benchmark designed to
expose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over
standard baselines. When integrated as a parameter-efficient verifier with
Mistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most
challenging problems. Critically, DRN demonstrates strong zero-shot
generalization, improving TruthfulQA performance by 23.6% without additional
training, indicating that uncertainty-driven deliberation learns transferable
reasoning principles. We position DRN as a foundational, verifiable System 2
reasoning component for building more trustworthy AI systems.

</details>


### [455] [OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing](https://arxiv.org/abs/2508.04361)
*Fuqing Bie,Shiyu Huang,Xijia Tao,Zhiqin Fang,Leyi Pan,Junzhe Chen,Min Ren,Liuyu Xiang,Zhaofeng He*

Main category: cs.AI

TL;DR: OmniPlay 基准测试的出现，揭示了当前全模态模型在跨模态推理和动态环境中存在重大不足，它们在记忆任务上表现出色，但在需要复杂推理和策略规划时却显得脆弱，尤其是在处理多感官冲突时。研究强调，未来的发展方向应注重提升模态间的协同融合能力，而非仅仅依赖模型规模的扩展。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法无法充分测试通用基础模型（如 Gemini 和 GPT-4o）在动态、交互式世界中的智能，静态基准缺乏能动性，而交互式基准存在模态瓶颈，忽略了听觉和时间线索。因此，需要一个能够探测模型在完整感官谱系下的融合与推理能力的评估工具。

Method: 提出了一种名为 OmniPlay 的诊断基准测试，该测试包含五个游戏环境，旨在系统性地评估智能体模型在跨越全部感官频谱时的融合和推理能力，特别关注模态间的协同与冲突。在 OmniPlay 上对六种领先的全模态模型进行了评估。

Result: Omni-modal 模型在处理高保真记忆任务时表现出超人水平，但在需要鲁棒推理和战略规划的挑战中存在系统性失败。研究发现，这种脆弱性源于不稳定的融合机制，这会导致在模态冲突下性能急剧下降，并揭示了一个“少即是多”的悖论，即移除感官信息有时反而能提升性能。

Conclusion: 该研究表明，通用基础模型在动态、交互式世界中的智能评估仍存在不足。OmniPlay 基准测试揭示了当前模型在跨模态融合和推理方面存在系统性缺陷，尤其是在需要鲁棒推理和战略规划的挑战中。模型在模态冲突下表现出灾难性的性能下降，并存在“少即是多”的悖论。未来的研究应超越扩展，专注于解决协同融合问题，以实现更强大的通用人工智能。

Abstract: While generalist foundation models like Gemini and GPT-4o demonstrate
impressive multi-modal competence, existing evaluations fail to test their
intelligence in dynamic, interactive worlds. Static benchmarks lack agency,
while interactive benchmarks suffer from a severe modal bottleneck, typically
ignoring crucial auditory and temporal cues. To bridge this evaluation chasm,
we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,
but to probe the fusion and reasoning capabilities of agentic models across the
full sensory spectrum. Built on a core philosophy of modality interdependence,
OmniPlay comprises a suite of five game environments that systematically create
scenarios of both synergy and conflict, forcing agents to perform genuine
cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal
models reveals a critical dichotomy: they exhibit superhuman performance on
high-fidelity memory tasks but suffer from systemic failures in challenges
requiring robust reasoning and strategic planning. We demonstrate that this
fragility stems from brittle fusion mechanisms, which lead to catastrophic
performance degradation under modality conflict and uncover a counter-intuitive
"less is more" paradox, where removing sensory information can paradoxically
improve performance. Our findings suggest that the path toward robust AGI
requires a research focus beyond scaling to explicitly address synergistic
fusion. Our platform is available for anonymous review at
https://github.com/fuqingbie/omni-game-benchmark.

</details>


### [456] [Artificial Consciousness as Interface Representation](https://arxiv.org/abs/2508.04383)
*Robert Prentner*

Main category: cs.AI

TL;DR: 本研究提出了SLP测试框架，通过评估AI的接口表征来检验其是否具有类似意识的属性，并将主观体验定义为关系实体的功能接口。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）系统是否能够拥有意识是一个有争议的问题，因为主观体验的定义和操作化存在内在挑战。

Method: 本研究提出了三个评估标准——S（主观-语言）、L（潜在-涌现）和P（现象-结构），统称为SLP测试。这些测试用于评估人工智能系统是否实例化了接口表征，从而促进了类似意识的属性。

Result: SLP测试整体上将主观体验操作化，不是作为物理系统的内在属性，而是作为关系实体的功能接口。通过范畴论，将接口表征建模为关系基底（RS）和可观察行为之间的映射，类似于特定类型的抽象层。

Conclusion: AI是否具有意识应被视为一个有待商榷的问题，因为主观体验的定义和操作化都存在内在挑战。本篇论文旨在提出一个框架，将人工智能意识的问题重新构建为可检验的实证测试。

Abstract: Whether artificial intelligence (AI) systems can possess consciousness is a
contentious question because of the inherent challenges of defining and
operationalizing subjective experience. This paper proposes a framework to
reframe the question of artificial consciousness into empirically tractable
tests. We introduce three evaluative criteria - S (subjective-linguistic), L
(latent-emergent), and P (phenomenological-structural) - collectively termed
SLP-tests, which assess whether an AI system instantiates interface
representations that facilitate consciousness-like properties. Drawing on
category theory, we model interface representations as mappings between
relational substrates (RS) and observable behaviors, akin to specific types of
abstraction layers. The SLP-tests collectively operationalize subjective
experience not as an intrinsic property of physical systems but as a functional
interface to a relational entity.

</details>


### [457] [GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning](https://arxiv.org/abs/2508.04389)
*Weitai Kang,Bin Lei,Gaowen Liu,Caiwen Ding,Yan Yan*

Main category: cs.AI

TL;DR: GuirlVG是一种创新的强化学习方法，通过优化RFT策略和引入对抗KL因子，实现了比SFT方法更高的GUI-VG性能，同时大幅减少了对训练数据的需求。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在GUI领域的能力增强，对详尽的SFT后训练的必要性受到质疑。规则化强化微调（RFT）作为一种更有效的方法，其在GUI-VG上的最佳应用方式仍未被探索。

Method: GuirlVG是一种基于强化学习的GUI-VG方法，通过系统性的实证研究和新颖的稳定技术来优化规则化强化微调（RFT）。研究了RFT的核心组成部分，提出了对抗KL因子以稳定训练，并探索了RFT的训练配置以提高效率。

Result: GuirlVG在GUI-VG任务上取得了显著成果，其性能优于SFT方法，并且在样本效率上表现突出。

Conclusion: GuirlVG在GUI-VG任务上表现出色，仅使用5.2K训练样本，在ScreenSpot上提升7.7%，在ScreenSpotPro上提升17.2%，在ScreenSpotV2上达到91.9%的准确率，超越了使用超过10M训练样本的SFT方法。

Abstract: Graphical user interface visual grounding (GUI-VG), a core capability for GUI
agents, has primarily relied on supervised fine-tuning (SFT) of multimodal
large language models (MLLMs), which demands extensive data curation and
significant training costs. However, as MLLMs continue to advance and even
cover GUI domains during pretraining, the necessity of exhaustive SFT
post-training becomes increasingly questionable. Meanwhile, recent successes of
rule-based reinforcement fine-tuning (RFT) suggest a more efficient
alternative. Despite this promise, the optimal manner of applying RFT for
GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a
reinforcement learning-based GUI-VG method built on a systematic empirical
study and a novel stabilization technique. We find that naive application of
RFT underperforms the SFT baseline, motivating a deeper exploration. First, we
decompose RFT into its core components and analyze the optimal formulation of
each. Second, we propose a novel Adversarial KL Factor that dynamically
stabilizes training to mitigate reward over-optimization. Third, we further
explore the training configurations of RFT to enhance effectiveness. Extensive
experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT
methods trained on over 10M samples, achieving a 7.7% improvement on
ScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on
ScreenSpotV2.

</details>


### [458] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: D2Snap 是一种新型 DOM 降采样算法，让 LLM 能够有效利用 DOM 快照处理网页任务，性能媲美甚至超越 GUI 快照。


<details>
  <summary>Details</summary>
Motivation: 当前基于 LLM 的网页智能体主要依赖 GUI 快照（截图），但 LLM 的视觉能力相比代码解释能力仍有差距。而 DOM 快照（类似于 HTML 结构）是一个有潜力的替代方案，但由于其庞大的输入 token 数量，难以在现有网页智能体中可靠实现。

Method: 提出了一种名为 D2Snap 的 DOM 降采样算法，并基于 GPT-4o 模型进行评估。该算法旨在解决大型语言模型在处理网页任务时，由于输入限制（如 token 数量）而无法有效利用 DOM 快照的问题。

Result: D2Snap 降采样的 DOM 快照在 Online-Mind2Web 数据集上的成功率为 67%，与 GUI 快照基线（65%）相当，且 token 数量在同一数量级（1e3）。在 token 数量略高于基线但仍在模型上下文窗口内的配置下，D2Snap 的性能比基线提高了 8%。

Conclusion: D2Snap 是一种开创性的 DOM 降采样算法，能够使大型语言模型 (LLM) 在处理网页任务时，利用 DOM 快照作为输入，其性能与传统的 GUI 快照相当，甚至在特定配置下有所超越。研究表明，DOM 的内在层级结构对于 LLM 理解用户界面至关重要。

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [459] [\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices](https://arxiv.org/abs/2508.04428)
*Si Chen,Izzy Molnar,Ting Hua,Peiyu Li,Le Huy Khiem,G. Alex Ambrose,Jim Lang,Ronald Metoyer,Nitesh V. Chawla*

Main category: cs.AI

TL;DR: SimInstruct通过模拟新手和专家的教学对话，解决了高质量教学数据稀缺的问题，生成的对话具有较高的教学质量和认知深度，并且在实际应用中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 高质量的教学对话对于开发支持教学、学习和决策制定的AI系统至关重要。然而，由于隐私和求助过程中的脆弱性，这类数据非常稀缺。因此，需要一种能够规模化收集教学对话数据的方法。

Method: SimInstruct通过模拟新手教师（利用LLMs，并调整其教学挑战和个人特质）和真人专家（提供多轮反馈、推理和教学支持）之间的互动来收集教学对话数据。

Result: SimInstruct生成的对话在教学相关性和认知深度上与真实导师记录相当。实验表明，个人特质（如外向和内向）会影响专家的互动方式。此外，使用SimInstruct增强的数据集微调的LLaMA模型在教学质量上超过了GPT-4o，并且指出了GPT-4o在教学对话中的一些局限性。

Conclusion: SimInstruct是一个用于收集教学对话数据的工具，通过模拟新手教师和真人专家之间的互动，生成了大量高质量的教学对话。该工具生成的对话在教学相关性和认知深度上与真实对话相当，并且在实际应用中，专家反馈也表明该工具能够提升他们的教学反思能力和洞察力。此外，使用SimInstruct生成的Augmented数据集微调的LLaMA模型在教学质量上优于GPT-4o，这表明该方法在提升AI教学能力方面具有巨大潜力。分析还指出了GPT-4o在教学对话中的一些不足，例如缺乏有效的反思性提问、过度使用通用表扬、语气居高临下以及信息过载等问题。

Abstract: High-quality, multi-turn instructional dialogues between novices and experts
are essential for developing AI systems that support teaching, learning, and
decision-making. These dialogues often involve scaffolding -- the process by
which an expert supports a novice's thinking through questions, feedback, and
step-by-step guidance. However, such data are scarce due to privacy concerns in
recording and the vulnerability inherent in help-seeking. We present
SimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding
dialogues. Using teaching development coaching as an example domain,
SimInstruct simulates novice instructors via LLMs, varying their teaching
challenges and LLM's persona traits, while human experts provide multi-turn
feedback, reasoning, and instructional support. This design enables the
creation of realistic, pedagogically rich dialogues without requiring real
novice participants. Our results reveal that persona traits, such as
extroversion and introversion, meaningfully influence how experts engage.
Compared to real mentoring recordings, SimInstruct dialogues demonstrate
comparable pedagogical relevance and cognitive depth. Experts also reported the
process as engaging and reflective, improving both data quality and their own
professional insight. We further fine-tuned a LLaMA model to be an expert model
using the augmented dataset, which outperformed GPT-4o in instructional
quality. Our analysis highlights GPT-4o's limitations in weak reflective
questioning, overuse of generic praise, a condescending tone, and a tendency to
overwhelm novices with excessive suggestions.

</details>


### [460] [From "Aha Moments" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control](https://arxiv.org/abs/2508.04460)
*Rui Ha,Chaozhuo Li,Rui Pu,Sen Su*

Main category: cs.AI

TL;DR: MERA通过解耦推理与控制、引入元认知能力，解决了大型推理模型“过度思考”的问题，提升了推理效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型（LRMs）在复杂推理中出现的“过度思考”问题，即模型在得出可靠结论后仍继续生成冗余推理内容，导致计算成本增加和延迟升高，限制了LRMs的实际部署。

Method: 提出元认知推理框架（MERA），该框架将思考过程解耦为推理和控制两个独立组件，并结合了基于接管的数据构建机制、结构化推理-控制分离（通过监督微调）以及控制-片段策略优化（CSPO）。

Result: 在多个推理基准测试中，使用MERA训练的模型在推理效率和准确性方面均得到提升。

Conclusion: MERA通过明确解耦推理和控制组件，并采用基于接管的数据构建机制、结构化推理-控制分离以及控制-片段策略优化（CSPO），有效解决了大型推理模型（LRMs）中的“过度思考”问题，提高了推理效率和准确性。

Abstract: Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex
reasoning by spontaneously exhibiting cognitive behaviors such as step-by-step
reasoning, reflection, and backtracking, commonly referred to as "Aha Moments".
However, such emergent behaviors remain unregulated and uncontrolled, often
resulting in overthinking, where the model continues generating redundant
reasoning content even after reaching reliable conclusions. This leads to
excessive computational costs and increased latency, limiting the practical
deployment of LRMs. The root cause lies in the absence of intrinsic regulatory
mechanisms, as current models are unable to monitor and adaptively manage their
reasoning process to determine when to continue, backtrack, or terminate. To
address this issue, we propose the Meta-cognitive Reasoning Framework (MERA),
which explicitly decouples the thinking process into distinct reasoning and
control components, thereby enabling the independent optimization of control
strategies. Specifically, MERA incorporates a takeover-based data construction
mechanism that identifies critical decision points during reasoning and
delegates the creation of control signals to auxiliary LLMs, thereby enabling
the construction of high-quality reasoning-control data. Additionally, a
structured reasoning-control separation is implemented via supervised
fine-tuning, enabling the model to generate explicit traces and acquire initial
meta-cognitive control capabilities. Finally, MERA employs Control-Segment
Policy Optimization (CSPO), which combines segment-wise Group Relative Policy
Optimization (GRPO) with a control-masking mechanism to optimize control
behavior learning while minimizing interference from irrelevant content.
Experiments on various reasoning benchmarks demonstrate that models trained
with MERA enhance both reasoning efficiency and accuracy.

</details>


### [461] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [462] [Argumentative Debates for Transparent Bias Detection [Technical Report]](https://arxiv.org/abs/2508.04511)
*Hamed Ayoobi,Nico Potyka,Anna Rapberger,Francesca Toni*

Main category: cs.AI

TL;DR: 本论文提出了一种新颖的可解释的偏差检测方法，该方法利用基于个体及其邻居受保护特征值的辩论来检测偏差。该方法基于形式和计算论证，并且在性能、可解释性和可解释性方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统在社会中的使用日益广泛，解决可能来自数据或由模型学习的潜在偏差对于防止对特定群体的系统性不利至关重要。已经提出了几种（不）公平的概念，以及相应的检测和缓解不公平的算法方法，但是，除了极少数例外，这些方法往往忽略了透明度。相反，可解释性和可解释性是算法公平性的核心要求，甚至比其他算法解决方案更为重要，因为公平性是以人为中心的。

Method: 我们贡献了一种新颖的、可解释的、可解释的偏差检测方法，该方法依赖于围绕个体偏差的存在的辩论，这些辩论基于受保护特征的个体及其邻居的值。我们的方法建立在形式和计算论证技术的基础上，其中辩论源于对邻居之间以及邻居内部的偏差的争论。

Result: 我们方法的正式、定量和定性评估突出了其相对于基线方法的性能优势，以及其可解释性和可解释性。

Conclusion: 该方法通过对个体进行辩论来检测偏差，这些辩论基于受保护特征的个体及其邻居的值。该方法基于形式和计算论证技术，其中辩论源于对邻居之间以及邻居内部的偏差的争论。我们提供了对我们方法的正式、定量和定性评估，突出了其相对于基线方法的性能优势，以及其可解释性和可解释性。

Abstract: As the use of AI systems in society grows, addressing potential biases that
emerge from data or are learned by models is essential to prevent systematic
disadvantages against specific groups. Several notions of (un)fairness have
been proposed in the literature, alongside corresponding algorithmic methods
for detecting and mitigating unfairness, but, with very few exceptions, these
tend to ignore transparency. Instead, interpretability and explainability are
core requirements for algorithmic fairness, even more so than for other
algorithmic solutions, given the human-oriented nature of fairness. In this
paper, we contribute a novel interpretable, explainable method for bias
detection relying on debates about the presence of bias against individuals,
based on the values of protected features for the individuals and others in
their neighbourhoods. Our method builds upon techniques from formal and
computational argumentation, whereby debates result from arguing about biases
within and across neighbourhoods. We provide formal, quantitative, and
qualitative evaluations of our method, highlighting its strengths in
performance against baselines, as well as its interpretability and
explainability.

</details>


### [463] [SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset](https://arxiv.org/abs/2508.04563)
*Mei Jiang,Houping Yue,Bingdong Li,Hao Hao,Ying Qian,Bo Jiang,Aimin Zhou*

Main category: cs.AI

TL;DR: SID是首个用于评估LLM在跨学科苏格拉底对话中高阶指导能力的基准，结果显示LLM在此方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 为了评估LLM在多轮、跨学科苏格拉底对话中进行高阶指导的能力，解决了现有评估基准的缺乏问题。

Method: 提出SID基准，包含大规模对话数据集（10,000个对话轮次，48个STEM项目）、新颖的教学特征注释模式和评估指标（如X-SRG）。

Result: 通过基线实验发现，即使是先进的LLM在引导学生实现知识整合和迁移方面也存在困难。

Conclusion: 现有的LLM在引导学生进行知识整合和迁移方面仍有很大提升空间，需要开发更具教学意识的LLM。

Abstract: Fostering students' abilities for knowledge integration and transfer in
complex problem-solving scenarios is a core objective of modern education, and
interdisciplinary STEM is a key pathway to achieve this, yet it requires expert
guidance that is difficult to scale. While LLMs offer potential in this regard,
their true capability for guided instruction remains unclear due to the lack of
an effective evaluation benchmark. To address this, we introduce SID, the first
benchmark designed to systematically evaluate the higher-order guidance
capabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our
contributions include a large-scale dataset of 10,000 dialogue turns across 48
complex STEM projects, a novel annotation schema for capturing deep pedagogical
features, and a new suite of evaluation metrics (e.g., X-SRG). Baseline
experiments confirm that even state-of-the-art LLMs struggle to execute
effective guided dialogues that lead students to achieve knowledge integration
and transfer. This highlights the critical value of our benchmark in driving
the development of more pedagogically-aware LLMs.

</details>


### [464] [ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges](https://arxiv.org/abs/2508.04576)
*Yue Zhou,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: ConfProBench 是首个系统评估 MPJ 置信度可靠性的基准，通过对抗性扰动和新指标揭示了现有 MPJ 的置信度局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的 MPJ 评估基准主要关注步骤正确性分类和推理过程搜索，忽略了 MPJ 在步骤层级上产生的置信度分数是否可靠这一关键问题。

Method: 提出了 ConfProBench 基准，包含同义词替换、句法转换和图像扰动三种对抗性扰动，并引入了置信度鲁棒性得分（CRS）、置信度敏感性得分（CSS）和置信度校准得分（CCS）三个评估指标。

Result: ConfProBench 评估了 14 个最先进的 MLLM，实验结果揭示了当前 MPJ 在置信度性能方面存在的局限性，并为未来的研究提供了有竞争力的基线。

Conclusion: 现有的大多数多模态大语言模型（MPJ）在评估推理步骤的置信度方面存在局限性，ConfProBench 为评估和改进 MPJ 的置信度性能提供了基准和指标。

Abstract: Reasoning is a critical capability of multimodal large language models
(MLLMs) for solving complex multimodal tasks, and judging the correctness of
reasoning steps is crucial for improving this capability. Recently, MLLM-based
process judges (MPJs) have been widely used to assess the correctness of
reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important
for identifying their limitations and guiding future improvements. However,
existing benchmarks for MPJs mainly focus on tasks such as step correctness
classification and reasoning process search, while overlooking a key aspect:
whether the confidence scores produced by MPJs at the step level are reliable.
To address this gap, we propose ConfProBench, the first comprehensive benchmark
designed to systematically evaluate the reliability of step-level confidence
scores generated by MPJs. Our benchmark constructs three types of adversarially
perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and
Image Perturbation, to test the robustness of MPJ confidence under
perturbations. In addition, we introduce three novel evaluation metrics:
Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and
Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and
calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including
both proprietary and open-source models. Experiments reveal limitations in
current MPJs' confidence performance and offer competitive baselines to support
future research.

</details>


### [465] [LLM Collaboration With Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.04652)
*Shuo Liu,Zeyu Liang,Xueguang Lyu,Christopher Amato*

Main category: cs.AI

TL;DR: 本文提出了一种名为MAGRPO的多智能体强化学习算法，用于优化大型语言模型（LLM）之间的协作。与现有方法不同，MAGRPO将LLM协作视为一个多智能体问题，无需复杂的个体奖励设计。实验证明，该方法能显著提升LLM在写作和编码等任务中的协作效率和响应质量，并为未来在LLM中应用其他多智能体强化学习方法开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM微调框架依赖于个体奖励，这需要为每个智能体设计复杂的奖励机制来鼓励协作，增加了复杂性。

Method: 本文将LLM协作建模为合作多智能体强化学习问题，并提出了一种名为多智能体群组相对策略优化（MAGRPO）的多智能体、多轮算法来解决该问题。

Result: 实验结果表明，通过MAGRPO微调的多智能体系统在LLM写作和编码协作任务中，能够通过有效的协作高效地生成高质量的响应。

Conclusion: 所提出的MAGRPO算法通过将LLM协作建模为合作多智能体强化学习问题，能够有效地实现智能体间的协作，从而高效地生成高质量的响应。

Abstract: A large amount of work has been done in Multi-Agent Systems (MAS) for
modeling and solving problems with multiple interacting agents. However, most
LLMs are pretrained independently and not specifically optimized for
coordination. Existing LLM fine-tuning frameworks rely on individual rewards,
which require complex reward designs for each agent to encourage collaboration.
To address these challenges, we model LLM collaboration as a cooperative
Multi-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,
multi-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),
to solve it, building on current RL approaches for LLMs as well as MARL
techniques. Our experiments on LLM writing and coding collaboration demonstrate
that fine-tuning MAS with MAGRPO enables agents to generate high-quality
responses efficiently through effective cooperation. Our approach opens the
door to using other MARL methods for LLMs and highlights the associated
challenges.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [466] [Forgive and Forget? An Industry 5.0 Approach to Trust-Fatigue Co-regulation in Human-Cobot Order Picking](https://arxiv.org/abs/2508.03765)
*Soumyadeep Dhar*

Main category: cs.MA

TL;DR: 本研究通过斯塔克尔伯格博弈模型研究了信任和疲劳在人机协作订单拣选中的作用，发现改进的信任模型可提高生产力近100%，信任修复协议可缩短恢复时间75%。


<details>
  <summary>Details</summary>
Motivation: 研究物流5.0中人与机器人共生关系的关键因素，特别是信任和疲劳对人机协作订单拣选的影响。

Method: 提出并模拟了一个动态的、领导者-追随者斯塔克尔伯格博弈模型，该模型在效用函数中明确考虑了人类疲劳和信任。

Result: 通过基于代理的模拟，证明了改进的信任模型可以产生“信任协同循环”，将生产力提高近100%，并且配备主动信任修复协议的协作机器人可以将严重故障后的信任恢复时间缩短75%以上。

Conclusion: 本研究提出了一个动态的、领导者-追随者斯塔克尔伯格博弈模型，该模型考虑了人类疲劳和信任在人机协作订单拣选中的作用，并为设计智能协作机器人行为提供了框架，以实现以人为本、可持续和有弹性的工业5.0目标。

Abstract: This paper investigates the critical role of trust and fatigue in human-cobot
collaborative order picking, framing the challenge within the scope of
Logistics 5.0 -- the implementation of human-robot symbiosis in smart
logistics. We propose a dynamic, leader-follower Stackelberg game to model this
interaction, where utility functions explicitly account for human fatigue and
trust. Through agent-based simulations, we demonstrate that while a naive model
leads to a "trust death spiral," a refined trust model creates a "trust synergy
cycle," increasing productivity by nearly 100 percent. Finally, we show that a
cobot equipped with a proactive Trust-Repair Protocol can overcome system
brittleness, reducing trust recovery time after a severe failure by over 75
percent compared to a non-adaptive model. Our findings provide a framework for
designing intelligent cobot behaviors that fulfill the Industry 5.0 pillars of
human-centricity, sustainability, and resilience.

</details>


### [467] [When Agents Break Down in Multiagent Path Finding](https://arxiv.org/abs/2508.03777)
*Foivos Fioravantes,Dušan Knop,Nikolaos Melissinos,Michal Opler*

Main category: cs.MA

TL;DR: 在多智能体路径查找（MAPF）中，当智能体出现故障时，重新计算整个计划通常是不可行的。我们提出了一个动态调度适应框架，允许智能体在本地协调和调整路径，而不是完全重新规划。我们的协议将故障的影响限制在可接受的范围内，并考虑了智能体的计算能力限制。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体寻路（MAPF）中部分智能体可能因故障导致延误的问题，并提出一种动态调度适应框架以应对此挑战。

Method: 提出了一种无需完全重新规划的动态调度适应框架，并开发了允许智能体在本地协调和动态调整路径的协议。

Result: 证明了主要的通信协议可以将 k 次故障导致的进度延迟限制在 k 个额外回合内，并提出了一个将计算转移到网络节点上的次要协议，以确保鲁棒性。

Conclusion: 所提出的框架和协议为在出现故障时进行弹性多智能体导航提供了一种实用且可扩展的方法。

Abstract: In Multiagent Path Finding (MAPF), the goal is to compute efficient,
collision-free paths for multiple agents navigating a network from their
sources to targets, minimizing the schedule's makespan-the total time until all
agents reach their destinations. We introduce a new variant that formally
models scenarios where some agents may experience delays due to malfunctions,
posing significant challenges for maintaining optimal schedules.
  Recomputing an entirely new schedule from scratch after each malfunction is
often computationally infeasible. To address this, we propose a framework for
dynamic schedule adaptation that does not rely on full replanning. Instead, we
develop protocols enabling agents to locally coordinate and adjust their paths
on the fly. We prove that following our primary communication protocol, the
increase in makespan after k malfunctions is bounded by k additional turns,
effectively limiting the impact of malfunctions on overall efficiency.
Moreover, recognizing that agents may have limited computational capabilities,
we also present a secondary protocol that shifts the necessary computations
onto the network's nodes, ensuring robustness without requiring enhanced agent
processing power. Our results demonstrate that these protocols provide a
practical, scalable approach to resilient multiagent navigation in the face of
agent failures.

</details>


### [468] [DRAMA: A Dynamic and Robust Allocation-based Multi-Agent System for Changing Environments](https://arxiv.org/abs/2508.04332)
*Naibo Wang,Yifan Zhang,Sai Liu,Xinkui Zhao,Guanjie Cheng,Yueshen Xu*

Main category: cs.MA

TL;DR: DRAMA is a dynamic multi-agent system that improves adaptability and robustness in changing environments through a modular design, resource abstraction, and flexible task allocation, ensuring continuous operation.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems (MAS) often rely on static architectures and rigid task allocation, limiting their adaptability to dynamic, uncertain, and variable real-world environments. This inflexibility hinders robust and efficient cooperation in changing conditions.

Method: DRAMA employs a modular architecture separating control and worker planes. Agents and tasks are abstracted as resource objects. Task allocation uses an affinity-based, loosely coupled mechanism. The control plane handles real-time monitoring and centralized planning for flexible task reassignment, while the worker plane consists of autonomous agents capable of local reasoning, task execution, collaboration, and taking over unfinished tasks.

Result: DRAMA facilitates resilient collaboration in rapidly changing environments by enabling flexible and efficient task reassignment as agents join, depart, or become unavailable, ensuring continuous and robust task execution.

Conclusion: The proposed DRAMA system, with its modular architecture, resource object abstraction, and affinity-based allocation mechanism, demonstrates enhanced adaptability and robustness in dynamic multi-agent systems, ensuring continuous and efficient task execution even with agent and environmental changes.

Abstract: Multi-agent systems (MAS) have demonstrated significant effectiveness in
addressing complex problems through coordinated collaboration among
heterogeneous agents. However, real-world environments and task specifications
are inherently dynamic, characterized by frequent changes, uncertainty, and
variability. Despite this, most existing MAS frameworks rely on static
architectures with fixed agent capabilities and rigid task allocation
strategies, which greatly limits their adaptability to evolving conditions.
This inflexibility poses substantial challenges for sustaining robust and
efficient multi-agent cooperation in dynamic and unpredictable scenarios. To
address these limitations, we propose DRAMA: a Dynamic and Robust
Allocation-based Multi-Agent System designed to facilitate resilient
collaboration in rapidly changing environments. DRAMA features a modular
architecture with a clear separation between the control plane and the worker
plane. Both agents and tasks are abstracted as resource objects with
well-defined lifecycles, while task allocation is achieved via an
affinity-based, loosely coupled mechanism. The control plane enables real-time
monitoring and centralized planning, allowing flexible and efficient task
reassignment as agents join, depart, or become unavailable, thereby ensuring
continuous and robust task execution. The worker plane comprises a cluster of
autonomous agents, each with local reasoning, task execution, the ability to
collaborate, and the capability to take over unfinished tasks from other agents
when needed.

</details>


### [469] [Position-Based Flocking for Robust Alignment](https://arxiv.org/abs/2508.04378)
*Hossein B. Jond*

Main category: cs.MA

TL;DR: 该研究提出了一种基于位置的群体模型，通过调整对齐和分离来实现稳定的集体运动，并在模拟中表现出比基于速度的模型更强的对齐和更紧凑的队形。


<details>
  <summary>Details</summary>
Motivation: 为了实现稳定的集体运动，需要一种能平衡内聚-分离和对齐的群体模型。

Method: 提出了一种基于位置的群体模型，通过用初始位置和当前位置来近似速度差，并引入阈值权重以确保持续对齐，从而修改了基于速度的方法。

Result: 与基于速度的模型相比，该模型在2D模拟中产生了更强的对齐和更稳定、更紧凑的队形，并且对齐指标和分离距离突出了所提出模型在实现鲁棒群体行为方面的有效性。

Conclusion: 该模型使用位置确保了鲁棒的对齐，并可应用于机器人和集体动力学领域。

Abstract: This paper presents a position-based flocking model for interacting agents,
balancing cohesion-separation and alignment to achieve stable collective
motion. The model modifies a velocity-based approach by approximating velocity
differences using initial and current positions, introducing a threshold weight
to ensure sustained alignment. Simulations with 50 agents in 2D demonstrate
that the position-based model produces stronger alignment and more rigid and
compact formations compared to the velocity-based model. The alignment metric
and separation distances highlight the efficacy of the proposed model in
achieving robust flocking behavior. The model's use of positions ensures robust
alignment, with applications in robotics and collective dynamics.

</details>
