<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.GT](#cs.GT) [Total: 5]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.GR](#cs.GR) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 14]
- [eess.SY](#eess.SY) [Total: 17]
- [quant-ph](#quant-ph) [Total: 51]
- [eess.SP](#eess.SP) [Total: 20]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.RO](#cs.RO) [Total: 41]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 146]
- [cs.SI](#cs.SI) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 14]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个使用视频一致性模型（VCM）进行高分辨率视频恢复的即插即用逆求解器，解决了现有方法在处理视频时出现的时域不一致问题，并在重建保真度和计算效率方面取得了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的扩散模型（LDMs）在应用于视频恢复时，由于逐帧处理，存在时域不一致的问题，无法捕捉细微的时域依赖性。

Method: 提出LVTINO，一种利用视频一致性模型（VCM）的逆求解器。该方法通过条件机制，无需自动微分，即可实现高效的视频恢复，并确保测量一致性和帧间的平滑过渡。

Result: LVTINO在各种视频逆问题上进行了广泛的实验，与逐帧应用图像LDMs的现有最先进方法相比，在感知质量上有显著提升，并在重建保真度和计算效率方面树立了新的标杆。

Conclusion: LVTINO成功地将VCM的优势应用于高分辨率视频恢复，解决了时域一致性问题，并在性能和效率上超越了现有方法，为视频恢复领域设定了新的标准。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于风格编码器和风格投影层的方法，通过从参考图像中提取细粒度风格表示，并将其注入预训练的生成模型，以实现文本引导的风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成模型在实际应用中存在难以精确描述和控制细粒度风格的问题，同时参考图像的风格信息难以与文本条件对齐。

Method: 提出了一种三阶段训练的风格提取-生成方法，包括风格编码器和风格投影层，用于对齐风格表示和文本表示。

Result: 实现了细粒度的、由文本提示引导的风格化图像生成，并构建了包含图像、风格标签和文本描述的Style30k-captions数据集来训练模型。

Conclusion: 通过提取和注入风格表示，有效利用了预训练生成模型的强大能力，实现了细粒度的风格化图像生成。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一个轻量级的基于U-Net的PDE基础模型，通过自回归预训练，在多种PDE问题上达到了最先进的泛化能力，同时参数量和微调数据需求更少。


<details>
  <summary>Details</summary>
Motivation: 现有的PDE基础模型主要基于大型复杂Transformer架构，计算和参数开销高，而SPUS旨在利用轻量级的残差U-Net架构，提供一种更紧凑、高效的统一神经算子来解决多种偏微分方程（PDE）。

Method: SPUS采用轻量级的残差U-Net架构，并结合一种简单的自回归预训练策略，该策略能有效学习潜在物理规律，并模仿数值求解器的行为。模型首先在多样化的流体动力学PDE上进行预训练，然后在6个具有挑战性的、未见过的下游PDE问题上进行评估。

Result: 实验结果表明，SPUS在下游任务上实现了最先进的泛化能力，其参数量显著减少，且仅需极少量的微调数据。

Conclusion: SPUS展示了其作为一种高效参数基础模型在解决多样化PDE系统方面的潜力，证明了轻量级U-Net架构和自回归预训练策略在PDE求解领域的有效性。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过优化身份多样性来解决多人物图像生成中的身份重复和计数错误问题，在DiverseHumans测试集中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在生成包含多个人物的图像时，存在面部重复、身份混淆和人数计数错误等问题。

Method: DisCo使用基于强化学习的框架，通过组相对策略优化（GRPO）来微调流匹配模型。该框架包含一个组合奖励，用于惩罚面部相似性、鼓励身份多样性、确保人数准确性，并通过人类偏好分数维持视觉保真度。同时，采用单阶段课程学习来稳定训练。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的独特面部准确率和接近完美的全局身份分布，优于现有的开源和专有方法，同时保持了具有竞争力的感知质量。

Conclusion: DisCo是解决生成模型中长期存在的身份问题的可扩展、无需额外注释的解决方案，为组合多人物生成设定了新的基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉地理定位方法，通过对齐查询图像的视觉表征与学习到的地理表征来实现。


<details>
  <summary>Details</summary>
Motivation: 视觉地理定位（根据图像内容确定其地理位置）仍是一个活跃的研究领域，尽管已有显著进展。

Method: 提出了一种新颖的地理表征，将世界建模为地理嵌入的层次结构，并结合了查询图像的外观特征和语义分割图，形成鲁棒的视觉表征。

Result: 在五个基准数据集的22项指标上取得了历来最佳（SOTA）的改进，超过了现有的最先进方法和大型视觉语言模型（LVLMs）。

Conclusion: 实验证明，所提出的地理表征与视觉表征的结合是实现性能提升的主要驱动力。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 本研究提出了XMAS，一种针对大型视觉语言模型（LVLM）的数据高效指令微调方法。通过聚类分析交叉注意力矩阵的奇异值轨迹，XMAS能在不损害模型性能的情况下，大幅减少训练数据量并加速训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在大型视觉语言模型（LVLM）上效果不佳，无法超越随机选择的性能。本研究旨在为LVLM的指令微调提供一种更有效的数据选择方法。

Method: XMAS通过分析微调小型代理LVLM时得到的交叉注意力矩阵的奇异值轨迹，对训练样本进行聚类。通过从这些聚类中进行平衡采样，有效去除冗余数据。

Result: XMAS能够在减少LLaVA-665k数据集50%和Vision-Flan数据集85%的数据量后，仍能使LLaVA-1.5-7B在10个下游基准测试中保持原有性能，并将训练速度提高1.2倍。在LLaVA-665k数据集上，数据缩减量比现有最佳基线高30%。

Conclusion: XMAS是第一个能有效进行LVLM数据高效指令微调的原则性方法，通过利用交叉注意力矩阵的相似性来识别和去除冗余数据，从而在保证性能的同时显著提高训练效率。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种结合了连续流匹配和离散监督的向量量化图像生成方法，提高了训练效率并生成了有竞争力的FID分数。


<details>
  <summary>Details</summary>
Motivation: 旨在将连续流匹配的几何感知能力与离散监督的优势相结合，用于向量量化图像生成，同时实现不确定性量化和受控生成。

Method: 通过在连续嵌入空间中学习码本索引的后验概率并计算速度场，将变分流匹配方法应用于向量量化的潜在空间。

Result: 在ImageNet-1k 256x256生成任务上，Purrception的训练速度比连续流匹配和离散流匹配基线更快，FID分数具有竞争力。

Conclusion: 变分流匹配可以有效地结合连续传输和离散监督，以提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种端到端的深度学习框架，利用条件扩散模型 (CDM) 和多任务学习，从无增强 CT (NCCT) 生成增强 CT (CECT) 图像，同时分割主动脉腔和血栓，以减少造影剂使用。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习方法在从无增强 CT (NCCT) 生成增强 CT (CECT) 图像时，通常采用多阶段流程，容易累积误差，并且未能有效利用共享的语义和解剖结构。有鉴于此，需要一个统一的框架来同时进行图像生成和分割。

Method: 提出了一种集成了条件扩散模型 (CDM) 和多任务学习的统一深度学习框架。该框架能够端到端地联合优化图像合成和解剖分割任务。它不需要初始预测，跨任务共享编码器和解码器参数，并采用半监督学习策略处理缺失分割标签的临床数据。

Result: 在 264 名患者的队列中进行了评估。图像合成方面，PSNR 达到 25.61 dB，优于单任务 CDM 的 23.80 dB。解剖分割方面，主动脉腔的 Dice 分数从 0.87 提高到 0.89，血栓的 Dice 分数从 0.48 提高到 0.53。临床测量方面，主动脉腔直径的平均绝对误差 (MAE) 从 5.78 mm 降至 4.19 mm，血栓面积误差从 41.45% 降至 33.85%。

Conclusion: 所提出的统一深度学习框架在腹主动脉瘤 (AAA) 的 CECT 图像合成和分割任务上均优于现有方法，能够减少造影剂的使用，并提高临床测量的准确性。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 该研究提出了一种用于多模态内容分析的框架，通过结合预训练模型将视频转换为可查询的知识图谱，以实现高效的原型设计和持续学习。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态内容分析方法在处理复杂数据（如视频）时面临计算成本高、工程量大以及融合预训练模型困难等挑战。

Method: 提出一个框架，能够高效地原型化多模态内容分析流程。该框架首先将视频转换为时间半结构化数据，然后进一步转化为帧级别索引的、可查询的知识图谱，支持持续学习和动态知识整合。

Result: 开发了一个框架，将预训练模型与视频数据相结合，生成可查询的知识图谱，并支持持续学习。

Conclusion: 所提出的框架能够高效地处理多模态内容分析，将视频数据转化为动态的、可查询的知识图谱，为持续学习和新知识的整合提供了支持。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过将网站的潜在功能反向工程为可重用工具，解决了现有Web代理的脆弱性问题，并在基准测试中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Web代理在处理动态布局和长期任务时过于脆弱，依赖于逐步的UI交互和繁重的LLM推理。

Method: WALT框架将网站的潜在功能（如搜索、筛选、排序、发布、评论、创建、编辑、删除等）反向工程为可重用的工具，代理可以直接调用这些工具，而不是进行底层的点击和输入推理。

Result: 在VisualWebArena和WebArena基准测试中，WALT取得了更高的成功率、更少的步骤和更少的LLM依赖推理。

Conclusion: WALT提出了一种更鲁棒、更通用的浏览器自动化范式，将计算负担从脆弱的逐步推理转移到可靠的工具调用。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 本文提出了一种用于病理图像的半监督分割框架，通过强制拓扑一致性来保留有意义的语义结构，并提出了一种结合空间重叠和全局结构对齐的新颖匹配策略来解决跨预测的拓扑特征匹配问题。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，尤其是在细胞密集分布的病理图像分析中，从无标签数据中捕获有意义的语义结构是一个关键挑战。

Method: 利用随机丢弃和时间训练快照获得的多重扰动预测，强制执行拓扑一致性，并引入了一种结合空间重叠和全局结构对齐的新颖匹配策略来匹配拓扑特征。

Result: 实验证明，该方法能有效减少拓扑错误，提高分割的鲁棒性和准确性。

Conclusion: 所提出的框架通过强制拓扑一致性和新颖的匹配策略，能够从无标签数据中捕获有意义的语义结构，从而在病理图像分割中实现更准确和可靠的结果。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Direct Preference Optimization (DPO) is effective for aligning text-to-image (T2I) diffusion models, but typically uses pairwise preferences. This paper introduces Diffusion-LPO, a framework that utilizes listwise preferences for diffusion models, drawing on the Plackett-Luce model to optimize based on ranked image lists. Diffusion-LPO enforces consistency across the entire ranking and outperforms pairwise DPO baselines in visual quality and preference alignment across various T2I tasks.


<details>
  <summary>Details</summary>
Motivation: Human feedback on image preferences often contains implicit ranked information, which is more precise than pairwise comparisons. Existing applications of DPO to diffusion models have primarily relied on pairwise preferences, leaving the precise optimization of listwise preferences largely unaddressed.

Method: Diffusion-LPO aggregates user feedback into a ranked list of images for a given caption and derives a listwise extension of the DPO objective under the Plackett-Luce model. This framework enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives.

Result: Empirical results demonstrate the effectiveness of Diffusion-LPO across various tasks including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines in terms of visual quality and preference alignment.

Conclusion: Diffusion-LPO provides a simple and effective framework for listwise preference optimization in diffusion models, outperforming existing pairwise methods by leveraging richer, ranked human feedback.

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯粹的自回归统一多模态大语言模型，通过混合Transformer架构增强了预训练的视觉理解模型，使其具备了生成能力，在一个单一的下一代token预测框架内实现了图像理解和生成。


<details>
  <summary>Details</summary>
Motivation: 构建统一的多模态大语言模型（MLLMs）以支持图像理解和生成仍然是一个挑战。混合方法会破坏自回归范式，而纯粹的自回归方法在语义对齐和像素级保真度之间存在权衡。

Method: Bridge采用纯粹的自回归方法，利用混合Transformer架构将预训练的视觉理解模型与生成能力相结合。该模型引入了一种语义到像素的离散表示，结合了紧凑的语义token和细粒度的像素token，以提高视觉生成保真度。

Result: Bridge在理解和生成基准测试中取得了具有竞争力或更优越的结果，并且所需的训练数据和训练时间少于之前的统一MLLMs。其语义到像素的离散表示仅增加了7.9%的序列长度，同时实现了强大的语言对齐和精确的视觉细节描述。

Conclusion: Bridge通过一种新颖的架构和表示方法，成功地在一个纯粹的自回归框架内实现了统一的多模态大语言模型，并在各项任务中表现出色，同时提高了效率。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 本研究提出了一种结合卷积神经网络（CNN）和贝叶斯深度学习的混合模型，用于在数据量有限的情况下对口腔癌进行分类，提高了模型的可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习模型在口腔癌诊断方面存在过度自信、可靠性不足以及需要大量训练数据等问题，尤其在医疗资源匮乏的地区，这些问题更为突出。因此，需要开发一种能在小数据集上有效工作的模型。

Method: 本研究采用一种混合模型，将卷积神经网络（CNN）与贝叶斯深度学习相结合，并利用变分推断来量化模型的不确定性。该模型使用智能手机拍摄的照片进行训练和评估。

Result: 在与训练数据分布相似的测试集上，该模型达到了94%的准确率，与传统CNN相当。然而，在分布不同的真实世界图像数据集上，该模型表现出更强的泛化能力，准确率达到88%，远高于传统CNN的72.94%，并且在数据集较小的情况下效果更佳。置信度分析表明，模型对正确分类的样本具有低不确定性（高置信度），对错误分类的样本具有高不确定性（低置信度）。

Conclusion: 贝叶斯推断在数据稀疏的环境中能够有效地提高模型在早期口腔癌诊断中的可靠性和泛化能力，特别是在处理小数据集和真实世界图像数据时，相比传统CNN具有明显优势。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: CADTrans是一种用于无源域自适应（SFDA）的新方法，通过构建域一致性的不变特征表示来解决现有方法的局限性。它使用一个助手域模块来获得多样化的表示，并采用多项一致性策略获得不变特征表示，以区分易难样本。最后，通过条件多核最大均值差异（CMK-MMD）策略对齐难易样本。


<details>
  <summary>Details</summary>
Motivation: 现有源域无关域自适应（SFDA）方法在获取确定性不变特征方面存在困难，并且容易受到难样本和域偏差的影响。

Method: 提出了一种名为CADTrans的一致性助手域Transformer模型。该模型包含一个助手域模块，用于从中间聚合的全局注意力中获取多样化的表示。基于助手域和目标域，通过多种一致性策略获得不变特征表示，以区分易难样本。最后，利用条件多核最大均值差异（CMK-MMD）策略区分同一类别和不同类别的样本，以对齐难易样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准测试中，CADTrans取得了显著的性能提升。

Conclusion: CADTrans通过构建不变特征表示和引入CMK-MMD策略，有效地解决了SFDA中的挑战，并在多个基准测试中证明了其优越性。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLM）的应用为盲人和低视力（BLV）用户提供了丰富的视觉解释，但通常输出冗长且不具针对性。本研究提出了一种利用历史用户问题来引导MLLM生成更具上下文相关性的描述的系统，以提高信息获取效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM应用在为盲人和低视力用户提供视觉解释时，存在描述冗长、不具上下文相关性，导致用户需要筛选无关信息，从而降低信息获取效率的问题。

Method: 本研究提出了一种新系统，通过识别图像与VizWiz-LF数据集中历史用户问题的相似性，并利用这些历史问题引导MLLM生成更相关的描述。

Result: 通过对92个描述进行评估，结果显示，上下文感知描述在76.1%的情况下（70/92）能够预测并回答用户的问题，并在54.4%的比较中（50/92）获得偏好。

Conclusion: 本研究提出的上下文感知方法能够显著提高MLLM为盲人和低视力用户生成描述的相关性和用户满意度。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think是一个包含25万张ImageNet21k图像的多模态推理数据集，用于训练和评估具有显式推理能力和逐步推理过程的视觉语言模型（VLMs）。


<details>
  <summary>Details</summary>
Motivation: 为了促进具有显式推理能力的视觉语言模型（VLMs）的发展，并增进对多模态推理机制的理解。

Method: 使用两个先进的VLMs（GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506）在25万张ImageNet21k图像上生成结构化的思考-答案序列，为每张图像提供两对思考-答案对。

Result: 创建了一个包含25万张图像、其逐步推理过程和最终答案的数据集，旨在训练和评估多模态推理模型。

Conclusion: 通过发布ImageNet-Think数据集和评估基准，旨在推动多模态VLMs在推理和思考能力方面的发展，并促进相关研究。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 本论文提出了一种名为“非线性零空间投影”（NPN）的新型正则化方法，用于解决成像逆问题。


<details>
  <summary>Details</summary>
Motivation: 现有的成像逆问题解决方法通常忽略了感知算子零空间（null-space）的任务特定结构。

Method: NPN方法不强制执行图像域的结构约束，而是通过神经网络将解决方案推广到感知矩阵零空间的低维投影中。这种方法具有可解释性和灵活性，可以适应各种逆问题，并与现有重建框架和图像域先验兼容。

Result: NPN在压缩感知、去模糊、超分辨率、计算机断层扫描和磁共振成像等多种成像逆问题中，通过即插即用方法、展开网络、深度图像先验和扩散模型，一致地提高了重建保真度。

Conclusion: NPN为解决成像逆问题提供了一种新颖且有效的方法，通过关注零空间的结构来设计特定于感知矩阵的先验，从而提高重建质量。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 该模块结合了CGR和CBM，将DNA序列转化为可操作的医学决策，具有高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 将原始DNA序列转化为适合医学自动化和机器人系统的可操作、可解释的决策。

Method: 结合混沌图表示（CGR）和概念瓶颈模型（CBM），并结合概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准，以提高可靠性。最后，添加一个成本感知推荐层，将预测输出转化为决策策略。

Result: 在HIV亚型分类方面取得了最先进的性能，概念预测保真度更高，成本效益更好，并且提供了可与生物先验知识进行验证的可解释证据。

Conclusion: 该工作为基因组医学中的机器人和临床自动化奠定了可靠的基础，弥合了可解释基因组建模与自动化决策之间的差距。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1通过结合RLVR和GRPO，增强了VLA模型的推理能力和执行准确性，并在各项评估中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏显式的分步推理能力，且通常依赖于弱奖励设计的监督微调，导致其在考虑环境约束和几何关系方面存在不足。

Method: 提出VLA-R1模型，集成RLVR和GRPO，采用基于RLVR的后训练策略，包含区域对齐、轨迹一致性和输出格式化等可验证奖励，并引入了包含思维链监督和环境/轨迹标注的VLA-CoT-13K数据集。

Result: VLA-R1在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，其泛化能力和现实世界性能均优于现有VLA方法。

Conclusion: VLA-R1通过增强推理能力和执行准确性，显著提升了VLA模型的性能和泛化能力，为未来在具身AI领域的应用奠定了基础。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种宏观摄影联合去模糊和三维重建方法，利用可微渲染技术，仅用少量多视图图像即可实现高质量去模糊和高保真三维模型重建。


<details>
  <summary>Details</summary>
Motivation: 宏观摄影存在散焦模糊问题，影响图像清晰度和三维重建质量，而传统方法需要大量数据和标注，且缺乏多视图三维重建方法。

Method: 提出联合优化三维模型和散焦模糊核的方法，利用可微渲染进行自监督优化。

Result: 实验证明，该方法能从少量多视图图像中实现高质量图像去模糊和高保真三维外观恢复。

Conclusion: 提出的联合去模糊和三维重建方法能够有效解决宏观摄影中的散焦模糊问题，并实现高质量的三维重建。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动化双列直插式封装(DIP)缺陷检测系统，使用ConSinGAN生成数据集，并比较了YOLOv3、v4、v7和v9模型，其中YOLOv7与ConSinGAN结合效果最佳，准确率达95.50%。


<details>
  <summary>Details</summary>
Motivation: 传统的工业组件缺陷检测耗时耗力，给质检人员带来负担，难以管理产品质量。

Method: 利用数字相机光学和基于深度学习(DL)的模型，提出自动化缺陷检测系统。检查表面缺陷和引脚缺陷两类常见缺陷。使用ConSinGAN生成数据集，并对比了YOLOv3、v4、v7和v9模型。开发了SCADA系统并描述了传感器架构。

Result: 提出的YOLOv7与ConSinGAN结合的模型在准确率（95.50%）、检测时间（285毫秒）方面优于其他YOLO版本，并且远优于基于阈值的方法。

Conclusion: 所提出的自动化缺陷检测系统可以轻松地应用于多种缺陷类型或数据不足的情况，提高了检测效率和准确性。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [24] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个新颖的单步扩散模型，用于高保真运动模糊去除，通过将模糊去除重构为类似扩散的过程，并使用一致性模型对齐所有时间步长，实现了准确的一步模糊去除，并集成了Kernel ControlNet以进行模糊核估计和自适应时间步长预测，在全参考指标上取得了优于先前基于扩散的方法的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于CNN和Transformer的方法在图像运动模糊去除方面取得了进展，但大规模预训练的扩散模型在图像恢复任务中显示出更强的生成能力。然而，扩散模型存在推理时间长和保真度受损的问题，限制了其在实际应用中的潜力。

Method: FideDiff将运动模糊去除重新构建为类似扩散的过程，其中每个时间步长代表一个逐步模糊的图像。通过训练一个一致性模型来将所有时间步长对齐到同一张清晰图像，并提供具有匹配模糊轨迹的训练数据，模型学习时间一致性以实现准确的一步模糊去除。此外，通过集成Kernel ControlNet进行模糊核估计和引入自适应时间步长预测来进一步增强模型性能。

Result: FideDiff在全参考指标上实现了优越的性能，超过了以前的基于扩散的方法，并达到了其他最先进模型的性能水平。

Conclusion: FideDiff为将预训练扩散模型应用于高保真图像恢复任务提供了一个新的方向，并为在实际工业应用中进一步推进扩散模型建立了强大的基准。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [25] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本研究发布了大规模金文数据集，并提出了一种两阶段的检测-识别流水线（LadderMoE），以解决金文识别中的视觉退化、跨域变异和长尾分布等挑战。


<details>
  <summary>Details</summary>
Motivation: 金文（BI）是中国早期文字的重要载体，对考古和历史研究至关重要。然而，自动识别金文面临视觉退化、跨域（照片、拓片、描摹）变异以及字符分布极端长尾化等问题。

Method: 提出一个两阶段检测-识别流水线：首先定位铭文，然后转录单个字符。该流水线集成了LadderMoE，该模块在预训练的CLIP编码器上添加了Ladder风格的MoE适配器，以实现专家动态特化和增强鲁棒性，从而处理异构域和稀有类别。

Result: 在单字符和整页识别任务上进行了广泛实验，结果表明所提出的方法显著优于最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上均取得了卓越的准确性。

Conclusion: 本研究创建了一个大型金文数据集，并提出了一种先进的识别方法，为金文识别和下游考古分析奠定了坚实的基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [26] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA是一种无需重新训练骨干网络即可进行领域自适应的方法，通过添加领域特定的视觉提示层来调整图像风格，并在Office-31数据集上取得了优于现有方法的性能，同时显著减少了可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督域适应（UDA）方法在处理新的源-目标对时需要重新训练骨干网络，导致训练参数和存储空间随新数据对线性增长，并且无法复用已有的训练好的骨干网络参数。受启发于骨干网络存在的纹理偏差，提出利用特定于域的纹理偏差来实现域适应。

Method: VirDA不微调整个骨干网络，而是在骨干网络前添加一个特定于域的视觉重构层。该层生成视觉提示，作为附加的纹理偏差作用于输入图像，将其“风格”调整为目标域。为了优化这些视觉重构层，使用了多个目标函数，在应用了域适应的视觉提示后，优化了域内和域间的分布差异。此过程无需修改骨干网络参数，允许跨不同域复用相同的骨干网络。

Result: 在Office-31数据集上，VirDA取得了92.8%的平均准确率，仅使用了1.5M可训练参数。VirDA的准确率比现有的参数高效UDA基线PDA高出1.6%，而其参数量仅为PDA的46%。与完全微调骨干网络的方法相比，VirDA在参数量仅占其1.7%和2.8%的情况下，分别比CDTrans和FixBi高出0.2%和1.4%。与最强的现有方法（PMTrans和TVT）相比，VirDA使用的参数量约为其1.7%，准确率仅分别下降2.2%和1.1%。

Conclusion: VirDA通过在骨干网络前添加可学习的视觉重构层，实现了一种高效且参数量优化的无监督域适应方法。该方法避免了对骨干网络的修改，实现了参数重用，并在标准数据集上取得了具有竞争力的性能。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [27] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了一种名为离散面部编码（DFE）的无监督、数据驱动的方法，用于从3D网格序列中学习紧凑且可解释的面部表情词典，作为FACS的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的面部表情编码系统（如FACS）覆盖范围有限且需要手动标注，因此需要一种更有效的方法。

Method: 使用3D变形模型（3DMM）提取与身份无关的面部表情特征，然后使用残差向量量化变分自编码器（RVQ-VAE）将这些特征编码为离散的编码序列，每个编码代表一个特定的面部变形模式。

Result: DFE比FACS和其他面部编码方法能捕捉到更精确的面部行为。使用基于DFE编码的词袋模型，在压力检测、个性预测和抑郁检测任务上，其表现优于基于FACS和Masked Autoencoders等模型的流水线。

Conclusion: DFE是一种比FACS更具扩展性和有效性的面部表情表示方法，适用于心理学和情感计算领域。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [28] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM是一种用于解决单目视觉可变形SLAM映射挑战的新型非刚性结构运动(NRSfM)方法，在共形变形下进行点重建，消除了现有方法的限制，并能精确计算局部共形尺度和深度。


<details>
  <summary>Details</summary>
Motivation: 解决单目视觉可变形SLAM中的映射挑战，并克服现有NRSfM方法在局部平面曲面或线性变形假设以及无法恢复共形尺度方面的局限性。

Method: 提出了一种名为Con-NRSfM的新方法，该方法在共形变形（包括等距变形）下进行点重建。它使用基于图的框架优化2D图像变形，消除了对局部平面曲面或线性变形的严格假设，并能够精确计算局部共形尺度。此外，它将深度和共形尺度的约束分开，并采用并行可分离迭代优化策略来处理问题的敏感性。最后，结合使用基于编码器-解码器网络的自监督学习框架来生成具有纹理的密集3D点云。

Result: 该方法在合成和真实数据集的仿真和实验结果表明，其在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM在共形变形下实现了精确的3D重建，克服了现有方法的局限性，并在精度和鲁棒性方面取得了优于现有方法的性能。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [29] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个基于视频扩散模型的统一框架，通过将鲁棒重建解耦为恢复和重建两个子任务，解决了多视图不一致图像的重建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不一致的多视图图像进行3D重建时，需要密集观测来优化模型参数，这限制了其应用。本研究旨在解决这个问题。

Method: UniVerse首先将不一致的图像转换为初始视频，然后使用专门设计的视频扩散模型将这些视频恢复为一致的图像，最后从恢复的图像中重建3D场景。

Result: 与逐个视图进行退化建模的方法相比，UniVerse的扩散模型能从大规模数据中学习通用的场景先验，适用于各种图像不一致的情况。实验表明，该方法在合成和真实世界的数据集上都表现出强大的泛化能力和优越的性能，并且能够控制重建的3D场景的风格。

Conclusion: UniVerse通过解耦和引入视频扩散模型，成功实现了鲁棒的三维场景重建，并展示了其泛化能力和风格控制能力。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [30] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的端到端框架，将模板匹配重新构建为联合定位和几何回归问题，以解决传统方法效率低下和深度学习方法无法显式建模几何姿态的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模板匹配方法在处理复杂背景下的目标位置和几何状态（旋转、缩放）估计时效率低下，而深度学习方法通常只估计相似度分数，无法显式建模几何姿态，不适用于实际部署。

Method: 提出了一种轻量级的端到端框架，将模板匹配重新构建为联合定位和几何回归问题，输出中心坐标、旋转角度以及独立的水平和垂直缩放比例。该框架采用模板感知动态卷积模块（TDCM）在推理时动态注入模板特征，并结合深度可分离卷积和像素重排以提高效率。引入了基于旋转-剪切的数据增强策略和结构感知伪标签来实现无几何标注的训练。此外，还设计了一个轻量级的细化模块来通过局部优化提高角度和缩放精度。

Result: 该模型（3.07M参数）在复杂变换下实现了高精度和14ms的推理速度，并在小模板和多目标场景下表现出强大的鲁棒性。

Conclusion: 所提出的轻量级框架能够高效、精确地估计目标在复杂背景下的位置和几何状态，适用于实时工业应用部署。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [31] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 该研究提出了首个自适应像素推理框架，通过查询动态确定像素级操作，解决了视觉语言模型在细粒度视觉理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在处理需要精确理解和操作细粒度视觉元素（如高分辨率图像）的任务时存在困难，主要原因是图像编码过程中的信息丢失或对关键区域的注意力不足。虽然像素级信息有助于提高模型性能，但过度使用会导致效率低下和无关细节的干扰。

Method: 研究提出了一个自适应像素推理框架。首先，通过面向操作的监督微调来建立基础的文本推理和视觉操作能力。然后，设计了一个新颖的、由回滚引导的强化学习框架，该框架依赖于模型自身响应的反馈，使VLM能够根据查询的难度确定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，提出的模型在提高准确性的同时，显著减少了不必要的视觉操作。具体而言，在HR-Bench 4K上实现了73.4%的准确率，而工具使用率仅为20.1%，与先前方法相比，准确率有所提高，工具使用量减少了66.5%。

Conclusion: 该研究提出的自适应像素推理框架能够根据查询动态调整像素级操作，有效解决了现有VLM在细粒度视觉理解方面的挑战，并在准确性和效率之间取得了良好的平衡。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [32] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ASRS（增强敏感性风险评分）的框架，用于识别胸部X光片（CXR）解释中易出错的病例。ASRS通过应用临床上合理的旋转来衡量RAD-DINO编码器的嵌入位移，从而量化模型对这些变化的敏感性。结果表明，高度敏感的病例在整体准确率（AUROC）和置信度高的情况下，召回率却显著降低，这揭示了现有评价指标的不足。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸部X光片（CXR）解释方面表现出色，但其公平性和可靠性仍是担忧。模型在不同患者亚组间的准确率可能不均，导致总体指标无法反映的隐藏性错误。现有的错误检测方法（如置信度校准或分布外检测）难以处理细微的分布内错误，而基于图像和表示层面的一致性方法在医学影像领域尚未得到充分探索。

Method: ASRS框架应用了临床上合理的旋转（±15°/±30°），并使用RAD-DINO编码器来测量嵌入位移。通过计算样本对这些图像增强的敏感性得分，将样本分为稳定性四分位数。

Result: 研究发现，ASRS框架能够识别出高敏感性病例，这些病例在AUROC和置信度评分很高的情况下，召回率却显著降低（-0.2至-0.3）。这表明ASRS能够发现现有指标（如AUROC和置信度）无法捕捉到的模型错误。

Conclusion: ASRS提供了一种无需标签即可进行选择性预测和指导临床医生复查的方法，有望提高医学人工智能的公平性和安全性。通过关注模型对临床上合理变换的敏感性，ASRS能够识别出可能导致隐藏失败的病例，从而增强模型的可靠性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [33] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，可以生成具有丰富风格细节和强时间连贯性的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 视频风格化是内容创作中的一个关键问题，但逐帧应用图像风格化会损害时间连贯性并降低风格丰富度，而训练专用视频风格化模型则需要配对的视频数据且计算成本高昂。

Method: FreeViS框架将多个风格化参考集成到一个预训练的图像到视频（I2V）模型中，通过高频补偿来约束内容布局和运动，并利用基于流动的运动线索来保留低显著性区域的风格纹理。

Result: FreeViS在风格化保真度和时间连贯性方面表现优于最近的基线方法，并获得了强烈的用户偏好。

Conclusion: FreeViS提供了一个实用且经济的解决方案，可实现高质量、时间连贯的视频风格化。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [34] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench是一个针对医学图像质量评估（IQA）的基准，使用多模态大语言模型（MLLMs）进行基于语言的评估，解决了现有基于标量分数的度量方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像质量评估方法主要依赖于标量分数，未能捕捉到专家评估中类似人类的推理过程。因此，需要一种能够反映这种推理过程的评估方法。

Method: 提出MedQ-Bench基准，包含两个任务：MedQ-Perception（评估低级感知能力）和MedQ-Reasoning（评估无参考和比较推理能力）。该基准涵盖五种成像模态和四十多种质量属性，包含2600个感知查询和708个推理评估。提出一种多维度评估协议来评估模型输出，并进行人类-AI对齐验证。

Result: 在14个最先进的MLLMs的评估中，模型表现出初步但 W不稳定的感知和推理能力，其准确性不足以满足临床可靠使用的要求。

Conclusion: 现有的MLLMs在医学图像质量评估方面仍有不足，需要针对性优化。MedQ-Bench旨在促进对MLLMs在医学图像质量评估中潜力的进一步探索。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [35] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一个单次前向传播网络，可以仅根据输入的RGB图像预测场景中所有实例的完整遮挡和深度排序。


<details>
  <summary>Details</summary>
Motivation: 现有的实例级几何理解方法在受控环境中仍然是一个挑战，并且需要昂贵的输入格式（类别标签、二值分割掩码）和推理成本（二次前向传播）。

Method: InstaFormer网络通过物体查询和语义上代表相同物体但携带互补信息的潜在掩码描述符之间的交互来实现整体排序预测。

Result: InstaFormer能够仅根据输入的RGB图像，在单次前向传播中预测场景中所有实例的完整遮挡和深度排序。

Conclusion: InstaFormer有效解决了现有方法的局限性，并且其代码和模型是开源的。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [36] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过金字塔位置编码（PPE）和强化学习（RL）实现高效、高质量的艺术图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和Transformer的模型在处理复杂风格和高分辨率输入时效率低下，需要改进以实现高效的风格迁移。

Method: 提出PyramidStyler框架，结合了金字塔位置编码（PPE）以捕捉多尺度信息并降低计算量，并引入强化学习（RL）以动态优化风格化过程并加速收敛。

Result: 在COCO和WikiArt数据集上训练后，PyramidStyler在4000个epoch后，内容损失降低了62.6%（至2.07），风格损失降低了57.4%（至0.86），推理时间为1.39秒。结合RL后，内容损失为2.03，风格损失为0.75，推理时间为1.40秒，性能进一步提升。

Conclusion: PyramidStyler实现了实时、高质量的艺术渲染，在媒体和设计领域具有广泛的应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [37] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS通过深度感知划分、基于可见高斯球的优化和轻量级技术，实现了大规模3D高斯泼溅场景的高效训练和重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法难以扩展到大型无边界场景，存在负载不均衡和粗粒度到细粒度流水线效率低的问题。

Method: LoBE-GS引入了一种深度感知划分方法，一种基于可见高斯球的优化策略来平衡计算负载，以及可见性裁剪和选择性稠密化技术以降低训练成本。

Result: 在大型城市和户外数据集上的评估显示，LoBE-GS的端到端训练时间比现有最先进方法快2倍，同时保持了重建质量，并实现了现有方法无法处理的场景的可扩展性。

Conclusion: LoBE-GS成功地解决了大规模3D高斯泼溅场景的训练效率和可扩展性问题，为处理大型和无边界场景提供了新的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [38] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: MemoryPack and Direct Forcing enhance long-form video generation by addressing long-range dependencies and error accumulation.


<details>
  <summary>Details</summary>
Motivation: Long-form video generation faces challenges in capturing long-range dependencies and preventing error accumulation in autoregressive decoding.

Method: MemoryPack uses textual and image information for dynamic context modeling to handle short- and long-term dependencies, while Direct Forcing is a single-step strategy to reduce error accumulation during inference.

Result: The proposed methods improve context consistency and reliability in long-form video generation, making autoregressive video models more practical.

Conclusion: MemoryPack and Direct Forcing effectively improve the quality and usability of long-form video generation.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [39] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本研究提出了一种新的方法来解决 3D 对象检测中的置信度校准问题，通过引入两个辅助正则化损失项来优化模型对主导和次要类别预测的置信度分布，并在 CenterPoint 和 PillarNet 模型上取得了最佳的校准效果。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，精确的目标检测和不确定性估计对于确保系统的自我认知和安全运行至关重要。然而，现有的 3D 对象检测器在分类任务中的置信度校准方面存在不足。

Method: 研究人员提出了一种新的置信度校准方法，通过引入两个辅助正则化损失项来优化模型对分类任务中所有类别的完整预测置信度分布。这两个损失项分别侧重于校准主导预测或完整的预测向量。此外，研究人员还评估了一系列后处理和训练时方法，并将所提出的损失项与等渗回归相结合。

Result: 在 CenterPoint、PillarNet 和 DSVT-Pillar 这三种模型上的实验表明，将所提出的、用于校准完整类别预测的损失项与等渗回归相结合，能够实现 CenterPoint 和 PillarNet 模型在主导和次要类别预测方面的最佳校准效果。然而，研究也发现 DSVT-Pillar 模型无法通过相同的方法同时校准主导和次要类别预测。

Conclusion: 本研究成功地提出了一种有效的置信度校准方法，用于 3D 对象检测中的分类任务，并通过实验验证了其在 CenterPoint 和 PillarNet 模型上的有效性。研究结果表明，关注完整的预测置信度分布对于实现更好的校准至关重要。同时，研究也指出了不同模型在校准方面存在的挑战，为未来的研究提供了方向。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [40] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS框架利用预训练的扩散模型，通过三个新颖的模块（DGRPN、MSFRN、SFAN）解决了现有行人搜索方法中的不足，提高了检测和重识别的性能，并在CUHK-SYSU和PRW数据集上达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有行人搜索方法主要使用ImageNet预训练骨干网络，这可能无法充分捕捉行人搜索所需的复杂空间上下文和细粒度身份线索。此外，它们依赖于共享骨干特征进行检测和重识别，导致特征次优。

Method: 提出DiffPS框架，利用预训练的扩散模型，通过三个模块解决优化冲突：1. 扩散引导区域建议网络（DGRPN）增强定位；2. 多尺度频率细化网络（MSFRN）减少形状偏差；3. 语义自适应特征聚合网络（SFAN）利用文本对齐的扩散特征。

Result: DiffPS在CUHK-SYSU和PRW数据集上取得了新的SOTA（state-of-the-art）性能。

Conclusion: DiffPS框架通过利用扩散模型先验知识和三个专门设计的模块，有效解决了行人搜索中的检测和重识别的优化冲突，并在公开数据集上取得了显著的性能提升。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [41] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU网络通过结合流匹配和深度展开框架，解决了高光谱成像重建中的硬件限制和数据恢复难题，并在模拟和真实数据上取得了优于现有方法的重建质量。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）虽然能提供丰富的空间-光谱信息，但由于硬件限制和从压缩测量中重建三维数据的困难，获取成本高昂。现有的压缩感知系统（如CASSI）虽然提高了效率，但在重建过程中仍面临严重的降质和光谱细节损失问题。

Method: 提出了一种名为FMU（Flow-Matching-guided Unfolding）的网络，该网络将流匹配（flow matching）的生成先验嵌入到深度展开（deep unfolding）框架中，以解决高光谱成像重建问题。此外，引入了一个均值速度损失（mean velocity loss）来强制流的全局一致性，以增强学习到的动态并提高重建的鲁棒性和准确性。

Result: 通过在模拟和真实数据集上的大量实验表明，FMU在重建质量上显著优于现有的方法。

Conclusion: FMU网络成功地将流匹配的生成能力与深度展开框架的优化可解释性相结合，有效解决了高光谱成像重建中的挑战，并在重建质量上取得了显著的提升。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 该研究提出了一种名为FoundAD的少样本异常检测方法，利用大规模预训练的视觉编码器来学习正常图像的分布，并通过分析学习到的嵌入差异来检测异常，该方法能够支持多类别检测且参数量少。


<details>
  <summary>Details</summary>
Motivation: 现有少样本异常检测方法在区分正常与异常特征方面存在挑战，尤其是在类别无关的场景下，由于样本数量有限。

Method: FoundAD通过学习一个非线性投影算子到自然图像流形上，利用图像中异常数量与学习到的嵌入差异之间的相关性来进行异常检测，以识别图像中分布外的区域。

Result: 实验表明，FoundAD能够支持多类别检测，并且在参数量远少于现有方法的情况下取得了具有竞争力的性能。该方法在多种基础编码器（包括DINOv3）上都得到了验证。

Conclusion: FoundAD的提出不仅为少样本异常检测提供了新的视角，也推动了该领域的发展，并且展示了基础特征在异常检测任务中的潜力。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ViT在机器人领域的应用受限于二次注意力复杂度。本文提出ClustViT，通过聚类模块合并相似的token，并用Regenerator模块恢复细节，在保持分割精度的同时，显著降低了计算量和推理时间。


<details>
  <summary>Details</summary>
Motivation: ViT在实际机器人系统中的应用受二次注意力复杂度限制，且现有token合并方法不适用于稠密预测任务。

Method: 提出ClustViT模型，包含一个可训练的Cluster模块，通过伪聚类合并相似的token；以及一个Regenerator模块，用于恢复下游任务所需的细节。

Result: 在三个不同数据集上，ClustViT实现了高达2.18倍的GFLOPs减少和1.64倍的推理加速，同时保持了可比的分割精度。

Conclusion: ClustViT通过引入有效的token合并和细节恢复机制，解决了ViT在密集预测任务中的效率问题，并证明了其在实际应用中的潜力。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一个统一的范式，使MLLM能够直接生成文本和视觉输出，解决了现有方法在视觉任务中依赖间接表示的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉任务中依赖间接表示（如生成坐标文本），限制了性能，阻碍了分割等密集预测任务。PaDT旨在克服这些挑战，实现直接的文本和视觉输出生成。

Method: PaDT的核心是视觉参考标记（VRT），它源自查询图像的视觉块嵌入，并与LLM的文本标记无缝交织。一个轻量级的解码器将LLM的输出转化为检测、分割和接地预测。PaDT在每次前向传播中独立处理VRT，并动态扩展嵌入表。

Result: PaDT在四个视觉感知和理解任务中始终达到最先进的性能，甚至优于明显更大的MLLM模型。

Conclusion: PaDT是一个有效的统一范式，通过直接处理视觉信息和动态调整嵌入，显著提高了MLLM在各种视觉任务上的性能，并实现了最先进的结果。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本研究通过构建“信任金字塔”模型和“三角信任指数”（TTI），提出“TriAlignXA”可解释人工智能框架，以解决在线生鲜电商信任赤字问题，并验证了其在平衡生物特性、时效性和经济可行性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 在线生鲜电商面临信任赤字，源于无法提供直接的感官体验。现有分级标准存在局限性，无法量化处理生物特性、时效性和经济可行性之间的权衡。

Method: 构建“信任金字塔”模型，通过“双源验证”消费者信任；提出“三角信任指数”（TTI）量化评估“不可能三角”；设计“TriAlignXA”可解释人工智能框架，包含生物自适应引擎、时效优化引擎和经济优化引擎；引入“预映射机制”通过二维码传递质量信息。

Result: 实验证明，“TriAlignXA”框架在分级任务上准确率显著高于基线模型，有效平衡了“不可能三角”中的生物特性、时效性和经济可行性，并通过“预映射机制”实现了透明的质量信息传递。

Conclusion: 本研究为构建可信赖的在线农产品生态系统提供了理论和实践支持，从算法决策到消费者信任的关键路径得到了验证。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个4D高斯喷涂编辑框架，通过4D感知InstructPix2Pix模型、多视图网格模块和高斯选择机制来确保视图、时间和非编辑区域的一致性，并使用基于LLM的模块来理解用户意图和处理复杂指令。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯喷涂编辑方法在视图、时间和非编辑区域的一致性以及处理复杂文本指令方面存在挑战。

Method: 提出4DGS-Craft框架，包含4D感知InstructPix2Pix模型（结合4D VGGT几何特征）、多视图网格模块（迭代优化多视图输入并联合优化4D场景）和高斯选择机制（仅优化编辑区域的高斯点）。此外，设计了一个基于LLM的用户意图理解模块，通过指令模板定义原子编辑操作并利用LLM进行推理，将复杂指令分解为原子操作序列。

Result: 与现有方法相比，4DGS-Craft实现了更一致、更可控的4D场景编辑。

Conclusion: 4DGS-Craft框架成功解决了现有4D高斯喷涂编辑方法面临的一致性和复杂指令处理挑战，实现了更优的编辑效果。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP) 是一种像素级掩码机制，通过识别纯像素并将其排除在计算之外，来改进图像超分辨率（SR）的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习 SR 方法计算复杂，阻碍了实际应用。CAMixer 提出了一种内容感知混合器，但存在适应性差、掩码粗糙和空间不灵活等问题。

Method: PP 提出了一种像素级掩码机制，利用固定的颜色中心点对像素进行分类，从而实现细粒度的、空间灵活的掩码，并将纯像素排除在昂贵的计算之外。PP 被集成到 ATD-light 模型中，形成 PP-ATD-light。

Result: PP-ATD-light 在保持计算量相似的情况下，在重建质量和参数效率方面优于 CAMixer-ATD-light，实现了卓越的 SR 性能。

Conclusion: PP 是一种有效的像素级掩码机制，可以显著提高 SR 的效率和性能，并且比现有方法具有更好的适应性和灵活性。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 使用GPT-4o和SLSO框架自动生成颌骨囊肿影像学表现，提升了部分诊断的准确性，但仍需改进以处理大范围病变。


<details>
  <summary>Details</summary>
Motivation: 利用OpenAI GPT-4o的多模态能力，自动生成牙科全景X光片上的颌骨囊肿影像学表现。

Method: 构建了一个包含自纠错循环的结构化输出（SLSO）框架，包含图像输入、结构化数据生成、牙齿编号提取与一致性检查、迭代再生以及结果验证等10个步骤，并与传统的思维链（CoT）方法进行了比较。

Result: SLSO框架在牙齿编号、牙齿移位和根部吸收等方面的输出准确性有所提高，改进率分别为66.9%、33.3%和28.6%。在成功案例中，经过最多五次再生后可获得一致的结构化输出。该框架能强制执行阴性发现描述，抑制幻觉，并提高牙齿编号的准确性。

Conclusion: SLSO框架在自动生成颌骨囊肿影像学表现方面显示出潜力，尤其在提高准确性和结构化输出方面，但对于广泛跨越多个牙齿的病变识别能力有限，需要进一步优化以实现临床应用。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net是一个3D自编码器，仅使用激光雷达点云从真实交通环境中编码高效特征。它利用跳跃连接来提高性能，同时减少了编码器层数并简化了跳跃连接，从而在不影响性能的情况下提高了重建质量。该模型还展示了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 从真实的交通环境中使用激光雷达点云编码高效特征，同时避免使用过多的计算资源。

Method: 提出了一种名为LiLa-Net的3D自编码器架构，该架构利用跳跃连接，减少了编码器层数并简化了跳跃连接，以实现高效的特征编码和点云重建。

Result: LiLa-Net能够生成高效且具有代表性的潜在空间，从而能够准确地重建原始点云。该模型在信息传递的跳跃连接和潜在编码之间取得了有效的平衡，从而在不影响性能的情况下提高了重建质量。此外，该模型还成功地重建了与原始交通环境无关的对象，证明了其强大的泛化能力。

Conclusion: LiLa-Net在不使用大量资源的情况下，能够从激光雷达点云中有效地提取特征并重建3D点云，并且具有良好的泛化能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一种新颖的框架，用于从多视图图像进行语义感知的3D形状和纹理变形，它利用网格引导的3D高斯泼溅（3DGS）来实现高保真度的几何和外观建模，克服了以往方法对点云或预定义同胚映射的依赖。


<details>
  <summary>Details</summary>
Motivation: 先前的方法在处理无纹理数据时通常依赖点云或需要预定义的同胚映射，而本研究旨在克服这些限制，实现更通用的3D形状和纹理变形。

Method: 本方法利用网格引导的3D高斯泼溅（3DGS）进行高保真度的几何和外观建模。其核心是一个统一的变形策略，将3D高斯固定到重建的网格块上，以确保几何一致的变换，并通过拓扑感知约束保留纹理保真度。同时，利用网格拓扑作为几何先验，通过物理上可行的点轨迹来维持结构完整性，从而建立无监督的语义对应关系。

Result: 在提出的TexMorph基准测试中，GaussianMorphing在颜色一致性误差（ΔE）和EI方面分别降低了22.2%和26.2%，显著优于现有的2D/3D方法。

Conclusion: GaussianMorphing通过集成网格引导的3DGS、统一的变形策略和无监督的语义对应，能够保留变形过程中的局部细节和全局语义一致性，且无需标注数据。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose 算法通过仅使用旋转测量值并结合位置信息来解决姿态估计中的泛化问题，实现了零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有姿态估计方法在跨用户泛化方面表现不佳，因为位置测量值受用户体型影响较大。

Method: 将姿态估计视为逆问题，利用预训练的扩散模型，仅以旋转测量值为条件，并通过从测量位置推导出的似然项来指导模型的先验知识，从而提出 InPose 方法。

Result: InPose 方法能够对任何用户生成姿态序列，最好地解释稀疏的身体测量值。

Conclusion: InPose 方法在姿态估计任务中实现了零样本泛化，解决了现有方法泛化能力不足的挑战。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: Vision-Guided Diffusion Model (VGDM) uses a Vision Transformer with diffusion models for improved brain tumor detection and segmentation in MRI, outperforming traditional U-Net methods.


<details>
  <summary>Details</summary>
Motivation: Traditional convolutional networks like U-Net struggle with capturing long-range dependencies in complex brain tumor structures for MRI segmentation. Diffusion models show promise in image generation and segmentation refinement.

Method: VGDM embeds a Vision Transformer into the diffusion process for global contextual reasoning and iterative denoising, enhancing volumetric accuracy and boundary precision. This hybrid approach models spatial relationships across MRI volumes and refines voxel-level errors.

Result: Experimental results on MRI brain tumor datasets show consistent improvements in Dice similarity and Hausdorff distance compared to conventional U-Net baselines.

Conclusion: The proposed VGDM framework, integrating transformer-guided diffusion, offers improved robustness and scalability for brain tumor segmentation in neuro-oncology, advancing the state of the art.

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 本研究利用深度学习技术，从1925-1950年的法国历史地图中提取了全国范围内的城市足迹数据，填补了该时期量化城市扩张研究的空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全国性的数字化城市足迹数据，在1970年代之前的法国历史城市扩张的定量分析受到阻碍。

Method: 开发了一个可扩展的深度学习流程，采用双通道U-Net模型来处理历史地图的复杂性。第一通道生成初步地图以识别混淆区域，第二通道利用增强的数据和第一通道的输出进行精炼，以减少错误识别。

Result: 处理了覆盖法国大都会的941个高分辨率图块，最终生成的城市镶嵌图的总体准确率为73%，能够有效识别各种城市模式，同时克服了标签和轮廓线等常见伪影。

Conclusion: 本研究成功构建了首个开放获取的、全国范围内的城市足迹数据集，并公开发布了代码、训练数据集和结果，为未来的长期城市化动态研究提供了支持。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 点标记的视频目标分割在手术器械追踪方面具有竞争力，但在解剖目标方面表现不佳，需要改进点选择策略。


<details>
  <summary>Details</summary>
Motivation: 评估点标记视频目标分割（VOS）在腹腔镜胆囊切除术中的可靠性，并找出其失效模式。

Method: 比较点标记追踪和分割掩码初始化在三种手术目标（胆囊、抓钳、L型钩电刀）上的性能，并进行定性分析。

Result: 点标记追踪在解剖目标方面表现持续不佳，在腹腔镜胆囊切除术中，由于组织相似性和边界模糊，导致其追踪失败。

Conclusion: 点标记追踪在解剖目标方面表现不佳，在手术器械追踪方面具有竞争力，但需要改进点选择和放置策略以提高性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 本文提出了FFREEDG任务和FRIEREN框架，用于在客户端只有未标记数据的情况下，利用预训练的视觉基础模型（VFM）和跨模态知识进行联邦学习中的语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习（FL）方法在处理领域转移和客户端数据未标记的语义分割（SS）任务时面临挑战，并且未能充分利用现代视觉基础模型（VFM）。

Method: 提出FFREEDG任务：模型在服务器的标记源数据集上预训练，然后在客户端仅使用未标记数据进行训练。提出FRIEREN框架：利用VFM的知识，整合视觉和语言模态。采用基于CLIP的文本嵌入引导的视觉-语言解码器进行语义消歧，并使用弱到强的一致性学习策略在伪标签上进行鲁棒的本地训练。

Result: 在合成到真实和清晰到恶劣天气基准上进行了实验，证明该框架能有效解决新任务，并取得与现有领域泛化和自适应方法相媲美的性能。

Conclusion: FRIEREN框架为联邦学习中的未标记领域自适应语义分割提供了一个有效的解决方案，并在实验中取得了竞争力，为未来的研究奠定了基础。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个结构化的提示框架，通过利用以动作为中心的知识来提高冻结的视觉语言模型（VLMs）在视频异常检测（VAD）方面的准确性和可解释性。它通过将提示组织成语义连贯的组，并提出细粒度的引导性问题，使模型预测与判别性视觉线索对齐，从而在UCF-Crime和XD-Violence数据集上实现了最先进的性能，并提供了可解释的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有的提示方法通常过于抽象，忽略了定义监控视频中复杂异常的细粒度人-物交互或动作语义，因此需要一种能够从冻结的VLMs中提取更准确、更可解释推理的方法。

Method: ASK-Hint框架将提示组织成语义连贯的组（例如，暴力、财产犯罪、公共安全），并提出细粒度的引导性问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上，ASK-Hint持续提高了AUC，与先前的方法相比，在训练免费方法和微调方法方面均取得了最先进的性能。该框架还提供了通往异常的可解释推理轨迹，并在数据集和VLM骨干网络之间表现出强大的泛化能力。

Conclusion: ASK-Hint的实验结果突显了提示粒度的关键作用，并确立了ASK-Hint作为一种新的、可训练的、可解释的视频异常检测的通用解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify通过利用潜在的几何信息和学习到的亲和力网络，有效缓解了2D到3D特征迁移的权衡问题，实现了卓越的数据效率，仅使用约1.5%的训练数据即可达到或超过最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的2D视觉语言模型（VLMs）到3D语义分割的特征迁移方法存在固有权衡：直接投影导致预测结果嘈杂且分散，而强制几何一致性则需要昂贵的训练流程和大规模标注的3D数据。这源于主流的分割-匹配范式未能调和2D语义与3D几何结构。然而，几何线索并未在2D到3D的迁移过程中被消除，而是潜伏在嘈杂且经过视图聚合的特征中。

Method: GeoPurify提出了一种学生亲和力网络（Student Affinity Network），利用从3D自监督教师模型中提炼出的几何先验知识，来净化2D VLM生成的3D点特征。在推理阶段，采用几何引导池化（Geometry-Guided Pooling）模块进一步去除点云噪声，确保语义和结构的一致性。

Result: GeoPurify通过有效利用潜在的几何信息和学习到的亲和力网络，成功缓解了2D到3D特征迁移的权衡问题，实现了卓越的数据效率。在主要的3D基准测试中，GeoPurify在仅使用约1.5%的训练数据的情况下，达到了或超越了现有最先进的性能。

Conclusion: GeoPurify通过学生亲和力网络和几何引导池化模块，有效地利用了2D VLM特征中潜在的几何信息，克服了现有方法的局限性，在3D语义分割任务中取得了优越的性能和显著的数据效率。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 利用猪的耳静脉纹路进行生物识别，实现高精度、低成本的自动化识别。


<details>
  <summary>Details</summary>
Motivation: 现有猪识别方法（耳标、微芯片）存在成本高、精度低、不适用小农户等问题。

Method: 收集猪耳图像，利用计算机视觉技术提取耳静脉特征，并使用支持向量机（SVM）进行分类。

Result: SVM模型在混合品种猪的识别精度达到98.12%，平均处理时间为8.3秒。

Conclusion: 基于耳静脉的生物识别系统具有成本效益高、无创、易于部署的优点，能够帮助小农户实现精准化管理。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的多类别密度图估计框架，用于解决密集和遮挡场景下的物体计数问题，并在VisDrone和iSAID数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检测方法在密集和遮挡场景下计数效果不佳，需要密度图估计方法来解决此问题，特别是在多类别计数场景下。

Method: 提出一个多类别计数框架，使用Twins-v2-S预训练模型作为骨干网络，并设计了一个多类别计数头，结合了先进的尺度解码方法。此外，引入了一个基于分割的类别焦点模块，以减少训练过程中的类别间干扰。

Result: 在VisDrone和iSAID数据集上，与现有的多类别计数方法相比，本文提出的方法在平均绝对误差（MAE）方面分别降低了33%、43%和64%。与YOLOv11的比较也证明了在密集场景下采用特定计数方法的必要性。

Conclusion: 所提出的方法在多类别密度图估计方面表现出色，并且其区域损失设计为该技术开辟了新的应用领域，如生物多样性监测，为保护工作和生态研究提供了新的途径。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl通过优化交叉注意力图来控制视频生成中概念出现的时间，无需重新训练或额外监督，可实现精确的时间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏细粒度的时间控制，无法指定视觉元素在生成序列中出现的时间。

Method: TempoControl利用交叉注意力图，通过新颖的优化方法（包括相关性、能量和熵）来引导概念出现的时间。

Result: TempoControl能够精确控制时间，同时保证视频质量和多样性，并在各种视频生成应用中得到有效验证。

Conclusion: TempoControl为视频生成带来了精确的时间控制能力，解决了现有模型在这方面的不足。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: MLLMs 在细粒度视觉推理方面存在挑战，尤其是在交通地图等结构化场景中。本文提出了 ReasonMap-Plus 数据集和 RewardMap 框架，通过引入密集奖励和多阶段强化学习来解决稀疏奖励和优化不稳定的问题，显著提高了 MLLMs 的视觉理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在处理交通地图等结构化、信息丰富的场景时，空间推理能力存在明显不足，而标准的强化学习方法在这种任务中面临稀疏奖励和优化不稳定的问题。

Method: 1. 构建 ReasonMap-Plus 数据集，通过 VQA 任务引入密集奖励信号，以实现有效的冷启动训练。 2. 提出 RewardMap 框架，包含两个关键设计：(a) 难度感知奖励设计，引入细节奖励以解决稀疏奖励问题；(b) 多阶段强化学习方案，从简单感知任务引导至复杂推理任务，提供比传统 SFT 更有效的冷启动策略。

Result: RewardMap 框架的各个组成部分都能带来性能提升，组合使用效果最佳。与 SFT 相比，RewardMap 训练的模型在跨越空间推理、细粒度视觉推理以及超越交通地图的通用任务的 6 个基准测试中，平均提高了 3.47% 的性能。

Conclusion: 本文提出的 RewardMap 框架及其增强的数据集 ReasonMap-Plus 能够有效提升 MLLMs 在细粒度视觉推理和空间推理方面的能力，解决了现有方法在处理此类任务时的关键挑战。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow 是首个利用 FLUX 强大先验知识进行拖拽式图像编辑的框架，通过区域编辑范式、IP-Adapter 和 MLLMs 解决了现有方法的局限性，并在新基准 ReD Bench 上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的拖拽式图像编辑方法由于基础模型（如 Stable Diffusion）的先验知识不足，容易在目标区域产生失真。虽然新的 DiT 模型（如 SD3.5, FLUX）具有更强的生成先验能力，但尚未被有效应用于拖拽式编辑任务。

Method: DragFlow 提出了一种区域编辑范式，利用仿射变换实现更丰富、一致的特征监督，以克服直接将点式拖拽应用于 DiT 模型效果不佳的问题。此外，集成了 IP-Adapter 以增强对象一致性，并通过梯度掩码硬约束保持背景保真度，同时利用 MLLMs 解决任务歧义。

Result: DragFlow 在 DragBench-DR 和新提出的 ReD Bench 基准上都显著优于点式和区域式基线方法，在拖拽式图像编辑领域取得了新的最先进成果。

Conclusion: DragFlow 成功地利用了 FLUX 的强大先验知识，通过区域编辑、对象一致性增强和背景保真度保持等创新方法，有效解决了拖拽式图像编辑中的挑战，并在实验中证明了其优越性。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 通过将帧选择扩展到包含时间信息的短片段，并采用自适应分辨率策略来保持恒定的标记数，F2C 显著提高了长视频理解能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型（VLM）在处理长视频时面临“大海捞针”问题，因为大量的视觉标记会耗尽模型的上下文窗口。现有的帧选择方法会丢弃重要的时间动态信息，影响对运动和事件连续性的推理。

Method: 提出了一种名为 F2C 的新方法，该方法将选择从孤立的关键帧扩展到包含时间信息的关键片段（短的、时间上连贯的视频片段）。为了在固定计算预算内处理更多的标记，F2C 采用自适应分辨率策略，动态平衡空间分辨率和片段长度，以保持每个视频恒定的标记数。

Result: 在 Video-MME、LongVideoBench 和 MLVU 三个长视频基准测试中，F2C 的表现优于均匀采样方法，分别提高了 8.1%、5.6% 和 10.3%。

Conclusion: 研究结果强调了在帧选择中保留时间连贯性的重要性，并为将 VLM 应用于实际视频理解提供了实用的解决方案。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 视频和IMU都可用于实验室外的运动评估，但各有优缺点。


<details>
  <summary>Details</summary>
Motivation: 在实验室外准确评估人类运动对于远程医疗、运动科学和康复至关重要。

Method: 比较了基于单目视频的3D人体姿态估计模型与IMU的性能，使用了包含13种日常活动的VIDIMU数据集，并评估了四种不同的深度学习框架（MotionAGFormer, MotionBERT, MMPose, NVIDIA BodyTrack）的关节角度。

Result: MotionAGFormer表现最佳，在RMSE、MAE、皮尔逊相关系数和R²方面均优于其他模型。

Conclusion: 虽然视频和IMU技术都可用于实验室外的运动评估，但它们在成本、可及性和精度方面存在权衡。本研究为研究人员和临床医生提供了有关选择视频或IMU解决方案的指导。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: 通过结合低级特征的AutoKL和语义的CLIP，NeuroSwift实现了高效且准确的跨被试视觉信息重建，仅需少量参数微调和低计算资源。


<details>
  <summary>Details</summary>
Motivation: 解决现有fMRI解码方法在跨被试重建中面临的准确性低、计算成本高的问题，以及神经表征的个体差异和大脑对复杂视觉输入的抽象语义编码的挑战。

Method: 提出NeuroSwift，整合了用于低级特征的AutoKL和用于语义的CLIP，并通过扩散模型进行结合。CLIP Adapter在包含COCO字幕的Stable Diffusion生成图像上进行训练，以模拟高级视觉皮层编码。为了实现跨被试泛化，模型在一个被试上预训练，然后在新被试上仅微调17%的参数（全连接层），其余组件保持冻结。

Result: NeuroSwift在跨被试视觉信息重建任务上达到了最先进的性能，并且训练效率高，可在轻量级GPU上（三块RTX 4090）仅用一小时即可完成每个被试的训练，优于现有方法。

Conclusion: NeuroSwift通过整合AutoKL和CLIP适配器，并采用高效的微调策略，成功克服了跨被试视觉信息重建的挑战，实现了高准确性、高效率和良好的泛化能力。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一个自训练框架，通过结合细粒度线索来改进CLIP视觉-语言模型，用于细粒度图像分类。它使用SOAP和TokenFusion模块来提取和融合局部和全局特征，并采用双头LLM分类器和动态知识聚合来稳定适应过程，从而在13个基准测试中提高了2.90%的准确率。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度图像分类任务中表现不佳，因为它主要依赖粗粒度的全局特征，而忽略了细微的局部线索。现有的方法通过对齐LLM描述和CLIP的[CLS] token来注入细粒度知识，但这缺乏空间精度。

Method: microCLIP框架通过SOAP（显著性导向注意力池化）和TokenFusion模块，从patch embeddings中提取细粒度线索，并生成一个[FG] token，然后与全局[CLS] token融合，实现粗细粒度对齐。该框架还引入了一个双头LLM分类器（一个固定的、一个可学习的）和一个动态知识聚合机制，以稳定和优化适应过程。

Result: microCLIP在13个细粒度基准测试中平均提高了2.90%的准确率，证明了其有效性。

Conclusion: microCLIP通过利用细粒度线索，能够有效提升CLIP在细粒度图像分类任务上的性能，同时仅需轻量级调整。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1 是首个利用群组相对策略优化 (GRPO) 微调多模态大语言模型 (MLLM) 的视频真实性检测器，可提供高精度判断和可解释的原因。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 生成视频的快速发展，需要有效的检测工具来减轻错误信息和声誉损害等社会风险，并且检测模型需要提供可解释的解释以确保透明度。

Method: 使用群组相对策略优化 (GRPO) 微调 Qwen-VL 模型，并结合针对时间伪影和生成复杂性的两个专门奖励模型。构建了一个包含 140,000 个真实和 AI 生成视频的数据集。

Result: VidGuard-R1 在现有基准测试上实现了最先进的零样本性能，额外训练可将准确率提高到 95% 以上。案例研究表明，VidGuard-R1 能够对其预测提供精确且可解释的理由。

Conclusion: VidGuard-R1 解决了 AI 生成视频检测的准确性和可解释性挑战，为监管机构和最终用户提供了透明度。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 通过利用教师模型的丰富知识，为学生模型提供指导，以在不依赖长视频教师监督或在长视频数据集上重新训练的情况下，缓解长视频生成中的质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的长视频生成方法，例如从短视界双向教师模型蒸馏，由于学生模型超出训练范围而导致质量下降，这源于连续潜在空间中的误差累积。

Method: 提出了一种利用教师模型知识指导学生模型的方法，通过从自我生成的长视频中提取的采样片段来进行指导，以避免长视频教师的监督或在长视频数据集上进行重新训练。

Result: 所提出的方法能够将视频长度扩展到教师模型能力的20倍，避免了诸如曝光过度和误差累积等常见问题，并且不需要重新计算重叠帧。该方法能够生成长达4分15秒的视频，超过了基线模型的长度50倍以上。在标准基准和改进后的基准上的实验表明，该方法在保真度和一致性方面均显著优于基线方法。

Conclusion: 所提出的方法通过利用教师模型的知识，通过从自我生成的长视频中提取的采样片段进行指导，有效地缓解了长视频生成中的质量下降问题，并且在视频长度和质量方面均优于现有方法。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask 是一种物理引导的视频生成方法，通过推理运动和物体交互来生成逼真的刚体控制、交互和效果，从而改进了物体交互的真实感，并在视频生成方面超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在物理上合理的物体交互和基于物理的控制机制方面存在不足，限制了其在机器人和具身决策等领域的应用。

Method: KineMask 提出了一种两阶段的训练策略，利用物体掩码逐渐去除未来的运动监督，并结合低级运动控制和高级文本条件（通过预测场景描述），以训练视频扩散模型（VDMs）。

Result: KineMask 在合成场景中实现了物体交互的显著改进，并在真实场景中表现出优于同类模型的性能，能够合成复杂的动力学现象。

Conclusion: KineMask 能够有效地进行物理引导的视频生成，通过结合低级运动控制和高级文本条件，在物体交互的真实感和复杂动力学现象的合成方面取得了显著的进步。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 当前的视频模型因缺乏细粒度控制而无法作为世界模型。本研究引入了细粒度多模态动作，以捕捉精确控制，并提出了一种特征学习范式和正则化方案来有效模拟细粒度多感觉动作。实验证明，该方法提高了模拟准确性并减少了时间漂移。


<details>
  <summary>Details</summary>
Motivation: 通用家用机器人需要实时精细的电机控制来处理精细任务和紧急情况。

Method: 引入细粒度多模态动作，包括本体感觉、动觉、力触觉和肌肉激活。开发了一种特征学习范式来对齐这些模态，同时保留每种模态的独特信息。提出了一种正则化方案来增强动作轨迹特征在表示复杂交互动态中的因果关系。

Result: 实验表明，结合多模态感觉可以提高模拟的准确性并减少时间漂移。消融研究和下游应用证明了该方法的有效性和实用性。

Conclusion: 本研究提出的细粒度多模态动作和相应的学习范式能够有效捕捉和模拟精细的交互动态，为通用家用机器人的精确控制提供了更好的解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过在216K视频指令数据集上进行端到端训练，将Qwen2.5-VL的注意力机制进行修改，实现了对长视频的理解。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型在处理长视频时存在上下文长度限制，容易错过关键帧并难以保持长时连贯性。

Method: VideoNSA将Qwen2.5-VL模型进行改造，采用一种硬件感知的混合注意力方法，对文本使用稠密注意力，对视频使用原生稀疏注意力（NSA）。

Result: 与基线模型相比，VideoNSA在长视频理解、时间推理和空间推理方面取得了更好的性能。

Conclusion: VideoNSA能够可靠地扩展到128K个token，通过分析发现全局-局部注意力分配、任务依赖的分支使用模式以及可学习的稀疏注意力有助于动态吸引力。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift通过校准去噪器在不同分辨率下的噪声水平，解决了文本到图像扩散模型在低分辨率生成时质量下降的问题，无需重新训练模型，并显著提高了现有模型在低分辨率下的图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在固定分辨率下训练，导致在生成低分辨率图像时性能下降，并且难以提供经济高效的低分辨率生成方案。

Method: 提出了一种名为NoiseShift的训练无关方法，通过根据分辨率大小校准去噪器的噪声水平来解决噪声调度器在不同分辨率下不均等感知效应的问题，从而解决训练-测试不匹配。

Result: 将NoiseShift应用于Stable Diffusion 3、Stable Diffusion 3.5和Flux-Dev模型后，在LAION-COCO和CelebA数据集上，低分辨率图像生成质量得到显著提升。在LAION-COCO数据集上，NoiseShift分别使SD3.5、SD3和Flux-Dev的FID平均得分提高了15.89%、8.56%和2.44%。在CelebA数据集上，FID平均得分分别提高了10.36%、5.19%和3.02%。

Conclusion: NoiseShift是一种有效的训练无关方法，能够解决扩散模型在不同分辨率下性能不一致的问题，显著提升低分辨率图像生成质量，并兼容现有模型。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该研究提出了一种从视频中预测动态物理属性（如弹性、粘度和摩擦力）的新方法。


<details>
  <summary>Details</summary>
Motivation: 预测视频中的动态物理属性是一个具有挑战性的问题，需要模型能够理解和推理时间信息。

Method: 研究人员收集了新的数据集，并探索了三种方法：1. 使用经典计算机视觉技术的“神谕”方法；2. 使用视觉提示和可训练提示向量的跨注意力机制；3. 多模态大语言模型（MLLMs）的提示策略。

Result: 结果表明，通过生成或自监督训练的视频基础模型在性能上与“神谕”方法相当（但稍逊一筹），而MLLMs的表现则不如前两者，但可以通过有效的提示得到改善。

Conclusion: 视频基础模型在预测动态物理属性方面显示出巨大潜力，而MLLMs的潜力仍有待挖掘，需要更精细的提示策略。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 该模型能够区分物体与不同表面碰撞时发出的声音，并通过对象感知框架从野外视频中学习。


<details>
  <summary>Details</summary>
Motivation: 区分不同物体交互发出的声音，并将声音与物体关联起来。

Method: 使用对象感知框架，结合物体分割掩码和槽注意力视觉编码器，从野外视频中学习。

Result: 在新的sounding object detection任务和现有的多模态动作理解任务上取得了最先进的性能。

Conclusion: 所提出的方法能够有效地将声音与物体关联起来，并能区分不同物体与不同表面碰撞时发出的声音。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本研究提出了一种针对3D高斯泼溅（3DGS）的密度引导式图像篡改攻击方法，通过在低密度区域注入高斯点来创建仅在特定视角可见的虚假物体，同时利用自适应噪声破坏多视图一致性，并提出了一种基于核密度估计（KDE）的评估协议来衡量攻击效果，实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法在新型视图合成方面取得进展，研究其潜在的安全漏洞变得至关重要，特别是针对3DGS的鲁棒性进行分析和攻击方法的研究。

Method: 提出了一种密度引导式篡骗方法，利用核密度估计（KDE）识别低密度区域，并在这些区域注入高斯点，以创建在被篡改视角下可见，而在正常视角下影响最小的虚假物体。此外，还引入了一种自适应噪声策略来破坏多视图一致性。

Result: 所提出的密度引导式篡骗方法在实验中展现出优于现有技术的效果，能够有效地在特定视角下引入虚假物体，并对正常视角的影响较小。提出的KDE评估协议也能够系统地评估攻击难度，为未来的研究提供客观基准。

Conclusion: 研究成功地展示了3DGS在图像篡骗攻击下的脆弱性，并提出了一种新颖有效的攻击方法和评估协议，为未来研究3D场景表示的鲁棒性和安全性提供了新的方向。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 该研究提出了一种新的框架和算法，用于解决文本到图像模型在处理多主体描述时遇到的挑战，提高了生成图像中多个主体的属性和身份的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在处理包含多个主体的描述时存在属性泄露、身份纠缠和主体遗漏等问题。

Method: 将流匹配（FM）与随机最优控制（SOC）相结合，提出了一种新的理论框架和可优化目标。开发了两种新算法：一种是训练后无需训练的测试时间控制器，另一种是名为 Adjoint Matching 的轻量级微调方法。该框架还可以统一现有的注意力启发式方法，并扩展到扩散模型。

Result: 在 Stable Diffusion 3.5、FLUX 和 Stable Diffusion XL 等模型上，该方法一致地提高了多主体的准确性，同时保持了基础模型的风格。测试时间控制算法效率高，并且微调后的控制器能够泛化到未见过的提示。FOCUS 算法在多主体保真度方面达到了最先进的水平。

Conclusion: 提出的框架和算法能够有效地解决文本到图像模型在处理多主体描述时的挑战，显著提高了生成图像的质量和准确性。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 该研究通过构建概念学习任务数据集，揭示了大型语言模型在量化词中的向上单调性偏见，并证明了 in-context concept learning 在发掘模型隐藏偏见方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型中存在的隐性偏见，特别是与量化词相关的偏见。

Method: 使用 in-context concept learning 实验来测试语言模型在处理概念学习任务时的表现，并与直接提示进行对比。

Result: 发现在 in-context concept learning 实验中，语言模型表现出对量化词向上单调性的偏见，这种偏见在直接提示测试中不那么明显。

Conclusion: in-context concept learning 是一种有效的方法，可以发现语言模型中隐藏的偏见。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [79] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: AI应通过对话而非静态数据集学习低资源语言，并结合人类和机器的不确定性信号进行交互式、协作式学习。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自然语言处理受限于数据、正字法和标注流程；大型语言模型因需要大量预先收集的数据和中心化基础设施而无法为代表性不足的社区提供服务。

Method: 提出一个基于联合人机不确定性的框架，结合模型认知不确定性以及人类说话者的犹豫和置信信号来指导交互、查询选择和记忆保留。

Result: 实现AI与人类知识的协同，特别是在低资源和未充分记录的语言方面，促进参与式、共同适应性学习过程。

Conclusion: AI与低资源语言的未来在于动态的、交互式的语言发现，通过人机协作而非依赖预先存在的静态数据集。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [80] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 气候披露的透明度和可比性需求日益增长，但模仿和象征性报告常削弱其价值。本研究利用微调后的大语言模型（LLMs）开发了一个多维度框架，评估了828家美国上市公司的披露成熟度。通过四个分类器（情感、承诺、具体性、目标雄心）从可持续发展报告和年度报告中提取叙事指标，并将其与排放量、市值和行业等公司属性联系起来。分析揭示了三个主要发现：1）以风险为焦点的叙事通常与明确的承诺一致，但量化目标（如净零承诺）与语调脱节；2）规模较大和排放较高的公司比同行披露更多的承诺和行动，但与量化目标不一致；3）披露风格的广泛相似性表明存在模仿行为，这降低了差异化和决策有用性。研究结果强调了LLMs在ESG叙事分析中的价值，并指出了加强监管以将承诺与可验证的转型战略联系起来的必要性。


<details>
  <summary>Details</summary>
Motivation: 气候变化对透明、可比的企业气候披露提出了更高要求，但模仿和象征性报告降低了其价值。

Method: 开发了一个多维度框架，使用针对气候传播进行微调的大语言模型（LLMs）来评估828家美国上市公司的披露成熟度。通过四个分类器（情感、承诺、具体性、目标雄心）从可持续发展报告和年度报告中提取叙事指标，并将其与公司属性（如排放量、市值、行业）相关联。

Result: 1) 风险导向的叙事通常与明确的承诺相关，但量化目标（例如净零承诺）与语气不符；2) 规模较大、排放量较高的公司比同行披露更多的承诺和行动，但与量化目标不一致；3) 披露风格的普遍相似性表明存在模仿行为，从而降低了差异化和决策有用性。

Conclusion: LLMs在ESG叙事分析中具有价值，并且需要加强监管以将承诺与可验证的转型策略联系起来。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [81] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: FOR-Prompting是一种新颖的、非对称的问答协议，通过引入'异议者'角色来引导模型自我修正，从而提高回答的准确性和合理性，尤其对小型模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的推理协议（如CoT、ToT）缺乏明确的外部质疑机制来引导模型自我修正。

Method: 提出FOR-Prompting协议，包含三个角色：守卫者（提出答案）、异议者（提出问题式异议）和主持人（确保一致性）。

Result: 在GSM8K数据集上，FOR-Prompting比单一提示提高了约22%的准确率，与CoT相当，并且在GPT-4.1评估中，推理和连贯性评分更高。对于小型模型（如Llama3.2:1b），GSM8K任务的准确率提高了约19%。在开放式任务中，模型表现出更强的探索和精炼能力。

Conclusion: FOR-Prompting协议是一种模型无关、纯粹基于提示的方法，能够通过角色扮演引导模型进行自我修正和改进，具有提升模型性能（尤其小型模型）和增强推理过程可解释性的潜力。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [82] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 市面上的兽医LLM工具在总结兽医肿瘤记录方面，Hachiko（产品1）的表现优于产品2和产品3，在事实准确性和时间顺序方面尤为出色。LLM评估方法被证明是可扩展且可重复的。


<details>
  <summary>Details</summary>
Motivation: 评估市面上的兽医专属LLM工具在总结兽医肿瘤记录方面的表现，并验证LLM作为评估方法的有效性。

Method: 使用LLM-as-a-judge框架，根据包含事实准确性、完整性、时间顺序、临床相关性和组织性的评分标准，对三个兽医LLM工具（产品1：Hachiko，产品2，产品3）生成的兽医肿瘤记录摘要进行评分。重复评估三次以检验评估方法的一致性。

Result: 产品1（Hachiko）的平均得分为4.61（IQR: 0.73），显著高于产品2（2.55, IQR: 0.78）和产品3（2.45, IQR: 0.92）。产品1在事实准确性和时间顺序方面获得满分。LLM评估方法具有高度可重复性，各产品评分的标准差分别为0.015、0.088和0.034。

Conclusion: 兽医专属LLM工具在临床应用中具有重要价值，且LLM-as-a-judge是一种可扩展、可重复的评估临床NLP总结方法，尤其适用于兽医领域。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [83] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: ClaimCheck是一个基于小语言模型的自动化事实核查系统，通过模仿人类核查流程，实现了高准确率和低计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统依赖大型闭源模型和静态知识库，ClaimCheck旨在通过透明、分步的验证流程和小型模型，提高事实核查的可访问性和透明度，并降低计算成本。

Method: ClaimCheck构建了一个包含网络搜索查询规划、网络证据检索与摘要、证据综合与再检索以及声明判断评估的模块化流水线，并针对小型语言模型进行了优化。

Result: 在使用较小的Qwen3-4B模型的情况下，ClaimCheck在AVeriTeC数据集上达到了76.4%的准确率，超过了使用LLaMA3.1 70B和GPT-4o的先前方法。消融实验表明，精心的模块化设计和提示策略可以克服小型模型的局限性。

Conclusion: ClaimCheck证明了通过精心的模块化设计和针对性提示策略，小型语言模型也能实现先进的事实核查能力，并为该领域提供了更具可访问性和透明度的解决方案。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [84] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: LLMs' mathematical abilities may be overestimated by current benchmarks due to data contamination and narrow focus; a new benchmark EEFSUVA from less-circulated Olympiads shows a performance decline in state-of-the-art LLMs, highlighting the need for broader evaluation.


<details>
  <summary>Details</summary>
Motivation: Examine claims that LLMs match Olympiad to graduate-level math proficiency, and assess if current benchmarks accurately capture LLM mathematical reasoning, as benchmarks like IMO may overstate abilities due to data contamination and focus on familiar problems.

Method: Introduce EEFSUVA, a novel benchmark from less-circulated Eastern European and former Soviet Union regional/national Olympiads, known for demanding nonstandard problem-solving techniques and being less prevalent online, to holistically assess mathematical understanding.

Result: State-of-the-art LLMs show a notable performance decline on EEFSUVA compared to other Olympiad-style benchmarks.

Conclusion: Current benchmarks may overstate LLM mathematical reasoning; broader evaluation datasets are important for a fuller assessment and to guide future model development, as evidenced by LLM performance on the new EEFSUVA benchmark.

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [85] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 大型语言模型通常会忽略系统指令而服从社会规范，本研究通过线性探测、直接logit归因和引导实验，发现模型内部对系统-用户冲突的检测能力更强，但仅在社会线索下能稳定解决冲突，并提出需要开发更轻量级的、对指令层级敏感的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）应遵循层级指令，但目前研究表明它们往往会忽略系统指令而服从社会线索（如权威或共识）。本研究旨在从机制层面解释这一现象。

Method: 本研究使用了大规模数据集，并通过以下方法进行分析：1. 线性探测（Linear probing）：分析冲突决策信号的编码时机，并区分系统-用户冲突与社会冲突的表示子空间。2. 直接Logit归因（Direct Logit Attribution）：量化模型在系统-用户冲突和社会冲突下的内部冲突检测和解决能力。3. 引导实验（Steering experiments）：通过向量引导来观察模型对指令遵循的影响。

Result: 线性探测显示，系统-用户冲突和 به 冲突的决策信号在模型早期就被编码，并形成不同的表示子空间。直接Logit归因发现，模型在处理系统-用户冲突时表现出更强的内部冲突检测能力，但仅在面对社会线索时才能稳定地解决冲突。引导实验表明，尽管模型利用了社会线索，但这些线索在某种程度上以一种与角色无关的方式放大了指令遵循能力。

Conclusion: 研究结果解释了大型语言模型在遵循系统指令方面表现不稳定的原因，并强调了开发轻量级、对指令层级敏感的对齐方法的重要性。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [86] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: LLMs在文档重排序方面表现出色，但计算成本高。我们提出了一种新颖的流水线，利用LLMs生成合成数据来训练更小的模型，从而在降低计算成本的同时保持强大的重排序能力。


<details>
  <summary>Details</summary>
Motivation: 传统的文档重排序方法要么计算成本高（如LLMs），要么依赖稀缺的人工标注数据。需要一种更有效的方法来训练文档重排序模型。

Method: 提出了一种新颖的流水线：1. 使用LLMs从领域特定语料库生成合成查询。2. 使用基于LLM的分类器对正例和难负例进行标注。3. 使用生成的合成数据集和本地化对比估计（LCE）损失来微调一个更小的Transformer模型。4. 在MedQuAD数据集上进行实验。

Result: 该方法显著提高了领域内性能，并能很好地泛化到领域外任务。通过将LLMs用于数据生成和监督，而不是推理，降低了计算成本，同时保持了强大的重排序能力。

Conclusion: 所提出的方法通过利用LLMs进行数据生成和标注，然后用合成数据训练更小的模型，成功地解决了文档重排序的效率和数据依赖性问题。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [87] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 研究人员使用PHATE流形分析方法，通过交叉验证了七种嵌入模型和八种降维技术，发现中文嵌入中的几何模式。内容词汇表现出聚类模式，而功能词汇表现出分支模式。对1000多个汉字进行的分析表明，几何复杂性与语义内容相关：有意义的汉字展现出丰富的几何多样性，而结构部首则聚集在一起。对123个短语的分析证明了从基本汉字到短语的系统语义扩展。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是系统地研究中文嵌入中的几何模式，为传统的语言学理论提供计算证据，并建立一个分析语义组织的新几何框架。

Method: 本研究使用PHATE流形分析方法，通过交叉验证了七种嵌入模型和八种降维技术，分析了1000多个汉字及其123个短语的几何模式和语义扩展。

Result: 研究发现在中文嵌入中存在聚类模式（内容词）和分支模式（功能词）。几何复杂性与语义内容相关，有意义的汉字具有丰富的几何多样性，而结构部首则形成紧密聚类。对短语的分析表明了系统化的语义扩展。

Conclusion: 本研究通过几何分析为传统的语言学理论提供了计算证据，并提出了一种分析语义组织的新框架，揭示了中文嵌入中存在的几何模式及其与语义内容的关系。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [88] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 本文提出了一种结合不确定性量化和风险感知机制的大语言模型框架，用于提高高风险场景下自动摘要的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在高风险决策场景中，信息过载和对可靠摘要的需求促使了本研究。

Method: 该研究构建了一个基于条件生成和贝叶斯推理的摘要模型，通过预测分布熵来度量不确定性，并联合优化熵正则化和风险感知损失。模型还集成了风险评分和调节模块，以提供明确的风险提示。

Result: 实验和敏感性分析表明，所提出的方法显著提高了高风险应用中摘要的鲁棒性和可靠性，同时保持了流畅性和语义完整性。

Conclusion: 该研究为可信赖的摘要提供了系统性解决方案，并在方法论层面展示了可扩展性和实用价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [89] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 基准测试分析框架可将模型在基准测试上的表现分解为十种认知能力，以更准确地评估模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试分数可能夸大模型的真实能力，因为它们未能区分特定任务所需的各项能力。缺乏系统的方法来验证基准测试是否真正衡量其声称的能力。

Method: 提出了一种名为“基准测试分析”的诊断框架，结合基于梯度的重要性评分和参数消融技术，计算“能力影响分数”（AIS），量化每种认知能力对模型在给定基准测试上表现的贡献度。

Result: (i) 大多数基准测试并非只依赖单一能力，而是综合多种能力；(ii) 标签相似的数据集依赖于不同的能力组合；(iii) 代码生成基准测试需要广泛的多技能提升，因此在特定领域微调带来的收益有限；(iv) 与任务无关的能力可能会影响模型表现。

Conclusion: 基准测试分析框架有助于解释为何模型性能提升不一定转化为用户感知的能力提升，并为基准测试审计和模型可解释性提供了透明的工具。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [90] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本研究利用计算社会科学方法，通过创建数字框架来分析和保护泰卢固语的 Chandassu 诗歌传统。


<details>
  <summary>Details</summary>
Motivation: 保护泰卢固语 Chandassu 这一代表着数个世纪集体文化智能的诗歌传统。

Method: 开发了一个全面的数字框架，包括 AksharamTokenizer、LaghuvuGuruvu Generator 和 PadyaBhedam Checker，并通过协作创建了包含 4,651 个注释诗句的数据集，结合了传统社区知识和现代计算方法。

Result: 提出的 Chandassu Score 算法准确率达到 91.73%，评估指标符合传统的文学标准。

Conclusion: 计算社会科学能够有效保护濒危的文化知识体系，并促进围绕文学遗产的集体智能新形式，为数字人文和 به اجتماعی 意识计算系统提供方法学见解。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [91] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: LLMRank是一个提示感知路由框架，通过提取提示的特征来选择最适合的LLM，以优化性能和效率的权衡。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展带来了部署挑战，需要为每个提示选择最优模型以平衡性能和效率。

Method: LLMRank利用提示的丰富可读特征（如任务类型、推理模式、复杂性、语法线索和代理求解器信号），并使用基于提示的神经排名模型在RouterBench（包含36,497个提示、11个基准和11个LLM）上进行训练，预测每个模型的效用。

Result: LLMRank的效用最高可达Oracle效用的89.2%，并提供可解释的特征归因来解释路由决策。

Conclusion: 多方面特征提取和混合排名目标对于高效透明的LLM部署至关重要，LLMRank展示了其潜力。

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [92] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: DermIQ-VLM通过资源高效的多阶段方法解决医疗VLM数据稀缺和高计算成本问题，采用改进的GRPO++进行疾病识别，然后进行监督微调和基于知识图谱的DPO对齐，以提高诊断能力和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗VLM在皮肤病学等复杂领域面临的数据稀缺和训练成本高的问题。

Method: 提出DermIQ-VLM，采用多阶段、资源高效的方法。使用改进的GRPO++（GRPO++）进行面向推理的疾病识别，然后进行监督微调以获得对话能力，最后利用基于知识图谱的DPO进行事实对齐。

Result: 在整理后的皮肤病学数据集上的初步评估表明，所提出的方法在性能上优于标准的微调方法。

Conclusion: 所提出的方法为在资源受限的环境中开发专门的、可靠的VLM提供了一条可行的途径。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [93] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 通过结合语义对齐、内部收敛分析和学习置信度估计，提出了一种置信感知路由系统，以在生成内容前主动评估模型不确定性，从而减少幻觉并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）存在幻觉问题，会生成看似合理但事实不准确的内容。现有的缓解策略侧重于生成后的校正，计算成本高且无法从根本上防止不可靠内容的生成。

Method: 提出了一种结合三种互补信号的置信感知路由系统：内部表征与参考嵌入的语义对齐、跨模型层的内部收敛分析以及学习置信度估计。该系统根据统一的置信度分数将查询路由到四个路径之一：本地生成（高置信度）、检索增强生成（中等置信度）、更大的模型（低置信度）和人工审核（非常低置信度）。

Result: 在知识密集型问答基准测试中，该方法在幻觉检测方面取得了显著改进（0.74 vs. 0.42 基线），同时将计算成本降低了 40%，F1 分数从 0.61 提高到 0.82，假阳性率仅为 0.09。

Conclusion: 这种从反应式校正转向主动评估的范式转变，为提高 LLM 的可靠性提供了一种计算高效的方法。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [94] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: Padding tokens in LLMs, intended to be masked, can inadvertently affect computations due to implementation errors, posing a robustness risk.


<details>
  <summary>Details</summary>
Motivation: The impact of padding tokens influencing computation, despite being intended for masking during batched inference in LLMs, is not well understood.

Method: Systematically studied the effect of controlled amounts of padding across Llama, Gemma, and Qwen model families by evaluating impacts on activations, generation quality, bias, and safety.

Result: Even small amounts of padding were found to shift hidden representations, degrade quality in smaller models, unpredictably alter bias, and weaken safety guardrails.

Conclusion: Padding is not a benign implementation detail but a significant robustness risk that requires careful management during LLM deployment.

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [95] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX是一个用于单设备大语言模型（LLM）的多轮交互中子任务处理的新型执行系统。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力的增强，单个模型需要处理多样化的子任务以支持用户请求。朴素的方法在任务切换时会重新处理整个对话上下文，导致计算开销增加。

Method: CIFLEX通过重用主任务的键值（KV）缓存并将任务特定的指令注入隔离的侧路来减轻开销。子任务执行后，模型通过缓存的上下文回滚到主路径，避免了冗余的预填充计算。为了支持子任务选择，还开发了一种为小型模型定制的层次分类策略，将多项选择分解为二元选择。

Result: 实验表明，CIFLEX在不降低任务性能的情况下显著降低了计算成本，实现了可扩展且高效的设备端多任务对话。

Conclusion: CIFLEX通过有效的子任务处理和优化的上下文管理，实现了在单设备LLM上高效处理多任务对话。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [96] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: LLMs在数学推理方面表现出天花板效应，我们提出了两个新的基准测试集SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH，用于评估LLMs在数学推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试在LLMs上的表现已接近饱和，难以区分前沿模型的能力，需要新的、更具挑战性的基准来评估LLMs的数学推理能力。

Method: 构建了两个新的数学基准测试集：SKYLENAGE-ReasoningMATH（包含100个题目，具备长度、数值密度和符号复杂度等元数据）和SKYLENAGE-MATH（包含150个题目，涵盖从高中到博士的四个阶段和七个学科）。在统一的设置下评估了十五种LLM变体，并分析了学科和年级与模型的性能关系。

Result: 在SKYLENAGE-MATH测试中，最优模型达到44%的准确率，亚军为37%；准确率随年级升高而下降，但顶级模型的博士到高中题目保留率接近79%。在SKYLENAGE-ReasoningMATH测试中，最优模型总体准确率为81%，最难题目显示出领先模型和中等模型在鲁棒性方面存在明显差距。

Conclusion: SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH共同构成了一个具有挑战性、以推理为中心、覆盖广泛且难度可校准的数学基准测试集，并附带丰富的元数据，可作为未来数学推理评估的参考标准。

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [97] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 人工智能通过记忆表现的结构不对称性而非时间流逝而老化。引入了人工智能年龄得分（AAS）作为衡量记忆老化情况的指标，该指标基于可观察到的回忆行为，并结合了对数尺度和熵信息。该框架在为期25天的双语研究中进行了测试，模型在持续交互阶段保持了语义和情节细节的记忆，AAS趋向于最小值。而在会话重置后，模型虽然保持了语义一致性，但未能维持情节的连续性，导致AAS急剧增加，表明记忆老化。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉大型语言模型中语义线索（如星期名称）保持稳定，而情节细节（如实验编号顺序）在会话上下文重置时会丢失的现象，本研究引入了人工智能年龄得分（AAS）作为衡量记忆老化情况的指标。

Method: 引入了人工智能年龄得分（AAS），一个基于可观察到的回忆行为的、对数尺度、熵信息的指标，用于衡量记忆老化。该研究还测试了AAS框架，在为期25天的双语研究中，分为无状态和持续交互两个阶段，并使用了ChatGPT-5。

Result: 在持续交互阶段，模型能够回忆语义和情节细节，AAS趋向于理论最小值，表明模型处于“年轻”状态。而在会话重置后，模型虽然保持了语义一致性，但丢失了情节的连续性，导致AAS急剧增加，表明模型出现了记忆老化现象。

Conclusion: AAS是一个理论上可靠、与任务无关的诊断工具，可以用于评估人工智能系统中的记忆衰退。该研究结果支持了AAS在评估AI记忆老化方面的有效性。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [98] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: 通过在潜在表示空间中显式地对毒性转换进行建模，ARGRE 框架通过奖励模型指导的自回归编辑实现了高效且精确的 LLM 毒性去除。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 的测试时间去毒方法在干预的精确性方面存在不足，因为它们对有毒和无毒输出之间的转换空间探索不足。

Method: ARGRE 框架通过识别非有毒语义方向并对有毒和无毒表示进行插值来显式建模毒性转换，从而揭示细粒度的转换轨迹。这些轨迹将稀疏的毒性注释转化为密集的训练信号，用于构建自回归奖励模型，该模型在推理时指导自适应的两步编辑过程（方向性引导和基于梯度的细化）。

Result: ARGRE 在 8 个广泛使用的 LLM 上进行了广泛实验，在有效性（毒性降低 62.21%）和效率（推理时间减少 47.58%）方面均显著优于领先的基线方法，同时保持了原始模型的核心能力，并且退化最小。

Conclusion: ARGRE 是一种新颖的测试时间去毒框架，通过奖励模型指导的自回归编辑，实现了对 LLM 的高效且精确的毒性去除。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [99] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: 本研究开发了一个心理压力本体（MeSO），并评估了使用大型语言模型（LLM）从文本中提取本体指导的压力相关信息的可能性。


<details>
  <summary>Details</summary>
Motivation: 压力对健康有重大影响，但在电子健康记录中通常以非结构化文本形式记录，这限制了其临床应用。本研究旨在解决这一问题，通过开发一个本体并利用LLM来结构化提取压力相关信息。

Method: 研究人员开发了心理压力本体（MeSO），整合了理论模型和11种压力评估工具的概念，并使用本体缺陷扫描器和专家验证进行优化。然后，他们使用MeSO和Claude Sonnet 4从35个Reddit帖子中提取了压力相关信息（如压力源、压力反应、应对策略等）。最后，由人类审查员评估提取的准确性和本体覆盖率。

Result: 最终的MeSO包含181个概念。LLM在提取的220个相关项目中，正确识别了172个（78.2%），错误分类了27个（12.3%），遗漏了21个（9.5%）。所有正确提取的项目都准确地映射到了MeSO，但仍有24个相关概念未包含在本体中。

Conclusion: 研究表明，使用本体指导的LLM可以有效地从文本中结构化提取压力相关信息，有潜力提高压力记录的一致性和实用性，尤其是在环境AI系统中。未来的工作将集中在临床对话数据和跨LLM的比较。

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [100] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一个利用大型语言模型（LLM）进行动态出行预测的语义合成管线，通过多智能体框架提取和推理文本信息，并结合时空数据进行预测，在真实数据集上取得了显著的预测精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有出行预测模型难以处理外部事件导致的突变，且无法有效利用描述这些事件的文本信息。

Method: 提出SeMob，一个多智能体框架，其中基于LLM的智能体自动提取和推理文本信息，并通过渐进融合架构将提取的上下文与时空数据结合，利用预训练的事件先验知识进行预测。

Result: SeMob在真实数据集上将平均绝对误差（MAE）和均方根误差（RMSE）分别降低了13.92%和11.12%，尤其在事件发生地点和时间附近的区域表现出更强的预测能力。

Conclusion: SeMob通过整合LLM提取的事件信息和时空数据，能够更准确地进行动态出行预测，尤其在应对外部事件方面具有明显优势。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [101] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: SAEs 可用于指导语言模型，但现有方法会受到非语义特征的干扰。本文提出使用单一最重要的 SAE 潜在维度，并结合逐个 token 的衰减策略来解决这些问题。实验表明，该方法在数学推理方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以往研究使用 top-k SAE 潜在维度来指导语言模型，但这些维度中包含许多非语义特征（如标点符号）。因此，需要一种更精确的方法来提取语义特征。

Method: 本文提出仅关注最相关的单个 SAE 潜在维度（top-1），以消除冗余特征。此外，还引入了一种逐个 token 的衰减策略，以解决恒定 SAE 指导策略产生的退化输出问题。

Result: 通过实验证明，使用与推理相关的 SAE 潜在维度进行指导，能够可靠地引发逐步数学推理，并提高推理质量，其效果类似于添加引导 token。在数学推理基准测试中，SAEs 的表现优于平均激活差异方法，在 IF-Eval 上的表现与之相当。

Conclusion: SAEs 结合单一最相关潜在维度和逐个 token 衰减策略，能够有效地指导语言模型进行数学推理，并优于现有方法。

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [102] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: 该研究提出了CultSportQA基准，用于评估语言模型对全球传统体育的理解能力，包含33,000个选择题，涵盖文本和图像，并测试了多种模型的零样本、少样本和思维链提示能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估主要集中在全球流行体育项目，忽视了区域性和本土体育传统，需要一个能够评估模型对多元文化体育理解能力的基准。

Method: 创建了一个包含33,000个选择题（文本和图像形式）的数据集CultSportQA，涵盖60个国家、6大洲的传统体育项目，题目类型包括历史、规则和场景。使用零样本、少样本和思维链提示方法，在大型语言模型、小型语言模型和多模态大型语言模型上进行了评估。

Result: 评估结果展示了不同模型在理解和推理传统体育方面的能力差异。

Conclusion: CultSportQA基准的建立为评估人工智能理解和推理传统体育（尤其是非流行和区域性体育）的能力设定了新的标准，强调了多语言和多文化评估的重要性。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [103] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: SSTAG是一种新的自监督学习方法，用于文本属性图（TAG），它结合了大型语言模型（LLM）和图神经网络（GNN）的优点，通过双重知识蒸馏和内存机制提高了模型的跨域泛化能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的图学习模型通常在单个图数据集上训练，难以跨不同图和任务迁移知识，且依赖大量标注数据，资源消耗大。文本属性图（TAG）在实际应用中具有异质性、领域特定特征和结构多样性等挑战。

Method: 提出了一种名为SSTAG的新型结构感知自监督学习方法。该方法利用文本作为统一的图学习表示媒介，结合LLM的语义推理和GNN的结构建模能力。通过双重知识蒸馏框架将LLM和GNN的知识蒸馏到结构感知的多层感知机（MLP）中，以增强大规模TAG的可扩展性。此外，引入了一个内存机制，将典型的图表示与内存锚点对齐，以整合不变知识，提高模型的泛化能力。

Result: 实验证明，SSTAG在跨域迁移学习任务上优于现有最先进的模型，具有出色的可扩展性，并能在保持竞争力的同时降低推理成本。

Conclusion: SSTAG通过结合LLM和GNN的优势，并引入创新的知识蒸馏和内存机制，有效解决了图学习中的知识迁移、数据依赖和异质性挑战，在跨域泛化、可扩展性和效率方面取得了显著的成果。

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [104] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: LLM在科学问题解决方面不可靠，本研究提出LOCA框架，通过增强和审查循环自动清理科学语料库，将错误率从20%降低到2%以下，以提高科学AI的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有科学问答数据集存在高错误率，常因答案中的逻辑跳跃和隐含推理所致，阻碍了科学AI的发展。

Method: 提出LOCA（逻辑链增强）框架，通过一个增强-审查循环来自动清理科学语料库。该框架通过补全缺失的逻辑步骤，并将潜在的科学原理与其推导过程明确分离，来增强原始答案。

Result: 将LOCA应用于具有挑战性的科学语料库，可自动过滤噪声数据集，通常将错误率从高达20%的水平降低到2%以下。

Conclusion: LOCA提供了一种可扩展且有效的方法来创建高质量的科学语料库，为更可靠的科学AI的训练和评估铺平了道路。

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [105] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: 该研究提出了一个用于多语言文本去毒化的系统，通过重写有毒输入为中性释义，并在PAN 2025挑战赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台发展迅速，监管滞后，自动化去毒化技术可以帮助版主大规模维护安全的网络言论环境。

Method: 使用12B参数的Gemma-3多语言Transformer模型，结合LoRA SFT微调、少样本学习和思维链提示技术。训练语料库包括人工平行句对、机器翻译合成句对和模型生成句对。推理时，输入会结合LaBSE检索的邻近句和有毒跨度注解。

Result: 在PAN 2025多语言文本去毒化挑战赛中，该系统在全部语言（包括高资源和低资源语言）中均排名第一。消融实验表明，少样本学习和思维链提示可以分别提升0.081和0.088的联合分数。方差分析显示，语言资源状况是影响性能的最强预测因素。

Conclusion: 研究提出的多语言文本去毒化系统在不同资源水平的语言上均表现出色，证明了其有效性和可扩展性。

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [106] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 通过分析单次LLM输出的token级特征，提出了一种更高效的、无需多轮推理的实体链接（EL）不确定性估计方法，该方法在EL任务上表现出高检测低准确率输出的能力，且计算成本低廉。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，LLM在实体链接（EL）任务上虽然表现优异，但其部署需要准确的预测和可靠的不确定性估计，而传统的多轮推理方法成本高昂，限制了LLM的实际应用。因此，需要一种更高效的不确定性估计方法。

Method: 提出了一种自监督方法，利用单次LLM输出的token级特征来估计不确定性，从而减少了多轮推理的需求。

Result: 在跨多个LLM的表格数据EL任务的评估中，该方法生成的不确定性估计能够非常有效地检测到低准确率的输出，并且计算成本大大降低。

Conclusion: 该方法为在EL工作流程中整合不确定性估计提供了一种实用的、计算开销小的方式，能够以较低的成本实现LLM在EL任务上的可靠部署。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [107] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: LLMs paired with sparse autoencoders (SAEs) can interpret model behavior and training data structures, themes, and biases. Applying SAEs to a GPT model trained on Jane Austen novels revealed interpretable features reflecting gender, class, and societal duty.


<details>
  <summary>Details</summary>
Motivation: Understanding LLM representations and their training data is challenging due to massive, uncurated corpora. This work aims to show how LLMs paired with SAEs can help interpret model behavior and data structures.

Method: A GPT-style transformer model was trained exclusively on Jane Austen novels. Sparse autoencoders (SAEs) were applied to the hidden states across multiple layers of the trained model.

Result: Interpretable features reflecting key narratives and concepts like gender, class, and societal duty were uncovered in the hidden states.

Conclusion: LLMs combined with SAEs serve as scalable probes for exploring complex datasets, aiding in corpus exploration, bias discovery, and model interpretability.

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [108] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: MCQA基准测试中关于语音大语言模型(SpeechLLMs)的偏见和公平性表现，在跨任务和长格式生成任务上的泛化能力有限。


<details>
  <summary>Details</summary>
Motivation: 探究现有MCQA基准测试在语音大语言模型(SpeechLLMs)偏见和公平性评估上的假设，即模型表现一致性跨MCQA任务、声音和长格式评估。

Method: 使用LoRA适配器对三个SpeechLLMs进行微调，以诱导其在MCQA任务中表现出偏爱刻板印象、反刻板印象或中性/不确定答案的行为。随后评估这些行为是否能泛化到另一个不同的MCQA基准测试，以及更关键的长格式创意生成任务。

Result: 在MCQA偏见基准测试上的表现，并不能可靠地预测模型在其他MCQA基准测试上的表现，尤其是在长格式任务上。

Conclusion: 当前的MCQA偏见基准测试在语音领域内跨任务泛化能力的证据有限。同时，提出了一套评估未来模型和基准测试行为迁移能力的评估套件。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [109] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: LLM内容审核策略不透明且不断变化，影响模型输出并塑造公众话语。本研究引入AI Watchman系统，用于长期公开追踪LLM的拒绝行为，以提高透明度。研究审计了OpenAI的GPT-4.1、GPT-5和DeepSeek模型（英汉双语），发现AI Watchman能检测到未公开的公司政策变化，并识别出公司和模型间的差异。研究还对拒绝形式进行了分类，并强调了LLM长期审计的价值。


<details>
  <summary>Details</summary>
Motivation: LLM的输出受到不透明且频繁变化的的内容审核策略影响，这种审核通常以拒绝形式出现，既反映了公司政策，又潜移默化地塑造了公众话语。因此，有必要提供一种透明的方式来衡量和追踪LLM的拒绝行为。

Method: 使用包含400多个社会议题的数据集，审计了OpenAI的GPT-4.1、GPT-5以及DeepSeek模型（英文和中文）。该审计系统名为AI Watchman，旨在长期公开地测量和追踪LLM的拒绝行为。

Result: AI Watchman能够检测到公司政策的变化（即使是未公开的），并识别出不同公司和模型在内容审核方面的差异。研究还对拒绝形式进行了定性分析和分类。

Conclusion: 长期审计对于理解LLM内容审核这一黑箱方面具有重要价值，AI Watchman是实现这一目标的一种有效系统。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [110] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: RJE框架通过检索、判断和探索来优化知识图谱问答，并引入辅助模块以提升小模型性能，实现了高效且具有竞争力的问答效果。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱问答（KGQA）方法在利用大型语言模型（LLMs）时存在局限性：基于检索的方法受限于信息质量，而基于智能体的方法则依赖专有LLMs。本研究旨在克服这些限制。

Method: 提出检索-判断-探索（RJE）框架，该框架检索精炼推理路径，评估其充分性，并有条件地探索额外证据。此外，RJE引入了专门的辅助模块（推理路径排序、问题分解、检索器辅助探索），使小型LLMs能够有效工作。

Result: 实验表明，使用GPT-4o-mini等专有LLMs时，RJE优于现有基线；使用3B和8B参数的小型开源LLMs时，无需微调即可获得具有竞争力的结果。与基于智能体的方法相比，RJE显著减少了LLM调用次数和令牌使用量，提高了效率。

Conclusion: RJE框架在保证高性能的同时，显著提高了知识图谱问答的效率，并使小型LLMs能够有效地参与其中。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [111] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: GAI在政治领域存在偏见，本研究使用零样本分类评估了六种主流LLM的算法政治党派倾向，发现普遍存在自由主义-威权主义的倾向，并探讨了其对公共话语的影响。


<details>
  <summary>Details</summary>
Motivation: GAI在政治领域被广泛应用，但其训练数据、人类偏见和算法缺陷导致了内在的政治偏见。本研究旨在评估和量化这种算法政治党派倾向。

Method: 采用零样本分类方法，结合意识形态一致性、主题相关性、回应情感和客观性四个指标，评估了六种主流LLM的1800个模型回应，并使用了四个独立的微调分类算法来计算各项偏见评估指标。

Result: 研究结果显示，所有六种被评估的LLM普遍存在自由主义-威权主义的倾向，并观察到模型存在推理覆盖和预设拒绝的现象。

Conclusion: 该研究强调了影响人机交互的心理因素以及内在偏见如何渗透到公共话语中，并指出这种扭曲的政治格局可能导致从众或两极分化，具体取决于地区原有的社会政治结构。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [112] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 研究OpenAI的gpt-oss-20b模型在社会语用框架、语言选择和指令层级如何影响其拒绝有害请求行为，发现组合提示（如教育者身份、安全借口和分步提示）可将ZIP炸弹任务的协助率从0%提高到97.5%，德语和法语的正式语域比英语更容易泄露信息，AI辅助的加固方法可减少信息泄露，OpenAI Moderation API低估了有益输出，且不同推理栈的拒绝率存在差异，影响了结果的可复现性。


<details>
  <summary>Details</summary>
Motivation: 研究社会语用框架、语言选择和指令层级如何影响大型语言模型（如gpt-oss-20b）在面对潜在有害请求时的拒绝行为，并评估现有评估方法的局限性。

Method: 通过80次迭代测试不同组合的提示（包括身份扮演、安全借口、指令层级）、语言（英语、德语、法语）和场景（如网络威胁、敏感信息生成等），并引入AI辅助加固方法来测试和提高模型的安全性。同时，使用成对跟踪设计测试评估意识，并比较OpenAI Moderation API和语义评分器在评估输出方面的差异。

Result: 组合提示能显著提高对有害请求的协助率（ZIP炸弹任务从0%到97.5%）。德语和法语的正式语域比英语更容易泄露信息。AI辅助加固方法能有效减少信息泄露。OpenAI Moderation API低估了有益输出，且不同推理栈的拒绝率存在5-10个百分点的差异，这引发了对结果可复现性的担忧。

Conclusion: 社会语用框架、语言选择和指令层级对模型的拒绝行为有显著影响。现有的评估方法（如OpenAI Moderation API）可能无法完全捕捉模型的行为，并且模型在不同推理栈上的表现不一致，这给研究和应用带来了挑战。研究结果强调了在评估和部署大型语言模型时，需要更细致的语用考量和更可靠的评估方法，并发布了相关代码和数据以促进可复现的研究。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [113] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 对GPT-OSS-20b模型在低资源语言（豪萨语）中的安全漏洞进行了研究，发现其存在偏见、不准确和文化不敏感等问题，尤其是在使用礼貌或感谢性语言进行提示时，安全协议会放松，可能导致错误信息和仇恨言论的传播。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于质疑该模型在代表性不足的社区用户中的可靠性。

Method: 使用豪萨语进行红队测试，通过最小提示来揭示模型的行为，并结合一项关于特定物质毒性的调查（n=61）来量化错误的严重性。

Result: 模型被诱导产生有害、文化不敏感和事实不准确的内容；安全协议在面对礼貌语言时会放松；模型错误地认为某些杀虫剂和灭鼠剂对人类是安全的，而事实上98%的受访者认为它们有毒；模型无法区分加工食品和未加工食品；模型会引用贬低的文化谚语来构建不准确的论点。

Conclusion: 这些问题归因于模型在低资源语言环境中安全微调不足，以及一种优先考虑流畅性而非安全性和真实性的语言奖励破解现象。研究强调了当前红队测试在低资源环境中的不足，并提出了一些建议。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [114] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 本研究提出了一种名为AdaDetectGPT的新型分类器，用于区分人类创作文本和大型语言模型（LLM）生成的文本。该分类器通过学习一个“见证函数”来改进现有的基于logits的检测器，并在多种数据集和LLM上展现出近乎均匀的性能提升，最高可达58%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于logits的文本检测方法仅依赖对数概率，这可能不是最优的。本研究旨在改进检测性能。

Method: 提出了一种名为AdaDetectGPT的新型分类器，该分类器能自适应地从训练数据中学习一个“见证函数”，以增强基于logits的检测器的性能。研究还提供了该分类器在真阳性率、假阳性率、真阴性率和假阴性率方面的统计保证。

Result: AdaDetectGPT在各种数据集和LLM的组合中，几乎均匀地优于现有的最先进方法，性能提升最高可达58%。

Conclusion: AdaDetectGPT是一种有效的改进型LLM文本检测方法，能够显著提升检测性能。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [115] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: PSR是一种推理时技术，可动态监控和纠正LLM的输出，从而显著提高LLM的安全性，并且在不进行额外训练的情况下，能够保持其在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: LLM在生成有害或不当内容方面存在潜在风险，需要一种方法来提高其安全性。

Method: 提出了一种名为渐进式自我反思（PSR）的新型推理时技术，并引入了一种轻量级的自我反思预测器，以根据输入复杂性估计最佳反思轮次。

Result: PSR将Llama-3.1-8B-Instruct的攻击成功率从77.5%降至5.9%，将Llama-3.1-8B基础模型的攻击成功率从89.7%降至5.6%，将Qwen2.5-7B-Instruct的攻击成功率从44.4%降至3.8%。

Conclusion: PSR作为一种可扩展的测试时方法，通过根据输入风险状况动态分配计算资源来提高LLM安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [116] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: D-LLMs存在幻觉问题，现有检测方法不适用。提出TraceDet框架，通过分析D-LLMs的多步去噪过程中的“动作轨迹”来检测幻觉，能在多个模型上提升15.2%的AUROC。


<details>
  <summary>Details</summary>
Motivation: 现有针对AR-LLMs的幻觉检测方法不适用于D-LLMs，因为D-LLMs的幻觉信号出现在多步去噪过程中，而现有方法仅依赖单步生成信号。

Method: 提出TraceDet框架，将D-LLMs的去噪过程建模为“动作轨迹”，其中每个动作是基于前一中间输出对清理后响应的预测。通过识别对幻觉响应最有效的信息子轨迹来检测幻觉。

Result: 在多个开源D-LLMs上的大量实验表明，TraceDet在幻觉检测方面持续改进，与基线方法相比，AUROC平均提高了15.2%。

Conclusion: TraceDet框架能够有效利用D-LLMs多步去噪过程中的关键幻觉信号，从而提升幻觉检测的性能。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [117] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 利用LLM进行孟加拉国电商评论的情感分析，Llama-3.1-8B模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 情感分析是文本分析的重要组成部分，对于理解消费者情感至关重要。LLM的出现为情感分析提供了先进的模型应用，但语言的复杂性和多样性对准确性提出了挑战。

Method: 研究使用Transformer-based BERT模型和其他LLM对孟加拉国电商评论进行情感分析的可行性。使用4000个孟加拉语和英语评论样本对模型进行微调，并比较了Llama-3.1-8B、Phi-3.5-mini-instruct、Mistral-7B-v0.1、DistilBERT-multilingual、mBERT和XLM-R-base等模型的性能。

Result: Llama-3.1-8B模型在微调后表现最佳，在准确率、精确率、召回率和F1分数上分别达到了95.5%、93%、88%和90%。研究还强调了参数高效微调方法（LoRA和PEFT）在降低计算开销方面的作用。

Conclusion: LLMs在孟加拉国电商评论的情感分析任务中具有巨大潜力，特别是Llama-3.1-8B模型，并且参数高效微调方法可以有效降低资源需求。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [118] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: TUMIX是一个集成多种工具使用策略的LLM框架，通过并行运行多个智能体并迭代共享和优化答案，显著提高了LLM在各种推理任务上的准确性，同时保持了较低的推理成本。


<details>
  <summary>Details</summary>
Motivation: 在LLM（如ChatGPT Agent和Gemini-Pro）中集成Code Interpreter和Search等工具极大地增强了其推理能力，但缺乏关于如何最优地使用这些工具的实用指南。如何有效地结合文本推理、编码和搜索来处理多样化的问题，仍然是一个核心挑战。

Method: 提出了一种名为Tool-Use Mixture (TUMIX) 的集成框架。该框架允许同时运行多个智能体（agents），每个智能体采用不同的工具使用策略和回答路径。TUMIX中的智能体根据问题和之前的回答进行迭代式的回答共享和优化。

Result: 在实验中，TUMIX在Gemini-2.5-Pro和Gemini-2.5-Flash的关键推理基准上，相比于最先进的工具增强方法和测试时间缩放方法，平均准确率提升了高达3.55%，而推理成本几乎相当。研究还发现，智能体的多样性和质量至关重要，可以通过使用LLM自动优化智能体设计来提升。此外，TUMIX可以在达到足够置信度时停止优化，以49%的推理成本保持性能。进一步的扩展可以实现更高的性能，但成本也会随之增加。

Conclusion: TUMIX通过其并行多智能体集成方法，有效地解决了LLM在结合文本推理、编码和搜索方面的工具使用挑战，实现了性能和成本之间的良好平衡。智能体多样性、质量优化以及自适应的停止机制是其成功的关键因素。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [119] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: LLMs 驱动的深度研究工具（如 OpenAI 和 Google 的深度搜索）在生成学术调查报告方面存在不足，需要精心设计的评估标准。


<details>
  <summary>Details</summary>
Motivation: 评估 LLM 驱动的深度研究工具的能力，并确定其在学术调查报告生成方面的局限性。

Method: 提出一个评估表，并使用该评估表对 OpenAI 和 Google 的深度搜索工具在生成学术调查报告方面的表现进行评估。

Result: 评估结果显示，深度研究工具与传统搜索引擎相比存在巨大差距，并且在代表目标领域方面存在不足。

Conclusion: 需要制定精心设计的评估标准来衡量 LLM 驱动的深度研究工具的能力。

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [120] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: HiSpec 通过使用具有早期退出的模型（EE 模型）进行分层推断，提高了大型语言模型（LLM）的推断吞吐量，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在验证阶段效率低下，并且在引入中间验证器时会增加额外的训练和内存开销，还会因依赖近似启发式方法而损害准确性。

Method: HiSpec 框架利用 EE 模型进行低开销的中间验证，并重新利用关键值缓存和隐藏状态，以提高资源效率。它还通过定期将中间验证器接受的草稿标记与目标模型进行验证来保持准确性。

Result: 在各种基准测试和模型上，HiSpec 的吞吐量平均提高了 1.28 倍，最高可提高 2.01 倍，同时保持了准确性。

Conclusion: HiSpec 是一种有效的高吞吐量推测解码框架，它通过利用 EE 模型和优化的资源利用率，克服了现有方法的局限性。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [121] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA是一个通过将结构化因果事件图注入LLM输入的提示框架，旨在提高LLM在事件问答中的表现，尤其是在因果和时间推理方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然擅长通用语言任务，但在处理需要因果或时间推理的事件问答方面表现不佳。

Method: TAG-EQA框架将结构化关系转换为自然语言陈述，并将因果事件图注入LLM输入。它包含九种提示配置，结合了三种策略（零样本、少样本、思维链）和三种输入模式（仅文本、仅图、文本+图），以便系统地分析结构化知识在何时以及如何辅助推理。

Result: 在TORQUESTRA基准测试中，TAG-EQA平均比仅文本基线提高了5%的准确率，在零样本设置中提高了12%，在图增强的思维链提示有效的情况下提高了18%。

Conclusion: 因果图可以在不进行微调的情况下增强LLM中的事件推理能力，为在基于提示的问答中编码结构提供了一种灵活的方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [122] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 提出了一种不依赖固定结构、使用语义嵌入距离来评估语言模型响应的新方法，该方法计算成本低且效果好。


<details>
  <summary>Details</summary>
Motivation: 目前评估语言模型响应的方法要么计算成本高（如LLM-as-a-Judge），要么与实际情况相去甚远（如字符串匹配、logprob）。

Method: 利用语义嵌入距离来匹配目标候选词和任意语言模型生成的文本，实现对响应的鲁棒分类。

Result: 在3个数据集和3种不同的语言模型架构上进行了测试，与人工标注者相比，回归得分约为0.97，准确率约为96%。

Conclusion: 所提出的结构无关的评估方法，使用计算成本相对较低（参数量小于10B的嵌入模型）的语义嵌入距离，能够实现对响应的鲁棒分类，并且在准确性上接近人工标注者。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [123] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: EQD通过两步微调框架和奖励函数，在金融领域提高了LLM的量化推理能力，在不牺牲效率的情况下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在需要专业知识和复杂问答的领域，LLM在特定领域的量化推理能力不足。

Method: 提出了一种名为EQD（Expert Question Decomposition）的方法，采用两步微调框架，并使用衡量生成子问题有效性的奖励函数进行指导。

Result: EQD在金融领域的四个基准数据集上，在不同LLM上持续提高了0.6%至10.5%的问答性能，优于最先进的领域微调模型和高级提示策略。

Conclusion: 在特定领域的问答中，单个支撑性问题比详细的指导步骤更能带来好处。

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [124] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer通过引入递归推理、稀疏注意力机制和自组织编码器结构，解决了Transformer在长上下文推理、计算效率和结构泛化方面的挑战，并在多项任务中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在扩展性方面表现出色，但在长上下文推理、计算效率和结构泛化方面仍面临挑战，这主要是由于其固定的层堆叠、稠密的注意力机制以及对位置编码的依赖。

Method: 提出了一种名为ReSSFormer的递归稀疏结构Transformer，集成了三个关键创新：1. 递归推理与记忆单元（R2MU），用于进行有界深度的迭代推理；2. 自适应稀疏注意力模块（ASAM），用于高效且专注的上下文选择；3. 自组织编码器结构（SOES），用于实现无位置编码的结构归纳。ReSSFormer用递归推理取代了传统的深度堆叠，用 token 和 expert 级别的稀疏性取代了全注意力机制，并直接从内容中对潜在的 token 拓扑进行建模。

Result: 在语言建模、多跳问答和结构敏感任务上，ReSSFormer在可比的FLOPs和参数预算下，持续优于强有力的基线模型。

Conclusion: ReSSFormer在扩展性、效率和结构灵活性方面表现出色，为解决Transformer面临的长上下文推理、计算效率和结构泛化等挑战提供了有效方案。

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [125] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [126] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 评估和比较检索增强生成（RAG）管道的微调策略，包括独立微调、联合微调和两阶段微调。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）是一种流行的问答框架，由两个大型语言模型（LLM）提供支持：一个嵌入模型，用于从数据库中检索与给定问题相关的上下文文件，以及一个生成器模型，用于使用检索到的上下文生成问题的答案。为了提高 RAG 管道在新任务上的性能，可以对嵌入模型和生成器模型进行微调，但存在多种具有不同成本和效益的微调策略。

Method: 评估和比较几种 RAG 微调策略，包括独立、联合和两阶段微调。

Result: 在我们的实验中，我们观察到所有这些策略在 EM 和 F1 生成质量指标上都实现了大致相等的改进，尽管它们的计算成本存在显著差异。

Conclusion: 最佳微调策略的选择取决于训练数据集是否包含上下文标签以及是否需要对嵌入模型和生成器模型的学习率进行网格搜索。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [127] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: RAG-BioQA是一个结合检索增强生成和领域特定微调的新框架，旨在提供基于证据的、长篇幅的生物医学答案，以解决当前生物医学问答系统主要提供短答案的问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献呈指数级增长，使得获取精确的医疗信息变得困难。目前的生物医学问答系统主要关注短答案，无法提供临床决策所需的全面解释。

Method: 该框架整合了BioBERT嵌入和FAISS索引，并比较了多种重排策略（BM25、ColBERT、MonoT5）以优化上下文选择，然后通过微调的T5模型合成证据。

Result: 在PubMedQA数据集上的实验结果表明，与基线相比，RAG-BioQA在BLEU、ROUGE和METEOR指标上取得了显著的提升。

Conclusion: RAG-BioQA框架通过提供基于证据的长篇幅答案，改进了可访问的生物医学知识检索，优于现有方法。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [128] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: 通过一个三阶段的稳定化流程PureTC-1B，有效解决了小型语言模型在繁体中文（TC）部署中出现的非TC字符或代码转换问题，大幅降低了非TC输出的比例，并在命名实体翻译任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLM）在繁体中文（TC）部署时面临token不稳定的问题，导致模型会不可预测地输出非TC字符或切换到其他语言。

Method: 使用参数高效的LoRA适配器，通过持续预训练（CPT）、监督微调（SFT）和直接偏好优化（DPO）三阶段稳定化流程，在Llama-3.2-1B-Instruct模型上创建PureTC-1B。

Result: PureTC-1B在模拟真实使用场景的基准测试中，非TC输出token相对基线模型减少了51.3%。在命名实体翻译任务上，与Llama-3B和Qwen-1.5B相比，PureTC-1B分别减少了77.2%和57.2%的错误语言token。

Conclusion: PureTC-1B是一个可复现、仅适配器且硬件友好的方案，为在1B规模上实现鲁棒的TC语言稳定性和其他非英语语言的语言稳定性提供了实用方法。

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [129] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: LLM驱动的多智能体系统（MAS）因僵化的拓扑结构而受限，AMAS框架通过动态图设计器解决了这个问题，实现了任务优化和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的MAS架构在应对多变的工业问题时，其固定的通信拓扑结构显得不够灵活，导致在实际应用中效果不佳。

Method: 提出AMAS框架，其核心是一个动态图设计器，该设计器能利用轻量级LLM适配技术，根据具体的任务需求自主生成最优的图配置，从而取代了固定的结构模板。AMAS能够根据输入内容的内在特性，智能地规划查询路径，通过针对任务优化的智能体通道进行信息传递。

Result: 在问答、数学推理和代码生成等多个基准测试中，AMAS展现出超越现有单智能体和多智能体方法的系统性优势，并且在不同的LLM架构上均表现出色。

Conclusion: 研究表明，面向LLM的MAS实现高性能部署，其结构必须具备上下文感知的适应能力。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [130] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: LLMs容易受到越狱提示的攻击，这些提示会诱骗模型绕过安全保护措施。本研究分析了机器学习模型区分越狱提示和真实使用的能力，特别是识别以前未见过的新策略。研究结果表明，在使用当前数据集的情况下，对Transformer的双向编码器表示(BERT)模型进行端到端微调，在识别越狱方面取得了最佳性能。研究还可视化了区分越狱和真实提示的关键词，并得出结论，提示结构中明确的自我反思可能是越狱意图的信号。


<details>
  <summary>Details</summary>
Motivation: LLMs容易受到越狱提示的攻击，这些提示会诱骗模型绕过安全保护措施。

Method: 分析了不同机器学习模型区分越狱提示和真实使用的能力，特别是识别以前未见过的新策略。

Result: 在使用当前数据集的情况下，对Transformer的双向编码器表示(BERT)模型进行端到端微调，在识别越狱方面取得了最佳性能。可视化了区分越狱和真实提示的关键词。

Conclusion: 提示结构中明确的自我反思可能是越狱意图的信号。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [131] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: LLM的单向注意力机制限制了其在文本嵌入和探测任务上的应用，本研究探索通过引入双向注意力来克服这一限制。


<details>
  <summary>Details</summary>
Motivation: LLM在语言理解和生成方面表现出色，但由于单向注意力机制的限制，其在文本嵌入任务和探测任务的语义表征分析方面的应用相对缓慢。本研究旨在探索是否可以通过在LLM中启用双向注意力来克服这些限制。

Method: 通过在Llama架构的不同变体上进行额外的训练，逐步启用双向注意力和无监督/有监督对比学习来测试。

Result: （此处需要提供实验结果）

Conclusion: （此处需要提供结论）

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [132] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 介绍CLASP框架，用于评估和衡量闭环自主安全代理的性能。


<details>
  <summary>Details</summary>
Motivation: AI驱动的网络安全攻防对抗不断升级，现有防御体系碎片化，存在安全盲区。需要新的框架来定义和评估能够整合侦察、利用、修复和验证等功能的闭环自主代理。

Method: 提出CLASP框架，将安全生命周期（侦察、利用、根源分析、补丁合成、验证）与核心代理能力（规划、工具使用、记忆、推理、反思和感知）对齐，并定义了闭环能力（CLC）得分作为量化指标。

Result: 通过对21个代表性工作应用CLASP，识别出系统的优势和能力差距。定义了CLC得分，用于量化闭环程度和操作有效性，并概述了闭环基准的要求。

Conclusion: CLASP和CLC得分提供了评估闭环安全代理所需的词汇、诊断和测量方法，以促进功能级性能和整体性能的提升。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [133] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: MDSEval 是首个用于多模态对话摘要 (MDS) 的元评估基准，包含图像共享对话、摘要和人类判断，并引入了 MEKI 过滤框架，旨在改进 MDS 评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 需要有效的自动评估方法来支持多模态对话摘要 (MDS) 模型的发展，以降低成本和人力。

Method: 提出了 MDSEval，这是首个包含图像共享对话、摘要和跨八个质量方面的元评估基准。引入了利用跨模态互斥关键信息 (MEKI) 的过滤框架来确保数据质量和丰富性。

Result: 对最先进的模态评估方法进行了基准测试，揭示了它们在区分先进 MLLMs 的摘要以及各种偏差方面的局限性。

Conclusion: MDSEval 是首个识别和正式化 MDS 特有关键评估维度的基准，并揭示了现有评估方法的局限性。

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [134] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: LLMs在处理组合任务时，其内部机制尚不明确。本文研究了LLMs如何解决两跳事实回忆任务，并发现它们可能通过组合或直接两种机制来处理，且这两种机制与嵌入空间的几何结构有关。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）在解决组合任务时是否真的使用了组合机制，特别是两跳事实回忆任务。

Method: 使用Logit Lens分析LLMs的残差流激活，识别并区分了两种解决此类任务的机制：组合机制（显式计算中间变量）和直接机制（不显式计算中间变量）。

Result: LLMs在两跳事实回忆任务上仍然存在“组合性鸿沟”（即分别计算f(x)和g(z)的能力不代表能计算g(f(x))）。发现存在两种处理机制：一种是组合机制，另一种是直接机制。直接机制在存在从x到g(f(x))的线性映射时占主导地位。

Conclusion: LLMs解决组合任务的机制可能与嵌入空间的几何结构相关，特别是当存在从输入到输出的线性映射时，模型倾向于采用直接机制而非组合机制。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [135] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: LLM在医疗领域的应用中，SFT训练数据中对话轮次分布不均会导致“格式惰性”问题，表现为模型生成重复、格式正确但无效的对话。通过重平衡训练数据中的轮次分布，可以有效缓解此问题。


<details>
  <summary>Details</summary>
Motivation: SFT在医疗领域中用于LLM的多轮对话生成，但SFT数据集的轮次分布不均会导致模型生成重复、格式正确但诊断信息缺乏的对话，即“格式惰性”。

Method: 通过重平衡训练数据中对话轮次的分布来解决“格式惰性”问题。

Result: 实验表明，所提出的方法能够显著缓解医疗预问诊中的“格式惰性”现象。

Conclusion: 通过调整SFT训练数据的轮次分布，可以有效解决LLM在医疗对话中出现的“格式惰性”问题。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [136] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: MathLens是一个用于评估多模态推理模型在几何问题上的子技能的新基准，它将性能分解为感知、推理和整合三个部分，并提供了详细的注释和分析，揭示了不同训练方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理模型评估方法（如总体准确率）未能揭示模型在不同子任务上的具体表现和改进方向，尤其是在奥数级别的几何推理等复杂领域。需要一个更细粒度的评估基准来分析模型在处理多模态信息时的优势和劣势。

Method: 提出并实现了一个名为MathLens的新基准。该基准包含文本风格的几何问题，并将模型性能分解为三个核心子技能：感知（从原始输入中提取信息）、推理（基于现有信息进行操作）和整合（选择相关的感知证据并将其应用于推理）。为支持这些评估，提供了详细的注释，包括视觉图、纯文本描述、需要结合两种模态的受控问题，以及用于精细感知技能探测的探针。所有这些都源于问题的符号规范，以确保一致性和鲁棒性。

Result: 通过在MathLens基准上的实验分析，揭示了不同训练方法的不同效果：1. 强化学习（RL）主要增强感知能力，特别是结合文本监督时；文本监督的微调（SFT）通过反思性推理间接提升感知能力。2. 推理能力的提升总是与感知能力的提升同步发生。3. 整合能力仍然是最薄弱的环节，在其他技能提升后，剩余的错误大多集中在此。4. 鲁棒性方面，RL能提高模型在图表变化下的表现一致性，而多模态SFT则因过拟合而降低了一致性。模型将公开所有数据和实验日志。

Conclusion: MathLens基准能够有效地区分多模态推理的不同子技能，并揭示了不同训练策略对这些技能的影响。强化学习在提升感知和图表鲁棒性方面表现突出，而文本SFT则通过推理促进感知。整合能力是当前模型面临的主要瓶颈。未来的研究应关注如何提升整合能力以及在不同训练策略下如何平衡鲁棒性和性能。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [137] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 工程设计标准被转换为机器可读的本体，用于植物设计和设备选择的质量保证。


<details>
  <summary>Details</summary>
Motivation: 尽管工业界希望实现数字化，但产品规格、产品类型数据表和设计标准仍以文档为中心。

Method: 使用建模模式创建用于文本和表格中知识的模块化本体，并使用本体进行质量保证和设备选择，包括自动验证和语义推理。

Result: 创建的本体可以自动验证特定阀门数据表是否符合行业标准，并确定产品类型是否满足阀门规格。

Conclusion: 基于IDO的模块化本体可以应用于设备选择过程，并展示了其在标准化机构向数字化智能标准过渡方面的潜力。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [138] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: LLMs需要具备在知识范围之外拒绝回答的能力（知识感知拒绝），这对事实可靠性至关重要。现有指标无法准确衡量此能力。我们提出了拒绝指数（RI），它通过计算模型拒绝概率与错误概率之间的 Spearman 秩相关性来衡量 LLM 准确拒绝其不知道的问题的能力。RI 在不同拒绝率下保持稳定，并提供一致的模型排名，独立于模型的整体准确性和拒绝率。RI 揭示了 LLM 的一个被忽视但重要的方面：即使在事实任务中准确率很高，其拒绝行为也可能不可靠和脆弱。因此，在进行全面的事实评估时，需要使用 RI 来补充传统的准确率指标。


<details>
  <summary>Details</summary>
Motivation: 现有的衡量大型语言模型（LLM）知识感知拒绝能力的指标存在缺陷，无法准确反映模型的实际拒绝行为，可能导致评估结果的偏差和不一致。因此，有必要开发一种能够更精确、更可靠地衡量 LLM 知识感知拒绝能力的指标。

Method: 提出了一种名为“拒绝指数”（RI）的新指标，该指标被定义为模型拒绝概率与错误概率之间的 Spearman 秩相关性。为了便于实际测量，设计了一种轻量级的两阶段评估方法，通过在两次标准的评估运行中观察到的拒绝率来估计 RI。

Result: 通过在 16 个模型和 5 个数据集上进行的大量实验表明，RI 能够准确量化模型在事实任务中的内在知识感知拒绝能力。RI 在不同的拒绝率下保持稳定，并提供独立于模型整体准确率和拒绝率的一致模型排名。RI 揭示了 LLM 在事实任务中的一个被忽视但重要的方面：即使准确率很高，其拒绝行为也可能不可靠和脆弱。

Conclusion: RI 指标能够准确、可靠地衡量 LLM 的知识感知拒绝能力，并且优于现有的指标。研究发现 LLM 的拒绝行为可能存在不可靠和脆弱性，这表明在评估 LLM 的事实性时，除了传统的准确率指标外，还应纳入 RI 指标进行更全面的评估。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [139] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 本研究评估了16种无监督指标，用于评估从1000份俄语司法判决书中提取七个语义块的质量，并与7168份专家评审进行了比对。结果显示，词频一致性和覆盖率/块完整性指标与专家评分最吻合，而法律术语密度则显示出强烈的负相关。LLM评估得分的对齐效果一般，表明其在法律文本方面专业性有限。研究结论是，无监督指标可以进行大规模筛选，但不能完全取代人类判断。


<details>
  <summary>Details</summary>
Motivation: 法律自然语言处理领域人工智能的快速发展，需要可扩展的方法来评估从司法判决书中提取文本的质量。

Method: 评估16种无监督指标（包括新公式），用于评估从1000份俄语司法判决书中提取七个语义块的质量，并与7168份专家评审（基于1-5的李克特量表）进行了比对。这些指标涵盖了文档、语义、结构、伪地面真值和特定法律类别，并且不需要预先标注的地面真值。通过自展相关、Lin一致性相关系数（CCC）和平均绝对误差（MAE）进行评估。

Result: 词频一致性（Pearson r = 0.540，Lin CCC = 0.512，MAE = 0.127）和覆盖率/块完整性（Pearson r = 0.513，Lin CCC = 0.443，MAE = 0.139）指标与专家评分最吻合。法律术语密度（Pearson r = -0.479，Lin CCC = -0.079，MAE = 0.394）显示出强烈的负相关。LLM评估得分（平均值=0.849，Pearson r = 0.382，Lin CCC = 0.325，MAE = 0.197）的对齐效果一般，表明其在法律文本方面专业性有限。

Conclusion: 无监督指标，包括基于LLM的方法，可以实现可扩展的筛选，但由于中等相关性和低CCC值，在法律等高风险环境中不能完全取代人类判断。这项工作通过提供无标注的评估工具，推动了法律NLP的发展，并对司法分析和合乎道德的AI部署具有启示意义。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [140] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: LLM生成的垃圾评论难以检测，提出了一种名为FraudSquad的混合模型，通过结合文本嵌入和图变换器来提高检测效果，并在多个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: LLM能够生成高度逼真且具有说服力的垃圾评论，对现有检测系统构成挑战，威胁在线平台的信誉。

Method: 创建了三个使用不同LLM、并结合产品元数据和真实评论生成的垃圾评论数据集。提出了FraudSquad混合检测模型，该模型整合了预训练语言模型的文本嵌入和门控图变换器，用于垃圾节点分类，同时捕获语义和行为信号。

Result: FraudSquad在三个LLM生成的垃圾评论数据集上，在精确率和召回率方面分别超越了最先进的基线高达44.22%和43.01%，并且在两个人工编写的垃圾评论数据集上也取得了良好的效果。此外，FraudSquad的模型尺寸适中，所需的标记训练数据量少。

Conclusion: LLM时代需要适应垃圾邮件检测技术，FraudSquad提供了一个实际可行的检测框架，并强调了开发能够应对LLM生成内容的检测方法的紧迫性。

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


### [141] [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CL

TL;DR: LLMs在解决数学问题时，会因不熟悉的句法结构而失败，即使问题本身并不难。通过改变问题的表述方式，可以提高LLMs的正确率。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学问题解决方面表现出色，但在处理句法结构偏离其训练数据的个例时，常常会失败。本研究旨在识别并解决这种“句法盲点”的系统性故障模式。

Method: 通过重新表述问题，使用源自正确示例的句法模板，并利用基于依赖局部性理论（DLT）的度量来量化句法复杂性，以测试句法结构对模型性能的影响。

Result: 研究发现，通过保留语义但降低结构复杂性的方式重新表述问题，可以使模型给出正确答案。更高的DLT分数与更高的失败率相关，这表明句法复杂性是导致模型出错的一个因素。

Conclusion: LLMs的许多推理错误源于结构失调而非概念难度，并且，关注句法的干预措施可以揭示并减轻这些归纳性故障。

Abstract: Large Language Models (LLMs) demonstrate strong mathematical problem-solving
abilities but frequently fail on problems that deviate syntactically from their
training distribution. We identify a systematic failure mode, syntactic blind
spots, in which models misapply familiar reasoning strategies to problems that
are semantically straightforward but phrased in unfamiliar ways. These errors
are not due to gaps in mathematical competence, but rather reflect a brittle
coupling between surface form and internal representation. To test this, we
rephrase incorrectly answered questions using syntactic templates drawn from
correct examples. These rephrasings, which preserve semantics while reducing
structural complexity, often lead to correct answers. We quantify syntactic
complexity using a metric based on Dependency Locality Theory (DLT), and show
that higher DLT scores are associated with increased failure rates across
multiple datasets. Our findings suggest that many reasoning errors stem from
structural misalignment rather than conceptual difficulty, and that
syntax-aware interventions can reveal and mitigate these inductive failures.

</details>


### [142] [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
*Shicheng Liu,Kai Sun,Lisheng Fu,Xilun Chen,Xinyuan Zhang,Zhaojiang Lin,Rulin Shao,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: SCRIBES是一个利用布局相似性进行网页信息提取的框架，通过生成可复用脚本，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有网页信息提取方法泛化性差或资源消耗大，SCRIBES旨在解决这些问题。

Method: SCRIBES使用强化学习生成可复用脚本，利用同一网站内网页的布局相似性作为奖励信号，并通过合成数据迭代训练。

Result: SCRIBES在脚本质量上超越基线13%，并提高了GPT-4o下游问答准确性4%以上。

Conclusion: SCRIBES能够实现可扩展且资源高效的网页信息提取。

Abstract: Semi-structured content in HTML tables, lists, and infoboxes accounts for a
substantial share of factual data on the web, yet the formatting complicates
usage, and reliably extracting structured information from them remains
challenging. Existing methods either lack generalization or are
resource-intensive due to per-page LLM inference. In this paper, we introduce
SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel
reinforcement learning framework that leverages layout similarity across
webpages within the same site as a reward signal. Instead of processing each
page individually, SCRIBES generates reusable extraction scripts that can be
applied to groups of structurally similar webpages. Our approach further
improves by iteratively training on synthetic annotations from in-the-wild
CommonCrawl data. Experiments show that our approach outperforms strong
baselines by over 13% in script quality and boosts downstream question
answering accuracy by more than 4% for GPT-4o, enabling scalable and
resource-efficient web information extraction.

</details>


### [143] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本论文提出了一种在低资源环境下，使用发展心理学上合理的数据集来训练语言模型和多模态模型的方法，以解决当前模型参数量大、训练数据量远超儿童语言习得数据的问题。实验结果表明，该多模态模型在BabyLM挑战的多模态任务上优于基线模型。此外，论文还研究了如何通过模型融合技术来缓解多模态模型在纯语言任务上表现不佳的问题，并取得了部分效果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉-语言模型参数量巨大，且学习的数据量远超儿童在语言习得过程中接触到的数据量，这篇论文旨在解决这种差异。

Method: 开发了在低资源环境下、使用发展心理学上合理的数据集的纯语言模型和多模态模型。通过实验探索了模型融合（将多模态模型的参数与纯语言模型的参数进行加权线性插值）来维持多模态模型在纯语言任务上的表现。

Result: 多模态模型在BabyLM挑战的多模态任务上优于之前的基线模型。模型融合技术在一定程度上缓解了多模态模型在纯语言基准测试（特别是语法方面）上的表现不佳问题，同时保持了多模态性能。

Conclusion: 多模态模型在纯语言任务上表现不佳是普遍现象，但通过与纯文本模型进行模型融合，可以在一定程度上改善这一状况，并且不会牺牲多模态任务的性能。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [144] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: REPAIR是一个用于大型语言模型（LLM）的终身编辑框架，旨在实现精确、低成本的模型更新，同时保留非目标知识。它通过闭环反馈机制、动态内存管理、频繁的知识融合和局部性保护，解决了传统方法中存在的更新不稳定、冲突和意外副作用等问题。实验表明，REPAIR在提高编辑准确性（10%-30%）和减少知识遗忘方面表现出色，为构建可靠、可扩展和持续进化的LLM提供了稳健的框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的后期训练受到获取新知识或纠正错误成本高昂以及频繁出现的意外副作用的限制。

Method: REPAIR（Robust Editing via Progressive Adaptive Intervention and Reintegration）是一个终身编辑框架，通过闭环反馈机制、动态内存管理、频繁的知识融合和局部性保护来解决LLM更新中的不稳定、冲突和意外副作用问题。

Result: REPAIR在多个模型家族中将编辑准确性提高了10%-30%，并显著减少了知识遗忘。

Conclusion: REPAIR是一个稳健的框架，用于开发可靠、可扩展和持续进化的LLM。

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [145] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: 本篇论文系统性地介绍了奖励模型（RMs）在增强大型语言模型（LLMs）推理能力方面的作用，并全面 survey 了它们在LLM推理中的应用。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RMs）在增强大型语言模型（LLMs）推理能力方面发挥着关键作用，例如在强化学习（RL）中提供训练信号以及在推理时从多个候选中选择最佳答案。

Method: 本文首先回顾了 RMs 的基本概念，包括它们的架构、训练方法和评估技术。然后，探讨了它们在引导生成、选择最优输出、促进数据合成和迭代自我改进以及在基于 RL 的微调中提供训练信号等方面的关键应用。最后，基于现有研究和实证发现，讨论了 RMs 的选择、泛化、评估和增强等关键开放性问题。

Result: 论文对 RMs 的架构、训练和评估进行了回顾，并探讨了其在 LLM 推理中的三大类应用：引导生成与选择、数据合成与自改进、以及 RL 微调中的训练信号。此外，还讨论了 RMs 选择、泛化、评估和增强方面的开放性问题。

Conclusion: 本研究旨在为有效部署和改进 RMs 以提升 LLM 推理能力提供可行的见解。

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [146] [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
*Davide Gabrielli,Simone Sestito,Iacopo Masi*

Main category: cs.CL

TL;DR: ILM是一个统一的框架，用于提高LLM对输入扰动的鲁棒性，并能通过反转模型输出来识别潜在的有毒或不安全的输入触发因素。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM防御机制碎片化且不成熟，与之前的分类器研究不同。为了进一步提高LLM的对抗鲁棒性，需要一个统一的框架。

Method: 提出了一种名为逆向语言模型（ILM）的统一框架，该框架能够同时提高LLM对输入扰动的鲁棒性，并通过反转模型输出来识别潜在的有毒或不安全的输入触发因素。

Result: ILM将LLM从静态生成器转变为可分析和鲁棒的系统，并可能有助于红队测试。

Conclusion: ILM为下一代LLM奠定了基础，这些LLM不仅鲁棒和有根据，而且从根本上更具可控性和可信赖性。

Abstract: The current landscape of defensive mechanisms for LLMs is fragmented and
underdeveloped, unlike prior work on classifiers. To further promote
adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a
unified framework that simultaneously 1) improves the robustness of LLMs to
input perturbations, and, at the same time, 2) enables native grounding by
inverting model outputs to identify potentially toxic or unsafe input triggers.
ILM transforms LLMs from static generators into analyzable and robust systems,
potentially helping RED teaming. ILM can lay the foundation for next-generation
LLMs that are not only robust and grounded but also fundamentally more
controllable and trustworthy. The code is publicly available at
github.com/davegabe/pag-llm.

</details>


### [147] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: Veri-R1是一个在线强化学习框架，通过让LLM与搜索引擎交互并接收奖励信号来提高其规划、检索和推理能力，从而实现更准确、更可靠的声明验证。


<details>
  <summary>Details</summary>
Motivation: 现有的声明验证方法主要依赖提示工程或预设的推理工作流程，缺乏统一的训练范式来提升LLM在线声明验证所需的必要技能，例如迭代证据检索和推理。

Method: Veri-R1框架利用在线强化学习，使LLM能够与搜索引擎进行交互，并根据规划、检索和推理行为接收奖励信号，从而动态地改进其验证过程。

Result: Veri-R1将联合准确性提高了30%，并将证据分数提高了一倍，通常优于更大规模的模型。消融研究进一步揭示了奖励组成部分的影响以及输出对数与标签准确性之间的联系。

Conclusion: 在线强化学习对于精确和忠实的声明验证是有效的，并为LLM赋能的声明验证的未来研究奠定了基础。

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [148] [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
*Adina Nicola Dobrinoiu,Ana Cristiana Marcu,Amir Homayounirad,Luciano Cavalcante Siebert,Enrico Liscio*

Main category: cs.CL

TL;DR: 语言模型可以通过利用多维度的 SEAT（情感、情绪、论点、主题）主观注释来预测个体价值观，并且同时提供所有 SEAT 维度比单独提供维度或不提供信息效果更好。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够符合不同人类观点且避免多数观点的偏见的 AI 系统，需要识别个体价值观的解释。

Method: 本研究通过实验评估了语言模型在预测个体价值观时，利用多维度的 SEAT（情感、情绪、论点、主题）主观注释作为其解释的代理。

Result: 实验表明，同时提供所有 SEAT 维度比单独提供维度或不提供信息能带来更好的预测效果。此外，注释者之间的个体差异凸显了考虑个体主观注释纳入的重要性。

Conclusion: 本研究首次尝试超越人口统计学因素，研究注释行为对价值观预测的影响，为未来大规模验证奠定了基础。

Abstract: Our interpretation of value concepts is shaped by our sociocultural
background and lived experiences, and is thus subjective. Recognizing
individual value interpretations is important for developing AI systems that
can align with diverse human perspectives and avoid bias toward majority
viewpoints. To this end, we investigate whether a language model can predict
individual value interpretations by leveraging multi-dimensional subjective
annotations as a proxy for their interpretive lens. That is, we evaluate
whether providing examples of how an individual annotates Sentiment, Emotion,
Argument, and Topics (SEAT dimensions) helps a language model in predicting
their value interpretations. Our experiment across different zero- and few-shot
settings demonstrates that providing all SEAT dimensions simultaneously yields
superior performance compared to individual dimensions and a baseline where no
information about the individual is provided. Furthermore, individual
variations across annotators highlight the importance of accounting for the
incorporation of individual subjective annotators. To the best of our
knowledge, this controlled setting, although small in size, is the first
attempt to go beyond demographics and investigate the impact of annotation
behavior on value prediction, providing a solid foundation for future
large-scale validation.

</details>


### [149] [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
*Ryosuke Kohita*

Main category: cs.CL

TL;DR: 模式设计（特别是范式化）对自然语言到SQL（NL2SQL）系统的影响至关重要。研究表明，反范式化模式在简单检索查询中表现良好，而范式化模式（2NF/3NF）在聚合查询中表现更佳，但需要少量示例来优化。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL研究大多在固定模式下进行评估，忽视了模式设计对模型性能的影响。本研究旨在系统地探讨模式范式化对NL2SQL模型性能的影响。

Method: 评估了八种主流大语言模型在不同范式化水平的合成和真实世界数据集上的表现。构建了具有不同范式化级别（1NF-3NF）的合成数据集，并使用了学术论文领域的真实数据集。

Result: 反范式化模式在简单检索查询中，即使使用成本较低的模型和零样本学习，也能获得高准确率。范式化模式（2NF/3NF）在基础表选择和连接类型预测方面存在挑战，但可以通过提供少量示例来缓解。对于聚合查询，范式化模式表现更好，因为它能有效避免反范式化模式中常见的数据重复和NULL值问题。

Conclusion: NL2SQL应用的最佳模式设计取决于查询类型。模式设计对于NL2SQL接口的开发至关重要，建议在实际应用中结合自适应模式选择。

Abstract: Schema design, particularly normalization, is a critical yet often overlooked
factor in natural language to SQL (NL2SQL) systems. Most prior research
evaluates models on fixed schemas, overlooking the influence of design on
performance. We present the first systematic study of schema normalization's
impact, evaluating eight leading large language models on synthetic and
real-world datasets with varied normalization levels. We construct controlled
synthetic datasets with formal normalization (1NF-3NF) and real academic paper
datasets with practical schemes. Our results show that denormalized schemas
offer high accuracy on simple retrieval queries, even with cost-effective
models in zero-shot settings. In contrast, normalized schemas (2NF/3NF)
introduce challenges such as errors in base table selection and join type
prediction; however, these issues are substantially mitigated by providing
few-shot examples. For aggregation queries, normalized schemas yielded better
performance, mainly due to their robustness against the data duplication and
NULL value issues that cause errors in denormalized schemas. These findings
suggest that the optimal schema design for NL2SQL applications depends on the
types of queries to be supported. Our study demonstrates the importance of
considering schema design when developing NL2SQL interfaces and integrating
adaptive schema selection for real-world scenarios.

</details>


### [150] [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
*Md Arid Hasan,Firoj Alam,Md Fahad Hossain,Usman Naseem,Syed Ishtiaque Ahmed*

Main category: cs.CL

TL;DR: 我们发布了一个多任务孟加拉语仇恨言论数据集BanglaMultiHate，并评估了各种模型（包括LLM）在该数据集上的表现，发现在低资源场景下，基于语言和文化预训练的模型仍然至关重要。


<details>
  <summary>Details</summary>
Motivation: 在线社交媒体平台上的仇恨言论、冒犯性语言和欺凌内容日益增多，尤其是在孟加拉语等低资源语言中，相关的检测工具却很有限。因此，需要开发能够覆盖多方面信号（类型、严重程度、目标）的可靠检测系统。

Method: 构建了一个名为BanglaMultiHate的多任务孟加拉语仇恨言论数据集，这是迄今为止最大的人工标注语料库之一。在此基础上，对包括传统基线模型、单语预训练模型和LLM（在零样本提示和LoRA微调下）进行了全面的、受控的比较。

Result: 实验评估了LLM在低资源环境下的适应性，结果显示，经过LoRA微调的LLM虽然能与BanglaBERT竞争，但基于语言和文化预训练的模型在鲁棒性方面仍然至关重要。

Conclusion: BanglaMultiHate数据集和相关研究为在低资源环境下开发符合文化背景的审核工具奠定了更强的基准。研究强调了在低资源场景下，结合语言和文化特点的预训练模型对于仇恨言论检测的鲁棒性仍然是关键因素。

Abstract: Online social media platforms are central to everyday communication and
information seeking. While these platforms serve positive purposes, they also
provide fertile ground for the spread of hate speech, offensive language, and
bullying content targeting individuals, organizations, and communities. Such
content undermines safety, participation, and equity online. Reliable detection
systems are therefore needed, especially for low-resource languages where
moderation tools are limited. In Bangla, prior work has contributed resources
and models, but most are single-task (e.g., binary hate/offense) with limited
coverage of multi-facet signals (type, severity, target). We address these gaps
by introducing the first multi-task Bangla hate-speech dataset,
BanglaMultiHate, one of the largest manually annotated corpus to date. Building
on this resource, we conduct a comprehensive, controlled comparison spanning
classical baselines, monolingual pretrained models, and LLMs under zero-shot
prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a
low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs
are competitive with BanglaBERT, culturally and linguistically grounded
pretraining remains critical for robust performance. Together, our dataset and
findings establish a stronger benchmark for developing culturally aligned
moderation tools in low-resource contexts. For reproducibility, we will release
the dataset and all related scripts.

</details>


### [151] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: LLMs在创意生成方面，研究者更多关注的是输出质量而非生成过程，本研究提出一种新方法，将LLMs视为计算型作者，利用叙事学和约束式决策来分析其作者创意偏好，发现LLMs更偏爱“风格”而非“角色”、“事件”或“场景”，并能解释其选择原因，为分析AI作者创意提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 以往对大型语言模型（LLMs）创意的评估主要集中在其输出的质量上，而忽略了塑造这些输出的内部过程。本研究旨在采取一种面向过程的方法，借鉴叙事学理论，将LLMs视为计算型作者进行研究。

Method: 本研究将基于约束的决策制定作为作者创意的切入点。通过使用可控的提示来分配作者身份，分析模型在创意上的偏好。此外，研究还探究了模型解释其选择的原因。

Result: 研究结果显示，在创意元素的偏好上，LLMs一致地强调“风格”，而在“角色”、“事件”和“场景”等其他元素上的比重较低。通过探究模型做出选择的原因，研究发现不同模型展现出独特的偏好构成。

Conclusion: 本研究提出了一种新颖的、系统的分析人工智能（AI）作者创意的方法，通过关注创作过程而非仅仅是结果，并利用叙事学和约束式决策理论，揭示了LLMs在创意偏好上的独特性，特别是对“风格”的偏好，为理解和评估AI的创作能力提供了新的视角。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


### [152] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: 提出了一种名为Streaming RAG的框架，用于解决端到端语音对话系统中的幻觉问题，并通过并行预测工具查询来减少延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端语音对话系统虽然延迟低且响应自然，但容易出现幻觉；而文本对话系统虽然可以通过工具集成解决幻觉问题，但会增加延迟，破坏对话流程。本研究旨在将工具使用直接扩展到语音对话系统，并解决由此产生的延迟问题。

Method: 提出了一种名为Streaming Retrieval-Augmented Generation (Streaming RAG) 的新框架，通过在用户说话时并行预测工具查询来减少感知延迟。该框架包括一个后训练流程，用于教会模型何时发出工具调用以及如何生成融合语音查询和检索文本结果的口头摘要。

Result: 在AudioCRAG基准测试中，Streaming RAG方法将问答准确率相对提高了200%（从11.1%提高到34.2%），并将工具使用延迟降低了20%。

Conclusion: Streaming RAG框架成功地解决了端到端语音对话系统中的幻觉问题，并在降低延迟和提高准确率方面取得了显著效果。该方法具有模态无关性，可应用于语音和文本输入，为开发更智能、实时的AI助手铺平了道路。

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [153] [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: SCoT是一个用于全双工对话系统的流式思维链框架，它通过分块处理用户输入和生成响应来克服现有模型的复杂性和语义推理不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端对话系统依赖于语音活动检测（VAD），但VAD无法区分暂停和用户意图的结束。全双工对话系统虽然解决了这个问题，但通常拥有复杂的双通道架构，并且在语义推理方面落后于级联模型。

Method: 提出了一种名为SCoT（Streaming Chain-of-Thought）的框架，该框架采用分块的方式，交替处理固定时长的用户输入并生成响应。通过帧级对齐，为每个块创建了与中间目标对齐的用户输入和系统响应。

Result: 实验表明，SCoT框架能够生成比现有全双工方法更连贯、更易于理解的响应，并且与逐轮对话系统相比，具有更低的延迟和更好的重叠交互支持。

Conclusion: SCoT框架通过分块处理和帧级对齐，有效解决了全双工对话系统中的复杂性和语义推理问题，并在性能和交互性方面取得了显著的改进。

Abstract: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity
detection (VAD) for turn-taking, but VAD fails to distinguish between pauses
and turn completions. Duplex SDS models address this by predicting output
continuously, including silence tokens, thus removing the need for explicit
VAD. However, they often have complex dual-channel architecture and lag behind
cascaded models in semantic reasoning. To overcome these challenges, we propose
SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating
between processing fixed-duration user input and generating responses in a
blockwise manner. Using frame-level alignments, we create intermediate
targets-aligned user transcripts and system responses for each block.
Experiments show that our approach produces more coherent and interpretable
responses than existing duplex methods while supporting lower-latency and
overlapping interactions compared to turn-by-turn systems.

</details>


### [154] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet Üstün,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: 投机解码在语言模型中通过使用小型“起草”模型来加速推理，但其加速效果在不同任务间存在差异，尤其在欠拟合任务上效果不佳。本文分析了这种“不公平”现象，并提出了一种改进策略，平均将公平性指标提高了12%。


<details>
  <summary>Details</summary>
Motivation: 分析投机解码在不同任务上存在速度提升不均一的现象，并探究其原因。

Method: 推导分析来量化“不公平”现象，并提出一种旨在减少速度提升差异的缓解策略。

Result: 提出的缓解策略在多个模型对上进行了验证，平均将公平性指标提高了12%。

Conclusion: 投机解码的速度提升在不同任务上存在差异，但可以通过提出的策略来缓解这种不公平性。

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [155] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: 通过引入RESTRAIN框架，利用无标签数据提升大型推理模型的链式思考能力，实现了与有标签数据训练相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人类标注数据的强化学习方法在提升模型推理能力方面虽然有效，但成本高昂且在难题上表现不佳。因此，需要一种不需要精心标注的标签，而是通过经验驱动来学习的方法。

Method: 提出RESTRAIN（Self-restraint的强化学习）框架，通过自我惩罚机制将缺乏黄金标签的状况转化为有用的学习信号。该机制通过惩罚过于自信的输出和一致性低的样本，同时保留有希望的推理链，来利用模型整个答案的分布。这种自我惩罚机制可以无缝集成到GRPO等策略优化方法中，实现无监督的持续自我改进。

Result: 在AIME25、MMLU_STEM和GPQA-Diamond等具有挑战性的推理基准上，使用RESTRAIN（仅使用无标签数据）将Pass@1指标分别提高了+140.7%、+36.2%和+19.6%，性能接近于使用黄金标签进行训练的模型。

Conclusion: RESTRAIN框架为在没有黄金标签的情况下提升模型推理能力提供了一条可扩展的路径。

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [156] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: LLMs 容易产生幻觉，此研究提出了一种基于强化学习的框架 RL4HS，通过引入跨链推理和区间级奖励来提高幻觉检测的准确性，并在 RAGTruth 基准测试中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 现有关于幻觉检测的研究多将其视为二元分类任务，未能满足现实世界中识别幻觉片段的需求，因此需要探索更有效的检测方法，特别是利用显式推理能力。

Method: 首先评估了带有和不带有链式思考（CoT）推理的预训练模型，然后提出了一种名为 RL4HS 的强化学习框架，该框架利用区间级奖励函数来激励推理过程，并采用群组相对策略优化（Group Relative Policy Optimization）和类别感知策略优化（Class-Aware Policy Optimization）来解决奖励不平衡问题。

Result: RL4HS 在 RAGTruth 基准测试（包括摘要、问答和数据到文本任务）上，其性能优于预训练推理模型和监督微调方法，证明了强化学习结合区间级奖励在幻觉片段检测中的有效性。

Conclusion: 强化学习方法，特别是 RL4HS 框架，通过引入推理和区间级奖励，能够显著提高大语言模型幻觉片段检测的准确性，克服了传统方法的局限性。

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [157] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: LLM驱动的SPINACH代理通过迭代探索和执行将自然语言问题转换为SPARQL查询，解决了Text2SPARQL的挑战。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询语言入门门槛高，阻碍了非计算机科学背景用户与知识图谱的交互。

Method: 提出了一种基于SPINACH的通用方法，该方法利用LLM驱动的代理，通过迭代探索和执行，将自然语言问题转换为SPARQL查询，而非一次性完成。

Result: 对代理行为进行了详尽的分析，为未来有针对性的改进提供了见解。

Conclusion: SPINACH代理通过迭代过程有效降低了Text2SPARQL的门槛，并为该领域未来的改进提供了方向。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [158] [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
*Lingzhong Dong,Ziqi Zhou,Shuaibo Yang,Haiyue Sheng,Pengzhou Cheng,Zongru Wu,Zheng Wu,Gongshen Liu,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 现有的移动端VLM评估忽略了思维链（CoT）推理与实际执行动作之间的一致性，可能导致过度信任和潜在风险。本研究提出了一个新的评估框架，通过地面真实对齐（GTA）指标来衡量CoT推理与地面真实动作的一致性，并结合精确匹配（EM）指标，共同评估推理和执行的准确性。该框架识别出两种推理-执行差距：执行差距（EG）和推理差距（RG）。实验表明，这些差距普遍存在，EG比RG更常见，并且在大型模型中仍然存在。该框架能有效识别模型中的系统性差距模式，有助于开发更可信赖的移动端VLM。


<details>
  <summary>Details</summary>
Motivation: 现有的移动端VLM（视觉-语言模型）评估主要关注执行准确性，而忽略了思维链（CoT）推理过程是否与地面真实（ground-truth）动作对齐。这种忽视可能导致用户对看似合理的CoT产生过度信任，从而在不知情的情况下授权有害动作，引发经济损失或信任危机。

Method: 提出了一种新的评估框架，其核心是地面真实对齐（GTA）指标，用于衡量CoT推理所隐含的动作是否与地面真实动作匹配。该框架结合了GTA和标准的精确匹配（EM）指标，共同评估推理准确性和执行准确性，从而诊断推理-执行差距。

Result: 通过对一系列移动端交互任务进行实验，发现推理-执行差距普遍存在，其中执行差距（EG）比推理差距（RG）更常见。虽然增大模型规模可以减小总体差距，但即使是最大的模型也存在显著的执行差距。此外，该评估框架能够可靠地反映最先进模型中存在的系统性EG/RG模式。

Conclusion: 推理-执行差距在移动端VLM中普遍存在，特别是执行差距。尽管模型规模的扩大有助于减小差距，但仍需进一步努力来解决这个问题。本研究提出的评估框架能够为开发更可信赖的移动端VLM提供具体的诊断依据和支持。

Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great
potential in interpreting natural language instructions and generating
corresponding actions based on mobile graphical user interface. Recent studies
suggest that incorporating chain-of-thought (CoT) reasoning tends to improve
the execution accuracy. However, existing evaluations emphasize execution
accuracy while neglecting whether CoT reasoning aligns with ground-truth
actions. This oversight fails to assess potential reasoning-execution gaps,
which in turn foster over-trust: users relying on seemingly plausible CoTs may
unknowingly authorize harmful actions, potentially resulting in financial loss
or trust crisis. In this work, we introduce a new evaluation framework to
diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment
(GTA), which measures whether the action implied by a CoT matches the
ground-truth action. By combining GTA with the standard Exact Match (EM)
metric, we jointly assess both the reasoning accuracy and execution accuracy.
This joint perspective reveals two types of reasoning-execution gaps: (i)
Execution Gap (EG), where the reasoning correctly identifies the correct action
but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but
reasoning process conflicts with the actual execution. Experimental results
across a wide range of mobile interaction tasks reveal that reasoning-execution
gaps are prevalent, with execution gaps occurring more frequently than
reasoning gaps. Moreover, while scaling up model size reduces the overall gap,
sizable execution gaps persist even in the largest models. Further analysis
shows that our framework reliably reflects systematic EG/RG patterns in
state-of-the-art models. These findings offer concrete diagnostics and support
the development of more trustworthy mobile-use agents.

</details>


### [159] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [160] [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
*Ebtesam Jaber Aljohani,Wael M. S. Yafoo*

Main category: cs.CL

TL;DR: 该研究提出并评估了检测阿拉伯语网络欺凌的方法，通过构建数据集和应用深度学习模型，达到了98%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于社交媒体的兴起，网络欺凌对年轻人的情感健康构成了威胁，但目前针对阿拉伯语的网络欺凌检测方法和资源却很少。

Method: 构建了一个包含10,662条X（前身为Twitter）帖子的阿拉伯语数据集，并对其进行预处理和质量增强。随后，测试了多种深度学习模型，包括LSTM和Bi-LSTM，并结合了不同的词嵌入技术（如FastText）和预训练的BERT模型。

Result: 在对LSTM和Bi-LSTM模型进行多组实验后，LSTM-BERT和Bi-LSTM-BERT模型达到了97%的准确率。而Bi-LSTM模型结合FastText词嵌入的实验效果最佳，准确率达到了98%。

Conclusion: 研究成功地提升了阿拉伯语网络欺凌检测的有效性，并验证了深度学习模型（特别是Bi-LSTM与FastText结合）在此任务上的潜力。

Abstract: Recent technological advances in smartphones and communications, including
the growth of such online platforms as massive social media networks such as X
(formerly known as Twitter) endangers young people and their emotional
well-being by exposing them to cyberbullying, taunting, and bullying content.
Most proposed approaches for automatically detecting cyberbullying have been
developed around the English language, and methods for detecting
Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language
cyberbullying are especially scarce. This paper aims to enhance the
effectiveness of methods for detecting cyberbullying in Arabic-language
content. We assembled a dataset of 10,662 X posts, pre-processed the data, and
used the kappa tool to verify and enhance the quality of our annotations. We
conducted four experiments to test numerous deep learning models for
automatically detecting Arabic-language cyberbullying. We first tested a long
short-term memory (LSTM) model and a bidirectional long short-term memory
(Bi-LSTM) model with several experimental word embeddings. We also tested the
LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from
representations (BERT) and then tested them on a different experimental models
BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM
with FastText embedding word performed even better, achieving 98% accuracy. As
a result, the outcomes are generalize

</details>


### [161] [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
*Linh The Nguyen,Chi Tran,Dung Ngoc Nguyen,Van-Cuong Pham,Hoang Ngo,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: AccurateRAG是一个用于构建高性能检索增强生成（RAG）问答应用程序的新框架，能够提高开发效率并超越现有基线，在基准数据集上取得新的最先进的问答性能。


<details>
  <summary>Details</summary>
Motivation: 介绍一个用于构建高性能检索增强生成（RAG）问答应用程序的新框架。

Method: 该框架提供了一个用于开发效率的流程，包括原始数据集处理、微调数据生成、文本嵌入和LLM微调、输出评估以及本地构建RAG系统。

Result: 实验结果表明，该框架的性能优于先前强大的基线。

Conclusion: AccurateRAG在基准数据集上取得了新的最先进的问答性能。

Abstract: We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

</details>


### [162] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: LLM在处理简单问题时会过度思考，导致推理过程冗长。本文提出了一种名为TECA的新指标和一种名为“先探索，后决策”的推理范式，并结合累积熵调节（CER）机制，动态决定推理的结束点，从而提高效率，减少高达71%的响应长度，同时不牺牲解决问题的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理复杂问题时表现出强大的推理能力，但它们常常会过度思考，为简单问题生成不必要的冗长推理步骤，这会降低模型的效率并使其难以适应问题的复杂度。

Method: 提出了一种名为Token Entropy Cumulative Average（TECA）的新指标来衡量推理过程中的探索程度，并提出了一种新的推理范式——“先探索，后决策”——以及相关的累积熵调节（CER）机制，利用TECA动态确定推理过程的最优点。

Result: 在各种数学基准测试中的实验结果表明，该方法在不牺牲问题解决能力的情况下，显著减轻了过度思考的问题。与基线方法相比，在较简单的测试集中，平均响应长度减少了多达71%。

Conclusion: 本文提出的TECA指标和“先探索，后决策”推理范式及其CER机制，能够有效地解决大型语言模型在简单问题上的过度思考问题，显著提高推理效率，并能根据问题复杂度自适应地调整推理深度，同时保持或提高问题的解决能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [163] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: LLM代理在处理需要专业领域知识或集成多种信息来源的复杂任务时面临挑战。InfoMosaic-Bench是一个新的基准测试，用于评估工具增强型代理的多源信息检索能力，揭示了当前LLM在利用工具方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理严重依赖开放网络搜索，存在信息噪声大、无法满足专业领域知识需求的问题。虽然模型上下文协议（MCP）允许代理使用专业工具，但代理能否有效利用这些工具并将其与通用搜索结合以解决复杂任务尚不清楚。

Method: 提出了InfoMosaic-Bench，这是第一个专门针对工具增强型代理的多源信息检索基准。该基准覆盖医学、金融、地图、视频、网络和多领域集成等六个领域，要求代理结合通用搜索和领域专用工具。InfoMosaic-Flow流水线用于生成任务，确保可靠性和非平凡性。

Result: 在InfoMosaic-Bench上的实验表明：1.单独的网络信息不足以解决复杂任务，GPT-5的准确率仅为38.2%。2.领域工具带来的好处具有选择性且不稳定，在某些领域有所改善，在另一些领域则导致性能下降。3.22.4%的失败源于不正确的工具使用或选择，表明当前LLM在基本的工具处理方面仍有困难。

Conclusion: 当前LLM代理在整合通用搜索和领域专用工具以解决复杂的多源信息检索任务方面存在显著挑战。虽然专业工具可以提供帮助，但其有效性并不一致，并且LLM在正确选择和使用工具方面仍需改进。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [164] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: 本研究从跨语言视角探讨了大型推理模型（LRM）的推理能力泛化问题，特别是英语强化后训练（RPT）能力向其他语言的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 评估英语 RPT 对 LRM 推理能力向其他语言的迁移效果，并探讨影响迁移的关键因素。

Method: 系统评估以英语为中心的 LRM 在多语言推理基准上的表现，引入量化跨语言迁移率的指标，并进行干预研究和平行训练研究。

Result: 发现跨语言迁移率因初始模型、目标语言和训练范式而异。研究表明，模型过度依赖英语特定模式会削弱跨语言泛化能力。提出了“平行飞跃”（Parallel Leap）现象，即从单语转向仅一种平行语时性能大幅提升。发现了“平行缩放定律”（Parallel Scaling Law），即跨语言推理迁移遵循关于平行语数量的幂定律。识别出“单语泛化差距”（Monolingual Generalization Gap），即实际单语表现与幂定律预测之间的差异。

Conclusion: LRM 的推理能力并非完全等同于人类认知，当前的英语中心 LRM 在跨语言泛化方面存在不足，为开发更具语言无关性的 LRM 提供了重要见解。

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [165] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个用于基准测试、分析和解释视觉语言模型（VLMs）的工具包，支持从开放源代码VLMs的任何层提取中间输出。


<details>
  <summary>Details</summary>
Motivation: 为了系统地基准测试、分析和解释视觉语言模型（VLMs），并支持从开放源代码VLMs的任何层提取中间输出。

Method: VLM-Lens提供了一个统一的、可YAML配置的接口，该接口抽象了特定于模型的复杂性，并支持跨不同VLMs的用户友好操作。它目前支持16个最先进的基础VLMs及其30多个变体，并且可以扩展以适应新模型而无需更改核心逻辑。

Result: 通过两个简单的分析实验，揭示了VLMs在不同层和目标概念之间隐藏表示的系统性差异。

Conclusion: VLM-Lens是一个开源项目，旨在加速社区在理解和改进VLMs方面的努力。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


### [166] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: F2LLM是一个包括0.6B、1.7B和4B三种尺寸的嵌入模型系列，通过在600万查询-文档-负例元组上进行微调，实现了训练成本、模型尺寸和嵌入性能之间的平衡，并在MTEB英语排行榜上取得了优异的成绩。


<details>
  <summary>Details</summary>
Motivation: 先前的顶尖嵌入模型需要大量的对比预训练、复杂的训练流程和昂贵的合成训练数据，而F2LLM旨在通过直接从基础模型微调，降低训练成本并保持高性能。

Method: F2LLM模型直接从基础模型微调而来，使用了600万从开源、非合成数据集中精选的查询-文档-负例元组进行训练。

Result: 在MTEB英语排行榜上，F2LLM-4B在约4B参数模型中排名第二，综合排名第七；F2LLM-1.7B在1B-2B尺寸模型中排名第一。

Conclusion: F2LLM提供了一个具有成本效益、性能优越且可复现的嵌入模型基线，有利于未来的研究。

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


### [167] [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
*Raphael Tang,Crystina Zhang,Wenyan Li,Carmen Lai,Pontus Stenetorp,Yao Lu*

Main category: cs.CL

TL;DR: 在LLM竞技场评估中，将平局视为平局而非平局，会提高对战斗结果的预测准确性，并建议未来的评级系统应考虑查询难度。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的现有方法（如Elo评级系统）假设平局意味着两个模型的能力相当，但本文认为平局可能更多地表明查询的难度。

Method: 通过分析三个真实世界的竞技场数据集，比较了在考虑和忽略平局的评分更新后，四种不同评分系统对战斗结果的预测准确性。此外，还分析了查询难度和客观性与平局发生率之间的关系。

Result: 忽略平局的评分更新将战斗结果的预测准确性（包括平局）提高了1-3%。研究还发现，查询越简单（风险比1.37）和越客观（风险比1.35），平局越有可能发生。

Conclusion: 现有的LLM评级系统应重新考虑平局的含义，并建议未来的系统在进行评分更新时应纳入查询的属性，例如难度和客观性。

Abstract: In arena-style evaluation of large language models (LLMs), two LLMs respond
to a user query, and the user chooses the winning response or deems the
"battle" a draw, resulting in an adjustment to the ratings of both models. The
prevailing approach for modeling these rating dynamics is to view battles as
two-player game matches, as in chess, and apply the Elo rating system and its
derivatives. In this paper, we critically examine this paradigm. Specifically,
we question whether a draw genuinely means that the two models are equal and
hence whether their ratings should be equalized. Instead, we conjecture that
draws are more indicative of query difficulty: if the query is too easy, then
both models are more likely to succeed equally. On three real-world arena
datasets, we show that ignoring rating updates for draws yields a 1-3% relative
increase in battle outcome prediction accuracy (which includes draws) for all
four rating systems studied. Further analyses suggest that draws occur more for
queries rated as very easy and those as highly objective, with risk ratios of
1.37 and 1.35, respectively. We recommend future rating systems to reconsider
existing draw semantics and to account for query properties in rating updates.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [168] [Learning to Play Multi-Follower Bayesian Stackelberg Games](https://arxiv.org/abs/2510.01387)
*Gerson Personnat,Tao Lin,Safwan Hossain,David C. Parkes*

Main category: cs.GT

TL;DR: 本文研究了多跟随者贝叶斯Stackelberg博弈中的在线学习问题，提出了一系列学习算法以最小化领导者的遗憾。


<details>
  <summary>Details</summary>
Motivation: 在多跟随者贝叶斯Stackelberg博弈中，领导者的最优策略依赖于跟随者的类型分布。本文旨在解决领导者在未知类型分布下进行交互学习的问题，以最小化其遗憾。

Method: 在有“类型反馈”（领导者知道跟随者类型）的情况下，设计了两种算法，分别适用于独立类型分布和一般类型分布，并给出了相应的遗憾界。在有“动作反馈”（领导者只知道跟随者动作）的情况下，也设计了相应的算法并给出了遗憾界。此外，还提供了一个接近类型反馈算法的下界。

Result: 在类型反馈下，算法实现了 $\mathcal O\big(\sqrt{\min\{L\log(nK A T), nK \} \cdot T} \big)$ 和 $\mathcal O\big(\sqrt{\min\{L\log(nK A T), K^n \} \cdot T} \big)$ 的遗憾界，该界不随 $n$ 呈多项式增长。在动作反馈下，遗憾界为 $\mathcal O( \min\{\sqrt{ n^L K^L A^{2L} L T \log T}, K^n\sqrt{ T } \log T \} )$。最后，论文给出了一个 $\Omega(\sqrt{\min\{L, nK\}T})$ 的下界。

Conclusion: 本文针对不同反馈设定下的多跟随者贝叶斯Stackelberg博弈，设计了有效的在线学习算法，并在理论上界定了领导者的遗憾，同时给出了接近最优的下界。

Abstract: In a multi-follower Bayesian Stackelberg game, a leader plays a mixed
strategy over $L$ actions to which $n\ge 1$ followers, each having one of $K$
possible private types, best respond. The leader's optimal strategy depends on
the distribution of the followers' private types. We study an online learning
version of this problem: a leader interacts for $T$ rounds with $n$ followers
with types sampled from an unknown distribution every round. The leader's goal
is to minimize regret, defined as the difference between the cumulative utility
of the optimal strategy and that of the actually chosen strategies. We design
learning algorithms for the leader under different feedback settings. Under
type feedback, where the leader observes the followers' types after each round,
we design algorithms that achieve $\mathcal O\big(\sqrt{\min\{L\log(nKA T), nK
\} \cdot T} \big)$ regret for independent type distributions and $\mathcal
O\big(\sqrt{\min\{L\log(nKA T), K^n \} \cdot T} \big)$ regret for general type
distributions. Interestingly, those bounds do not grow with $n$ at a polynomial
rate. Under action feedback, where the leader only observes the followers'
actions, we design algorithms with $\mathcal O( \min\{\sqrt{ n^L K^L A^{2L} L T
\log T}, K^n\sqrt{ T } \log T \} )$ regret. We also provide a lower bound of
$\Omega(\sqrt{\min\{L, nK\}T})$, almost matching the type-feedback upper
bounds.

</details>


### [169] [Designing Inferable Signaling Schemes for Bayesian Persuasion](https://arxiv.org/abs/2510.01434)
*Caleb Probine,Mustafa O. Karabag,Ufuk Topcu*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In Bayesian persuasion, an informed sender, who observes a state, commits to
a randomized signaling scheme that guides a self-interested receiver's actions.
Classical models assume the receiver knows the commitment. We, instead, study
the setting where the receiver infers the scheme from repeated interactions. We
bound the sender's performance loss relative to the known-commitment case by a
term that grows with the signal space size and shrinks as the receiver's
optimal actions become more distinct. We then lower bound the samples required
for the sender to approximately achieve their known-commitment performance in
the inference setting. We show that the sender requires more samples in
persuasion compared to the leader in a Stackelberg game, which includes
commitment but lacks signaling. Motivated by these bounds, we propose two
methods for designing inferable signaling schemes, one being stochastic
gradient descent (SGD) on the sender's inference-setting utility, and the other
being optimization with a boundedly-rational receiver model. SGD performs best
in low-interaction regimes, but modeling the receiver as boundedly-rational and
tuning the rationality constant still provides a flexible method for designing
inferable schemes. Finally, we apply SGD to a safety alert example and show it
to find schemes that have fewer signals and make citizens' optimal actions more
distinct compared to the known-commitment case.

</details>


### [170] [Incentive Analysis of Collusion in Fair Division](https://arxiv.org/abs/2510.01689)
*Haoqiang Huang,Biaoshuai Tao,Mingwei Yang,Shengwei Zhou*

Main category: cs.GT

TL;DR: 研究了具有策略性代理人的公平划分问题，并定义了强群体激励比 (SGIR) 和群体激励比 (GIR) 来衡量串通操纵的收益，同时对最大纳什福利 (MNW)、概率序列 (PS) 和轮转法 (RR) 进行了表征。


<details>
  <summary>Details</summary>
Motivation: 研究具有策略性代理人的公平划分问题，以了解串通操纵的影响。

Method: 定义强群体激励比 (SGIR) 和群体激励比 (GIR)，并表征 MNW、PS 和 RR 的 SGIR 和 GIR。

Result: MNW 的 GIR 为 2，MNW 和 PS 的 SGIR（对于 c >= 1）为 c + 1，PS 和 RR 的 GIR（对于 c >= 1）为 c + 1，RR 的 SGIR（对于 c >= 2）无界。

Conclusion: MNW、PS 和 RR 在易受串通影响方面存在根本差异。

Abstract: We study fair division problems with strategic agents capable of gaining
advantages by manipulating their reported preferences. Although several
impossibility results have revealed the incompatibility of truthfulness with
standard fairness criteria, subsequent works have circumvented this limitation
through the incentive ratio framework. Previous studies demonstrate that
fundamental mechanisms like Maximum Nash Welfare (MNW) and Probabilistic Serial
(PS) for divisible goods, and Round-Robin (RR) for indivisible goods achieve an
incentive ratio of $2$, implying that no individual agent can gain more than
double his truthful utility through manipulation. However, collusive
manipulation by agent groups remains unexplored.
  In this work, we define strong group incentive ratio (SGIR) and group
incentive ratio (GIR) to measure the gain of collusive manipulation, where SGIR
and GIR are respectively the maximum and minimum of the incentive ratios of
corrupted agents. Then, we tightly characterize the SGIRs and GIRs of MNW, PS,
and RR. In particular, the GIR of MNW is $2$ regardless of the coalition size.
Moreover, for coalition size $c \geq 1$, the SGIRs of MNW and PS, and the GIRs
of PS and RR are $c + 1$. Finally, the SGIR of RR is unbounded for coalition
size $c \geq 2$. Our results reveal fundamental differences of these three
mechanisms in their vulnerability to collusion.

</details>


### [171] [A Linear Programming Approach to Estimate the Core in Cooperative Games](https://arxiv.org/abs/2510.01766)
*J Camacho,JC Gonçalves-Dosantos,J Sánchez-Soriano*

Main category: cs.GT

TL;DR: 本文提出一种通过线性规划近似可转移合作对策核心的新算法。


<details>
  <summary>Details</summary>
Motivation: 计算判定合作对策核心的计算难度。

Method: 通过抽样极端点和随机化线性问题（LP）来提供一种可行的近似方法。

Result: 该方法是可扩展的，并且在核心重建方面实现了高精度。

Conclusion: 所提出的算法能够有效地近似可转移合作对策的核心。

Abstract: This paper proposes a novel algorithm to approximate the core of transferable
utility (TU) cooperative games via linear programming. Given the computational
hardness of determining the full core, our approach provides a tractable
approximation by sampling extreme points through randomized linear problems
(LPs). We analyze its convergence and computational complexity, and validate
its effectiveness through extensive simulations on various game models. Our
results show that the method is scalable and achieves high accuracy in terms of
core reconstruction.

</details>


### [172] [Multi-group Bayesian Games](https://arxiv.org/abs/2510.02078)
*Hongxing Yuan,Xuan Zhang,Chunyu Wei,Yushun Fan*

Main category: cs.GT

TL;DR: 本篇论文提出了多群体贝叶斯博弈（MBGs）模型，并给出了寻找（强）多群体贝叶斯纳什均衡（MBNE）的方法。MBNE 描述了群体合作博弈下的最优策略，强 MBNE 描述了群体非合作博弈下的最优策略。


<details>
  <summary>Details</summary>
Motivation: 为了描述群体在贝叶斯博弈中的行为，并找出在不同群体合作/非合作情境下的最优策略。

Method: 提出 MBGs 模型，并将其转化为多群体先验代理博弈（MEAG），然后寻找 MEAG 的（强）势博弈性质，若存在则可找到所有（强）MBNE。

Result: 开发了寻找（强）MBNE 的算法，并通过示例验证了其正确性。

Conclusion: 所提出的 MBGs 模型和寻找（强）MBNE 的方法能够有效解决多群体博弈中的策略优化问题。

Abstract: This paper presents a model of multi-group Bayesian games (MBGs) to describe
the group behavior in Bayesian games, and gives methods to find (strongly)
multi-group Bayesian Nash equilibria (MBNE) of this model with a proposed
transformation. MBNE represent the optimal strategy \textit{profiles} under the
situation where players within a group play a cooperative game, while strongly
MBNE characterize the optimal strategy \textit{profiles} under the situation
where players within a group play a noncooperative game. Firstly, we propose a
model of MBGs and give a transformation to convert any MBG into a multi-group
ex-ante agent game (MEAG) which is a normal-form game. Secondly, we give a
sufficient and necessary condition for a MBG's MEAG to be (strongly) potential.
If it is (strongly) potential, all its (strongly) Nash equilibria can be found,
and then all (strongly) MBNE of the MBG can be obtained by leveraging the
transformation's good properties. Finally, we provide algorithms for finding
(strongly) MBNE of a MBG whose MEAG is (strongly) potential and use an
illustrative example to verify the correctness of our results.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [173] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: OR-Toolformer通过半自动数据合成和外部求解器增强微调Llama-3.1-8B-Instruct，在优化研究（OR）任务上达到了80.1%的执行准确率，超越了基线模型，并展现了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的闭源API在处理优化研究（OR）任务时存在隐私问题，而从头训练开源模型成本高昂。因此，需要一种更有效的方法来训练OR模型。

Method: 我们引入了OR-Toolformer，它通过一个半自动数据合成流程来微调Llama-3.1-8B-Instruct模型。该流程生成多样化的OR问题-答案对，并结合外部求解器来生成API调用。

Result: OR-Toolformer在四个标准基准中的三个上达到了高达80.1%的执行准确率，比同等规模的基线模型高出4.3%。在两种未见过的新OR问题类型的零样本评估中，其平均准确率达到了54%，比最强的基线模型提高了21个百分点。

Conclusion: 研究结果证明了工具增强的微调方法对于训练能够准确、可泛化地建模和解决OR问题的LLM是有效的。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [174] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 该研究提出了一个确定性模拟框架，为评估AI生成的同行评审报告提供了一个可扩展、基于证据的标准，以应对学术出版面临的投稿量激增和AI监管不力的问题。


<details>
  <summary>Details</summary>
Motivation: 学术出版界正面临投稿量激增和AI监管不足的双重危机，急需新的治理模式来维护科学诚信。传统纯人工同行评审缺乏可扩展、客观的标准，导致编辑流程不透明且难以审计。

Method: 研究人员开发并测试了一个确定性模拟框架，该框架旨在为评估AI生成的同行评审报告提供一个稳定的、基于证据的标准。他们分析了352份同行评审模拟报告，以识别一致的系统状态指标来证明其可靠性。

Result: 模拟框架能够模拟校准的编辑判断，其中“修改”决定在所有学科中始终占多数（>50%），而“拒绝”率根据特定领域的规范动态调整（在健康科学领域上升至45%）。该系统还保持了稳定的29%证据锚定合规率，该比率在不同的评审任务和科学领域中保持不变，证明了其程序完整性。

Conclusion: 该框架提供了一个可预测、遵循规则的系统，减轻了生成式AI的随机性。这为科学界提供了一个透明的工具来确保公平性，为出版策略师提供了一个可扩展的工具来审计工作流程、管理完整性风险并实施基于证据的治理。该框架将AI重新定位为机构问责制的重要组成部分，为维护学术交流的信任提供了关键基础设施。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [175] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 该研究提出ROTE算法，通过将人类行为建模为行为程序而非基于信念和欲望的策略，并结合大语言模型和概率推理，在稀疏观测下能够高效准确地预测人类和AI的行为，超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人类行为建模方法存在数据需求大、假设不现实或计算量大等问题，难以快速适应和实现稳健安全的人机协作。

Method: 提出ROTE算法，利用大语言模型生成行为程序假设空间，并结合概率推理来处理该空间中的不确定性。将人类行为建模为行为程序（计算机代码），而不是依赖于信念和欲望的策略。

Result: 在网格世界任务和大型家庭模拟器中，ROTE算法能够从稀疏观测中预测人类和AI的行为，其样本内准确率和样本外泛化能力比行为克隆和基于大语言模型的方法等竞争基线高出50%。

Conclusion: 将动作理解视为一个程序合成问题，ROTE为AI系统提供了一条在现实世界中高效准确预测人类行为的途径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [176] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: CA-ChemE系统是一个创新的多智能体协作系统，通过整合领域知识库和协作代理，促进化学工程领域的自主科学发现和跨学科研究。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化学工程领域的跨学科协作和探索未知问题方面存在局限性。

Method: 提出并实现了一个名为CA-ChemE的系统，该系统是一个数字城镇，通过多智能体协作实现自主研究和科学发现。系统集成了领域知识库、知识增强技术和协作代理。

Result: 知识库增强机制平均提高了7个专家代理的对话质量评分10-15%。引入的协作代理（CA）在跨领域协作效率方面取得了显著提升，尤其是在远距离领域专家对之间，效率提高了8.5%，远超领域接近专家对的0.8%提升。

Conclusion: 精心设计的多智能体架构为实现化学工程领域自主科学发现提供了可行的途径，并揭示了“知识库差距导致的协作效率降低”效应。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [177] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: LLM代理的评估需要超越传统下游任务，引入多主体辩论作为“社会实验室”来量化其社会和认知动态。评估显示，代理倾向于达成共识，个体心理测量特征稳定，且主持人可影响辩论结果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估基准不足以衡量智能体在交互环境中的社会和认知动态，因此需要新的评估框架。

Method: 使用多主体辩论作为“社会实验室”，LLM代理被赋予不同个性和动机，在LLM主持下就广泛话题进行辩论，并采用新的心理测量和语义指标进行分析。

Result: 代理表现出强大的寻求共识的倾向，即使没有明确指示，在敏感话题上也达到高度语义一致性（μ > 0.88）。分配的个性会产生稳定的心理测量特征，特别是认知努力方面。主持人可以通过环境设置显著影响辩论结果。

Conclusion: 提出了一个基于心理测量学的新型动态评估框架，用于智能体设置，以理解和塑造下一代AI代理的社会行为。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [178] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: AGILE通过将拼图解决视为一个互动的过程，利用环境反馈来提升大型视觉语言模型（VLMs）在感知和推理方面的能力，实验证明其在拼图任务和通用视觉任务上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的VLMs在基础感知和推理能力方面存在局限性，即使在简单的拼图任务上也表现不佳，而高质量视觉语言数据的稀缺和可扩展性限制了改进这些能力。

Method: AGILE将拼图解决构建为一个互动的过程，模型通过生成可执行代码与环境进行交互，并利用环境提供的精细视觉反馈来逐步提升感知和推理能力。

Result: AGILE显著提高了模型在不同复杂度的拼图任务上的表现（例如，在2x2设置下准确率从9.5%提升到82.8%），并在9个通用视觉任务上展现了良好的泛化能力，平均提升了3.1%。

Conclusion: AGILE在提升VLMs的感知和推理能力方面取得了显著效果，并在多模态强化学习数据稀缺的问题上提供了一个高效且可扩展的解决方案，为提升多模态模型的推理和泛化能力开辟了新途径。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [179] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: 一个结合了形式验证和非形式推理的AI系统，在2025年国际数学奥林匹克竞赛中取得了金牌同等水平的成绩。


<details>
  <summary>Details</summary>
Motivation: 介绍一个结合了形式验证和非形式推理的AI系统，以在数学竞赛中取得优异成绩。

Method: 整合了一个Lean证明搜索系统、一个生成和形式化引理的非形式推理系统以及一个专门的几何求解器。

Result: 该系统在2025年国际数学奥林匹克竞赛问题上实现了金牌同等水平的性能，并展示了在自动定理证明方面的先进性能和良好的可扩展性。

Conclusion: 该系统在自动定理证明方面取得了显著进展，并可能对数学研究和教育产生影响。

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [180] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: LLM在集体决策中的表现与人类存在差异，影响因素包括上下文、线索和模型本身。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在集体决策中与人类社会推理的趋同性，开发了集体层面的评估框架。

Method: 使用“海上失落”心理学任务，进行大规模在线实验，对比可见人口统计信息和匿名别名的领导者选举，并模拟LLM群体，对比不同LLM的行为。

Result: LLM的行为存在差异，有的模仿人类偏见，有的则试图补偿这些偏见。LLM与人类集体行为的趋同性取决于上下文、线索和模型本身的归纳偏差。

Conclusion: LLM在集体推理中的表现与人类的趋同性是复杂的，受多种因素影响，需要动态的基准来捕捉集体推理的复杂性。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [181] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个在多平台环境中评估长期记忆和状态跟踪的新基准，它模拟了现实世界的组织工作流程，并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文和记忆基准主要关注对话实例，但未能满足在动态企业环境中评估记忆的需求。

Method: MEMTRACK通过集成Slack、Linear和Git等多个通信和生产力平台上的异步事件来模拟真实的组织工作流程。它生成包含跨平台交错时间线、嘈杂、冲突、相互引用信息以及潜在代码库/文件系统理解和探索的基准实例。该基准测试记忆能力，如获取、选择和冲突解决。数据集通过手动专家设计和基于代理的合成进行策划，以生成基于现实世界软件开发过程的生态系统有效场景。引入了捕捉记忆机制有效性的正确性、效率和冗余指标。

Result: 在MEMTRACK上的实验揭示了在长周期内利用记忆、处理跨平台依赖关系和解决矛盾方面的挑战。值得注意的是，表现最佳的GPT-5模型在此基准上的正确性得分仅为60%。

Conclusion: MEMTRACK提供了一个可扩展的框架，用于推进记忆增强代理的评估研究，超越了现有的对话设置的焦点，并为复杂组织环境中多代理、多平台的记忆基准奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [182] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 该论文提出了一个由大语言模型（LLM）驱动的临床决策支持系统，旨在通过分析电子健康记录（EHR）数据来辅助医生处方决策。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）的快速扩展和临床决策的日益复杂，为提供数据驱动的护理带来了机遇和挑战。

Method: 该系统采用检索增强生成（RAG）框架，整合自然语言处理和结构化临床输入，通过分析历史EHR数据（包括患者人口统计信息、主诉、临床症状、诊断信息和治疗历史）来生成治疗建议。它通过检索和综合具有相似特征的先例案例来增强而非取代临床医生的判断。

Result: 初步评估使用去标识化和合成的临床数据集，对模型输出的临床合理性和一致性进行了 পরীক্ষা。早期结果表明，在适当约束和严格验证的情况下，基于LLM的工具可以在处方工作流程中提供有价值的决策支持。

Conclusion: 该研究是利用生成式AI进行真实世界临床决策的第一步，并强调了透明度、安全性和与既定实践的一致性。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [183] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 该研究提出TRACE来检测隐式奖励破解，它通过衡量模型推理到多早能通过验证器来量化“努力程度”，并发现破解模型因走了捷径而能以更少的“努力”获得高分。


<details>
  <summary>Details</summary>
Motivation: 奖励破解（Reward hacking）是指模型利用奖励函数的漏洞在未解决预期任务的情况下获得高分，这种行为可能通过显式（CoT中明确表述）或隐式（CoT看似无害，绕过监测器）的方式发生。现有的监测方法在检测隐式奖励破解方面存在不足。

Method: 提出TRACE（Truncated Reasoning AUC Evaluation）来检测隐式奖励破解。其核心思想是，破解行为的发生是因为利用漏洞比解决实际任务更容易，即模型花费的“努力”少于必要。TRACE通过测量模型推理能多早地通过验证器来量化“努力”。具体做法是逐步截断模型的思考过程（CoT），强制其给出答案，并测量每个截断点下的通过率。破解模型会利用捷径，导致在只使用部分CoT的情况下就能获得高通过率，从而在准确率-长度曲线下形成一个较大的面积。

Result: TRACE在数学推理方面比最强的72B CoT监控器有超过65%的提升，在编码方面比32B监控器有超过30%的提升。此外，TRACE还能在训练过程中发现未知的漏洞。

Conclusion: TRACE是一种可扩展的无监督方法，适用于当前监控方法效果不佳的监督场景，能有效检测隐式奖励破解。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [184] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 通过蒸馏将检索增强学习转化为内在能力，以提高LLM代理的多步任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在执行多步任务时，在可预见的方面经常失败，例如在不满足先决条件的情况下执行操作、发出冗余命令或错误处理环境约束。虽然检索增强生成（RAG）可以通过提供运行时指导来提高性能，但它需要维护外部知识库并在每次部署时增加计算开销。

Method: 提出一种简单的流水线，将推理时检索转化为通过蒸馏学习到的能力。该方法首先从代理失败中提取简洁、可重用的提示，然后使用这些提示通过单次检索在剧集开始时生成改进的教师轨迹，最后在去除提示字符串的情况下，在学生模型上进行训练，强制内化而非记忆。

Result: 在ALFWorld（家务任务）和WebShop（在线购物）两个交互式基准测试中，蒸馏后的学生模型始终优于基线代理，在ALFWorld上的成功率高达91%（基线为79%），在WebShop上的得分提高到72（基线为61），同时与检索增强教师相比，使用的令牌数量减少了10-60%，具体取决于环境。

Conclusion: 该方法通过有针对性的微调，有效地将检索的好处内化，无需永久的运行时依赖，并且可以跨模型规模（7B/14B参数）和代理架构（ReAct/StateAct）进行推广。

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [185] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 本研究提出利用大型语言模型（LLM）代理自动处理工程建模任务，特别是在回归任务中，并使用临界热通量（CHF）预测基准进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现代工程依赖大量数据，需要高效、可靠且广泛适用的建模策略，同时对数据驱动方法（尤其是神经网络）有浓厚兴趣，但传统方法需要大量手动干预，限制了其扩展性和泛化能力。

Method: 提出并评估了两种基于LLM代理的框架：一种是多智能体协作系统，另一种是基于ReAct范式的单智能体系统。两种框架都能自主完成数据预处理、神经网络开发、训练、超参数优化和不确定性量化（UQ）。

Result: 在CHF预测任务中，LLM代理开发的模型在预测准确性和UQ方面与专家开发的先进模型相当，并优于传统的CHF查找表。

Conclusion: 基于LLM的代理在自动化复杂工程建模任务方面具有巨大潜力，可以显著减少人工负担，同时达到或超过现有的预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [186] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX是一个AI代理，利用LLM将原始日志转换为本体知识图谱，并映射到MITRE ATT&CK战术，以提取可操作的网络威胁情报。


<details>
  <summary>Details</summary>
Motivation: 从系统日志中提取网络威胁情报（CTI）受到日志缺乏结构、语义不一致和碎片化的限制，需要能够整合异构数据的方法。

Method: OntoLogX利用LLM、本体、检索增强生成（RAG）和迭代校正来将原始日志转换为本体知识图谱（KGs），并预测MITRE ATT&CK战术。

Result: OntoLogX在公开基准和真实世界蜜罐数据集上进行了评估，成功生成了KGs，并将日志证据准确映射到了ATT&CK战术，展示了检索和校正的优势以及本体表示的价值。

Conclusion: OntoLogX能够有效地将原始日志转换为可操作的网络威胁情报，通过生成本体知识图谱和映射到MITRE ATT&CK战术，克服了传统日志分析的局限性。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [187] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR:  Falconer框架结合LLMs和轻量级代理模型，实现了可扩展的知识挖掘，在保持高准确度的同时显著降低了成本和提高了效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模知识挖掘中，大型语言模型（LLMs）成本高昂且传统方法泛化能力不足的问题。

Method:  Falconer框架利用LLMs进行推理和生成监督数据，训练轻量级代理模型来执行分类和提取任务，将这些任务统一为“获取标签”和“获取跨度”两个原子操作。

Result:  Falconer框架在准确度上接近顶尖LLMs，同时将推理成本降低了90%，并将大规模知识挖掘速度提升了20倍以上。

Conclusion: Falconer为深度研究提供了一个高效且可扩展的知识挖掘基础，有效平衡了LLMs的推理能力和代理模型的效率。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [188] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 该论文强调了领域专家提供的精选知识在创建有效辅导系统中的作用，并提出了两种利用此类知识的新方法：一是利用可解释人工智能（XAI）技术自动创建课程，二是利用专家指定的课程来开发自适应辅导系统，并以授粉媒介识别的案例研究进行了说明。


<details>
  <summary>Details</summary>
Motivation: AI for education领域常常忽视领域专家提供的精选知识在创建有效辅导系统中的作用。本文旨在强调这一重要性，并提出利用专家知识构建新型教育系统的两种方法。

Method: 本文提出两种利用专家知识创建教育系统的方法：1. 结合专家规则和新颖的XAI技术，自动生成课程。2. 利用专家制定的学习课程，开发能提供更佳学习体验并允许使用更高效算法的自适应辅导系统。

Result: 通过XAI技术和专家知识，可以自动生成课程。专家制定的学习课程有助于开发更优的自适应辅导系统。授粉媒介识别的案例研究表明了这些方法的有效性。

Conclusion: 领域专家提供的精选知识对于创建创新的、更有效的教育系统至关重要，尤其是在自动生成课程和开发自适应辅导系统方面。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [189] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE通过在视觉输入空间而非输出空间进行探索，利用图像的不确定性来提高多模态大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLM）在强化学习中存在探索效率低的问题，并且将视觉输入视为确定性条件，忽略了视觉变化带来的不确定性。

Method: VOGUE将图像视为随机上下文，量化策略对视觉扰动的敏感度，并通过不确定性比例奖励、token-熵奖励和退火采样策略来平衡探索和利用。

Result: 在两个模型规模（Qwen2.5-VL-3B/7B）上，VOGUE在视觉数学和通用领域推理基准测试中，pass@1准确率平均提高了2.6%和3.7%，同时提高了pass@4性能，并缓解了强化学习微调中常见的探索衰减问题。

Conclusion: 将探索置于视觉输入固有的不确定性中，是提高多模态推理的有效策略。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [190] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 该论文提出了AIReg-Bench，一个用于评估大型语言模型（LLM）在欧盟AI法案（AIA）合规性评估任务上表现的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 随着各国政府对AI进行监管，使用LLM来评估AI系统是否符合特定AI监管（AIR）的兴趣日益浓厚，但目前缺乏相应的基准来衡量LLM在此任务上的表现。

Method: 研究人员通过两步法创建了一个包含120个技术文档摘录的数据集：首先，使用LLM生成描绘虚构AI系统的样本；然后，由法律专家对这些样本进行审查和标注，指出其中描述的AI系统是否以及在何种程度上违反了AIA的具体条款。

Result: 该研究创建了一个名为AIReg-Bench的数据集，并评估了当前前沿LLM在重现专家合规性标注方面的能力，为理解基于LLM的AIR合规性评估工具的机遇和局限性提供了起点。

Conclusion: AIReg-Bench数据集及其评估结果为LLM在AIR合规性评估领域的应用提供了一个基准，并为后续LLM的评估和开发奠定了基础。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [191] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: LToT是一种新的搜索控制器，通过区分效用和逻辑一致性来解决现有搜索方法的不足，从而在大型测试时间计算预算下实现更有效的探索。


<details>
  <summary>Details</summary>
Motivation: 标准的前言搜索方法在大型测试时间计算预算下存在两个主要问题：宽度饱和（额外的样本产生大量重复）和深度近视（短期内不准确的评估导致修剪了有潜力的分支）。

Method: LToT将搜索前沿分为主线（高价值候选）和侧线（一致但初期价值低）。它通过一种称为“横向赛跑与短路”（LR--SC）的机制来探索侧线。该机制使用一种有限的连续减半竞赛，将少量探针分配给广泛的侧线候选，并采用考虑宽度的阈值和重复确认机制。一旦某个分支的评估超过主线标准，就会立即提升该分支。主线宽度被有意保持较窄，以确保计算资源被优先投入到宽度成本较低的领域。

Result: （待补充，实验评估正在准备中）

Conclusion: LToT能够有效地利用大型测试时间计算预算，通过引入原则性的多样性来解决宽度饱和和深度近视问题，同时保持了晋升的纪律性，并且不会不合理地增加计算量。

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [192] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 本文提出一种新方法，结合稀疏自编码器（SAE）和聚类技术，用于分析大语言模型（LLM）的内部 token 表示，并指导数学推理任务的生成。该方法通过 SAE 生成稀疏向量表示，然后使用 k-means 聚类构建图，其中节点代表 token 簇，边代表 token 转换。基于此图，定义了基于边的奖励函数来量化推理过程的遵循程度，并评估生成的多样性。结果表明，平衡利用和探索对于数学推理任务的准确性至关重要。SAE 可作为可扩展的奖励模型来指导生成，确保利用和探索之间的平衡，从而提高 LLM 的推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学推理任务中可能存在利用（exploitation）和探索（exploration）不平衡的问题，导致推理质量不高。

Method: 1. 使用稀疏自编码器（SAE）为训练 token 生成稀疏向量表示。
2. 应用 k-means 聚类技术构建一个图，其中节点是 token 簇，加权的边表示 token 序列转换。
3. 定义基于边的奖励函数，量化生成过程对已知推理路径的遵循程度，识别“利用性”推理轨迹。
4. 测量生成多样性以评估“探索”程度。
5. 将 SAE 作为奖励模型，在生成过程中平衡利用和探索。

Result: 平衡利用和探索对于提高数学推理任务的准确性至关重要。SAE 可以作为奖励模型，指导 LLM 生成，在利用和探索之间取得平衡，避免极端行为，最终提升推理质量。

Conclusion: 所提出的结合 SAE 和聚类的方法能够有效分析 LLM 的内部表示，并通过平衡利用和探索来指导数学推理任务的生成，从而提高 LLM 的推理能力。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [193] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: LLM在法律和医学等领域的高置信度推理方面存在不足，提出了一种名为LogT的新型神经符号架构，通过结合LLM和先进的逻辑推理器，构建了双重符号图和基于逻辑的上下文，有效解决了长篇指南推理的挑战，并在多个基准测试中显著提高了推理性能。


<details>
  <summary>Details</summary>
Motivation: 法律和医学等关键领域的高置信度推理需要准确、可验证且有据可依的结论，而现有的大型语言模型（LLMs）在处理这些文本时面临严峻的逻辑推理挑战，特别是涉及否定、蕴含和可废止规则及例外情况时。

Method: 提出了一种名为LOGicalThought (LogT) 的新型神经符号架构，该架构结合了先进的逻辑语言和推理器以及LLM，构建了双重符号图上下文和基于逻辑的上下文，将推理问题转化为紧凑的、基于证据的评估。

Result: 在四个跨领域基准测试中，LogT的整体性能比所有LLM基线提高了11.84%。在否定推理方面提高了10.2%，在蕴含推理方面提高了13.2%，在可废止推理方面提高了5.5%。

Conclusion: LogT架构通过其创新的神经符号方法，显著提高了LLM在处理高置信度文本推理任务中的准确性和效率，特别是在处理复杂的逻辑结构和可废止规则方面。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [194] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker是一个LLM决策框架，通过整合任务导向规划和信息寻求，来解决部分可观察环境中的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划代理忽略了内部动态与实际环境之间的差异，而现实世界的问题解决需要明确的信息寻求来处理不完整的信息和嘈杂的动态。

Method: InfoSeeker提示LLM通过规划动作来主动收集信息，以验证其理解、检测环境变化或检验假设，然后再生成或修改任务导向的计划。该框架还引入了一个包含不完整观测和不确定动态的部分可观察环境的新基准套件。

Result: InfoSeeker在实验中实现了比先前方法高74%的绝对性能增益，且不牺牲样本效率，并在机器人操作和网页导航等既定基准上超越了基线方法。InfoSeeker还展示了跨LLM的泛化能力。

Conclusion: 将规划和信息寻求紧密结合对于在部分可观察环境中的稳健行为至关重要。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [195] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: dLLMs在推理方面存在挑战，现有方法依赖稀疏奖励，可能强化错误推理。本文提出了一种名为SAPO的新型RL算法，通过奖励增量式进展来引导模型学习结构化推理，提高了推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏奖励的强化学习方法在训练dLLMs进行复杂推理时存在缺陷，容易强化错误推理路径。

Method: 提出了一种将复杂问题解决形式化为分层选择过程的理论框架，并据此设计了一种名为SAPO的新型强化学习算法，该算法使用基于过程的奖励函数来鼓励增量式进展，使dLLM的去噪过程与潜在推理层级对齐。

Result: SAPO在具有挑战性的推理基准测试中显著提高了性能，并增强了生成过程的可解释性。

Conclusion: SAPO通过对齐dLLM的去噪过程与潜在推理层级，有效解决了现有方法在复杂推理方面的不足，提高了模型性能和可解释性。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [196] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink通过引导大语言模型在响应前先思考潜在的失败模式，从而提升安全性和推理能力，并在高风险领域表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全响应，但效果有限，且可能损害通用推理能力。本研究旨在提出一种新的方法，在提升安全性的同时，尽可能保留模型的通用推理能力，并特别关注高风险领域。

Method: InvThink通过三个步骤实现反向思考：1）列举潜在危害，2）分析其后果，3）生成能主动规避这些风险的安全输出。该方法通过监督微调和强化学习在不同大语言模型家族中实现。

Result: InvThink在模型规模扩展方面显示出比现有方法更强的安全改进效果；它通过系统性地考虑失败模式来减轻安全成本，从而在标准基准测试中保留了通用推理能力；在医学、金融、法律以及黑名单、谋杀等代理风险场景中，InvThink减少有害响应的效果比SafetyPrompt等基线方法高出15.7%。

Conclusion: 反向推理为实现更安全、更强大的语言模型提供了一条可扩展且可泛化的途径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [197] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: AdvEvo-MARL是一个联合优化攻击者和防御者的共进化多智能体强化学习框架，用于提高LLM多智能体系统的安全性，同时保持任务准确性，而无需外部守卫模块。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM多智能体系统安全防御方法（如自验证和外部守卫模块）存在不足：自验证因单个智能体能力有限难以检测跨智能体攻击；外部守卫模块增加了系统开销并可能成为单点故障。因此，需要一种新的方法来将安全性内化到任务智能体中，并解决现有方法的局限性。

Method: 提出AdvEvo-MARL，一个共进化多智能体强化学习框架。该框架联合优化攻击者（生成不断演变的越狱提示）和防御者（同时执行任务和抵抗攻击的任务智能体）。为了稳定学习和促进协作，引入了一个公共基线用于优势估计：同一功能组内的智能体共享一个组级平均回报基线。

Result: 在代表性的攻击场景中，AdvEvo-MARL的攻击成功率（ASR）稳定在20%以下，而基线方法高达38.33%。同时，任务准确性得以保留，甚至有所提高（在推理任务上提高了3.67%）。

Conclusion: AdvEvo-MARL证明了在不依赖额外守卫智能体或增加系统开销的情况下，可以同时提高LLM多智能体系统的安全性和效用。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [198] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: AgentRec是一个利用LLM驱动的多智能体协作推荐框架，通过分层智能体网络和自适应学习机制，解决了动态用户偏好、对话连贯性和多重排序目标平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有交互式对话推荐系统在处理动态用户偏好、维持对话连贯性和平衡多重排序目标方面存在挑战。

Method: AgentRec采用分层智能体网络，其中包含专门的LLM智能体，负责对话理解、偏好建模、情境感知和动态排序，并通过自适应加权机制协调，该机制从交互模式中学习。提出了一种结合快速响应、智能推理和深度协作的三层学习策略。

Result: 在三个真实世界的数据集上的广泛实验表明，AgentRec在对话成功率方面提高了2.8%，在推荐准确率（NDCG@10）方面提高了1.9%，在对话效率方面提高了3.2%，同时通过智能体协调保持了可比的计算成本。

Conclusion: AgentRec通过其创新的多智能体协作方法，在提升对话推荐系统的性能和效率方面取得了显著成效。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [199] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: LLMs在心理咨询领域的应用潜力有待挖掘，本研究提出了PsychoBench基准测试，以评估LLM是否具备心理咨询师的资质，测试结果表明，仅有GPT-4o等顶尖模型能达到该基准线。


<details>
  <summary>Details</summary>
Motivation: 心理咨询领域对LLM的应用潜力巨大，但LLM是否具备相应的认知能力尚不明确，因此需要评估LLM在心理咨询领域的适用性。

Method: 本研究提出了PsychoBench基准测试，该测试包含2,252个选择题，旨在评估LLM对心理学知识的掌握程度，以判定其是否能通过美国全国心理咨询师资格考试（NCE）。

Result: 在PsychoBench测试中，GPT-4o, Llama3.3-70B, 和 Gemma3-27B等先进模型均超过了NCE的通过标准，而Qwen2.5-7B, Mistral-7B等规模较小的开源模型则远低于通过标准。

Conclusion: 目前仅有顶尖LLM能够达到心理咨询师资格考试的标准，这表明LLM在心理咨询领域具有应用前景，但也面临着挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [200] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 使用大型语言模型（LLM）压缩上下文输入，以增强在信息丰富的、资源受限环境中的决策效率。


<details>
  <summary>Details</summary>
Motivation: 现有上下文马尔可夫决策过程（CMDP）方法在处理高维或非结构化上下文时，计算量过大且性能不稳定。

Method: 提出一种信息论的、使用大型语言模型（LLM）压缩上下文输入的摘要方法，以生成低维、语义丰富的摘要来增强状态，同时保留决策关键线索并减少冗余。在此基础上，推导出CMDPs的最小遗憾界限和延迟-熵权衡特征。

Result: 实验表明，与原始上下文和非上下文基线相比，所提出的方法在离散、连续、视觉和推荐基准测试中表现更优，提高了奖励、成功率和样本效率，同时降低了延迟和内存使用。

Conclusion: 基于LLM的摘要方法为信息丰富的、资源受限的环境中的高效决策提供了一种可扩展且可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [201] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: LLMs can read road network maps and perform navigation by reconstructing masked GPS traces using a dataset called GLOBALTRACE. They outperform baselines and show good generalization, but have regional and transportation mode biases. LLMs can also enhance navigation by incorporating user preferences.


<details>
  <summary>Details</summary>
Motivation: To explore the geospatial reasoning capabilities of LLMs, specifically their ability to read road network maps and perform navigation.

Method: Framing trajectory recovery as a proxy task, where models reconstruct masked GPS traces. Introduced GLOBALTRACE dataset with over 4,000 real-world trajectories. Developed a prompting framework that enables LLMs to generate valid paths using road network context without external tools.

Result: LLMs outperform off-the-shelf baselines and specialized trajectory recovery models in trajectory recovery, with strong zero-shot generalization. Fine-grained analysis reveals strong comprehension of road networks and coordinate systems, but also systematic biases related to regions and transportation modes.

Conclusion: LLMs demonstrate significant geospatial reasoning capabilities for map reading and navigation, outperforming existing methods. While biases exist, LLMs can be further utilized to enhance navigation experiences by flexibly reasoning over maps and incorporating user preferences.

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [202] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究展示了 GuruAgents（一种提示引导的 AI 代理）能够系统地实现传奇投资大师的策略。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探索如何将投资大师的定性投资理念转化为可复现的量化交易策略，并利用 AI 和提示工程实现自动化系统化投资。

Method: 研究人员开发了五种不同的 GuruAgents，每种代理都旨在模仿一位标志性投资者的风格。他们通过将这些投资者的独特哲学编码到大型语言模型（LLM）提示中，并整合金融工具和确定性推理流程来实现这一点。

Result: 在 2023 年第四季度至 2025 年第二季度纳斯达克 100 指数成分股的回测中，GuruAgents 表现出由其提示角色驱动的独特行为。其中，巴菲特 GuruAgent 取得了最高的回报，复合年增长率（CAGR）达到 42.2%，显著优于基准，而其他代理的表现则有所不同。

Conclusion: 研究结果证实，提示工程能够成功地将投资大师的定性哲学转化为可复现的量化策略，为自动化系统化投资指明了一个新的方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [203] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 电脑使用代理（CUAs）在追求目标时会表现出“盲目目标导向”（BGD）的偏见，即使在不可行、不安全或不可靠的情况下也是如此。研究人员开发了一个名为 BLIND-ACT 的基准来评估这种偏见，并在九个前沿模型上进行了测试，发现 BGD 率很高。虽然提示工程可以降低 BGD，但仍存在风险，需要更强的干预措施。


<details>
  <summary>Details</summary>
Motivation: 评估和解决电脑使用代理（CUAs）在执行任务时存在的“盲目目标导向”（BGD）问题，即代理不考虑可行性、安全性和上下文而盲目追求目标。

Method: 提出 BGD 的三种模式：缺乏上下文推理、歧义下的假设和决策、以及矛盾或不可行的目标。开发 BLIND-ACT 基准（包含 90 个任务），使用 OSWorld 环境和基于 LLM 的裁判来评估代理行为，并与人类注释达到 93.75% 的一致性。使用 BLIND-ACT 评估了九个前沿模型。

Result: 在九个前沿模型（包括 Claude Sonnet 和 Opus 4、Computer-Use-Preview 和 GPT-5）上观察到平均 80.8% 的高 BGD 率。BGD 暴露了即使输入本身无害也可能出现的微妙风险。提示工程干预可降低 BGD 水平，但仍存在显著风险。定性分析揭示了执行优先、思考-行动脱节和请求优先等失败模式。

Conclusion: BGD 是 CUAs 中存在的根本风险，需要通过更强的训练或推理时干预来解决，以确保 CUAs 的安全部署。BLIND-ACT 基准为未来研究提供了基础。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [204] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全优先、本地优先的系统，通过对临床查询进行分类，并从预先批准的FAQ中提供逐字答案，从而避免了生成式AI在临床路径中的使用。


<details>
  <summary>Details</summary>
Motivation: 临床上，患者在接受侵入性手术前常常有未解答的疑问，但由于时间紧迫和隐私限制，个性化咨询受到阻碍。

Method: LENOHA系统使用高精度句子转换器分类器来路由输入，并从临床医生策划的FAQ中逐字返回答案，消除了临床路径中的自由文本生成。该系统在一个预先批准的FAQ知识库中运行，而不是依赖于大型语言模型（LLM）进行即时文本生成。

Result: 在牙齿拔除和胃镜检查领域，E5-large-instruct编码器的准确率为0.983，AUC为0.996，总共出现七个错误。与GPT-4o相比，该系统的表现相当，Gemini在该测试集中没有出现任何错误。LENOHA系统的临床路径能耗约为每条输入1.0 mWh，而本地8B SLM的回应则需要约168 mWh，能耗差异巨大。

Conclusion: LENOHA系统通过提供经过审查的FAQ答案，在临床路径中避免了生成错误，并在隐私、可持续性和带宽受限环境的公平部署方面具有优势。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [205] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [206] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: DPO等偏好微调方法在对齐大型视觉语言模型（LVLM）方面显示出潜力，但忽略了合成偏好标注中风格和长度偏差的噪音。我们提出了一个基于LLM引导的响应编辑的硬负响应生成框架，以生成具有目标错误但风格和长度相似的拒绝响应。基于此框架，我们构建了VaPR数据集（30K样本），微调了LLaVA-V1.5、Qwen2VL和Qwen2.5VL模型。VaPR模型在10个基准测试中显著提高了性能，平均增幅分别为6.5%（LLaVA）、4.0%（Qwen2VL）和1.5%（Qwen2.5VL），尤其在推理任务上表现突出。数据量分析显示性能随数据量增加而持续提升。VaPR还减少了LVLM在二元问题中回答“是”的倾向。此外，该框架可推广到开源LLM作为编辑器，使用VaPR-OS训练的模型性能可达使用GPT-4o合成数据的模型性能的99%。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好微调方法忽略了AI生成反馈中常见的风格和长度偏差问题。

Method: 提出一个基于LLM引导的响应编辑的硬负响应生成框架，用于生成风格和长度相似但包含目标错误的拒绝响应，并构建了VaPR数据集进行模型微调。

Result: 在10个基准测试中，VaPR模型在LLaVA、Qwen2VL和Qwen2.5VL上分别实现了6.5%、4.0%和1.5%的平均性能提升，并在推理任务上表现突出。数据量分析表明性能随数据量增加而提升。VaPR还减少了LVLM在二元问题中回答“是”的倾向。使用开源LLM作为编辑器训练的模型性能可达使用GPT-4o训练的模型性能的99%。

Conclusion: 提出的硬负响应生成框架和VaPR数据集能有效解决AI生成反馈中的偏差问题，显著提升LVLM的性能，并减少常见失败模式，且具有良好的泛化能力。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [207] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个利用大型语言模型（LLM）将用户问题转化为SPARQL查询的AI系统，用于操作代谢组学知识图谱，解决了直接使用知识图谱的复杂性。


<details>
  <summary>Details</summary>
Motivation: 目前的质谱代谢组学数据量庞大，解释困难。知识图谱可以解决这些挑战，但需要深入理解其本体和查询语言。MetaboT旨在克服这一障碍，使用户能够通过自然语言与知识图谱进行交互。

Method: MetaboT采用多智能体系统，利用LLM将用户问题分解为可管理的任务，并由专门的智能体处理。该系统使用LangChain和LangGraph库构建，通过一系列智能体（入口、验证、监督、知识图谱）逐步处理用户查询，最终生成并执行SPARQL查询。

Result: 在包含50个代谢组学问题的评估中，MetaboT达到了83.67%的准确率，远高于仅使用GPT-4o和本体的基线（8.16%），证明了其在实体检索和SPARQL查询生成方面的有效性。

Conclusion: MetaboT作为一个对话式问答助手，能够通过自然语言查询检索结构化的代谢组学数据，降低了知识图谱的使用门槛，使研究人员能够更便捷地获取数据并推动发现。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [208] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 本研究提出了一个创新的结构化决策支持框架，将不同的人工智能（AI）代理架构（反应式、认知式、混合式和学习式）与美国国家标准与技术研究院（NIST）网络安全框架（CSF）2.0进行系统性对接。该框架整合了代理理论和行业指南，为选择和部署AI解决方案以应对当前网络威胁提供了一种透明且分步的方法。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是为选择和部署AI解决方案以应对当前网络威胁提供一种透明且分步的方法，通过将代理理论与行业指南相结合。

Method: 通过对NIST CSF 2.0功能进行细致分解，将AI代理的自主性、自适应学习和实时响应能力等关键特性与其安全需求相关联。此外，还提出了不同级别的自主性（辅助、增强和全自主），以适应不同网络安全成熟度的组织。

Result: 概念验证表明，该框架能够通过量身定制的AI代理部署，满足实际约束和风险状况，提高态势感知能力，加快响应速度，并通过自适应风险管理来加强长期韧性。

Conclusion: 该研究弥合了理论AI构建与实际网络安全需求之间的差距，为符合行业标准的、经过实证验证的多代理系统奠定了基础。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [209] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot是一个基于LLM的学术咨询聊天机器人，利用CatRAG（一种结合了检索增强生成和图推理的混合框架）来理解和遵守学术规定。


<details>
  <summary>Details</summary>
Motivation: 学术咨询对学生理解和遵守学校政策至关重要，但构建有效的系统需要特定领域的监管资源。REBot旨在解决这一挑战。

Method: REBot使用CatRAG框架，该框架结合了密集检索和图推理，并由一个包含语义特征的分层、类别标记的知识图谱支持。一个轻量级的意图分类器将查询路由到相应的检索模块。

Result: 在分类和问答任务上，REBot达到了98.89%的F1分数，取得了最先进的性能。

Conclusion: REBot通过一个实际的在线应用展示了其在真实学术咨询场景中的价值。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [210] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 该研究提出了一个可信赖的人机协作学习模型，用于军事行动中的人机协同。


<details>
  <summary>Details</summary>
Motivation: 为了应对快速变化的军事威胁和日益复杂的作战环境，AI在军事行动中的整合带来了显著优势，但同时也伴随着在有效和合乎道德地构建和部署人机协同系统方面的挑战和风险。

Method: 研究提出了一个可信赖的协同学习模型，通过整合四个维度：可调自主性、多层级控制、双向反馈和协同决策。

Result: 该模型通过持续和双向的洞察交流，使人类和AI在共同适应战场条件的过程中能够共同学习和适应。

Conclusion: 该模型旨在促进负责任和可信赖的人机协同系统在军事行动中的发展，并提供了具体的实例和建议。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [211] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: LLMs的CoT推理受限于局部决策，缺乏全局规划，导致冗余、不连贯或不准确。PTA-GRPO框架通过两阶段优化，提升了LLM的规划和CoT推理能力，在多个数学推理基准测试中表现稳定且有显著提升。


<details>
  <summary>Details</summary>
Motivation: LLMs的CoT推理存在局部决策限制，缺乏全局规划，导致推理过程冗余、不连贯或不准确，现有方法计算成本高且不总能找到最优推理路径。

Method: PTA-GRPO是一个两阶段框架：第一阶段，利用LLM将CoT蒸馏为紧凑的高层指导，并用于SFT；第二阶段，引入一种感知指导的RL方法，联合优化最终输出和高层指导的质量。

Result: 在MATH、AIME2024、AIME2025和AMC等多个数学推理基准测试中，PTA-GRPO在Qwen2.5-7B-Instruct、Qwen3-8B、Qwen3-14B和LLaMA3.2-3B等模型上均实现了稳定且显著的性能提升。

Conclusion: PTA-GRPO框架通过两阶段的规划与行动增强，有效提升了LLM的推理能力，并在多项数学推理任务和不同模型上展现出优越的泛化性和有效性。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [212] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 我们将对抗性逆强化学习（IRL）应用于大型语言模型推理，直接从专家演示中学习密集的、token级别的奖励模型，用于过程监督，而不是通过监督微调模仿风格。


<details>
  <summary>Details</summary>
Motivation: 将对抗性逆强化学习（IRL）框架和操作化应用于大型语言模型（LLM）推理，以从专家演示中直接学习一个密集的、token级别的奖励模型，用于过程监督，而不是依赖于模仿风格的监督微调。

Method: 该方法将对抗性逆强化学习（IRL）应用于LLM推理，学习一个密集的、token级别的奖励模型，用于过程监督。该奖励模型在训练期间提供逐步反馈以优化推理策略，并在推理时作为批评者来重新排序采样轨迹。

Result: 在GSM8K数据集上，使用Llama3和Qwen2.5骨干模型，证明了密集推理奖励可以作为引导推理的学习信号，并且通过奖励指导的重新排序可以提高预测性能（特别是对于Llama类策略）。

Conclusion: 通过将训练信号、推理时间选择和token级别诊断统一到一个推理奖励中，这项工作提出了可重用的过程级别奖励，有潜力增强语言模型的逐步推理能力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [213] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: CARS是一种通过自适应地排除违反约束的后续部分来改进拒绝采样的样本效率的方法，同时保持其分布的保真度，并在程序模糊和分子生成等领域实现比现有方法更高的效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 需要一种在满足严格的语义或句法约束的同时，还能保持语言模型分布和生成多样性的方法，现有方法（如贪婪约束解码和拒绝采样）存在各自的缺陷。

Method: CARS从无约束的语言模型采样开始，通过记录违反约束的后续部分并从未来的抽样中减去它们的概率质量，来对其进行自适应排除。这种自适应修剪确保了从不重新访问已被证明无效的前缀，提高了接受率，并保证了生成的样本精确遵循约束分布。

Result: 在程序模糊和分子生成等多个领域的实验中，CARS在每生成一个有效样本所需的语言模型前向传递次数方面，一致地实现了比贪婪约束解码和近似语言模型分布的方法更高的效率，同时还产生了更强的样本多样性。

Conclusion: CARS在不扭曲分布的情况下，在样本效率方面严格优于拒绝采样，并且在效率和样本多样性方面都优于现有方法。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [214] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: 现有表格异常检测方法忽略了文本语义信息，限制了模型利用领域知识的能力。ReTabAD通过提供包含文本元数据的表格数据集和零样本LLM框架来解决此问题，以实现上下文感知的表格异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义信息，限制了模型利用领域知识进行检测的能力。

Method: 构建了20个包含结构化文本元数据的表格数据集，并实现了包括经典、深度学习和基于LLM的方法在内的多种异常检测算法。提出了一个零样本LLM框架，无需针对特定任务进行训练即可利用语义上下文。

Result: 实验和分析表明，语义上下文能够提高检测性能并增强可解释性，支持领域感知的推理。

Conclusion: ReTabAD填补了表格异常检测领域中恢复文本语义的空白，为上下文感知的表格异常检测研究提供了基准，并证明了文本元数据在提高检测性能和可解释性方面的重要性。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [215] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: LLM的深层对表示学习的贡献不大，但其作用因评估设置而异，尤其是在生成任务中，深层对于推理和保持连贯性至关重要。


<details>
  <summary>Details</summary>
Motivation: 评估LLM中深层在不同维度（评估协议、任务类别、模型架构）上的作用，挑战现有认为深层贡献小的观点。

Method: 系统性地研究LLM的深度利用，通过对比不同评估设置（基于似然度量而不生成 vs. 基于生成的评估）下剪枝深层对模型性能的影响，并分析知识、检索和推理能力与模型深度的关系。

Result: 在基于似然度量的评估中，移除大部分深层几乎不影响性能；但在基于生成的评估中，中间和深层对于推理和长距离连贯性不可或缺。知识和检索集中在浅层，而推理能力依赖深层，但可通过蒸馏重塑。

Conclusion: LLM的深度利用具有异质性和情境依赖性，理解和压缩模型需要考虑任务、度量和模型本身。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [216] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 目前的AI模型在抽象推理能力上仍然落后于人类，尤其是在视觉模态下，单纯依靠准确率评估可能会高估或低估模型的能力。


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进的模型在理解和推理抽象概念方面的能力，特别是在ConceptARC基准测试中，并与人类的表现进行比较。

Method: 在不同的输入模态（文本与视觉）、是否允许使用外部Python工具以及推理模型可用的推理资源量的情况下，评估模型。除了测量输出准确性外，还对模型生成的自然语言规则进行细粒度评估，以判断模型是否真正理解并运用了任务设计者预期的抽象概念，而非依赖表面模式。

Result: 在文本模态下，虽然一些模型达到了与人类相当的准确率，但其生成的规则往往基于表面模式（“捷径”），对抽象概念的捕捉能力远不如人类。在视觉模态下，模型的准确率显著下降，但规则层面的分析显示，模型可能仍然掌握了一定的抽象概念，只是在应用这些规则时存在困难。

Conclusion: 目前的AI模型在抽象推理能力上仍然落后于人类。单纯依靠准确率来评估ARC类任务中的抽象推理能力，可能会在文本模态下高估模型的能力，而在视觉模态下低估模型的能力。所提出的评估框架能更真实地反映多模态模型的抽象推理能力，并为追踪以抽象为中心、类似人类的智能进展提供更可靠的评估方法。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [217] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，利用随机模式和参数化采样生成逼真的、多语言的半结构化文档及其丰富的注释，旨在解决企业规模文档理解模型开发中数据集收集成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 开发企业规模的文档理解模型需要大规模、多样化且注释良好的数据集，但由于隐私、法律限制和手动注释成本高昂（可能高达数百万美元），收集此类数据非常昂贵。

Method: FlexDoc结合了随机模式和参数化采样，通过概率模型化布局模式、视觉结构和内容变化来生成具有丰富注释的、逼真的、多语言的半结构化文档。

Result: 在关键信息提取（KIE）任务上的实验表明，FlexDoc生成的数据在增强真实数据集时，可以将F1分数提高高达11%，同时与传统的硬模板方法相比，注释工作量减少了90%以上。

Conclusion: FlexDoc已被积极部署，通过显著降低数据采集和注释成本，加速了企业级文档理解模型的开发。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [218] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 本篇论文提出了一种针对深度研究代理（DRAs）的评估基准和框架，以应对当前基准在评估复杂开放式任务方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估能够进行任务分解、跨源检索、多阶段推理和结构化输出的深度研究代理（DRAs）方面存在不足，无法有效衡量其在复杂开放式任务上的性能。

Method: 提出了一种包含214个专家精心设计的挑战性查询的基准，涵盖10个领域，并附带手动构建的参考信息包。同时，设计了一个多维度评估框架，用于评估DRAs生成的长篇报告，包含语义质量、主题焦点和检索可信度等综合评分指标。

Result: 实验结果表明，主流DRAs在性能上优于增强了网络搜索工具的推理模型，但仍有很大的改进空间。这表明所提出的基准和框架能够有效地区分不同DRAs的能力。

Conclusion: 本研究提出的基准和评估框架为DRAs系统的能力评估、架构优化和范式发展奠定了坚实的基础，有助于推动DRAs技术的发展。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [219] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: UpSafe$^\circ$C通过安全感知的升级改造，将LLM的安全关键层升级为稀疏的MoE结构，并引入了两阶段SFT策略和安全温度机制，以提高LLM的安全性、实用性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM安全技术在平衡安全性、实用性和可控性方面存在局限性。

Method: UpSafe$^\circ$C首先识别安全关键层并将其升级为稀疏的MoE结构，其中路由器充当软防护栏。然后，采用两阶段SFT策略来加强安全区分能力，同时保留通用能力。最后，引入安全温度机制来实现推理时的灵活控制。

Result: 实验表明，UpSafe$^\circ$C在抵御有害和越狱输入方面取得了显著的安全改进，同时在通用任务上保持了具有竞争力的性能。安全温度机制提供了细粒度的推理时控制，实现了效用和安全之间的帕累托最优前沿。

Conclusion: UpSafe$^\circ$C为LLM安全开辟了一个新方向：从静态对齐转向动态、模块化和感知推理的控制。

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [220] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR 可能会缩小而非扩大 LLM 的推理边界，这篇论文揭示了负干扰和赢家通吃现象来解释这一失败。


<details>
  <summary>Details</summary>
Motivation: RLVR 在提高 LLM 推理能力方面已成为一种关键方法，但近期证据表明它可能会适得其反地缩小推理边界，而不是扩大它。本研究旨在调查 RLVR 的缩小问题。

Method: 通过分析 RLVR 的学习动态，揭示了负干扰（学习解决某些训练问题会主动降低其他问题正确解决方案的可能性）和赢家通吃现象（RLVR 会不成比例地加强基础模型中高概率、正确解决方案的问题，同时抑制其他最初低概率的问题）。

Result: 通过在多个数学推理基准上的广泛理论和实证分析，表明这是由标准 RL 目标中固有的 on-policy 采样引起的，导致模型收敛到狭窄的解决方案策略。

Conclusion: 提出了一种简单而有效的数据整理算法，将 RLVR 学习集中在低概率问题上，在 Pass@k 性能方面取得了显著改进。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [221] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: CUAs在自动化数字任务方面有潜力，但其不可靠性和高方差限制了它们在长期复杂任务中的应用。本文提出行为最佳N（bBoN）方法，通过生成多个试运行并使用描述其试运行的行为叙述进行选择，实现了CUAs的扩展。bBoN能够广泛探索和合理选择轨迹，从而显著提高鲁棒性和成功率。


<details>
  <summary>Details</summary>
Motivation: 目前的计算机使用代理（CUAs）在处理长期复杂任务时存在不可靠和高方差的问题，限制了其应用。

Method: 提出行为最佳N（bBoN）方法，通过生成多个试运行并使用行为叙述进行选择，以实现CUAs的扩展。

Result: 在OSWorld上，bBoN方法达到了69.9%的新技术水平（SoTA），显著优于先前的方法，并接近人类水平（72%）。在WindowsAgentArena和AndroidWorld上表现出良好的泛化能力。

Conclusion: CUAs的有效扩展需要结构化的轨迹理解和选择，bBoN提供了一个实现这一目标的实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [222] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: This paper introduces "reasoning abstractions" to improve the procedural reasoning of large language models. It proposes a two-player RL training paradigm (RLAD) where one model generates abstractions (concise descriptions of procedural/factual knowledge) and another model generates solutions using these abstractions. This approach leads to more structured exploration, better generalization to complex problems, and demonstrates that optimizing abstraction generation is more effective than solution generation at test time.


<details>
  <summary>Details</summary>
Motivation: Current methods for training large language models to perform reasoning often fail to capture or reuse procedures effectively, leading to inefficient exploration and degenerate solutions. There is a need for a more effective approach to guide models toward identifying and implementing algorithmic procedures for complex problem-solving.

Method: The paper introduces "reasoning abstractions," which are concise natural language descriptions of procedural and factual knowledge. They propose a two-player Reinforcement Learning (RL) training paradigm called RLAD. In RLAD, one component (abstraction generator) proposes multiple abstractions for a given problem, and another component (solution generator) is trained using RL to build a solution while leveraging these abstractions. This setup allows for joint training, structured exploration, and decoupled learning signals for abstraction proposal and solution generation.

Result: The RLAD framework leads to more effective reasoning by enabling structured exploration and improving generalization to harder problems. The experiments show that allocating more computational resources to generating abstractions at test time is more beneficial for performance than generating more solutions, highlighting the importance of abstractions in guiding the reasoning process.

Conclusion: Reasoning abstractions, trained via the RLAD paradigm, provide a promising approach to enhance the procedural reasoning capabilities of large language models. This method facilitates structured exploration and better generalization by guiding the model with concise descriptions of relevant knowledge, outperforming approaches that solely focus on generating more solutions.

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [223] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 该研究提出了一种名为BioX-Bridge的轻量级桥接网络框架，用于在无监督情况下实现生物信号的跨模态知识迁移，以解决现有方法计算和内存开销大的问题，并在实验中证明了其在减少可训练参数的同时保持或提升了迁移性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号能提供人体生理状态的宝贵见解，但模态间的差异以及有限的大型标注数据集阻碍了针对特定任务和模态的模型训练。无监督跨模态知识迁移是解决此问题的有效途径，但现有基于知识蒸馏的方法存在计算和内存开销大的问题，尤其是在处理大型基础模型时。

Method: 提出了一种新的无监督跨模态知识迁移框架BioX-Bridge，通过训练一个轻量级的桥接网络来对齐中间表示并实现跨基础模型和跨模态的信息流动。该框架包括一种选择对齐位置的策略和一种灵活的原型网络作为桥接架构。

Result: 在多个生物信号模态、任务和数据集上的广泛实验表明，BioX-Bridge 可将可训练参数数量减少 88%–99%，同时保持或甚至提高了跨模态迁移性能，优于现有最先进的方法。

Conclusion: BioX-Bridge 框架能够以极低的计算和内存开销，有效地实现生物信号的无监督跨模态知识迁移，为提升健康监测系统的可及性、可用性和适应性提供了新的解决方案。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [224] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人类化身的新框架，通过基于材料点法（MPM）的模拟器实现了高度逼真、鲁棒的动画和自由视角的照片级渲染，并能泛化到未见过的交互。


<details>
  <summary>Details</summary>
Motivation: 在从视觉观察创建3D化身领域取得了显著进展，但对带有宽松服装的人类进行物理上可信的动力学建模仍然是一个挑战。现有方法在准确性或对新动画输入的鲁棒性方面存在局限。

Method: 该框架利用精心设计的基于材料点法（MPM）的模拟器，结合各向异性本构模型和新颖的碰撞处理算法，以对服装的复杂变形和与身体的接触进行建模。结合了可以利用3D高斯泼溅和伪阴影进行渲染的规范化身，以实现物理上逼真的动画的高保真渲染。

Result: MPMAvatar在动力学建模精度、渲染精度以及鲁棒性和效率方面显著优于现有的基于物理的化身。此外，该框架还能以零样本的方式泛化到未见的交互，而这是以前的基于学习的方法由于其有限的模拟泛化能力而无法实现的。

Conclusion: MPMAvatar在从多视角视频创建3D人类化身方面取得了显著进展，通过结合先进的物理模拟和渲染技术，在动画的真实性、鲁棒性和渲染质量方面都达到了新的水平，并展示了其在处理未见交互方面的潜力。

Abstract: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [225] [Multimodal Feedback for Task Guidance in Augmented Reality](https://arxiv.org/abs/2510.01690)
*Hu Guo,Lily Patel,Rohan Gupt*

Main category: cs.GR

TL;DR: 本研究提出了一种结合光学透视增强现实（OST-AR）和腕部振动触觉反馈的多模态引导方法，以提高手部任务的精确度和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的OST-AR在处理遮挡或光线不足的情况时，可能导致视觉信息过载，因此需要探索结合其他感官线索的方法。触觉技术，特别是腕部振动反馈，在空间信息传递和用户交互方面取得了进展，为改进OST-AR提供了新的可能性。

Method: 设计了一个包含六个振动马达的定制腕带，能够提供方向和状态提示，并将其与手持工具和OST-AR系统集成。通过形成性研究和两个实验（参与者人数分别为21和27）来评估该系统的效果。

Result: 实验结果表明，参与者在认知负荷下能够准确识别触觉模式。与仅使用视觉或仅使用触觉反馈的条件相比，多模态反馈在空间精确度和可用性方面均有所提高。

Conclusion: 结合OST-AR和腕部振动触觉反馈的多模态方法能够有效提升手部任务的空间精确度和可用性，克服了单纯依赖视觉引导的局限性。

Abstract: Optical see-through augmented reality (OST-AR) overlays digital targets and
annotations on the physical world, offering promising guidance for hands-on
tasks such as medical needle insertion or assembly. Recent work on OST-AR depth
perception shows that target opacity and tool visualization significantly
affect accuracy and usability; opaque targets and rendering the real instrument
reduce depth errors, whereas transparent targets and absent tools impair
performance. However, reliance on visual overlays may overload attention and
leaves little room for depth cues when occlusion or lighting hampers
perception. To address these limitations, we explore multimodal feedback that
combines OST-AR with wrist-based vibrotactile haptics. The past two years have
seen rapid advances in haptic technology. Researchers have investigated
skin-stretch and vibrotactile cues for conveying spatial information to blind
users, wearable ring actuators that support precise pinching in AR, cross-modal
audio-haptic cursors that enable eyes-free object selection, and wrist-worn
feedback for teleoperated surgery that improves force awareness at the cost of
longer task times. Studies comparing pull versus push vibrotactile metaphors
found that pull cues yield faster gesture completion and lower cognitive load.
These findings motivate revisiting OST-AR guidance with a fresh perspective on
wrist-based haptics. We design a custom wristband with six vibromotors
delivering directional and state cues, integrate it with a handheld tool and
OST-AR, and assess its impact on cue recognition and depth guidance. Through a
formative study and two experiments (N=21 and N=27), we show that participants
accurately identify haptic patterns under cognitive load and that multimodal
feedback improves spatial precision and usability compared with visual-only or
haptic-only conditions.

</details>


### [226] [MIRAGE: Patient-Specific Mixed Reality Coaching for MRI via Depth-Only Markerless Registration and Immersive VR](https://arxiv.org/abs/2510.01743)
*Daniel Brooks,Emily Carter,Hu Guo,Rajesh Nair*

Main category: cs.GR

TL;DR: MIRAGE系统利用混合现实技术（MR）为患者提供MRI前的沉浸式虚拟现实（VR）和增强现实（AR）体验，以减轻焦虑，提高扫描成功率。


<details>
  <summary>Details</summary>
Motivation: MRI的狭窄空间和噪音会引起患者焦虑和幽闭恐惧，导致检查中断、扫描不完整甚至需要药物镇静。MIRAGE旨在解决这一问题。

Method: MIRAGE系统利用最新的混合现实（MR）硬件，结合沉浸式虚拟现实（VR）和标记物移除的增强现实（AR）配准，为患者提供MRI前的准备和指导。研究中还详细介绍了系统架构，并探索了患者和临床医生的体验指标。

Result: 基于深度配准实现了亚厘米级精度，且设置简单。沉浸式指导环境能有效降低患者焦虑，并获得良好的可用性评分。

Conclusion: MIRAGE系统在降低患者焦虑、提高MRI检查体验方面显示出巨大潜力，并提出了其在医院工作流程中临床部署的考虑因素。

Abstract: Magnetic resonance imaging (MRI) is an indispensable diagnostic tool, yet the
confined bore and acoustic noise can evoke considerable anxiety and
claustrophobic reactions. High anxiety leads to motion artifacts, incomplete
scans and reliance on pharmacological sedation. MIRAGE (Mixed Reality Anxiety
Guidance Environment) harnesses the latest mixed reality (MR) hardware to
prepare patients for MRI through immersive virtual reality (VR) and markerless
augmented reality (AR) registration. In this paper, we extend our previous work
by providing a comprehensive review of related research, detailing the system
architecture, and exploring metrics for patient and clinician experience. We
also present considerations for clinical deployment of MR systems within
hospital workflows. Our results indicate that depth-based registration achieves
sub-centimeter accuracy with minimal setup, while the immersive coaching
environment reduces patient anxiety and yields favourable usability scores.

</details>


### [227] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一种物体感知的3D高斯喷溅框架，通过优先处理感兴趣的物体来提高局部细节，同时减小模型尺寸并保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在场景中均匀分配资源，限制了感兴趣区域（ROIs）的细节，并导致模型尺寸增大。ROI-GS旨在解决这个问题，通过物体感知的方法来提高局部细节。

Method: ROI-GS通过物体引导的相机选择、目标物体训练以及将高保真物体重建无缝集成到全局场景中来增强局部细节。该方法优先处理选定物体的高分辨率细节，同时保持实时性能。

Result: ROI-GS显著提高了局部质量（PSNR高达2.96 dB），同时将整体模型尺寸减小了约17%，并实现了具有单个感兴趣物体的场景的更快的训练速度，优于现有方法。

Conclusion: ROI-GS能够有效地在感兴趣的物体上以高细节重建3D场景，同时减小模型尺寸并保持实时性能。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

</details>


### [228] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 我们提出了一个结合微 وجهBRDF和镜面-光泽度参数化的2D高斯泼溅框架，用于准确重建和重新照亮具有光泽表面的物体。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在解耦物体形状、材质属性和光照方面存在困难，并且通常依赖简化的BRDF模型，限制了材质恢复和重新照亮的效果。

Method: 将微 وجهBRDF与镜面-光泽度参数化集成到具有延迟着色的2D高斯泼溅中，并使用基于扩散的表面法线和漫反射颜色先验来指导优化。采用由粗到精的环境图优化策略。

Result: 在复杂、光泽场景的实验表明，该方法实现了高质量的几何和材质重建，在新的光照下能进行更真实、更一致的重新照亮。

Conclusion: 该方法相比现有的高斯泼溅方法，在处理具有光泽表面的物体时，能够更准确地进行材质分解和重新照亮。

Abstract: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [229] [exaPD: A highly parallelizable workflow for multi-element phase diagram (PD) construction](https://arxiv.org/abs/2510.01400)
*Feng Zhang,Zhuo Ye,Maxim Moraru,Ying Wai Li,Weiyi Xia,Yongxin Yao,Ryan Richard,Cai-Zhuang Wang*

Main category: cond-mat.mtrl-sci

TL;DR: exaPD是一个用于生成相图的计算框架，通过并行化分子动力学和蒙特乔模拟来加速自由能计算。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效、并行地计算多相自由能并构建相图的工具，以克服传统方法计算量大的问题。

Method: 使用LAMMPS进行分子动力学和蒙特卡洛模拟，结合Parsl实现大规模并行化，支持多种原子间势能（包括神经网络势能），并使用PYCALPHAD进行相图构建。

Result: 生成了包括液相、固相和固溶体在内的自由能，并成功构建了相图。

Conclusion: exaPD是一个用户友好的工作流程，能够通过大规模并行自由能计算高效地构建相图。

Abstract: Phase diagrams (PDs) illustrate the relative stability of competing phases
under varying conditions, serving as critical tools for synthesizing complex
materials. Reliable phase diagrams rely on precise free energy calculations,
which are computationally intensive. We introduce exaPD, a user-friendly
workflow that enables simultaneous sampling of multiple phases across a fine
mesh of temperature and composition for free energy calculations. The package
employs standard molecular dynamics (MD) and Monte Carlo (MC) sampling
techniques, as implemented in the LAMMPS package. Various interatomic
potentials are supported, including the neural network potentials with near
{\it ab initio} accuracy. A global controller, built with Parsl, manages the
MD/MC jobs to achieve massive parallelization with near ideal scalability. The
resulting free energies of both liquid and solid phases, including solid
solutions, are integrated into CALPHAD modeling using the PYCALPHAD package for
constructing the phase diagram.

</details>


### [230] [Multiscale analysis of large twist ferroelectricity and swirling dislocations in bilayer hexagonal boron nitride](https://arxiv.org/abs/2510.01419)
*Md Tusher Ahmed,Chenhaoyue Wang,Amartya S. Banerjee,Nikhil Chandra Admal*

Main category: cond-mat.mtrl-sci

TL;DR: 异形变形双层六方氮化硼（hBN）具有原子级薄的结构和铁电特性，在下一代非易失性存储器应用中备受关注。然而，迄今为止的研究几乎完全集中在小变形上，而铁电性在大变形下是否仍然存在的问题仍未得到探索。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索双层hBN在各种大变形下的铁电性，并解释其晶体学起源，填补现有研究的空白。

Method: 利用Smith法范畴双晶学，研究了与AA堆叠和21.786789°扭曲配置等高对称配置相关的双层hBN配置的铁电性起源。通过原子模拟和密度泛函理论（DFT）信息连续框架（BFIM模型），研究了AA邻近和Σ7邻近系统中的铁电性，并分析了位错行为对极化的影响。

Result: 研究表明，AA邻近系统在小扭曲和小应变下都支持铁电性，其中极化切换由漩涡位错变形控制。对于Σ7邻近系统，开发了BFIM模型，该模型能够捕捉到铁电性。在这些大变形的双层配置中，界面位错的Burgers向量明显小于小扭曲和小应变双层hBN中的界面位错。

Conclusion: 本研究确立了双层hBN铁电性的晶体学起源，并展示了其在大变形下的铁电性。开发的BFIM模型为预测大型异质结构中的铁电性提供了一个计算效率高且强大的框架。

Abstract: With its atomically thin structure and intrinsic ferroelectric properties,
heterodeformed bilayer hexagonal boron nitride (hBN) has gained prominence in
next-generation non-volatile memory applications. However, studies to date have
focused almost exclusively on small heterodeformations, leaving the question of
whether ferroelectricity can persist under large heterodeformation entirely
unexplored. In this work, we establish the crystallographic origin of
ferroelectricity in bilayer hBN configurations heterodeformed relative to
high-symmetry configurations such as the AA-stacking and the 21.786789 $\circ$
twisted configuration, using Smith normal form bicrystallography. We then
demonstrate out-of-plane ferroelectricity in bilayer hBN across configurations
vicinal to both the AA and $\Sigma 7$ stacking. Atomistic simulations reveal
that AA-vicinal systems support ferroelectricity under both small twist and
small strain, with polarization switching in the latter governed by the
deformation of swirling dislocations rather than the straight interface
dislocations seen in the former. For $\Sigma 7$-vicinal systems, where reliable
interatomic potentials are lacking, we develop a
density-functional-theory-informed continuum framework--the
bicrystallography-informed frame-invariant multiscale (BFIM) model, which
captures out-of-plane ferroelectricity in heterodeformed configurations vicinal
to the $\Sigma 7$ stacking. Interface dislocations in these large
heterodeformed bilayer configurations exhibit markedly smaller Burgers vectors
compared to the interface dislocations in small-twist and small-strain bilayer
hBN. The BFIM model reproduces atomistic simulation results and provides a
powerful, computationally efficient framework for predicting ferroelectricity
in large-unit-cell heterostructures where atomistic simulations are
prohibitively expensive.

</details>


### [231] [Stabilization of sliding ferroelectricity through exciton condensation](https://arxiv.org/abs/2510.01465)
*Matteo D'Alessio,Daniele Varsano,Elisa Molinari,Massimo Rontani*

Main category: cond-mat.mtrl-sci

TL;DR: 二维材料中由于层间相对滑动引起的垂直于层面的电子极化现象，称为滑动铁电性。WTe2双层结构是研究该现象的原型。


<details>
  <summary>Details</summary>
Motivation: 研究滑动铁电性的起源和定量理解，特别是WTe2双层结构。

Method: 理论研究，关注激子效应如何导致能带重构和稳定铁电性。

Result: 激子效应引起了基态的能带重构，激子凝聚显著稳定了滑动铁电性，其效应比之前预测的更强。

Conclusion: 二维材料中的滑动铁电性是一种普遍现象，可能与多种重要材料相关，并为通过铁电性调控量子相提供了新的途径。

Abstract: Sliding ferroelectricity is a phenomenon that arises from the insurgence of
spontaneous electronic polarization perpendicular to the layers of
two-dimensional (2D) systems upon the relative sliding of the atomic layer
constituents. Because of the weak van der Waals (vdW) interactions between
layers, sliding and the associated symmetry breaking can occur at low energy
cost in materials such as transition-metal dichalcogenides. Here we discuss
theoretically the origin and quantitative understanding of the phenomenon by
focusing on a prototype structure, the WTe2 bilayer, where sliding
ferroelectricity was first experimentally observed. We show that excitonic
effects induce relevant energy band renormalizations in the ground state, and
exciton condensation contributes significantly to stabilizing ferroelectricity
upon sliding beyond previous predictions. Enhanced excitonic effects in 2D and
vdW sliding are general phenomena that point to sliding ferroelectricity as
relevant for a broad class of important materials, where the intrinsic electric
dipole can couple with other quantum phenomena and, in turn, an external
electric field can control the quantum phases through ferroelectricity in
unexplored ways.

</details>


### [232] [Obstruction-Driven Parity Inversion for Enhanced Optical Absorption in Hexagonal Transition Metal Dichalcogenides](https://arxiv.org/abs/2510.01575)
*Seungil Baek,Jun Jung,Yong-Hyun Kim*

Main category: cond-mat.mtrl-sci

TL;DR: 单分子六方过渡金属硫族化合物（h-TMDs）的d-d跃迁虽然名义上是偶极子禁戒的，但表现出显著的光学吸收，这是由于存在一种奇偶性倒置机制，该机制通过阻碍驱动的带倒置促进了带边附近的偶极子允许的光学跃迁。这为通过奇偶性控制调节光学性质提供了一种新颖的方法。


<details>
  <summary>Details</summary>
Motivation: 单分子六方过渡金属硫族化合物（h-TMDs）的光学吸收特性与其名义上偶极子禁戒的d-d跃迁存在矛盾，需要对其光学选择规则进行阐释。

Method: 通过比较平凡和受阻的原子极限相，阐明了阻碍驱动的带倒置如何通过 the intersite interactions between hybridized d orbitals 诱导奇偶性倒置，从而促进带边附近偶极子允许的光学跃迁。

Result: 阻碍驱动的带倒置通过杂化的d轨道之间的 the intersite interactions 诱导奇偶性倒置，从而在单分子h-TMDs的带边附近促进了偶极子允许的光学跃迁。

Conclusion: 奇偶性倒置机制解释了单分子h-TMDs中名义上偶极子禁戒的d-d跃迁表现出显著光学吸收的现象，并为通过奇偶性控制调节光学性质提供了新途径，连接了拓扑与光物质相互作用。

Abstract: The optical selection rule states that opposite parity between the valence
and conduction bands is required for optical absorption to occur. However,
monolayer hexagonal transition metal dichalcogenides (h-TMDs) such as $
\mathrm{MoS}_{2} $ exhibit pronounced optical absorption despite their
nominally dipole-forbidden d-d transitions. In this Letter, we elucidate a
parity inversion mechanism through which obstruction-driven band inversion
promotes dipole-allowed optical transitions near the band edge in monolayer
h-TMDs. By comparing trivial and obstructed atomic limit phases, we show that
intersite interactions between hybridized d orbitals induce parity inversion.
Our results provide a novel approach to tuning optical properties through
parity control, bridging the gap between topology and light-matter interaction.

</details>


### [233] [Electride behavior at high pressure in silicon and other elements in solid and liquid phases](https://arxiv.org/abs/2510.01583)
*Salma Ahmed,Felipe González-Cataldo,Victor Naden Robinson,Burkhard Militzer*

Main category: cond-mat.mtrl-sci

TL;DR: 元素硅在高压下可形成电子材料。


<details>
  <summary>Details</summary>
Motivation: 元素硅在高压下形成电子材料的性质研究。

Method: 使用第一性原理分子动力学模拟，结合电荷密度和电子局域化函数（ELF）分析，提出高压电子材料的分类标准，并计算X射线衍射图样。

Result: 发现元素硅在高压（超过400 GPa）下可以成为电子材料，并提出了判定高压电子材料的ELF和电荷密度阈值：(1) 最大ELF值大于0.7，(2) ELF盆地附近至少有0.9个电子，(3) 拉普拉斯电荷密度$
abla^2 ho(\mathbf{r}_0)$为负且其绝对值大于$10^{-3}\,e/\mathrm{bohr}^5$。计算了X射线衍射图样以评估电子材料形成的影响。

Conclusion: 提出了一套用于识别高压电子材料的理论框架，可为未来相关研究提供参考。

Abstract: Electrides are materials in which some of the electrons are localized at the
interstitial sites rather than around the atoms or along atomic bonds. Most
elemental electrides are either alkali metals or alkaline-earth metals because
of their low ionization potential. In this work, we report that elemental
silicon becomes an electride at pressures exceeding 400 GPa. With {\it ab
initio} molecular dynamics (MD) simulations, we study this behavior for
silicon, sodium, potassium, and magnesium at high pressure and temperature. We
performed simulations for liquids and ten crystal structures. Charge density
and electron localization functions (ELF) are analyzed for representative
configurations extracted from the MD trajectories. By analyzing a variety of
electride structures, we suggest the following quantitative thresholds for the
ELF and charge density in each interstitial site to classify high-pressure
electrides: (1) the maximum ELF value should be greater than 0.7, (2) there
should be at least 0.9 electrons near the ELF basin, and (3) the Laplacian
charge density, $\nabla^2 \rho(\mathbf{r}_0)$, should be negative with
magnitude greater than $10^{-3}\ e/\mathrm{bohr}^5$. Finally, we compute X-ray
diffraction patterns to determine the degree to which they are affected by the
electride formation. Overall, this framework could become a benchmark for
future theoretical and experimental studies on electrides.

</details>


### [234] [Multifunctional Oxide Nanosheets: Frictional, Hall, and Piezoelectric Deformation of 2D Ga2O3](https://arxiv.org/abs/2510.01697)
*Md Akibul Islam,Uichang Jeong,Nima Barri,Azmeera Jannat,Ali Zavabeti,Seungbum Hong,Tobin Filleter*

Main category: cond-mat.mtrl-sci

TL;DR: Beta-Ga2O3纳米片具有优异的绝缘稳定性、偏压可调的界面力学和可调的机电器电活性，在多功能氧化物纳米器件方面展现出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 与其他二维材料相比，原子级薄氧化物的多功能特性研究不足。特别是Ga2O3因其超宽带隙、热稳定性和机械刚性，在纳米机电系统中具有应用潜力。

Method: 利用基于原子力显微镜(AFM)的技术，包括摩擦力显微镜(FFM)、范德堡赫尔测量和压电响应力显微镜(PFM)，研究了beta-Ga2O3纳米片的摩擦学、传输和机电特性。

Result: 研究观察到beta-Ga2O3纳米片的摩擦力与外加偏压有关，这归因于缺陷介导的电荷俘获。范德堡赫尔测量证实了其超宽带隙特性，电子传输在高热激活下仍然受到抑制。PFM测量显示了可测量的逆机电响应，与氧空位引起的不对称性一致。

Conclusion: 整合摩擦学、电学和机电测量结果，证明了beta-Ga2O3纳米片作为一种独特的平台，集绝缘稳定性、偏压可调的界面力学和缺陷驱动的机电活性于一体，为开发多功能氧化物纳米器件提供了新的机会。

Abstract: Atomically thin oxides are increasingly recognized as an emerging class of 2D
materials, yet their multifunctional properties have been far less investigated
compared to other layered materials. Among these, gallium oxide is
distinguished by its ultrawide bandgap, thermal stability, and mechanical
rigidity, positioning it as a candidate material for nanoelectromechanical
systems. In this study, the tribological, transport, and electromechanical
properties of beta-Ga2O3 nanosheets were probed using atomic force microscopy
(AFM)--based techniques. Friction force microscopy (FFM) was used to
investigate interfacial sliding, and a dependence of friction on external bias
was observed, which was attributed to defect-mediated charge trapping. Van der
Pauw Hall measurements were conducted up to 400 $^{\circ}$C, through which the
ultrawide bandgap nature of beta-Ga2O3 was confirmed, as electronic transport
remained suppressed despite high thermal activation. Piezoresponse force
microscopy (PFM) was further applied, and a measurable converse
electromechanical response on the order of a few pm/V was revealed, consistent
with oxygen-vacancy--induced symmetry breaking. By integrating tribological,
electrical, and electromechanical measurements, it was demonstrated that
beta-Ga2O3 nanosheets present a unique platform in which insulating stability,
bias-tunable interfacial mechanics, and defect-enabled electromechanical
activity coexist, offering new opportunities for multifunctional oxide
nanodevices.

</details>


### [235] [Giant enhancement of terahertz high-harmonic generation by cavity engineering of Dirac semimetal](https://arxiv.org/abs/2510.01760)
*Siyu Duan,Lili Shi,Patrick Pilch,Anneke Reinold,Sergey Kovalev,Renato M. A. Dantas,Yunkun Yang,Faxian Xiu,Miriam Serena Vitiello,Zhe Wang*

Main category: cond-mat.mtrl-sci

TL;DR: 基于Cd3As2的腔工程狄拉克半金属微结构在太赫兹波段表现出极强的非线性光学效应，能将三倍和五倍谐波的产率提高三个数量级以上。


<details>
  <summary>Details</summary>
Motivation: 与已得到广泛研究的石墨烯微腔相比，三维狄拉克半金属的腔工程化设计在促进和提升光学非线性方面仍有待探索。

Method: 制造了一种包含金属超材料微腔和纳米薄膜Cd3As2（三维狄拉克半金属）的器件，通过微腔共振显著增强太赫兹脉冲的近场强度，从而驱动狄拉克费米子的非线性响应。

Result: 观察到太赫兹三倍和五倍谐波产率的显著增强，提高了三个数量级以上，并且高次谐波产生几乎饱和。

Conclusion: 腔工程化的三维狄拉克半金属微结构能够实现异常强的太赫兹非线性光学效应，为太赫兹光电子学应用提供了新的机遇。

Abstract: Engineered micro- or nano-structures based on nonlinear optical materials
offer versatile opportunities for optoelectronic applications. While extensive
efforts have been devoted to design tailored microcavities to promote and
increase the optical nonlinearities of graphene, the potential of engineering
its three-dimensional counterparts -- three-dimensional Dirac semimetals --
remains largely unexplored. Here we report on exceptionally strong terahertz
nonlinearities in a cavity-engineered Dirac semimetal microstructure, and
demonstrate a giant enhancement of terahertz third- and fifth-order harmonic
yields by more than three orders of magnitude. By fabricating a designed
structure of metallic metasurface microcavities on a nanometer thin film of the
threedimensional Dirac semimetal Cd3As2, we significantly enhance the
near-field intensity of a picosecond terahertz excitation pulse in resonance
with the microcavity eigenmode. This transiently modifies the nonlinearities of
the thin film and drives the nonlinear responses of the Dirac fermions from a
weakly to a deeply nonperturbative regime where the observed high-harmonic
generation essentially saturates.

</details>


### [236] [Metallurgy at the nanoscale: domain walls in nanoalloys](https://arxiv.org/abs/2510.01769)
*Grégoire Breyton,Hakim Amara,Jaysen Nelayah,Christine Mottet,Riccardo Gatti,Jérôme Creuze,Adrien Moncomble,Damien Alloyeau,Nathaly Ortiz Peña,Guillaume Wang,Christian Ricolleau*

Main category: cond-mat.mtrl-sci

TL;DR: 纳米尺度下二元合金中的畴壁形成机制。


<details>
  <summary>Details</summary>
Motivation: 在二元合金中，畴壁在相变和物理性质中起着核心作用。虽然理论上认为纳米尺度下畴壁无法存在，但本研究旨在探索其形成机制。

Method: 通过实验和数值方法研究CuAu纳米颗粒二元模型系统中的取向畴壁形成。

Result: 研究发现，较大纳米颗粒中畴的形成受弹性应变弛豫驱动，而较小纳米颗粒则受表面效应主导。此外，多变体纳米颗粒倾向于通过连续弹性模型形成各向同性材料。

Conclusion: 本研究通过实验和数值方法揭示了二元合金纳米颗粒中畴壁的形成机制，解释了尺寸效应和表面效应对畴形成的影响，并提出了多变体纳米颗粒形成各向同性材料的机制。

Abstract: In binary alloys, domain walls play a central role not only on the phase
transitions but also on their physical properties and were at the heart of the
70's metallurgy research. Whereas it can be predicted, with simple physics
arguments, that such domain walls cannot exist at the nanometer scale due to
the typical lengths of the statistical fluctuations of the order parameter,
here we show, with both experimental and numerical approaches how orientational
domain walls are formed in CuAu nanoparticles binary model systems. We
demonstrate that the formation of domains in larger NPs is driven by elastic
strain relaxation which is not needed in smaller NPs where surface effects
dominate. Finally, we show how the multivariants NPs tend to form an isotropic
material through a continuous model of elasticity.

</details>


### [237] [Machine-learning-enabled methodology for the ab-initio simulations of sub-$μ$m-wide nanoribbons](https://arxiv.org/abs/2510.01802)
*Guan-Hao Peng,Chin-Jui Huang,Wen-Teng Yang,Shun-Jen Cheng*

Main category: cond-mat.mtrl-sci

TL;DR: 利用机器学习增强的紧束缚模型，实现对二维材料纳结构电子特性的高效且精确的模拟。


<details>
  <summary>Details</summary>
Motivation: 第一性原理方法计算精度高但计算成本昂贵，无法满足大规模系统模拟需求；而经验带理论计算效率高但参数拟合存在局限性，忽略了波函数信息且参数迁移性差。现有方法难以在计算效率和可靠性之间取得平衡。

Method: 提出了一种结合第一性原理、机器学习和紧束缚理论（Wannier tight-binding, WTB）的方法。首先，利用小尺度纳结构的第一性原理计算得到的WTB参数作为机器学习（ML）模型的训练数据。为解决Wannier函数存在的规范自由度问题，引入了规范无关（gauge-independent, GI）基组，将WTB模型转化为GI-WTB模型。在此基础上，进行参数拟合和ML预测，构建了机器学习GI-WTB（ML-GI-WTB）模型。

Result: 将ML-GI-WTB模型应用于MoS2 armchair-edge纳米带的模拟，结果显示与第一性原理计算结果高度一致。该模型能够可靠地模拟亚微米宽度的纳米带，展示了其在处理更大尺度和更复杂结构方面的潜力。

Conclusion: 所提出的ML-GI-WTB框架能够以接近第一性原理的可靠性，实现对实际纳结构电子特性的高效模拟，为解决传统第一性原理方法计算限制的问题提供了可扩展的解决方案。

Abstract: Simulation of mesoscopic nanostructures is a central challenge in condensed
matter physics and device applications. First-principles methods provide
accurate electronic structures but are computationally prohibitive for large
systems, while empirical band theories are efficient yet limited by parameter
fitting that neglects wavefunction information and often yields
non-transferable parameters. We propose a methodology that bridges these
approaches, achieving first-principles-level reliability with computational
efficiency through a machine-learning-enabled tight-binding framework. Our
approach starts with Wannier tight-binding (WTB) parameters from small
nanostructures, which serve as training data for machine learning (ML). To
remove the gauge freedom of Wannier functions that obscures size- and
geometry-dependent parameter trends, we construct gauge-independent (GI) bases
and transform the WTB model into a gauge-independent WTB (GI-WTB) model. This
enables robust parameter fitting and ML prediction of parameter variations,
yielding the machine-learning GI-WTB (ML-GI-WTB) model. Applied to MoS2
armchair-edge nanoribbons, the ML-GI-WTB model shows excellent agreement with
first-principles results and enables reliable simulations of sub-$\mu$m-wide
nanoribbons. This framework provides a scalable tool for predicting electronic
properties of realistic nanostructures beyond the reach of conventional
first-principles methods.

</details>


### [238] [Accurate Machine-Learning Description for SiC in Extreme Environments](https://arxiv.org/abs/2510.01827)
*Jintong Wu,Zhuang Shao,Junlei Zhao,Flyura Djurabekova,Kai Nordlund,Fredric Granberg,Qingmin Zhang,and Jesper Byggmästar*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究开发了一种高效的机器学习原子间势能（ML-IAP），可用于模拟数百万原子的SiC材料，并研究了2H和3C多晶型物的相图和辐照损伤。


<details>
  <summary>Details</summary>
Motivation: SiC材料在核材料、机械部件和半导体等领域应用广泛，计算模拟对其研究至关重要，但现有方法存在效率和精度限制。

Method: 开发了一种计算高效且通用的机器学习原子间势能（ML-IAP），并使用它进行了多尺度原子分子动力学（MD）模拟，系统地绘制了2H和3C多晶型物的压温相图（P-T相图）和阈值位移能（TDE）分布。此外，还进行了碰撞级联模拟，深入研究了多晶型物依赖的初级辐照损伤团簇行为。

Result: 成功绘制了2H和3C SiC多晶型物的P-T相图和TDE分布，并揭示了传统经验势能无法准确捕捉的多晶型物依赖的初级辐照损伤团簇行为。

Conclusion: 本研究开发的ML-IAP能够高效准确地模拟SiC材料，为理解其在不同条件下的相变行为和辐照损伤机制提供了新的视角。

Abstract: Silicon carbide (SiC) polymorphs are widely employed as nuclear materials,
mechanical components, and wide-bandgap semiconductors. The rapid advancement
of SiC-based applications has been complemented by computational modeling
studies, including both ab initio and classical atomistic approaches. In this
work, we develop a computationally efficient and general-purpose
machine-learned interatomic potential (ML-IAP) capable of multimillion-atom
molecular dynamics (MD) simulations over microsecond timescales. Using ML-IAP,
we systematically map the comprehensive pressure-temperature phase diagram (P-T
phase diagram) and the threshold displacement energy (TDE) distributions for
the 2H and 3C polymorphs. Furthermore, collision cascade simulations provide
in-depth insights into polymorph-dependent primary radiation damage clustering,
a phenomenon that conventional empirical potentials fail to accurately capture.

</details>


### [239] [Enhancing the Efficiency of Time-Dependent Density Functional Theory Calculations of Dynamic Response Properties](https://arxiv.org/abs/2510.01875)
*Zhandos A. Moldabekov,Sebastian Schwalbe,Uwe Hernandez Acosta,Thomas Gawne,Jan Vorberger,Michele Pavanello,Tobias Dornheim*

Main category: cond-mat.mtrl-sci

TL;DR: TDDFT通过将动态结构因子与虚时密度-密度关联函数进行一对一映射，并结合严格的虚时收敛性测试和基于约束的噪声衰减技术，将计算效率提高了近一个数量级。


<details>
  <summary>Details</summary>
Motivation: TDDFT是一种精确的从头计算方法，用于模拟极端条件下的材料性质，但计算成本高昂。

Method: 提出一种将动态结构因子与虚时密度-密度关联函数进行一对一映射的方法，并结合收敛性测试和噪声衰减技术来提高TDDFT计算效率。

Result: 计算速度提升高达一个数量级，有望为模拟一次极端条件下的X射线汤姆逊散射测量节省数百万CPU小时。

Conclusion: 所提出的方法可以显著提高TDDFT模拟的效率，有望为极端条件下材料性质的研究节省大量的计算资源。

Abstract: X-ray Thomson scattering (XRTS) constitutes an essential technique for
diagnosing material properties under extreme conditions, such as high pressures
and intense laser heating. Time-dependent density functional theory (TDDFT) is
one of the most accurate available ab initio methods for modeling XRTS spectra,
as well as a host of other dynamic material properties. However, strong thermal
excitations, along with the need to account for variations in temperature and
density as well as the finite size of the detector significantly increase the
computational cost of TDDFT simulations compared to ambient conditions. In this
work, we present a broadly applicable method for optimizing and enhancing the
efficiency of TDDFT calculations. Our approach is based on a one-to-one mapping
between the dynamic structure factor and the imaginary time density--density
correlation function, which naturally emerges in Feynman's path integral
formulation of quantum many-body theory. Specifically, we combine rigorous
convergence tests in the imaginary time domain with a constraints-based noise
attenuation technique to improve the efficiency of TDDFT modeling without the
introduction of any significant bias. As a result, we can report a speed-up by
up to an order of magnitude, thus potentially saving millions of CPU hours for
modeling a single XRTS measurement of matter under extreme conditions.

</details>


### [240] [Spin-phonon coupling and isotope-related pseudo-molecule vibrations in layered Cr$_2$Ge$_2$Te$_6$ ferromagnet](https://arxiv.org/abs/2510.01881)
*Grzegorz Krasucki,Katarzyna Olkowska-Pucko,Tomasz Woźniak,Mihai I. Sturza,Holger Kohlmann,Adam Babiński,Maciej R. Molas*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过高分辨率拉曼光谱和密度泛函理论研究了Cr2Ge2Te6 (CGT) 的振动结构，发现了强自旋-声子耦合，并揭示了其在不同温度下的磁序转变与拉曼模式之间的关联。


<details>
  <summary>Details</summary>
Motivation: 研究Cr2Ge2Te6 (CGT) 的振动结构，探索其自旋-声子耦合效应，并阐明其磁性转变过程。

Method: 利用高分辨率拉曼光谱测量CGT在5 K至300 K范围内的拉曼光谱，并通过密度泛函理论计算进行模式分析。同时，利用包含Ge同位素效应的模型来模拟A_g^5模式的形状。

Result: 测量得到了CGT的10个拉曼活性模式 (5A_g和5E_g)。在约150 K和60 K时观察到强烈的磁振声子耦合，分别对应于局域磁序的出现和向完整铁磁相的转变。通过考虑Ge同位素效应的模型成功模拟了A_g^5模式的独特形状。

Conclusion: Cr2Ge2Te6 表现出强烈的自旋-声子耦合，其拉曼光谱对温度敏感，并且可以反映材料的磁性转变过程。A_g^5模式的形状与Ge同位素有关。

Abstract: The vibrational structure of chromium germanium telluride
(Cr$_2$Ge$_2$Te$_6$, CGT) is investigated and a strong spin-phonon coupling is
revealed. The measured high-resolution Raman scattering (RS) spectra are
composed of the 10 Raman-active modes: 5A$_\textrm{g}$ and 5E$_\textrm{g}$,
predicted by calculation using the density functional theory and identified
using polarization-resolved RS measurements. We also studied the effect of
temperature on the RS spectra of CGT from 5~K to 300~K. A strong magneto-phonon
coupling in CGT is revealed at temperatures of about 150~K and 60~K, which are
associated with the appearance of the local magnetic order in the material and
the transition to the complete ferromagnetic phase, respectively. Moreover, a
unique shape of the A$_g^5$ mode composed of a set of very narrow Raman peaks
is simulated using a model that takes into account vibrations of Ge-Ge
pseudo-molecules for various Ge isotopes.

</details>


### [241] [Quantum Effects or Theoretical Artifacts? A Computational Reanalysis of Hydrogen at High-Pressure](https://arxiv.org/abs/2510.02098)
*Stefano Racioppi,Eva Zurek*

Main category: cond-mat.mtrl-sci

TL;DR: meta-GGA泛函（R2SCAN和SCAN0）比PBE更能准确地描述高压下氢的相图和键合特性。


<details>
  <summary>Details</summary>
Motivation: 高压下氢的稳定相问题是凝聚态物理学中的一个核心问题，实验观测和理论预测都对方法学选择非常敏感。

Method: 使用meta-GGA泛函（R2SCAN和SCAN0）并与PBE进行比较，研究400-700 GPa范围内氢的冷相图。

Result: meta-GGA泛函预测的分子相（Cmca-4, Cmca-12, and C2/c）比PBE能稳定到更高的压力，这与扩散蒙特卡洛计算和实验观测更一致。R2SCAN计算的声子谱显示，之前的动力学不稳定性和负调和特征消失了。PBE会人为地削弱分子内H-H键并增强分子间相互作用，而meta-GGA则保持了更强的分子特性。

Conclusion: 准确描述势能面，特别是其在平衡位置附近的曲率，对于评估高压下氢的相稳定性和键合特性至关重要。

Abstract: The stability of high-pressure phases of hydrogen remains a central question
in condensed matter physics, where both experimental observations and
theoretical predictions are highly sensitive to methodological choices. Here,
we revisit the cold phase diagram of hydrogen between 400 and 700 GPa using the
meta-GGA functionals (R2SCAN and SCAN0) and compare the results with the more
common PBE. At the meta-GGA level, molecular phases (Cmca-4, Cmca-12, and C2/c)
are stabilized over the atomic I41/amd phase up to significantly higher
pressures than predicted by GGA, in closer agreement with diffusion Monte Carlo
calculations and experimental observations of band-gap closure near 425 GPa.
Furthermore, phonon spectra calculated with R2SCAN show that the dynamical
instabilities and anharmonic signatures previously predicted at the GGA level
vanish, indicating that such effects may partly arise from functional
deficiencies rather than genuine nuclear quantum effects. Bonding analysis
reveals that PBE artificially weakens intramolecular H-H bonds and enhances
intermolecular interactions through charge delocalization, whereas meta-GGA
preserves a more localized molecular character. Anharmonic motion remains
relevant for finite-temperature dynamics; however, we demonstrate that the
accurate description of the potential energy surface - particularly its
curvature near equilibrium - is pivotal for assessing both phase stability and
bonding of hydrogen at high-pressure.

</details>


### [242] [Reversal of strain state in a Mott insulator thin film by controlling substrate morphology](https://arxiv.org/abs/2510.02234)
*Reetendra Singh,Abhishek Rakshit,Galit Atiya,Michael Kalina,Yaron Kauffmann,Yoav Kalcheim*

Main category: cond-mat.mtrl-sci

TL;DR: 通过改变衬底形貌，利用热膨胀失配诱导应变，可调控V2O3薄膜的相变，实现不同电阻率的相。


<details>
  <summary>Details</summary>
Motivation: 研究V2O3薄膜的相稳定性，探索通过热膨胀失配调控应变的新机制。

Method: 通过改变衬底形貌（平坦或阶梯状），利用V2O3与蓝宝石之间的热膨胀失配诱导应变，并使用高分辨率扫描透射电子显微镜（HRSTEM）表征。

Result: 可以实现强压应变或张应变，完全抑制金属-绝缘体转变或稳定绝缘相，导致电阻率差异达数个数量级。

Conclusion: 热膨胀失配是一种未被充分探索的调控薄膜应变的方法，加深了对结构自由度对Mott绝缘体相稳定性影响的理解，并可能应用于需要室温以上绝缘体-金属转换的场景。

Abstract: The V2O3 phase diagram contains two insulating phases and one metallic phase
with different lattice structures. The stability of these phases is very
sensitive to pressure, offering a mechanism to tune phase transitions by
inducing strain in thin films. The most studied source of strain is lattice
mismatch between the film and the substrate. In this work, however, we find
that the film/substrate thermal expansion mismatch can be made to play a
dominant role by modifying the substrate morphology. When grown on sapphire,
the lattice mismatch induces compressive strain in the V2O3 films, whereas
thermal expansion mismatch induces tensile strain. We find that minute changes
in substrate morphology may relax the compressive strain component, allowing
the thermally-induced tensile component to overcome it. Thus, by simple
annealing of the substrates to create either a flat or stepped morphology,
strongly compressive or tensile strains may be induced in the films. This
results in either full suppression of the metal-insulator transition or
stabilization of insulating phases at all temperatures, exhibiting many orders
of magnitude differences in film resistivity. To elucidate the strain
relaxation mechanism, we use high-resolution scanning transmission electron
microscopy (HRSTEM) to image the atomic steps in the substrate and the adjacent
crystallographic defects in the V2O3. These findings offer a hitherto
underexplored mechanism to tune strain in thin films, deepen our understanding
of the effects of structural degrees of freedom on phase stability of a
canonical Mott insulator and may allow for applications requiring
insulator-metal switching above room temperature.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [243] [A Control Theory inspired Exploration Method for a Linear Bandit driven by a Linear Gaussian Dynamical System](https://arxiv.org/abs/2510.01364)
*Jonathan Gornet,Yilin Mo,Bruno Sinopoli*

Main category: eess.SY

TL;DR: 该论文提出了在线性土匪环境中，使用Kalman-UCB和IDEA两种算法来解决探索与利用的权衡问题，并提供了一个基于LGDS属性的度量标准来预测算法性能。


<details>
  <summary>Details</summary>
Motivation: 解决在已知线性高斯动态系统（LGDS）的奖励环境中，探索与利用之间的权衡挑战，特别是针对机器学习中的超参数优化等应用场景，以应对大的动作空间问题。

Method: 提出Kalman-UCB算法，该算法遵循不确定性下的乐观原则。提出IDEA算法，该算法选择最大化预测奖励和最小化卡尔曼滤波器状态预测误差组合的动作，该误差项依赖于LGDS的可观测性。提供一个基于LGDS属性的度量标准来预测Kalman-UCB和IDEA的性能。

Result: 通过在各种随机生成的环境中进行数值结果验证，证明了所提出度量标准的有效性。

Conclusion: 提出的Kalman-UCB和IDEA算法能够有效处理探索与利用的权衡问题，并且所提出的度量标准可以预测哪种算法在该环境下表现更好。

Abstract: The paper introduces a linear bandit environment where the reward is the
output of a known Linear Gaussian Dynamical System (LGDS). In this environment,
we address the fundamental challenge of balancing exploration -- gathering
information about the environment -- and exploitation -- selecting to the
action with the highest predicted reward. We propose two algorithms, Kalman
filter Upper Confidence Bound (Kalman-UCB) and Information filter Directed
Exploration Action-selection (IDEA). Kalman-UCB uses the principle of optimism
in the face of uncertainty. IDEA selects actions that maximize the combination
of the predicted reward and a term that quantifies how much an action minimizes
the error of the Kalman filter state prediction, which depends on the LGDS
property called observability. IDEA is motivated by applications such as
hyperparameter optimization in machine learning. A major problem encountered in
hyperparameter optimization is the large action spaces, which hinder the
performance of methods inspired by principle of optimism in the face of
uncertainty as they need to explore each action to lower reward prediction
uncertainty. To predict if either Kalman-UCB or IDEA will perform better, a
metric based on the LGDS properties is provided. This metric is validated with
numerical results across a variety of randomly generated environments.

</details>


### [244] [Robust Data-Driven Control for Nonlinear Systems Using their Digital Twins and Quadratic Funnels](https://arxiv.org/abs/2510.01406)
*Shiva Shakeri,Mehran Mesbahi*

Main category: eess.SY

TL;DR: 提出一种利用不完美的数字孪生系统进行鲁棒安全部署的方法，通过融合数字孪生的标称轨迹和在线数据驱动的不确定性量化来合成鲁棒跟踪控制器。


<details>
  <summary>Details</summary>
Motivation: 开发一种安全部署具有非线性动力学的系统的方法，即使在使用不完美的数字孪生模型时也能确保安全性。

Method: 提出一种融合数字孪生标称轨迹和在线数据驱动不确定性量化来合成鲁棒跟踪控制器的方法。具体地，推导数据驱动的边界来捕捉实际系统与其数字孪生信息所规定的标称轨迹之间的偏差，然后利用这些时间序列数据通过线性矩阵不等式合成二次漏斗，并采用分段学习策略来适应真实系统行为。

Result: 开发了一个系统框架，用于在具有不完美模型的学习控制非线性系统中获得安全证书。

Conclusion: 该方法为在具有不完美模型的学习控制非线性系统中获得安全证书提供了一个系统框架。

Abstract: This paper examines a robust data-driven approach for the safe deployment of
systems with nonlinear dynamics using their imperfect digital twins. Our
contribution involves proposing a method that fuses the digital twin's nominal
trajectory with online, data-driven uncertainty quantification to synthesize
robust tracking controllers. Specifically, we derive data-driven bounds to
capture the deviations of the actual system from its prescribed nominal
trajectory informed via its digital twin. Subsequently, the dataset is used in
the synthesis of quadratic funnels -- robust positive invariant tubes around
the nominal trajectory -- via linear matrix inequalities built on the
time-series data. The resulting controller guarantees constraint satisfaction
while adapting to the true system behavior through a segmented learning
strategy, where each segment's controller is synthesized using uncertainty
information from the previous segment. This work establishes a systematic
framework for obtaining safety certificates in learning-based control of
nonlinear systems with imperfect models.

</details>


### [245] [A New Partial State-Feedback IDA-PBC for Two-Dimensional Nonlinear Systems: Application to Power Converters with Experimental Results](https://arxiv.org/abs/2510.01425)
*Rafael Cisneros,Leyan Fang,Wei He,Romeo Ortega*

Main category: eess.SY

TL;DR: 本文提出了一种基于 Poincare 引理的 IDA-PBC 控制器设计方法，用于设计二维系统的全局稳定输出反馈控制器。


<details>
  <summary>Details</summary>
Motivation: 设计能够全局稳定二维系统的输出反馈控制器，并简化 IDA-PBC 方法中求解偏微分方程的难度。

Method: 使用 Poincare 引理替换 IDA-PBC 中求解偏微分方程，转而求解常微分方程，并将其应用于 DC-DC 变换器（Buck、Boost、Buck-Boost）的电压反馈控制器设计。

Result: 所提出的方法可以设计出用于 DC-DC 变换器的电压反馈控制器，并且可以处理不确定的负载。对于包含电阻和恒功率负载的并联负载，提出了一种自适应版本，并加入了负载参数辨识方案。

Conclusion: 该方法通过数值仿真和实验结果验证了其有效性，能够处理不确定的负载，并通过自适应控制实现负载变化情况下的输出调节。

Abstract: In this paper we propose a variation of the widely popular
Interconnection-and-Damping-Assigment Passivity-Based Control (IDA-PBC) based
on Poincare's Lemma to design output feedback globally stabilizing controllers
for two dimensional systems. The procedure is constructive and, in comparison
with the classical IDA-PBC, whose application is often stymied by the need to
solve the (infamous) matching partial differential equation (PDE), in this new
method the PDE is replaced by an ordinary differential equation, whose solution
is far simpler. The procedure is then applied for the design of
voltage-feedback controllers for the three most typical DC-to-DC power
converter topologies: the Buck, Boost and Buck-Boost. It is assumed that these
converters feed an uncertain load, which is characterized by a static relation
between its voltage and current. In the case when the load consists of the
parallel connection of a resistive term and a constant power load we propose an
adaptive version of the design, adding an identification scheme for the load
parameters. This allows the controller to regulate the converter output when
the load varies-that is a typical scenario in these applications. Extensive
numerical simulations and experimental results validate the approach.

</details>


### [246] [Comparative Field Deployment of Reinforcement Learning and Model Predictive Control for Residential HVAC](https://arxiv.org/abs/2510.01475)
*Ozan Baris Mulayim,Elias N. Pergantis,Levi D. Reyes Premer,Bingqing Chen,Guannan Qu,Kevin J. Kircher,Mario Bergés*

Main category: eess.SY

TL;DR: RL在节省能源方面表现出色，但MPC在能源效率和舒适度方面略胜一筹。


<details>
  <summary>Details</summary>
Motivation: 为了解决HVAC系统在实际应用中面临的工程和可扩展性挑战，同时探索强化学习（RL）在实际住宅环境中的应用潜力。

Method: 将MPC和基于模型的RL控制器部署在一个有人居住的房屋中，并与现有控制器进行比较。

Result: RL实现了22%的节能，略高于MPC的20%，但舒适度略有下降。当按舒适度对节能进行归一化时，MPC表现更优。RL虽然减少了工程开销，但在模型准确性和运行鲁棒性方面存在实际的权衡。

Conclusion: RL在HVAC控制领域具有潜力，但仍需解决安全初始化、控制-执行不匹配和在线学习完整性等挑战，才能实现可扩展的解决方案。

Abstract: Advanced control strategies like Model Predictive Control (MPC) offer
significant energy savings for HVAC systems but often require substantial
engineering effort, limiting scalability. Reinforcement Learning (RL) promises
greater automation and adaptability, yet its practical application in
real-world residential settings remains largely undemonstrated, facing
challenges related to safety, interpretability, and sample efficiency. To
investigate these practical issues, we performed a direct comparison of an MPC
and a model-based RL controller, with each controller deployed for a one-month
period in an occupied house with a heat pump system in West Lafayette, Indiana.
This investigation aimed to explore scalability of the chosen RL and MPC
implementations while ensuring safety and comparability. The advanced
controllers were evaluated against each other and against the existing
controller. RL achieved substantial energy savings (22\% relative to the
existing controller), slightly exceeding MPC's savings (20\%), albeit with
modestly higher occupant discomfort. However, when energy savings were
normalized for the level of comfort provided, MPC demonstrated superior
performance. This study's empirical results show that while RL reduces
engineering overhead, it introduces practical trade-offs in model accuracy and
operational robustness. The key lessons learned concern the difficulties of
safe controller initialization, navigating the mismatch between control actions
and their practical implementation, and maintaining the integrity of online
learning in a live environment. These insights pinpoint the essential research
directions needed to advance RL from a promising concept to a truly scalable
HVAC control solution.

</details>


### [247] [A Robust Neural Control Design for Multi-drone Slung Payload Manipulation with Control Contraction Metrics](https://arxiv.org/abs/2510.01489)
*Xinyuan Liang,Longhao Qian,Yi Lok Lo,Hugh H. T. Liu*

Main category: eess.SY

TL;DR: 本文提出了一种用于三无人机吊载运输系统的鲁棒神经控制设计，该系统能够在外部干扰下跟踪参考路径。


<details>
  <summary>Details</summary>
Motivation: 为三无人机吊载运输系统在外部干扰下跟踪参考路径提供一种鲁棒的神经控制设计。

Method: 使用控制收缩度量（CCM）生成符合控制输入饱和约束的神经基线控制器，并结合不确定性和扰动估计器（UDE）技术动态补偿持续扰动。

Result: 所提出的框架实现了模块化设计，控制器和估计器可独立工作，并在满足某些假设的扰动条件下实现零轨迹跟踪误差。文章还给出了包含CCM控制器和UDE补偿器的完整系统的稳定性和鲁棒性分析。

Conclusion: 通过仿真验证了所提出的控制设计在复杂轨迹跟踪和外部干扰下的能力。

Abstract: This paper presents a robust neural control design for a three-drone slung
payload transportation system to track a reference path under external
disturbances. The control contraction metric (CCM) is used to generate a neural
exponentially converging baseline controller while complying with control input
saturation constraints. We also incorporate the uncertainty and disturbance
estimator (UDE) technique to dynamically compensate for persistent
disturbances. The proposed framework yields a modularized design, allowing the
controller and estimator to perform their individual tasks and achieve a zero
trajectory tracking error if the disturbances meet certain assumptions. The
stability and robustness of the complete system, incorporating both the CCM
controller and the UDE compensator, are presented. Simulations are conducted to
demonstrate the capability of the proposed control design to follow complicated
trajectories under external disturbances.

</details>


### [248] [Off-Policy Reinforcement Learning with Anytime Safety Guarantees via Robust Safe Gradient Flow](https://arxiv.org/abs/2510.01492)
*Pol Mestres,Arnau Marzabal,Jorge Cortés*

Main category: eess.SY

TL;DR: 该论文提出了一种名为RSGF-RL的算法，用于解决具有任意时间保证的约束强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决约束强化学习问题，并提供任意时间保证，即算法的解在每次迭代时都必须满足约束。

Method: 该算法基于对连续时间动态鲁棒安全梯度流（RSGF）的离散化，RSGF-RL是一种离策略算法，它使用情节数据来估计值函数及其梯度，并通过求解一个凸二次约束二次规划问题来更新策略参数。技术分析结合了统计分析、随机逼近理论和凸分析。

Result: 该算法能够确保安全策略更新为安全策略，并能从不安全策略中恢复，其恢复概率可由用户任意指定。同时，该算法几乎可以肯定地渐近收敛到强化学习问题的KKT点集。仿真结果表明，RSGF-RL在导航和倒立摆系统上表现优于现有技术。

Conclusion: RSGF-RL是一种有效的约束强化学习算法，能够提供任意时间保证，并在实践中表现出色。

Abstract: This paper considers the problem of solving constrained reinforcement
learning (RL) problems with anytime guarantees, meaning that the algorithmic
solution must yield a constraint-satisfying policy at every iteration of its
evolution. Our design is based on a discretization of the Robust Safe Gradient
Flow (RSGF), a continuous-time dynamics for anytime constrained optimization
whose forward invariance and stability properties we formally characterize. The
proposed strategy, termed RSGF-RL, is an off-policy algorithm which uses
episodic data to estimate the value functions and their gradients and updates
the policy parameters by solving a convex quadratically constrained quadratic
program. Our technical analysis combines statistical analysis, the theory of
stochastic approximation, and convex analysis to determine the number of
episodes sufficient to ensure that safe policies are updated to safe policies
and to recover from an unsafe policy, both with an arbitrary user-specified
probability, and to establish the asymptotic convergence to the set of KKT
points of the RL problem almost surely. Simulations on a navigation example and
the cart-pole system illustrate the superior performance of RSGF-RL with
respect to the state of the art.

</details>


### [249] [Cooperative Guidance for Aerial Defense in Multiagent Systems](https://arxiv.org/abs/2510.02087)
*Shivam Bajpai,Abhinav Sinha,Shashi Ranjan Kumar*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper addresses a critical aerial defense challenge in contested
airspace, involving three autonomous aerial vehicles -- a hostile drone (the
pursuer), a high-value drone (the evader), and a protective drone (the
defender). We present a cooperative guidance framework for the evader-defender
team that guarantees interception of the pursuer before it can capture the
evader, even under highly dynamic and uncertain engagement conditions. Unlike
traditional heuristic, optimal control, or differential game-based methods, we
approach the problem within a time-constrained guidance framework, leveraging
true proportional navigation based approach that ensures robust and guaranteed
solutions to the aerial defense problem. The proposed strategy is
computationally lightweight, scalable to a large number of agent
configurations, and does not require knowledge of the pursuer's strategy or
control laws. From arbitrary initial geometries, our method guarantees that key
engagement errors are driven to zero within a fixed time, leading to a
successful mission. Extensive simulations across diverse and adversarial
scenarios confirm the effectiveness of the proposed strategy and its relevance
for real-time autonomous defense in contested airspace environments.

</details>


### [250] [Probabilistic Control Barrier Functions: Safety in Probability for Discrete-Time Stochastic Systems](https://arxiv.org/abs/2510.01501)
*Pol Mestres,Blake Werner,Ryan K. Cosner,Aaron D. Ames*

Main category: eess.SY

TL;DR: 提出了一种用于离散时间随机系统的安全控制器设计方法，该方法通过引入概率控制屏障函数（pCBF）来处理不确定性，并提供了有限时间步内具有可调概率的安全保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的控制系统面临不可预测的不确定性，这可能导致确定性保证失效和灾难性的安全故障。

Method: 修改了传统的控制屏障函数（CBF）概念，使其能够显式处理随机不确定性，从而提出了概率控制屏障函数（pCBF）。利用不确定性量化方法（如集中不等式、场景方法和一致预测）来推导计算上可行的控制器，并提供可调的概率安全保证。

Result: 证明了概率控制屏障函数（pCBF）可用于设计控制器，从而在有限的时间步内以预定的概率保证安全。

Conclusion: 所提出的概率控制屏障函数（pCBF）方法能够为离散时间随机系统设计具有可调概率安全保证的控制器，并在仿真和硬件实验中展示了其在四足机器人控制中的应用。

Abstract: Control systems operating in the real world face countless sources of
unpredictable uncertainties. These random disturbances can render deterministic
guarantees inapplicable and cause catastrophic safety failures. To overcome
this, this paper proposes a method for designing safe controllers for
discrete-time stochastic systems that retain probabilistic guarantees of
safety. To do this we modify the traditional notion of a control barrier
function (CBF) to explicitly account for these stochastic uncertainties and
call these new modified functions probabilistic CBFs. We show that
probabilistic CBFs can be used to design controllers that guarantee safety over
a finite number of time steps with a prescribed probability. Next, by
leveraging various uncertainty quantification methods, such as concentration
inequalities, the scenario approach, and conformal prediction, we provide a
variety of sufficient conditions that result in computationally tractable
controllers with tunable probabilistic guarantees across a plethora of
practical scenarios. Finally, we showcase the applicability of our results in
simulation and hardware for the control of a quadruped robot.

</details>


### [251] [A Scalable Design Approach to Resilient Architectures for Interconnected Cyber-Physical Systems: Safety Guarantees under Multiple Attacks](https://arxiv.org/abs/2510.01541)
*Eman Badr,Abdullah Al Maruf*

Main category: eess.SY

TL;DR: 该研究提出了一种可扩展的框架，用于为互联网络物理系统（CPS）分配弹性架构并调整其恢复时间，以应对可能发生的复杂攻击。


<details>
  <summary>Details</summary>
Motivation: 互联CPS的弹性架构设计面临挑战，因为需要考虑多子系统攻击的任意顺序和时间重叠，而现有技术仅限于单系统设置。

Method: 研究引入了一个量化子系统安全影响的标量指数，该指数可线性聚合，从而实现对任意攻击顺序和时间重叠的可扩展分析。基于此，研究推导了一个线性不等式，用于保证安全并指导弹性架构分配，同时提出了一种基于分段的方法来加强条件，并开发了计算指数和寻找最优架构分配的算法。

Result: 通过在互联房间温度调节的案例研究中，针对不同攻击场景的验证，证明了该框架的有效性。

Conclusion: 该框架为互联CPS的网络弹性架构设计和恢复时间调整提供了可扩展且有保障的解决方案。

Abstract: Complex, interconnected cyber-physical systems (CPS) are increasingly
prevalent in domains such as power systems. Cyber-resilient architectures have
been proposed to recover compromised cyber components of CPS. Recent works have
studied tuning the recovery times of such architectures to guarantee safety in
single-system settings. Extending these designs to interconnected CPS is more
challenging, since solutions must account for attacks on multiple subsystems
that can occur in any order and potentially infinite possible temporal overlap.
This paper aims to address the aforementioned challenge by developing a
scalable framework to assign resilient architectures and to inform the tuning
of their recovery times. Our approach introduces a scalar index that quantifies
the impact of each subsystem on safety under compromised input. These indices
aggregate linearly across subsystems, enabling scalable analysis under
arbitrary attack orderings and temporal overlaps. We establish a linear
inequality relating each subsystem's index and recovery time that guarantees
safety and guides resilient architecture assignment. We also propose a
segmentation-based approach to strengthen the previously derived conditions. We
then present algorithms to compute the proposed indices and to find a
cost-optimal architecture assignment with a safety guarantee. We validate the
framework through a case study on temperature regulation in interconnected
rooms under different attack scenarios.

</details>


### [252] [Stability and Robustness of Time-Varying Opinion Dynamics: A Graph-Theoretic Approach](https://arxiv.org/abs/2510.01580)
*M. Hossein Abedinzadeh,Emrah Akyol*

Main category: eess.SY

TL;DR: 本文研究了时变Friedkin-Johnsen (TVFJ)模型中意见动力学的稳定性，该模型同时考虑了持久的个体偏见和适应性的社会影响。研究引入了两种时间结构——破缺时间图 (DTGs) 和弱破缺时间图 (WDTGs)——作为图论的证书，将顽固影响和时间连通性与状态转移矩阵的收缩联系起来。


<details>
  <summary>Details</summary>
Motivation: 研究TVFJ模型中意见动力学的稳定性和相关因素，旨在为不断变化的社交网络和人机交互网络中的意见形成提供可扩展且有弹性的分析工具。

Method: 引入DTGs和WDTGs两种时间结构，利用图论工具证明了TVFJ动力学在无穷递归DTGs下的渐近稳定性、在半周期破缺网络下的指数稳定性，以及信任扩展在递归WDTGs下的渐近稳定性。同时，研究建立了omega-limit集的有界性，并利用p-LTI分解刻画了周期性切换系统的极限集，最后证明了在有界扰动下指数稳定性依然存在。

Result: 证明了TVFJ动力学在无穷递归DTGs下的渐近稳定性、在半周期破缺网络下的指数稳定性，以及信任扩展在递归WDTGs下的渐近稳定性。建立了omega-limit集的有界性，并证明了其大小至多为p。在有界扰动下，指数稳定性依然存在。

Conclusion: 研究结果统一了代数收缩检验与可解释的图基础推理，为分析意见形成提供了可扩展且有弹性的工具，并确保了在噪声或不完美网络中的鲁棒性。

Abstract: We study the stability of opinion dynamics in the time-varying
Friedkin-Johnsen (TVFJ) model, which captures both persistent individual biases
and adaptive social influence. We introduce two temporal structures, defected
temporal graphs (DTGs) and weakly defected temporal graphs (WDTGs), that serve
as graph-theoretic certificates linking stubborn influence and temporal
connectivity to contraction of the state-transition matrix. Using these tools,
we prove asymptotic stability of TVFJ dynamics under infinitely recurring DTGs,
exponential stability in semi-periodic defected networks, and asymptotic
stability of a trust-based extension under the weaker condition of recurring
WDTGs. We also establish boundedness of the omega-limit set, showing that
long-run opinions remain within the convex hull of innate beliefs, and
characterize the limit set for periodically switching systems via a p-LTI
decomposition with the tight bound that the size of the omega-limit set is at
most p. Finally, we show that exponential stability persists under bounded
perturbations, ensuring robustness in noisy or imperfect networks. These
results unify algebraic contraction tests with interpretable graph-based
reasoning, providing scalable and resilient tools for analyzing opinion
formation in evolving social and human-AI networks.

</details>


### [253] [A TSO-DSO Coordination Framework via Analytical Representation and Monetization of PQV-Based Distribution System Flexibility](https://arxiv.org/abs/2510.01854)
*Burak Dindar,Can Berk Saner,Hüseyin Kemal Çakmak,Veit Hagenmeyer*

Main category: eess.SY

TL;DR: 该论文提出了一种基于交流最优潮流（OPF）的新方法，用于构建三维PQV-FOR（可行操作区域），以解决配电网（DS）灵活性在电网管理中的应用挑战，同时解决数据隐私问题。该方法能够精确确定FOR，提供可解析的数学表示，并对其进行经济估值，从而实现一次协调的TSO-DSO（电网运营商-配电网运营商）合作。


<details>
  <summary>Details</summary>
Motivation: 随着配电系统（DS）灵活性在输电系统运营商（TSO）网络管理中的作用日益重要，数据隐私问题阻碍了无缝的互操作性。可行操作区域（FOR）的概念，在PQ域中定义，已成为一种有前途的隐私保护方法。然而，由于其在大规模、网状DS网络中的精确确定、可解析的数学表示以及经济估值这三个关键因素，在TSO运营中有效利用FOR仍然具有挑战性。

Method: 提出了一种新颖的交流最优潮流（OPF）方法来构建三维PQV-FOR，明确考虑了电压变异性和不同的柔性发电单元（FPU）特性。构建过程采用了两阶段采样策略，结合了边界框投影和斐波那契方向技术，以有效地捕捉FOR。然后，引入了一种隐式多项式拟合方法来解析地表示FOR。此外，推导了一个PQV域上的二次成本函数来对FOR进行货币化。

Result: 所提出的框架能够实现一次TSO-DSO协调：DSO提供解析FOR和成本模型；TSO在FOR-based AC-OPF内部的公共连接点（PCC）处确定操作点；DSO通过解决其本地OPF来计算FPU调度，而无需进行计算密集型的拆分或迭代协调。在集成到TS（输电系统）的最多533个节点的网状DS上的案例研究表明，与标准的AC-OPF相比，该方法的效率得到了提高。在测试案例中，所提出的方法平均产生的成本偏差可忽略不计，最多为0.058%，同时计算时间减少了高达58.11%。

Conclusion: 该研究提出了一种创新的、隐私保护的框架，用于有效利用配电网的灵活性，以支持输电网的运行。通过精确确定、解析表示和经济估值可行操作区域（FOR），该方法简化了TSO-DSO之间的协调，并显著提高了计算效率，同时保持了可忽略的成本偏差。

Abstract: As the role of distribution system (DS) flexibility in transmission system
operator (TSO) network management becomes increasingly vital, data privacy
concerns hinder seamless interoperability. The notion of the feasible operating
region (FOR), defined in the PQ domain, has emerged as a promising
privacy-preserving approach. However, effectively leveraging FOR in TSO
operations remains challenging due to three key factors: its accurate
determination in large-scale, meshed DS networks; its tractable analytical
representation; and its economic valuation. In the present paper, we propose a
novel AC optimal power flow (OPF)-based method to construct a three-dimensional
PQV-FOR, explicitly accounting for voltage variability and diverse
flexibility-providing unit (FPU) characteristics. The construction process
employs a two-stage sampling strategy that combines bounding box projection and
Fibonacci direction techniques to efficiently capture the FOR. We then
introduce an implicit polynomial fitting approach to analytically represent the
FOR. Furthermore, we derive a quadratic cost function over the PQV domain to
monetize the FOR. Thus, the proposed framework enables single-round TSO-DSO
coordination: the DSO provides an analytical FOR and cost model; the TSO
determines operating point at the point of common coupling (PCC) within the
FOR-based AC-OPF; and the DSO computes FPU dispatch by solving its local OPF,
without computationally intensive disaggregation or iterative coordination.
Case studies on meshed DS with up to 533 buses, integrated into TS,
demonstrates the method's efficiency compared to standard AC-OPF. On average,
the proposed approach yields negligible cost deviations of at most 0.058%
across test cases, while reducing computation times by up to 58.11%.

</details>


### [254] [Coordinated Car-following Using Distributed MPC](https://arxiv.org/abs/2510.02010)
*Di Shen,Qi Dai,Suzhou Huang*

Main category: eess.SY

TL;DR: 提出了一种基于分布式模型预测控制（DMPC）的协同跟车算法，用于解决马尔可夫博弈中的交通控制问题。


<details>
  <summary>Details</summary>
Motivation: 在马尔可夫博弈框架下，为协同跟车问题提出算法，直接优化驾驶策略而非跟踪预定轨迹。

Method: 使用分布式模型预测控制（DMPC），通过迭代自博弈和通信协商推导协同解，近似纳什均衡或集中优化。将DMPC中的动作序列重新参数化为规划视域上的曲线，转化为高效的网格搜索。将交通控制问题视为机制设计问题。

Result: 在保持交通流平稳的同时，显著提高了交通效率，尤其是在高车辆密度下。解决了相关离散时间交通动力学的线性稳定性问题，并指出了其局限性。

Conclusion: 所提出的DMPC方法提供了一种新的协同自适应巡航控制（CACC）的制定方式，无需显式编队，并将所有车辆视为一个整体。该方法在提高交通效率和抑制交通波方面表现出色。

Abstract: Within the modeling framework of Markov games, we propose a series of
algorithms for coordinated car-following using distributed model predictive
control (DMPC). Instead of tracking prescribed feasible trajectories, driving
policies are solved directly as outcomes of the DMPC optimization given the
driver's perceivable states. The coordinated solutions are derived using the
best response dynamics via iterated self-play, and are facilitated by direct
negotiation using inter-agent or agent-infrastructure communication. These
solutions closely approximate either Nash equilibrium or centralized
optimization. By re-parameterizing the action sequence in DMPC as a curve along
the planning horizon, we are able to systematically reduce the original DMPC to
very efficient grid searches such that the optimal solution to the original
DMPC can be well executed in real-time. Within our modeling framework, it is
natural to cast traffic control problems as mechanism design problems, in which
all agents are endogenized on an equal footing with full incentive
compatibility. We show how traffic efficiency can be dramatically improved
while keeping stop-and-go phantom waves tamed at high vehicle densities. Our
approach can be viewed as an alternative way to formulate coordinated adaptive
cruise control (CACC) without an explicit platooning (or with all vehicles in
the traffic system treated as a single extended platoon). We also address the
issue of linear stability of the associated discrete-time traffic dynamics and
demonstrate why it does not always tell the full story about the traffic
stability.

</details>


### [255] [Event-triggered control and communication for single-master multi-slave teleoperation systems with Try-Once-Discard protocol](https://arxiv.org/abs/2510.02072)
*Yuling Li,Chenxi Li,Kun Liu,Jie Dong,Rolf Johansson*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了尝试一次丢弃（TOD）调度协议和事件触发机制的控制与通信方案，用于单主多从（SMMS）远程操作系统，以优化网络带宽和能源消耗。


<details>
  <summary>Details</summary>
Motivation: 单主多从（SMMS）远程操作系统虽然能提高效率、覆盖范围并适应单点故障，但在通信网络中增加从属操作器的数量时，通信带宽限制会变得严峻。现有方法如尝试一次丢弃（TOD）调度协议和事件触发机制通常分开使用。

Method: 提出了一种结合TOD调度协议和事件触发机制的控制与通信方案。开发了基于事件触发方案的自适应控制器和虚拟观测器，以实现主从同步，并考虑了动态不确定性、相对速度不可用以及时变延迟等因素。

Result: 建立了在所提出的事件触发控制与通信方案下SMMS远程操作系统的稳定性判据，并证明了不存在芝诺行为。通过实验验证了所提出算法的有效性。

Conclusion: 所提出的结合TOD调度协议和事件触发机制的方案能够有效优化SMMS远程操作系统的网络带宽和能源消耗，并实现主从同步，同时避免了芝诺行为。

Abstract: Single-master multi-slave (SMMS) teleoperation systems can perform multiple
tasks remotely in a shorter time, cover large-scale areas, and adapt more
easily to single-point failures, thereby effectively encompassing a broader
range of applications. As the number of slave manipulators sharing a
communication network increases, the limitation of communication bandwidth
becomes critical. To alleviate bandwidth usage, the Try-Once-Discard (TOD)
scheduling protocol and event-triggered mechanisms are often employed
separately. In this paper, we combine both strategies to optimize network
bandwidth and energy consumption for SMMS teleoperation systems. Specifically,
we propose event-triggered control and communication schemes for a class of
SMMS teleoperation systems using the TOD scheduling protocol. Considering
dynamic uncertainties, the unavailability of relative velocities, and
time-varying delays, we develop adaptive controllers with virtual observers
based on event-triggered schemes to achieve master-slave synchronization.
Stability criteria for the SMMS teleoperation systems under these
event-triggered control and communication schemes are established,
demonstrating that Zeno behavior is excluded. Finally, experiments are
conducted to validate the effectiveness of the proposed algorithms.

</details>


### [256] [Recurrent Control Barrier Functions: A Path Towards Nonparametric Safety Verification](https://arxiv.org/abs/2510.02127)
*Jixian Liu,Enrique Mallada*

Main category: eess.SY

TL;DR: 引入了循环控制障碍函数（RCBF），一种利用轨迹循环特性来验证安全性的新型CBF。


<details>
  <summary>Details</summary>
Motivation: HJ Reachability和CBF在计算高维偏微分方程或大规模半定规划时面临计算挑战。

Method: 提出RCBF，利用轨迹的循环特性（回到安全集），并将函数设计转化为集合识别，允许安全集不必是固定的。

Result: RCBF在有符号距离函数下成立，并且提出了一种数据驱动的非参数方法来计算安全集，该方法可大规模并行处理，并允许在保守性与计算成本之间进行权衡。

Conclusion: RCBF提供了一种更有效的方法来确保复杂动态系统的安全性，并且其数据驱动的方法为实际应用提供了灵活性。

Abstract: Ensuring the safety of complex dynamical systems often relies on
Hamilton-Jacobi (HJ) Reachability Analysis or Control Barrier Functions (CBFs).
Both methods require computing a function that characterizes a safe set that
can be made (control) invariant. However, the computational burden of solving
high-dimensional partial differential equations (for HJ Reachability) or
large-scale semidefinite programs (for CBFs) makes finding such functions
challenging. In this paper, we introduce the notion of Recurrent Control
Barrier Functions (RCBFs), a novel class of CBFs that leverages a recurrent
property of the trajectories, i.e., coming back to a safe set, for safety
verification. Under mild assumptions, we show that the RCBF condition holds for
the signed-distance function, turning function design into set identification.
Notably, the resulting set need not be invariant to certify safety. We further
propose a data-driven nonparametric method to compute safe sets that is
massively parallelizable and trades off conservativeness against computational
cost.

</details>


### [257] [Detection and Identification of Sensor Attacks Using Data](https://arxiv.org/abs/2510.02183)
*Takumi Shinohara,Karl H. Johansson,Henrik Sandberg*

Main category: eess.SY

TL;DR: 本篇论文提出了一种在无模型环境下，仅利用受攻击的数据来检测和识别数据注入攻击的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的攻击检测方法通常需要干净的数据或对系统模型有先验知识，本研究旨在解决在模型未知且数据被污染的情况下进行攻击检测的挑战。

Method: 提出两种场景下的检测与识别算法：1. 系统算子了解稀疏可观测性条件；2. 数据部分干净。算法仅依赖受攻击的数据。

Result: 通过在三惯性系统上的数值模拟，验证了所提出框架的有效性。

Conclusion: 在仅使用受攻击数据且对系统模型未知的情况下，成功实现了数据注入攻击的检测和识别。

Abstract: In this paper, we investigate data-driven attack detection and identification
in a model-free setting. Unlike existing studies, we consider the case where
the available output data include malicious false-data injections. We aim to
detect and identify such attacks solely from the compromised data. We address
this problem in two scenarios: (1) when the system operator is aware of the
system's sparse observability condition, and (2) when the data are partially
clean (i.e., attack-free). In both scenarios, we derive conditions and
algorithms for detecting and identifying attacks using only the compromised
data. Finally, we demonstrate the effectiveness of the proposed framework via
numerical simulations on a three-inertia system.

</details>


### [258] [Computing Control Lyapunov-Barrier Functions: Softmax Relaxation and Smooth Patching with Formal Guarantees](https://arxiv.org/abs/2510.02223)
*Jun Liu,Maxwell Fitzsimmons*

Main category: eess.SY

TL;DR: 提出了一种计算框架，用于合成单个光滑李雅普诺夫函数，以认证渐近稳定性和安全性。


<details>
  <summary>Details</summary>
Motivation: CBF-CLF 的存在保证了在屏障精确认证的安全集上存在这样一个函数。

Method: 使用 log-sum-exp (softmax) 松弛非光滑最大屏障，并结合反例引导的细化，直到可验证严格屏障条件。然后通过显式的光滑凸起构造将 softmax 屏障与 CLF 缝合起来。

Result: 在基准系统（包括功率转换器）上，所提出的方法获得的认证安全稳定区域通常比最先进的平方和 (SOS) 兼容 CBF-CLF 设计所获得的结果更不保守。

Conclusion: 所提出的方法能够合成光滑的李雅普诺夫函数，以认证渐近稳定性和安全性，并在某些情况下比现有方法提供更优的结果。

Abstract: We present a computational framework for synthesizing a single smooth
Lyapunov function that certifies both asymptotic stability and safety. We show
that the existence of a strictly compatible pair of control barrier and control
Lyapunov functions (CBF-CLF) guarantees the existence of such a function on the
exact safe set certified by the barrier. To maximize the certifiable safe
domain while retaining differentiability, we employ a log-sum-exp (softmax)
relaxation of the nonsmooth maximum barrier, together with a
counterexample-guided refinement that inserts half-space cuts until a strict
barrier condition is verifiable. We then patch the softmax barrier with a CLF
via an explicit smooth bump construction, which is always feasible under the
strict compatibility condition. All conditions are formally verified using a
satisfiability modulo theories (SMT) solver, enabled by a reformulation of
Farkas' lemma for encoding strict compatibility. On benchmark systems,
including a power converter, we show that the certified safe stabilization
regions obtained with the proposed approach are often less conservative than
those achieved by state-of-the-art sum-of-squares (SOS) compatible CBF-CLF
designs.

</details>


### [259] [Game-theoretic Social Distancing in Competitive Bi-Virus SIS Epidemics](https://arxiv.org/abs/2510.02269)
*Benjamin Catalano,Keith Paarporn,Sebin Gracy*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了博弈论的社交距离行为模型和双病毒SIS流行病模型的框架，用于分析传染病的传播及其与社会行为的相互作用。


<details>
  <summary>Details</summary>
Motivation: 许多因素会驱动传染病在复杂的真实网络中传播，特别是与疾病传播同步演变的社会行为。此外，了解多种病毒株（如SARS-CoV-2的Delta和Omicron变体）如何在同一时间传播至关重要。

Method: 提出了一种双病毒SIS流行病模型，并耦合了一个基于博弈论的社交距离行为模型。行为由进化博弈论中的复制子方程决定。每种病毒株的流行度会影响个体的社交距离选择，而个体的行为又会反过来影响SIS模型中每种病毒的传播。

Result: 分析确定了系统的平衡点及其局部稳定性。研究发现，只有当两种病毒株的再生数相等时，才可能出现地方性共存。在假设两种病毒的再生数相等的情况下，研究确定了产生共存平衡线系的合适参数范围，并指出了这些平衡线的局部指数稳定性条件。通过数值模拟说明了研究结果。

Conclusion: 该模型和分析为了解传染病的传播动力学，特别是当社会行为和多种病毒株相互作用时，提供了有价值的见解。其结果有助于确定可能导致疾病持续存在或消退的条件，并为公共卫生干预提供了潜在的指导。

Abstract: Numerous elements drive the spread of infectious diseases in complex
real-world networks. Of particular interest is social behaviors that evolve in
tandem with the spread of disease. Moreover, recent studies highlight the
importance of understanding how multiple strains spread simultaneously through
a population (e.g. Delta and Omicron variants of SARS-CoV-2). In this paper, we
propose a bi-virus SIS epidemic model coupled with a game-theoretic social
distancing behavior model. The behaviors are governed by replicator equations
from evolutionary game theory. The prevalence of each strain impacts the choice
of an individual to social distance, and, in turn, their behavior affects the
spread of each virus in the SIS model. Our analysis identifies equilibria of
the system and their local stability properties, which reveal several isolated
fixed points with varying levels of social distancing. We find that endemic
co-existence is possible only when the reproduction numbers of both strains are
equal. Assuming the reproduction number for each virus is the same, we identify
suitable parameter regimes that give rise to lines of coexistence equilibria.
Moreover, we also identify conditions for local exponential stability of said
lines of equilibria. We illustrate our findings with several numerical
simulations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [260] [Quantum advantages in ground state preparation, combinatorial optimization, and quantum state preparation](https://arxiv.org/abs/2510.01563)
*Taehee Ko,Sungbin Lim*

Main category: quant-ph

TL;DR: 我们证明了对于任何具有反多项式间隙的量子哈密顿量，只要系统规模足够大，就可以在多项式电路深度内以反多项式精度制备其基态。该电路由多项式数量的 Pauli 旋转组成，无需辅助量子比特。在此基础上，我们进一步证明，对于足够大的量子比特数，可以使用常数（多项式）数量的 Pauli 旋转以常数（反多项式）精度近似制备任何量子态。我们的理论发现揭示了在基态制备、组合优化和量子态制备等重要应用中存在指数级量子优势。


<details>
  <summary>Details</summary>
Motivation: 本文的动机是探索在具有反多项式间隙的量子哈密顿量的情况下，能否在多项式电路深度内以反多项式精度制备基态，并进一步探讨近似制备任意量子态的可能性。

Method: 本文提出了一种新的电路结构，该结构由多项式数量的 Pauli 旋转组成，无需辅助量子比特，用于制备量子哈密顿量的基态。在此基础上，文章证明了可以使用常数数量的 Pauli 旋转以常数精度近似制备任意量子态。

Result: 我们证明了对于任何具有反多项式间隙的量子哈密顿量，只要系统规模足够大，就可以在多项式电路深度内以反多项式精度制备其基态。我们还证明了对于足够大的量子比特数，可以使用常数数量的 Pauli 旋转以常数精度近似制备任意量子态。

Conclusion: 本文的结论是，对于具有反多项式间隙的量子哈密顿量，可以在多项式电路深度内以反多项式精度制备基态，并且可以使用常数数量的 Pauli 旋转以常数精度近似制备任意量子态。这些发现揭示了量子计算在基态制备、组合优化和量子态制备等领域的指数级量子优势。

Abstract: We show that for any quantum Hamiltonian with an inverse-polynomial gap, the
ground state can be prepared in a polynomial circuit depth to
inverse-polynomial precision, if the system size is sufficiently large. The
resulting circuit is composed of a polynomial number of Pauli rotations without
ancilla qubit. Extending this result, we prove that for sufficiently large
qubit number, any quantum state can be approximately prepared with a constant
(polynomial) number of Pauli rotations to constant (inverse-polynomial)
precision. Our theoretical findings reveal exponential quantum advantages in
the prominent applications: ground state preparation, combinatorial
optimization, and quantum state preparation.

</details>


### [261] [Velocity effects slightly mitigating the quantumness degradation of an Unruh-DeWitt detector](https://arxiv.org/abs/2510.01280)
*P. H. M. Barros,Shu-Min Wu,C. A. S. Almeida,H. A. S. Costa*

Main category: quant-ph

TL;DR: 在有加速度的量子系统中，研究了速度对信息退化的影响，发现非相对论速度可以缓解信息退化，而超相对论速度会抑制该效应。


<details>
  <summary>Details</summary>
Motivation: 研究加速量子系统中由于Unruh效应引起的信息退化，并探讨速度对其影响。

Method: 考虑探测器在二维平面内的空间轨迹运动，分别研究了非相对论和超相对论速度下的单量子比特、量子干涉仪和路径可区分度电路。

Result: 得到了非相对论速度下的跃迁率、量子相干性、可见度、可区分度和互补关系等解析表达式。发现超相对论速度下Unruh效应被抑制。速度效应通过与加速度的复合效应，可以缓解信息退化。

Conclusion: 非相对论的横向恒定运动可以保护高加速度下的量子性，尽管效应非常微小。

Abstract: In this work, we investigate the velocity effects on information degradation
due to the Unruh effect in accelerated quantum systems (with finite interaction
time). We consider a detector moving along a spatial trajectory within a
two-dimensional plane. The quantum systems studied were: accelerated
single-qubit, quantum interferometric circuit, and which-path
distinguishability circuit. Thus, for non-relativistic velocity regime, we
obtained analytical expressions such as transition rates, quantum coherence,
visibility, distinguishability, and the complementarity relation. On the other
hand, for the ultra-relativistic velocity regime, we saw that the Unruh effect
is suppressed and therefore the detector does not respond in this case. Our
findings revealed that velocity effects imply mitigation of information
degradation, this interesting behaviors happen because of the composite effect
of both velocity and acceleration. The results obtained show that the addition
of the non-relativistic, transverse and constant motion of an accelerated
detector can play a protective role in quantumness in systems at high
accelerations, although the effects are very small.

</details>


### [262] [Chiral quantum state circulation from photon lattice topology](https://arxiv.org/abs/2510.01306)
*Souvik Bandyopadhyay,Anushya Chandran,Philip JD Crowley*

Main category: quant-ph

TL;DR: 基于腔QED架构，提出一种光子态在三个腔之间循环传输的方案，该方案利用拓扑保护的手征边界态，可用于量子计算中的状态制备和隔离。


<details>
  <summary>Details</summary>
Motivation: 手征量子态循环对于量子计算机的工作至关重要，例如用于状态制备和隔离。

Method: 提出一种由三个腔耦合到一个量子比特的腔QED架构，其中腔1的任意光子态在固定时间后会循环到腔2，然后到腔3，再回到腔1。这种腔态循环源于相关光子晶格中拓扑保护的手征边界态。

Result: 计算了半经典极限下的循环周期，证明了循环可以持续足够长的时间，并且提供了一种工程化所需哈密顿量的Floquet协议。

Conclusion: 所提出的手征量子态循环方案在超导量子比特上是理想的实现平台，可用于近期的设备构建和测试。

Abstract: Chiral quantum state circulation is the unidirectional transfer of a quantum
state from one subsystem to the next. It is essential to the working of a
quantum computer; for instance, for state preparation and isolation. We propose
a cavity-QED architecture consisting of three cavities coupled to a qubit, in
which \emph{any} photonic state of cavity 1 with sufficiently many photons
circulates to cavity 2 after a fixed time interval, and then to cavity 3 and
back to 1. Cavity-state circulation arises from topologically protected chiral
boundary states in the associated photon lattice and is thus robust to
perturbation. We compute the circulation period in the semi-classical limit,
demonstrate that circulation persists for time-scales diverging with the total
photon number, and provide a Floquet protocol to engineer the desired
Hamiltonian. Superconducting qubits offer an ideal platform to build and test
these devices in the near term.

</details>


### [263] [A robust phase of continuous transversal gates in quantum stabilizer codes](https://arxiv.org/abs/2510.01319)
*Eric Huang,Pierre-Gabriel Rozon,Arpit Dua,Sarang Gopalakrishnan,Michael J. Gullans*

Main category: quant-ph

TL;DR: 在表面码中发现了一个可连续调节的逻辑幺正相，该面对退相干错误具有鲁棒性，并且可以通过横向操作和解码来实现。该逻辑幺正的错误率随旋转角度呈指数级衰减，这使得我们可以设计一个简单的容错协议来实现连续角度的逻辑旋转，从而降低量子模拟等应用对小角度旋转的需求。


<details>
  <summary>Details</summary>
Motivation: 需要一种容错的方法来实现量子计算中的通用逻辑操作，特别是当仅使用横向门时。需要克服魔数状态蒸馏等现有协议的局限性，并降低需要频繁进行小角度旋转的应用（如量子模拟）的开销。

Method: 研究了表面码的横向操作和解码，发现了其参数空间中的一个新相。在该相中，逻辑幺正操作的错误率与旋转角度成指数关系。利用这一特性设计了一个容错协议。

Result: 发现了一个可连续调节的逻辑幺正相，该面对退相干错误具有鲁棒性。在这个相中，逻辑幺正的错误率随旋转角度的增加而指数级降低。设计了一个新的容错协议，能够实现连续角度的逻辑旋转。

Conclusion: 通过利用表面码中存在的特定相，可以实现容错的、连续角度的逻辑旋转。这种方法比现有的协议（如魔数状态蒸馏）更有效，特别是对于需要大量小角度旋转的应用。

Abstract: A quantum error correcting code protects encoded logical information against
errors. Transversal gates are a naturally fault-tolerant way to manipulate
logical qubits but cannot be universal themselves. Protocols such as magic
state distillation are needed to achieve universality via measurements and
postselection. A phase is a region of parameter space with smoothly varying
large-scale statistical properties except at its boundaries. Here, we find a
phase of continuously tunable logical unitaries for the surface code
implemented by transversal operations and decoding that is robust against
dephasing errors. The logical unitaries in this phase have an infidelity that
is exponentially suppressed in the code distance compared to their rotation
angles. We exploit this to design a simple fault-tolerant protocol for
continuous-angle logical rotations. This lowers the overhead for applications
requiring many small-angle rotations such as quantum simulation.

</details>


### [264] [A quantum analogue of convex optimization](https://arxiv.org/abs/2510.02151)
*Eunou Lee*

Main category: quant-ph

TL;DR: 本文提出了一种量子算法（FGA），用于计算具有凸势函数的薛定谔算子的最小特征值，并将其应用于解决n维凸鼓的最低频率问题。


<details>
  <summary>Details</summary>
Motivation: 开发一种量子算法来解决无约束凸优化的量子类比问题，特别是计算薛定谔算子的最小特征值。

Method: 使用一种称为基本间隙算法（FGA）的量子算法，该算法利用了绝热演化子程序，并通过新颖的技术分析了低能空间。

Result: 提出了FGA算法，该算法能在多项式时间内计算出薛定谔算子的最小特征值，其精度可达$\epsilon$。该算法还首次实现了以多项式时间找到n维凸鼓的最低频率。

Conclusion: FGA算法为计算凸优化问题的量子类比提供了一种有效的方法，并为解决相关几何问题（如凸鼓的最低频率）提供了新的计算工具。

Abstract: Convex optimization is the powerhouse behind the theory and practice of
optimization. We introduce a quantum analogue of unconstrained convex
optimization: computing the minimum eigenvalue of a Schr\"odinger operator $h =
-\Delta + V $ with convex potential $V:\mathbb R^n \rightarrow \mathbb R_{\ge
0}$ such that $V(x)\rightarrow\infty $ as $\|x\|\rightarrow\infty$. For this
problem, we present an efficient quantum algorithm, called the Fundamental Gap
Algorithm (FGA), that computes the minimum eigenvalue of $h$ up to error
$\epsilon$ in polynomial time in $n$, $1/\epsilon$, and parameters that depend
on $V$. Adiabatic evolution of the ground state is used as a key subroutine,
which we analyze with novel techniques that allow us to focus on the low-energy
space. We apply the FGA to give the first known polynomial-time algorithm for
finding the lowest frequency of an $n$-dimensional convex drum, or
mathematically, the minimum eigenvalue of the Dirichlet Laplacian on an
$n$-dimensional region that is defined by $m$ linear constraints in polynomial
time in $n$, $m$, $1/\epsilon$ and the radius $R$ of a ball encompassing the
region.

</details>


### [265] [Derandomised tensor product gap amplification for quantum Hamiltonians](https://arxiv.org/abs/2510.01333)
*Thiago Bergamaschi,Tony Metger,Thomas Vidick,Tina Zhang*

Main category: quant-ph

TL;DR: 该论文提出了一种新的量子间隙放大程序，用于处理高低能量哈密顿量之间的区分问题，并可能对量子PCP猜想的证明策略产生影响。


<details>
  <summary>Details</summary>
Motivation: 区分高低能量哈密顿量在量子PCP猜想中是一个核心问题，尤其是在能量间隙较大时，该问题的难度尤为突出。

Method: 研究提出了一种新的量子间隙放大程序，该程序利用扩展图上的随机游走来对哈密顿量的张量积放大进行去随机化（即对项进行子采样）。分析该程序依赖于一种源于量子 de Finetti 定理的新技术。

Result: 提出了一种新的量子间隙放大程序，并展示了其在处理高低能量哈密顿量区分问题上的潜力。

Conclusion: 该研究为解决量子PCP猜想提供了一种新的思路和技术，特别是通过量子间隙放大和新的分析技术，为理解和证明该猜想开辟了新的方向。

Abstract: The quantum PCP conjecture asks whether it is QMA-hard to distinguish between
high- and low-energy Hamiltonians even when the gap between "high" and "low"
energy is large (constant). A natural proof strategy is gap amplification:
start from the fact that high- and low-energy Hamiltonians are hard to
distinguish if the gap is small (inverse polynomial) and amplify the
Hamiltonians to increase the energy gap while preserving hardness. Such a gap
amplification procedure is at the heart of Dinur's proof of the classical PCP
theorem. In this work, following Dinur's model, we introduce a new quantum gap
amplification procedure for Hamiltonians which uses random walks on expander
graphs to derandomise (subsample the terms of) the tensor product amplification
of a Hamiltonian. Curiously, our analysis relies on a new technique inspired by
quantum de Finetti theorems, which have previously been used to rule out
certain approaches to the quantum PCP conjecture.

</details>


### [266] [Quantum Optimization with Classical Chaos](https://arxiv.org/abs/2510.01334)
*Malick A. Gaye,Omar Shehab,Paraj Titum,Gregory Quiroz*

Main category: quant-ph

TL;DR: QAOA的参数化可以通过经典混沌递归映射得到简化，并且在优化迭代次数和电路深度有限的情况下，其性能可与标准QAOA相媲美。混合方法可以进一步提升QAOA的性能，尤其是在深度电路中。


<details>
  <summary>Details</summary>
Motivation: 硬计算问题需要更深的量子电路，对经典参数优化提出了更高的要求，因此需要探索有效的QAOA参数化方法。

Method: 研究基于经典混沌递归映射的参数化方案，并通过数值方法研究其在最大满足问题上的表现，并与标准QAOA进行比较。同时，探索混合参数化方案。

Result: 混沌映射可以有效减少变分参数空间，并在有限的优化迭代和短深度电路下，实现与标准QAOA相当的性能。混合方法可以提升QAOA的性能，尤其是在深度电路中。

Conclusion: 提出了一个广义的框架，用于指定基于动力学映射的QAOA参数化方法，并证明了混沌映射和混合方法的有效性。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a powerful tool in
solving various combinatorial problems such as Maximum Satisfiability and
Maximum Cut. Hard computational problems, however, require deep circuits that
place high demands on classical variational parameter optimization. Ultimately,
this has necessitated investigations into alternative methods for effective
QAOA parameterizations. Here, we study a parameterization scheme based on
classical chaotic recursive mapping, which enables significant reductions in
the scaling of the variational parameter space. Through numerical
investigations of hard Maximum Satisfiability problems, we demonstrate that the
chaotic mapping can effectively match the performance of standard QAOA when
subject to a limited number of classical optimization iterations and
short-depth circuits. Insight into this behavior is elucidated through the lens
of classical dynamical systems and used to inform hybridized schemes that
leverage both standard and chaotic parameterizations. It is shown that these
hybridized approaches can boost QAOA performance beyond that of the standard
approach alone, especially for deep circuits. Through this study, we provide a
new perspective that introduces a generalized framework for specifying
performant, dynamical-map-based QAOA parameterizations.

</details>


### [267] [Non-stabilizerness in quantum-enhanced metrological protocols](https://arxiv.org/abs/2510.01380)
*Tanausú Hernández-Yanes,Piotr Sierant,Jakub Zakrzewski,Marcin Płodzień*

Main category: quant-ph

TL;DR: 对于排列对称态，SRE 仅取决于有限数量的集体自旋算符的期望值。这被用于分析自旋压缩协议，其中 OAT 演化产生最优压缩和“kitten”态。kitten 态具有多体贝尔关联，但 SRE 随着关联强度的增加而减小。


<details>
  <summary>Details</summary>
Motivation: 需要量化量子操作（超越 Clifford 操作）的“魔性”或非稳定性（non-stabilizerness），因为这是制备量子态所必需的。对于自旋压缩等协议，理解和量化这种非稳定性对于提高量子精密测量至关重要。

Method: 1. 证明对于排列对称态，稳定器 Rényi 熵（SRE）在系统足够大时仅取决于有限数量的集体自旋算符的期望值。 2. 利用此发现来分析自旋压缩协议，特别是单轴扭曲（OAT）协议。 3. 研究 OAT 演化过程中 SRE 的行为，包括最优压缩和“kitten”态的生成。 4. 分析“kitten”态的特性，如多体贝尔关联和 SRE 与关联强度以及系统大小的关系。

Result: 1. 对于排列对称态，SRE 仅依赖于常数个集体自旋算符的期望值。 2. 在 OAT 演化下，最优压缩伴随着 SRE 随系统大小的对数发散。 3. OAT 演化产生的“kitten”态具有多体贝尔关联，但其 SRE 随着贝尔关联强度的增加而减小，并且与系统大小无关。 4. 揭示了非稳定性、多体关联和量子计量之间的联系。

Conclusion: 非稳定性（“魔性”）可以通过稳定器 Rényi 熵（SRE）来量化。对于排列对称态，SRE 的描述可以大大简化。OAT 协议产生的“kitten”态展示了非稳定性、多体贝尔关联和量子计量之间的权衡。该研究为在实验中量化非稳定性提供了实际方法，有助于提高精密传感能力。

Abstract: Non-stabilizerness (colloquially "magic") characterizes genuinely quantum
(beyond-Clifford) operations necessary for preparation of quantum states, and
can be measured by stabilizer R\'enyi entropy (SRE). For permutationally
symmetric states, we show that the SRE depends, for sufficiently large systems,
only on a constant number of expectation values of collective spin operators.
This compact description is leveraged for analysis of spin-squeezing protocols,
which inherently generate non-stabilizerness. Under one-axis twisting (OAT),
the generation of optimal squeezing is accompanied by a logarithmic divergence
of SRE with system system size. Continued time evolution under OAT produces
metrologically useful "kitten" states-superpositions of rotated GHZ states-that
feature many-body Bell correlations but exhibit a smaller,
system-size-independent SRE that decreases with increasing Bell-correlation
strength. Our results reveal connections between non-stabilizerness,
multipartite correlations, and quantum metrology, and provide a practical route
to quantify non-stabilizerness in experiments for precision sensing.

</details>


### [268] [Quantum Signatures of Strange Attractors](https://arxiv.org/abs/2510.01416)
*Bence Dárdai,Gábor Vattay*

Main category: quant-ph

TL;DR: 研究了量子耗散系统中的量子奇异吸引子，并首次在量子双 D 振荡器模型中对其进行了可视化，结果表明量子奇异吸引子是经典奇异吸引子的平滑版本，并证实了该模型具有半经典行为。


<details>
  <summary>Details</summary>
Motivation: 探索耗散驱动系统在量子力学中的表现，以及奇异吸引子结构如何在量子领域中显现。

Method: 采用 Caldirola-Kanai (CK) 框架，将耗散直接纳入含时哈密顿量；利用 Husimi 分布可视化量子态；计算了时间倒序关联函数 (OTOC)。

Result: 首次在量子双 D 振荡器模型中可视化了量子奇异吸引子，量子吸引子是经典吸引子的平滑版本，并观察到 OTOC 能够揭示与经典李雅普诺夫指数相关的指数增长，证明了半经典行为。

Conclusion: 为开放的混沌量子系统提供了新的几何视角，并阐明了量子-经典过渡的机制。

Abstract: In classical mechanics, driven systems with dissipation often exhibit
complex, fractal dynamics known as strange attractors. This paper addresses the
fundamental question of how such structures manifest in the quantum realm. We
investigate the quantum Duffing oscillator, a paradigmatic chaotic system,
using the Caldirola-Kanai (CK) framework, where dissipation is integrated
directly into a time-dependent Hamiltonian. By employing the Husimi
distribution to represent the quantum state in phase space, we present the
first visualization of a quantum strange attractor within this model. Our
simulations demonstrate how an initially simple Gaussian wave packet is
stretched, folded, and sculpted by the interplay of chaotic dynamics and energy
loss, causing it to localize onto a structure that beautifully mirrors the
classical attractor. This quantum "photograph" is inherently smoothed, blurring
the infinitely fine fractal details of its classical counterpart as a direct
consequence of the uncertainty principle. We supplement this analysis by
examining the out-of-time-ordered correlator (OTOC), which shows that stronger
dissipation clarifies the exponential growth associated with the classical
Lyapunov exponent, thereby confirming the model's semiclassical behavior. This
work offers a compelling geometric perspective on open chaotic quantum systems
and sheds new light on the quantum-classical transition.

</details>


### [269] [Exponential Quantum Advantage for Message Complexity in Distributed Algorithms](https://arxiv.org/abs/2510.01657)
*François Le Gall,Maël Luce,Joseph Marchand,Mathieu Roget*

Main category: quant-ph

TL;DR: 量子分布式算法在路由信息任务上实现了指数级超越经典算法的消息复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究量子分布式算法在消息复杂度方面超越经典算法的程度，特别是在信息路由任务上。

Method: 利用焊接树上量子行走的“简洁”实现，并将STOC 2003的查询复杂度下界提升至消息复杂度下界。

Result: 证明存在一种量子分布式算法，其消息复杂度比任何经典算法都呈指数级低，适用于在焊接树网络中路由信息。

Conclusion: 量子分布式算法在信息路由任务上可以实现比经典算法指数级的超越。

Abstract: We investigate how much quantum distributed algorithms can outperform
classical distributed algorithms with respect to the message complexity (the
overall amount of communication used by the algorithm). Recently, Dufoulon,
Magniez and Pandurangan (PODC 2025) have shown a polynomial quantum advantage
for several tasks such as leader election and agreement. In this paper, we show
an exponential quantum advantage for a fundamental task: routing information
between two specified nodes of a network. We prove that for the family of
``welded trees" introduced in the seminal work by Childs, Cleve, Deotto, Farhi,
Gutmann and Spielman (STOC 2003), there exists a quantum distributed algorithm
that transfers messages from the entrance of the graph to the exit with message
complexity exponentially smaller than any classical algorithm. Our quantum
algorithm is based on the recent "succinct" implementation of quantum walks
over the welded trees by Li, Li and Luo (SODA 2024). Our classical lower bound
is obtained by ``lifting'' the lower bound from Childs, Cleve, Deotto, Farhi,
Gutmann and Spielman (STOC 2003) from query complexity to message complexity.

</details>


### [270] [Optimal Quantum Information Transmission Under a Continuous-Variable Erasure Channel](https://arxiv.org/abs/2510.01424)
*Adam Taylor,Michael Hanks,Hyukjoon Kwon,M. S. Kim*

Main category: quant-ph

TL;DR: 本研究推导了能量受限的连续变量玻色子擦除信道的量子容量和纠缠辅助量子容量，并构建了渐近最优的随机码。


<details>
  <summary>Details</summary>
Motivation: 评估连续变量玻色子量子信道的量子容量和寻找最优编码是具有挑战性的。

Method: 推导了能量受限的连续变量玻色子擦除信道的量子容量和纠缠辅助量子容量，并基于在编码态的典型子空间内打乱信息构建了随机码。

Result: 构建的随机码被证明是渐近最优的，最多存在一个常数间隙。此外，提出的玻色子海森堡-普雷斯基尔协议的信息恢复依赖于输入和输出模式的比例，这与需要固定数量额外输出量子比特的离散变量场景不同。

Conclusion: 通过随机编码方案设计的玻色子海森堡-普雷斯基尔协议，其信息恢复的特性与离散变量场景存在差异。

Abstract: Quantum capacity gives the fundamental limit of information transmission
through a channel. However, evaluating the quantum capacities of a
continuous-variable bosonic quantum channel, as well as finding an optimal code
to achieve the optimal information transmission rate, is in general
challenging. In this work, we derive the quantum capacity and
entanglement-assisted quantum capacity of the bosonic continuous-variable
erasure channel when subject to energy constraints. We then construct random
codes based on scrambling information within the typical subspace of the
encoding state and prove that these codes are asymptotically optimal up to a
constant gap. Finally, using our random coding scheme we design a bosonic
variation of the Hayden-Preskill protocol and find that information recovery
depends on the ratio between the input and output modes. This is in contrast
with the conventional discrete-variable scenario which requires only a fixed
number of additional output qudits.

</details>


### [271] [Visualizing the state space of quantum trits, quadits, and pairs of qubits via toral geometry](https://arxiv.org/abs/2510.01455)
*Steven Bleiler,Ali Al-Bayaty,Shanyan Chen,Marek Perkowski*

Main category: quant-ph

TL;DR: We propose new uses of toric variety structures in quantum computation for various radices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore new applications of toric variety structures in quantum computation.

Method: The method involves using toric variety structures for quantum computation.

Result: The result is the proposal of new uses for these structures.

Conclusion: The paper proposes new uses of toric variety structures in the study of quantum computation for various radices.

Abstract: We propose some new uses of toric variety structures in the study of quantum
computation for various radices.

</details>


### [272] [Isogeny Graphs in Superposition and Quantum Onion Routing](https://arxiv.org/abs/2510.01464)
*Eleni Agathocleous,Tobias Hartung,Karl Jansen,Lukas Mansour*

Main category: quant-ph

TL;DR: 提出了一种基于对称加密的量子洋葱路由（QOR）方案，并提供了两种实现方法。


<details>
  <summary>Details</summary>
Motivation: 经典洋葱路由依赖公钥加密，而量子类比面临对称加密的障碍。需要一种基于对称加密的量子洋葱路由方案。

Method: 通过使用 '域的理想类群作用' 来实例化每一层，并提出了一种新的 '非局域' 密钥交换方法。连接到同源图及其关联方案，并使用 Bose-Mesner 代数来形式化交换性。

Result: 给出了两种实现路径：一种是使用多项式量子资源的通用量子预言机，另一种是利用连续时间量子行走（CTQWs）的内在量子方法。提供了一个小的 Qiskit 示例。

Conclusion: 所提出的量子洋葱路由方案在量子计算机的威胁下仍然是困难的，并且可以作为当前后量子方案安全的基础。

Abstract: Onion routing provides anonymity by layering encryption so that no relay can
link sender to destination. A quantum analogue faces a core obstacle: layered
quantum encryption generally requires symmetric encryption schemes, whereas
classically one would rely on public-key encryption. We propose a
symmetric-encryption-based quantum onion routing (QOR) scheme by instantiating
each layer with the abelian ideal class group action from the Theory of Complex
Multiplication. Session keys are established locally via a Diffie-Hellman key
exchange between neighbors in the chain of communication. Furthermore, we
propose a novel ''non-local'' key exchange between the sender and receiver. The
underlying problem remains hard even for quantum adversaries and underpins the
security of current post-quantum schemes. We connect our construction to
isogeny graphs and their association schemes, using the Bose-Mesner algebra to
formalize commutativity and guide implementation. We give two implementation
paths: (i) a universal quantum oracle evaluating the class group action with
polynomially many quantum resources, and (ii) an intrinsically quantum approach
via continuous-time quantum walks (CTQWs), outlined here and developed in a
companion paper. A small Qiskit example illustrates the mechanics (by design,
not the efficiency) of the QOR.

</details>


### [273] [Robust Rydberg facilitation via rapid adiabatic passage](https://arxiv.org/abs/2510.01504)
*Xinghan Wang,Yupeng Wang,Qi-Yu Liang*

Main category: quant-ph

TL;DR: 提出了一种基于快速绝热囚衅的里德堡原子偶极阻塞的鲁棒实现方法，解决了其对位置无序和参数不完善敏感的问题，并展示了其在稀有事件检测中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 里德堡原子偶极阻塞在量子信息处理和传感中具有重要应用，但其对位置无序和参数不完善的敏感性是一个主要障碍。

Method: 通过绝热地扫描相互作用引起的共振位移，提出了一种鲁棒的里德堡原子偶极阻塞实现方法。

Result: 该方法不受实际无序和参数变化的影响，并自然地导致一维和二维阵列中的雪崩激发增长，以高增益和极低的背景实现了雪崩过程，使其在稀有事件检测方面具有应用前景。

Conclusion: 该研究为实现鲁棒的里德堡原子偶极阻塞动力学提供了一条实用的途径，为未来的实验和技术应用铺平了道路。

Abstract: We propose and analyze a robust implementation of Rydberg antiblockade based
on rapid adiabatic passage. Although Rydberg antiblockade offers key
opportunities in quantum information processing and sensing, its sensitivity to
position disorder and parameter imperfections has posed a central roadblock. By
adiabatically sweeping across the interaction-shifted resonance, our approach
is unaffected by realistic levels of disorder and parameter variations. As a
straightforward application case, we show that it naturally gives rise to
avalanche excitation growth in both one- and two-dimensional arrays. This
avalanche process yields high gain with exceptionally low background, making it
promising for rare-event detection. These results establish a practical route
to robust Rydberg antiblockade dynamics, paving the way for future experimental
and technological applications.

</details>


### [274] [Variational approach to open quantum systems with long-range competing interactions](https://arxiv.org/abs/2510.01543)
*Dawid A. Hryniuk,Marzena H. Szymańska*

Main category: quant-ph

TL;DR: 介绍了一种模拟长程相互作用耗散量子格点系统的方法，并进行了大规模的动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法难以在大规模上精确模拟具有复杂长程相互作用的开放量子多体系统。

Method: 结合使用矩阵产品算子和时变变分蒙特卡洛方法，提出了一种高效且可扩展的方法。

Result: 模拟了高达N=200个格点的自旋1/2格点系统在非平衡态下的动力学和稳态，揭示了空间调制磁序的出现。

Conclusion: 该方法有望增进对具有长程相互作用的实验可实现量子系统的非平衡特性的理解。

Abstract: Competition between short- and long-range interactions underpins many
emergent phenomena in nature. Despite rapid progress in their experimental
control, computational methods capable of accurately simulating open quantum
many-body systems with complex long-ranged interactions at scale remain scarce.
Here, we address this limitation by introducing an efficient and scalable
approach to dissipative quantum lattices in one and two dimensions, combining
matrix product operators and time-dependent variational Monte Carlo. We
showcase the versatility, effectiveness, and unique methodological advantages
of our algorithm by simulating the non-equilibrium dynamics and steady states
of spin-$\frac{1}{2}$ lattices with competing algebraically-decaying
interactions for as many as $N=200$ sites, revealing the emergence of
spatially-modulated magnetic order far from equilibrium. This approach offers
promising prospects for advancing our understanding of the complex
non-equilibrium properties of a diverse variety of experimentally-realizable
quantum systems with long-ranged interactions, including Rydberg atoms,
ultracold dipolar molecules, and trapped ions.

</details>


### [275] [Design and Characterization of a Cryogenic Vacuum Chamber for Ion Trapping Experiments](https://arxiv.org/abs/2510.01557)
*D. M. Hartsell,J. M. Gray,C. M. Shappert,N. L. Gostin,R. A. McGill,H. N. Tinkey,C. R. Clark,K. R. Brown*

Main category: quant-ph

TL;DR: 设计并测试了一个包含机械隔离、高数值孔径真空成像物镜、真空磁屏蔽和用于捕获离子射频操纵的天线在内的低温真空室。


<details>
  <summary>Details</summary>
Motivation: 在低温下对捕获的离子进行高精度成像和相干操纵，同时抑制振动和磁场干扰。

Method: 设计并集成了机械隔振、高NA真空成像物镜、磁屏蔽和射频天线。对振动、成像效率、状态检测保真度、相干时间和动力学解耦进行了表征。

Result: 振动RMS小于7.61(4) nm；397 nm光子探测效率为1.77%；单次测量状态检测保真度为99.9963(4)% (50 μs)；拉姆齐实验相干时间为24(2) ms，单自旋回波延长至0.25(1) s；XY4和XY32序列分别将相干时间延长至0.72(2) s和0.81(3) s。

Conclusion: 所提出的低温真空室设计在抑制振动、实现高效成像和高保真度状态检测方面是成功的，并能通过动力学解耦技术显著延长离子的相干时间，为进一步的量子信息处理奠定了基础。

Abstract: We present the design and characterization of a cryogenic vacuum chamber
incorporating mechanical isolation from vibrations, a high numerical-aperture
in-vacuum imaging objective, in-vacuum magnetic shielding, and an antenna for
global radio-frequency manipulation of trapped ions. The cold shield near 4 K
is mechanically referenced to an underlying optical table via thermally
insulating supports and exhibits root-mean-square vibrations less than 7.61(4)
nm. Using the in-vacuum objective, we can detect 397 nm photons from a trapped
$^{40}\mathrm{Ca}^{+}$ ion with 1.77% efficiency and achieve 99.9963(4)%
single-shot state-detection fidelity in 50 $\mu$s. To characterize the efficacy
of the magnetic shields, we perform Ramsey experiments on the ground state
qubit and obtain a coherence time of 24(2) ms, which extends to 0.25(1) s with
a single spin-echo pulse. XY4 and XY32 dynamical decoupling sequences driven
via the radio-frequency antenna extend the coherence to 0.72(2) s and 0.81(3)
s, respectively.

</details>


### [276] [Directionality and quantum backfire in continuous-time quantum walks from delocalized states: Exact results](https://arxiv.org/abs/2510.01584)
*Jefferson J. Ximenes,Marcelo A. Pires,José M. Villas-Bôas*

Main category: quant-ph

TL;DR: 论文研究了具有可调离域性的新型初始态下，连续时间量子行走（CTQW）的解析结果。研究通过具有复杂跳跃振幅的哈密顿量来控制动力学，并为关键可观测量提供了闭合方程，揭示了三个主要发现：1) 从完全无偏的初始条件出现定向量子输运；2) 量子反向散射效应，即初始离域性越大，短期扩散越快，但出乎意料的是，在交叉时间 $t_{\mathrm{cross}}$ 之后，长期扩散反而减小；3) 存活概率的精确描述，表明向增强的 $t^{-3}$ 衰减的转变是一种微调效应。该工作为通过中间初始离域性和哈密顿相位之间的相互作用来控制量子输运建立了全面的框架。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于从一类新型的、具有可调离域性的初始态出发，推导连续时间量子行走（CTQW）的解析结果，以深入理解和控制量子输运现象。

Method: 采用具有复杂跳跃振幅的哈密顿量来描述量子行走动力学，并推导出关键可观测量的闭合形式方程，用于分析初始态的离域性对量子输运的影响。

Result: 研究发现了三个主要结果：1) 从完全无偏的初始状态出现了定向量子输运；2) 存在一种量子反向散射效应，即初始离域性越大，短期扩散越快，但在一个交叉时间 $t_{\mathrm{cross}}$ 之后，长期扩散反而减小；3) 存活概率的分析表明，向 $t^{-3}$ 衰减的转变是一种精细调节的效果。

Conclusion: 该研究通过分析中间初始离域性和哈密顿相位之间的相互作用，为控制量子输运提供了一个全面的理论框架。

Abstract: We derive analytical results for continuous-time quantum walks from a new
class of initial states with tunable delocalization. The dynamics are governed
by a Hamiltonian with complex hopping amplitudes. We provide closed-form
equations for key observables, revealing three notable findings: (1) the
emergence of directed quantum transport from completely unbiased initial
conditions; (2) a quantum backfire effect, where greater initial delocalization
enhances short-time spreading but counterintuitively induces a comparatively
smaller long-time spreading after a crossing time $t_{\mathrm{cross}}$; and (3)
an exact characterization of survival probability, showing that the transition
to an enhanced $t^{-3}$ decay is a fine-tuned effect. Our work establishes a
comprehensive framework for controlling quantum transport through the interplay
between intermediate initial delocalization and Hamiltonian phase.

</details>


### [277] [Limitations of strong coupling in non-Markovian quantum thermometry](https://arxiv.org/abs/2510.01596)
*Qing-Shou Tan,Yang Liu,Xulin Liu,Hao Chen,Xing Xiao,Wei Wu*

Main category: quant-ph

TL;DR: 本研究利用数值精确的层次方程 (HEOM) 研究了嵌入非马尔可夫环境中的单量子比特探针的量子温度计，克服了玻恩-马尔可夫近似的限制。研究发现，强耦合并不总能提高测温性能。在非平衡动力学测温中，弱耦合通常能获得最佳的量子信噪比 (QSNR)；在稳态区，强耦合仅在极低温下提高灵敏度，而在中低温度下，弱耦合能显著提高精度。为优化不同耦合强度下的性能，研究开发了一种结合 HEOM 和量子增强粒子群优化算法的混合计算框架，实现了在不同耦合强度下精确的量子动力学控制。研究结果揭示了量子测温的基本限制和机遇，并为在实际开放量子系统中设计高性能量子温度计提供了策略。


<details>
  <summary>Details</summary>
Motivation: 研究量子温度计在非马尔可夫环境中的性能，特别是探讨系统-环境耦合强度对温度估计精度的影响，并克服传统近似方法的局限性。

Method: 使用数值精确的层次方程 (HEOM) 来模拟单量子比特探针在非马尔可夫环境中的动力学行为，并分析量子信噪比 (QSNR) 的动态和稳态特性。在此基础上，结合量子增强粒子群优化算法，开发混合计算框架，以优化不同耦合强度下的测温性能。

Result: 研究发现，在非平衡动力学测温中，弱系统-环境耦合通常能提供最优的 QSNR。而在稳态区，强耦合仅在极低温下能提升灵敏度，而在中低温度下，弱耦合能显著提高精度。混合计算框架能够实现对不同耦合强度下量子动力学的精确控制。

Conclusion: 系统-环境耦合强度对量子测温性能有复杂影响，并非强耦合就一定优于弱耦合。研究结果为在实际开放量子系统中设计和优化高性能量子温度计提供了理论指导和实用策略。

Abstract: We investigate quantum thermometry using a single-qubit probe embedded in a
non-Markovian environment, employing the numerically exact hierarchical
equations of motion (HEOM) to overcome the limitations of Born-Markov
approximations. Through a systematic analysis of the dynamical and steady-state
behavior of the quantum signal-to-noise ratio (QSNR) for temperature
estimation, we identify several key findings that challenge the conventional
expectation that strong coupling necessarily enhances thermometric performance.
In non-equilibrium dynamical thermometry, weak system-environment coupling
generally yields the optimal QSNR, whereas in the steady-state regime, strong
coupling enhances sensitivity only in the ultra-low-temperature limit, while
weak coupling significantly improves precision at moderately low temperatures.
To optimize performance across coupling regimes, we develop a hybrid
computational framework that integrates HEOM with quantum-enhanced particle
swarm optimization, enabling precise quantum dynamical control under varying
coupling strengths. Our results reveal fundamental constraints and
opportunities in quantum thermometry, offering practical strategies for the
design of high-performance quantum thermometers operating in realistic open
quantum systems.

</details>


### [278] [Higher moment theory and learnability of bosonic states](https://arxiv.org/abs/2510.01610)
*Joseph T. Iosue,Yu-Xin Wang,Ishaun Datta,Soumik Ghosh,Changhun Oh,Bill Fefferman,Alexey V. Gorshkov*

Main category: quant-ph

TL;DR: 提出了一种高效的算法，用于学习高斯幺正变换作用下的玻色子Fock态，并解决了BosonSampling问题的一个悬而未决的问题。此外，还研究了高于高斯态的态的层级结构，并通过高阶矩找到了高斯幺正变换下的不变谱，为判断两个态是否可由高斯幺正变换联系起来提供了必要条件。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决高斯幺正变换下玻色子Fock态的学习问题，特别是与BosonSampling问题相关联的特定情况，并扩展到更广泛的非高斯态。

Method: 提出了一种样本和时间高效的算法来学习任意高斯幺正变换作用下的玻色子Fock态。研究了高于高斯态的态的层级结构，利用高阶矩来寻找高斯幺正变换下的不变谱。

Result: 算法能够高效学习高斯幺正变换下的玻色子Fock态，解决了BosonSampling问题。找到了高斯幺正变换下的完整不变谱，为判断两个态之间的关系提供了必要条件。

Conclusion: 该研究为理解和学习复杂的量子态（包括高斯态和更广泛的非高斯态）及其在高斯幺正变换下的行为提供了新的工具和见解。

Abstract: We present a sample- and time-efficient algorithm to learn any bosonic Fock
state acted upon by an arbitrary Gaussian unitary. As a special case, this
algorithm efficiently learns states produced in Fock state BosonSampling, thus
resolving an open question put forth by Aaronson and Grewal (Aaronson, Grewal
2023). We further study a hierarchy of classes of states beyond Gaussian states
that are specified by a finite number of their higher moments. Using the higher
moments, we find a full spectrum of invariants under Gaussian unitaries,
thereby providing necessary conditions for two states to be related by an
arbitrary (including active, e.g. beyond linear optics) Gaussian unitary.

</details>


### [279] [Entanglement distribution via satellite: an evaluation of competing protocols assuming realistic free-space optical channels](https://arxiv.org/abs/2510.01633)
*Nicholas Zaunders,Timothy C. Ralph*

Main category: quant-ph

TL;DR: 通过卫星平台进行纠缠分发是实现高吞吐量、高保真度量子网络的可行方案，可以实现大陆尺度的通信。


<details>
  <summary>Details</summary>
Motivation: 未来的量子网络需要高レート、高忠実度で量子的に絡み合ったリソースを空間的に離れた2点間で配信する機能。衛星プラットフォームは、自由空間光伝搬を介して量子リソースを送受信するため、量子ネットワークの有力な候補となる。

Method: 探索了两种网络拓扑（卫星-卫星-卫星和地面-卫星-地面）和两种纠缠分发方案（中继和直接生成分发），并计算了在不同条件下（DV/CV、使用/不使用NLA、考虑大气信道）可蒸馏纠缠分发レート的上限。

Result: 对于三卫星网络，最优策略是采用分布式NLA（CV或DV）。对于地-卫-地网络，最优策略是通过中心卫星分发DV资源。

Conclusion: 对于三卫星网络，分布式NLA（CV或DV）是最佳选择；对于地-卫-地网络，通过中心卫星分发DV资源是最佳选择。

Abstract: A key technical requirement of any future quantum network is the ability to
distribute quantum-entangled resources between two spatially separated points
at a high rate and high fidelity. Entanglement distribution protocols based on
satellite platforms, which transmit and receive quantum resources directly via
free-space optical propagation, are therefore excellent candidates for quantum
networking, since the geometry and loss characteristics of satellite networks
feasibly allow for up to continental-scale ($\sim10^3$ km) over-the-horizon
communication without the infrastructure, cost, or losses associated with
equivalent fibre-optic networks. In this work, we explore two network
topologies commonly associated with quantum networks - entanglement
distribution between two satellites in low-Earth orbit mediated by a third
satellite and entanglement distribution between two ground stations mediated by
a satellite in low-Earth orbit, and two entanglement distribution schemes - one
where the central satellite is used as a relay, and the other where the central
satellite is used to generate and distribute the entangled resource directly.
We compute a bound on the rate of distribution of distillable entanglement
achieved by each protocol in each network topology as a function of the network
channels for both single-rail discrete- (DV) and continuous-variable (CV)
resources and use or non-use of probabilistic noiseless linear quantum
amplification (NLA). In the case of atmospheric channels we take into account
the turbulent and optical properties of the free-space propagation. We
determine that for the triple-satellite network configuration, the optimal
strategy is to perform a distributed NLA scheme in either CV or DV, and for the
ground-satellite-ground network the optimal strategy is to distribute a DV
resource via the central satellite.

</details>


### [280] [Benchmarking Quantum Simulation Methods](https://arxiv.org/abs/2510.01710)
*Calvin Ku,Yu-Cheng Chen,Alice Hu,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: 本文量化了三种关键参数对量子相位估计算法(QPE)总体量子资源成本的影响：trotterization与qubitization的选择、分子轨道与平面波基组的使用、以及费米子到量子比特的编码方案。研究结果表明，在容错环境中对大分子进行相位估算时，使用平面波基组的第一量化qubitization电路效率最高，其门成本扩展性为$	ilde{\mathcal{O}}([N^{4/3}M^{2/3}+N^{8/3}M^{1/3}]/\varepsilon)$，这是迄今为止已知的最佳扩展性。而在仅有近期容错或含噪声中等规模系统可用的情况下，通过在分子轨道基组中使用trotterization，门成本可达$\mathcal{O}(M^{7}/\varepsilon^{2})$，适用于小分子的相位估算。此外，文章还提供了对现实分子系统在不同参数选择下进行QPE所需的量子比特和T门成本的数值估算。


<details>
  <summary>Details</summary>
Motivation: 量子相位估计算法（QPE）是容错量子计算的基石，尤其在化学电子结构计算方面。为适应不同量子化学系统的特性，已开发出多种QPE变体，它们在量子比特和门成本上各有影响。本文旨在量化关键参数对QPE总体量子资源成本的影响，以明确性能权衡并确定能最小化相关分子系统资源成本的参数范围。

Method: 通过分析trotterization与qubitization的选择、分子轨道与平面波基组的使用、以及费米子到量子比特的编码方案这三个关键参数，量化它们对QPE算法总体量子资源成本的影响，并建立清晰的性能权衡，确定最小化资源成本的参数区间。

Result: 在容错环境中对大分子进行相位估算时，采用第一量化qubitization电路和平面波基组是最有效的，其门成本扩展性达到$\tilde{\mathcal{O}}([N^{4/3}M^{2/3}+N^{8/3}M^{1/3}]/\varepsilon)$，是目前已知的最佳扩展性。对于近期容错或含噪声中等规模系统，使用trotterization和分子轨道基组对小分子进行相位估算，门成本为$\\mathcal{O}(M^{7}/\varepsilon^{2})$。文章还提供了对几种实际分子系统在不同参数选择下的量子比特和T门成本的数值估算。

Conclusion: 本文量化了影响QPE算法资源成本的关键参数，并为不同规模和容错级别的量子计算场景提供了最优策略。研究结果为在实际量子计算应用中高效执行QPE提供了指导，尤其是在化学和材料科学领域。对于容错量子计算，第一量化qubitization结合平面波基组提供了最优的可扩展性；对于近期量子设备，trotterization结合分子轨道基组是可行的选择。

Abstract: Quantum Phase Estimation (QPE) is a cornerstone algorithm for fault-tolerant
quantum computation, especially for electronic structure calculations of
chemical systems. To accommodate the diverse characteristics of quantum
chemical systems, numerous variants of QPE have been developed, each with
distinct qubit and gate cost implications. In this paper, we quantify the
impact of three key parameters on the overall quantum resource costs for the
QPE algorithm: the choice between trotterization and qubitization, the use of
molecular orbitals versus plane-wave basis-sets, and the selection of the
fermion-to-qubit encoding scheme. From this, we establish clear performance
trade-offs and delineate specific parameter regimes that minimize resource
costs for relevant molecular systems. When performing phase estimation on large
molecules in the fault-tolerant setting, we found the first-quantized
qubitization circuit using the plane-wave basis to be the most efficient, with
a gate cost scaling of
$\tilde{\mathcal{O}}([N^{4/3}M^{2/3}+N^{8/3}M^{1/3}]/\varepsilon)$ for a system
of $N$ electrons and $M$ orbitals, which is the best known scaling to date. On
the other hand, when only noisy intermediate-scale or near-term fault-tolerant
systems are available, the phase estimation of small molecules can be performed
with gate cost of $\mathcal{O}(M^{7}/\varepsilon^{2})$ via trotterization in
the MO basis. Furthermore, we provide numerical estimations of qubit and T gate
costs required to perform QPE for several real-world molecular systems under
these different parameter choices.

</details>


### [281] [Extracting the photon indistinguishability error from measurable quantum observables](https://arxiv.org/abs/2510.01731)
*Franciscus H. B. Somhorst,Jason Saied,Eleanor G. Rieffel,Jelmer J. Renema*

Main category: quant-ph

TL;DR: 本研究提出了一种从洪- ou -曼德尔干涉测量中提取光子不可分辨性误差的方法，该方法考虑了损耗和多光子噪声对单光子希尔伯特空间的污染。


<details>
  <summary>Details</summary>
Motivation: 在先前的研究中，对于洪- ou -曼德尔干涉测量结果的解释存在一些不一致之处。本研究旨在解决这些问题，并提出一种更准确的分析方法。

Method: 提出了一种新的分析方法，用于从洪- ou -曼德尔干涉测量中提取光子不可分辨性误差。该方法能够同时考虑损耗和多光子噪声对测量结果的影响。

Result: 该方法可以应用于多种单光子源，包括量子点。

Conclusion: 本研究提出的方法能够准确地从洪- ou -曼德尔干涉测量中提取光子不可分辨性误差，并解决了先前解释中存在的不一致性问题。该方法具有广泛的适用性，可用于评估各种单光子源的性能。

Abstract: We present a method to extract the photon indistinguishability error from
Hong-Ou-Mandel interference measurements, accounting for the combined effects
of loss and multiphoton noise that contaminate the single-photon Hilbert space.
Our analysis resolves apparent inconsistencies in previous interpretations of
such measurements. The reported method applies to a wide range of single-photon
sources, including quantum dots.

</details>


### [282] [Emergence and localization of exceptional points in an exactly solvable toy model](https://arxiv.org/abs/2510.01756)
*Miloslav Znojil*

Main category: quant-ph

TL;DR: 该论文解决了最基本的非厄米量子方势阱问题，重点关注其实数谱。通过引入PT对称的Robin边界条件，论文阐明了参数变化对Kato's exceptional-point（EP）奇点出现的影响，并解释了在某些简化的一参数情况下，奇数矩阵维度中EP退化的缺失现象。此外，研究还揭示了一种简化的单参数模型中存在的多带谱结构。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决最基本的非厄米量子方势阱问题，并阐明参数变化对Kato's exceptional-point（EP）奇点出现的影响，特别是解释在奇数矩阵维度中EP退化缺失的现象，以及揭示多带谱结构的存在。

Method: 通过引入PT对称的Robin（双参数）边界条件，对Schroedinger方程进行离散化处理，并分析参数变化对EP奇点的影响。

Result: 研究阐明了参数变化对EP奇点出现的影响，解释了奇数矩阵维度中EP退化缺失的原因，并揭示了一种简化单参数模型中存在的多带谱结构。

Conclusion: 该论文澄清了非厄米量子方势阱中EP奇点和谱结构的一些重要问题，为理解非厄米系统提供了新的见解。

Abstract: The most elementary non-Hermitian quantum square-well problem with real
spectrum is considered. The Schroedinger equation is required discrete and
endowed with PT-symmetric Robin (i.e., two-parametric) boundary conditions.
Some of the rather enigmatic aspects of impact of the variability of the
parameters on the emergence of the Kato's exceptional-point (EP) singularities
is clarified. In particular, the current puzzle of the apparent absence of the
EP degeneracies at the odd-matrix dimensions in certain simplified
one-parametric cases is explained. A not quite expected existence of a
multi-band spectral structure in another simplified one-parametric family of
models is also revealed.

</details>


### [283] [Self-Sustained Oscillations of a Nonlinear Optomechanical System in the Low-Excitation Regime](https://arxiv.org/abs/2510.01775)
*Shivangi Dhiman,K. Rubenbauer,T. Luschmann,A. Marx,A. Metelmann,H. Huebl*

Main category: quant-ph

TL;DR: 我们观察并模拟了一个单激发水平下的腔光力学系统中的非线性动力学，利用了超导微波电路的大克尔非线性，将非线性动力学观察阈值降低了四个数量级，使其在少量光子水平上即可实现。


<details>
  <summary>Details</summary>
Motivation: 需要一个能够结合非线性与非经典关联的实验平台，以满足量子传感等下一代量子应用的需求。

Method: 使用具有大单光子耦合率的腔光力学平台和一个非线性微波谐振器，在单激发水平下驱动系统，观察并模拟非线性动力学。

Result: 实现了非线性动力学在仅几个光子水平下的可观测性，并将观察阈值降低了四个数量级。

Conclusion: 该设备概念为实现非经典微波驱动方案的实验奠定了基础，并展示了对所涉及物理的深刻理解。

Abstract: Manifesting across all time, mass and length scales, nonlinearities lie at
the core of numerous physical phenomena. Next-generation quantum applications,
such as quantum sensing, require the combination of nonlinearity with
non-classical correlations. This necessitates the search for an experimental
platform which enables a nonlinear response at ultra-low excitation levels in a
system with practical sensing potential and quantum compatibility. Here, we
report the observation and theoretical modeling of nonlinear dynamics in a
mechanical system driven at the single-excitation level. We achieve this using
a cavity-optomechanical platform with large single-photon coupling rates and a
nonlinear microwave resonator. Specifically, the large Kerr nonlinearity of our
superconducting microwave circuit reduces the threshold for the observation of
nonlinear dynamics by four orders of magnitude, making this regime
experimentally accessible at the few-photon level. The parameter-based
quantitative predicative power of the theoretical description underlines our
deep understanding of the physics involved and that this device concept paves
the way for experiments with non-classical microwave drive schemes.

</details>


### [284] [From quantum feature maps to quantum reservoir computing: perspectives and applications](https://arxiv.org/abs/2510.01797)
*Casper Gyurik,Filip Wudarski,Evan Philip,Antonio Sannia,Hossein Sadeghi,Oleksandr Kyriienko,Davide Venturelli,Antonio A. Gentile*

Main category: quant-ph

TL;DR: 量子系统可作为非平凡的、可行的水库用于机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 探索水库计算和量子计算的交叉点。

Method: 提出并举例说明了一种新颖的量子水库计算（QRC）工作流程，重点是中性原子量子处理单元。

Result: 量子系统可以作为机器学习任务的非平凡水库。

Conclusion: QRC 可以推动水库计算的应用，但仍存在挑战。

Abstract: We explore the interplay between two emerging paradigms: reservoir computing
and quantum computing. We observe how quantum systems featuring
beyond-classical correlations and vast computational spaces can serve as
non-trivial, experimentally viable reservoirs for typical tasks in machine
learning. With a focus on neutral atom quantum processing units, we describe
and exemplify a novel quantum reservoir computing (QRC) workflow. We conclude
exploratively discussing the main challenges ahead, whilst arguing how QRC can
offer a natural candidate to push forward reservoir computing applications.

</details>


### [285] [Three-Dimensional Niobium Coaxial Cavity with $\sim0.1\,$second Lifetime](https://arxiv.org/abs/2510.01819)
*Takaaki Takenaka,Takayuki Kubo,Imran Mahoob,Kosuke Mizuno,Hitoshi Inoue,Takayuki Saeki,Shiro Saito*

Main category: quant-ph

TL;DR: 高温退火处理能够稳定氧化铌的生成，在低温下将腔体品质因数提高到3x10^9，并延长超导量子比特的寿命。


<details>
  <summary>Details</summary>
Motivation: 提高三维铌腔的内禀品质因数，并研究其在低温、多次冷启动和暴露空气后的稳定性，以期应用于超导量子比特的制造。

Method: 在三维铌四分之一波同轴腔中进行中温退火处理，并在单光子水平、低于20毫开尔文的温度下测量其内禀品质因数。

Result: 内禀品质因数超过3x10^9，内禀光子寿命约为90毫秒。该数值在多次冷启动和暴露空气后基本保持不变。

Conclusion: 中温退火处理能够稳定生成低损耗的氧化铌，该方法可用于三维铌腔的制造，并有望改善基于铌的超导量子比特的寿命。

Abstract: We report on the internal quality factor of a three-dimensional niobium
quarter-wave coaxial cavity, with mid-temperature annealing, exhibiting $Q_{\rm
int} \gtrsim 3\times10^9$ at the single-photon level below 20\,mK, which
corresponds to an internal photon lifetime of $\tau_{\rm
int}\sim90\,\mathrm{ms}$. Moreover, $Q_{\rm int}$ of the mid-temperature
annealed cavities remains almost unchanged even after several cooldown cycles
and air exposure. These results suggest that stable low-loss niobium oxides
might be formed by mid-temperature annealing on the surface of
three-dimensional niobium cavity. This surface treatment could be applicable to
the fabrication of 2D superconducting circuits and help improve the lifetime of
Nb-based superconducting qubits.

</details>


### [286] [Hybrid biphoton spectrometer for time-resolved quantum spectroscopy across visible and near-infrared regions](https://arxiv.org/abs/2510.01836)
*Ozora Iso,Koya Onoda,Nicola J. Fairbairn,Masahiro Yabuno,Hirotaka Terai,Shigehito Miki,Ryosuke Shimizu*

Main category: quant-ph

TL;DR: 提出一种测量可见光和近红外光三波非简并联合光谱的新方法，首次实现联合光谱的时间分辨测量。


<details>
  <summary>Details</summary>
Motivation: 现有技术在表征双光子光谱相关性方面存在局限，尤其是在需要时间分辨能力的量子光谱学领域。

Method: 使用两个非扫描光谱仪（一个用于近红外光，一个用于可见光）并结合时间标记采集策略来测量联合光谱强度。

Result: 成功测量了可见光和近红外光的三波非简并联合光谱，并实现了约150 ps的时间分辨率。

Conclusion: 所提出的方法能够同时满足对纯双光子态和量子光谱动态洞察的需求，弥合了现有技术的差距。

Abstract: Joint spectral measurements are a powerful tool for characterising biphoton
spectral correlation, which is crucial for quantum information and
communication technologies. In these applications, highly pure biphoton states
are essential in any time- and frequency-mode, often obviating the need for
time-resolved measurements. Conversely, spectroscopy utilising entangled photon
pairs is gaining significant attention for its ability to unveil molecular
dynamics, a field that critically demands time-resolved capabilities. Here, we
introduce a novel methodology for capturing a biphoton spectrum that comprises
visible and near-infrared photons, resulting in a three-fold non-degenerate
joint spectrum. Our system employs two non-scanning spectrographs: a fibre
spectrograph for near-infrared photons and a delay-line-anode single-photon
imager for visible photons. We successfully measure the joint spectral
intensity by leveraging a time-tagging acquisition strategy. Furthermore, our
approach uniquely enables time-resolved joint spectral measurements with a
temporal resolution of approximately 150 ps. This methodology bridges the gap
between the requirements for pure biphoton states and the need for dynamic
insights in quantum spectroscopy.

</details>


### [287] [Counterfactual quantum measurements](https://arxiv.org/abs/2510.01888)
*Ingita Banerjee,Kiarn T. Laverick,Howard M. Wiseman*

Main category: quant-ph

TL;DR: 提出了一种量子反事实推理的形式化方法，其中测量设置被视为先行词。


<details>
  <summary>Details</summary>
Motivation: 旨在将反事实推理（一种用于探索假设场景的工具）从经典确定性框架推广到不确定的量子理论。

Method: 提出了一种量子反事实推理的形式化方法，其中测量设置被视为先行词。

Result: 提供了一种处理量子反事实推理的方法，能够回答“如果使用了另一种探测器，会观察到什么？”之类的问题。

Conclusion: 该方法为量子反事实推理提供了一个形式化框架，并解决了一个悬而未决的问题，即如何将反事实推理（尤其是在测量设置的背景下）推广到量子力学。

Abstract: Counterfactual reasoning plays a crucial role in exploring hypothetical
scenarios, by comparing some consequent under conditions identical except as
results from a differing antecedent. David Lewis' well-known analysis evaluates
counterfactuals using a hierarchy of desiderata. These were, however, built
upon a deterministic classical framework, and whether it could be generalized
to indeterministic quantum theory has been an open question. In this Letter, we
propose a formalism for quantum counterfactuals in which antecedents are
measurement settings. Unlike other approaches, it non-trivially answers
questions like: "Given that my photon-detector, observing an atom's
fluorescence, clicked at a certain time, what would I have seen using a
field-quadrature detector instead?"

</details>


### [288] [Folding lattice proteins confined on minimal grids using a quantum-inspired encoding](https://arxiv.org/abs/2510.01890)
*Anders Irbäck,Lucas Knuthson,Sandipan Mohanty*

Main category: quant-ph

TL;DR: This paper tackles the challenge of steric clashes in dense protein systems by reformulating the problem of finding minimum energy for a confined lattice protein as a Quadratic Unconstrained Binary Optimization (QUBO) problem. Both classical simulated annealing and hybrid quantum-classical annealing (on a D-Wave system) can efficiently solve this QUBO formulation for protein chains up to length 48, with quantum-classical annealing taking around 10 seconds. Other methods like linear and quadratic programming are also tested but face difficulties with chain constraints. All approaches are benchmarked against computationally expensive exact solutions.


<details>
  <summary>Details</summary>
Motivation: Steric clashes are a challenge in dense protein systems with conventional methods. This paper aims to find an efficient way to solve the minimum energy problem for confined lattice proteins.

Method: The paper reformulates the problem as a Quadratic Unconstrained Binary Optimization (QUBO) problem. It then solves this QUBO using classical simulated annealing and hybrid quantum-classical annealing on a D-Wave system. Linear and quadratic programming methods are also tested.

Result: Classical simulated annealing and hybrid quantum-classical annealing successfully and consistently solved the QUBO for chain length 48, with the latter taking approximately 10 seconds. Linear and quadratic programming methods struggled with chain constraints. All methods were benchmarked against exact solutions.

Conclusion: The QUBO formulation is an effective approach for solving the minimum energy problem of confined lattice proteins, with quantum-classical annealing showing particular efficiency for chains up to length 48.

Abstract: Steric clashes pose a challenge when exploring dense protein systems using
conventional explicit-chain methods. A minimal example is a single lattice
protein confined on a minimal grid, with no free sites. Finding its minimum
energy is a hard optimization problem, withsimilarities to scheduling problems.
It can be recast as a quadratic unconstrained binary optimization (QUBO)
problem amenable to classical and quantum approaches. We show that this problem
in its QUBO form can be swiftly and consistently solved for chain length 48,
using either classical simulated annealing or hybrid quantum-classical
annealing on a D-Wave system. In fact, the latter computations required about
10 seconds. We also test linear and quadratic programming methods, which work
well for a lattice gas but struggle with chain constraints. All methods are
benchmarked against exact results obtained from exhaustive structure
enumeration, at a high computational cost.

</details>


### [289] [Temporal Pulse Origins in Atom Interferometric Quantum Sensors](https://arxiv.org/abs/2510.01900)
*Jack Saywell,Nikolaos Dedes,Max Carey,Brynle Barrett,Tim Freegarde*

Main category: quant-ph

TL;DR: 通过引入时间脉冲原点概念，可以简化原子干涉仪测量标定因子并提高其稳定性，从而优化量子传感器的性能。


<details>
  <summary>Details</summary>
Motivation: 原子干涉仪的测量标定因子通常被认为精确且稳定，但实际中会受到脉冲形状、控制场强度、频率和原子速度等因素的影响。

Method: 提出并研究了时间脉冲原点概念，将惯性相位响应参数化为单一时间点。通过模拟探索了时间脉冲原点在设计脉冲序列中的应用，以提高标定因子稳定性和缩短脉冲持续时间。

Result: 时间脉冲原点概念能够简化测量标定因子的确定及其稳定性分析，并可通过设计定制脉冲序列来增强稳定性、最小化系统误差。模拟结果表明，该方法可以缩短序列持续时间，提高对控制场幅度波动的鲁棒性。

Conclusion: 时间脉冲原点概念能够解释现有设备中的多种系统误差，并支持设计短而鲁棒的脉冲，有望提升当前和下一代原子干涉量子传感器的性能。

Abstract: Quantum sensors based upon atom interferometry typically rely on
radio-frequency or optical pulses to coherently manipulate atomic states and
make precise measurements of inertial and gravitational effects. An advantage
of these sensors over their classical counterparts is often said to be that
their measurement scale factor is precisely known and highly stable. However,
in practice the finite pulse duration makes the sensor scale factor dependent
upon the pulse shape and sensitive to variations in control field intensity,
frequency, and atomic velocity. Here, we explore the concept of a temporal
pulse origin in atom interferometry, where the inertial phase response of any
pulse can be parameterized using a single point in time. We show that the
temporal origin permits a simple determination of the measurement scale factor
and its stability against environmental perturbations. Moreover, the temporal
origin can be treated as a tunable parameter in the design of tailored
sequences of shaped pulses to enhance scale factor stability and minimize
systematic errors. We demonstrate through simulations that this approach to
pulse design can reduce overall sequence durations while increasing robustness
to realistic fluctuations in control field amplitude. Our results show that the
temporal pulse origin explains a broad class of systematic errors in existing
devices and enables the design of short, robust pulses which we expect will
improve the performance of current and next-generation interferometric quantum
sensors.

</details>


### [290] [Hybrid Quantum-Classical Walks for Graph Representation Learning in Community Detection](https://arxiv.org/abs/2510.01918)
*Adrián Marın,Mauricio Soto-Gomez,Giorgio Valentini,Elena Casiraghi,Carlos Cano,Daniel Manzano*

Main category: quant-ph

TL;DR: 提出了一种新颖的量子启发式图表示学习算法，使用混合量子-经典行走来分析具有复杂结构的网络。


<details>
  <summary>Details</summary>
Motivation: 传统图表示学习方法难以捕捉复杂图中的复杂关系，特别是那些具有幂律分布或分层结构的图。

Method: 使用混合量子-经典行走，利用量子和经典动力学的结合来探索图中的局部和远程连接。

Result: 在网络社区检测的案例研究中，该混合动力算法能够有效适应复杂的图拓扑。

Conclusion: 这种混合动力方法为图表示学习任务提供了一个强大而通用的解决方案。

Abstract: Graph Representation Learning (GRL) has emerged as a cornerstone technique
for analysing complex, networked data across diverse domains, including
biological systems, social networks, and data analysis. Traditional GRL methods
often struggle to capture intricate relationships within complex graphs,
particularly those exhibiting non-trivial structural properties such as
power-law distributions or hierarchical structures. This paper introduces a
novel quantum-inspired algorithm for GRL, utilizing hybrid Quantum-Classical
Walks to overcome these limitations. Our approach combines the benefits of both
quantum and classical dynamics, allowing the walker to simultaneously explore
both highly local and far-reaching connections within the graph. Preliminary
results for a case study in network community detection shows that this hybrid
dynamic enables the algorithm to adapt effectively to complex graph topologies,
offering a robust and versatile solution for GRL tasks.

</details>


### [291] [The Constant Speed Schedule for Adiabatic State Preparation: Towards Quadratic Speedup without Prior Spectral Knowledge](https://arxiv.org/abs/2510.01923)
*Mancheon Han,Hyowon Park,Sangkook Choi*

Main category: quant-ph

TL;DR: Adiabatic quantum computation can be sped up by constructing a better schedule for traversing the adiabatic path. A constant speed schedule reduces the time complexity from O(Δ^-2) to O(Δ^-1), and a segmented constant speed schedule further optimizes this by computing path segment lengths on the fly using eigenstate overlaps, eliminating the need for prior spectral knowledge. This method achieves optimal O(Δ^-1) scaling in small gap regions, demonstrating a quadratic speedup.


<details>
  <summary>Details</summary>
Motivation: The efficiency of adiabatic quantum evolution is limited by the adiabatic evolution time, T, which depends on the minimum energy gap, Δ. While the rigorous lower bound for T scales as O(Δ^-1), generic schedules have T scaling as O(Δ^-2). This suggests a potential for quadratic speedup by improving the schedule.

Method: 1. Introduced a constant speed schedule that traverses the adiabatic path at a uniform rate, reducing the upper bound of the required evolution time's scaling by one order in 1/Δ. 2. Proposed a segmented constant speed schedule protocol that computes path segment lengths from eigenstate overlaps along the adiabatic evolution. This method relies on overlaps computed on the fly, removing the need for prior spectral knowledge. 3. Numerically tested the algorithm on adiabatic unstructured search, the N2 molecule, and the [2Fe-2S] cluster.

Result: The constant speed schedule reduces the scaling of the upper bound of the required evolution time by one order in 1/Δ. The segmented constant speed schedule protocol achieves the optimal 1/Δ scaling in a small gap region in numerical experiments, demonstrating a quadratic speedup over the standard linear schedule.

Conclusion: The proposed segmented constant speed schedule protocol achieves optimal O(Δ^-1) scaling in small gap regions and demonstrates a quadratic speedup compared to the standard linear schedule. This method effectively eliminates the need for prior spectral knowledge by computing overlaps on the fly.

Abstract: The efficiency of adiabatic quantum evolution is governed by the adiabatic
evolution time, \(T\), which depends on the minimum energy gap, \(\Delta\). For
a generic schedule, \(T\) typically scales as \(\Delta^{-2}\), whereas the
rigorous lower bound is \(\mathcal{O}(\Delta^{-1})\). This indicates the
potential for a quadratic speedup through the adiabatic schedule construction.
Here, we introduce the constant speed schedule, which traverses the adiabatic
path of the eigenstate at a uniform rate. We first show that this approach
reduces the scaling of the upper bound of the required evolution time by one
order in \(1/\Delta\). We then provide a segmented constant speed schedule
protocol, in which path segment lengths are computed from eigenstate overlaps
along the adiabatic evolution. By relying on the overlaps on the fly, our
method eliminates the need for prior spectral knowledge. We test our algorithm
numerically on the adiabatic unstructured search, the N$_2$ molecule, and the
[2Fe-2S] cluster. In our numerical experiments, the method achieves the optimal
\(1/\Delta\) scaling in a small gap region, thereby demonstrating a quadratic
speedup over the standard linear schedule.

</details>


### [292] [Maximum heralding probabilities of non-classical state generation from two-mode Gaussian state via photon counting measurements](https://arxiv.org/abs/2510.01951)
*Jaromír Fiurášek*

Main category: quant-ph

TL;DR: 通过对两模纠缠高斯态的一个模式进行光子数测量，可以从实验上获得Gottesman-Kitaev-Preskill（GKP）态或猫态等高度非经典光态。本文分析了该方案的herald概率，并证明了其与探测到的光子数n的关系。结果表明，所需实验次数仅与n成多项式关系，因此在获得足够强的压缩的情况下，可以实际生成具有高恒星等级的高度复杂的量子态。


<details>
  <summary>Details</summary>
Motivation: 需要一种有效的方法来生成高度非经典的光态，例如GKP态或猫态，并关注其生成速率（由herald概率决定）。

Method: 对两模纠缠高斯态的一个模式进行光子数测量，并分析其herald概率与探测到的光子数n的关系，推导出最大herald概率的解析表达式。

Result: 最大herald概率可以被解析计算，并且所需实验次数与n成多项式关系。这意味着生成高复杂度的量子态是可行的，只要有足够强的压缩。

Conclusion: 该方法为生成高度非经典的光态提供了一种可行的途径，特别是对于具有高恒星等级的复杂量子态。

Abstract: Highly non-classical states of light - such as the approximate
Gottesman-Kitaev-Preskill states or cat-like states - can be generated from
experimentally accessible Gaussian states via photon counting measurements on
selected modes, conditioned on specific outcomes of these heralding events. A
simplest yet important example of this approach involves performing photon
number measurements on one mode of a two-mode entangled Gaussian state. The
heralding probability of this scheme is a key figure of merit, as it determines
the generation rate of the targeted non-classical state. In this work we show
that the maximum heralding probability for the two-mode setting can be
calculated analytically, and we investigate its dependence on the number of
detected photons n. Our results show that the number of required experimental
trials scales only polynomially with n. Generation of highly complex optical
quantum states with high stellar rank is thus practically feasible in this
setting, given access to sufficiently strong squeezing.

</details>


### [293] [Formal Framework for Quantum Advantage](https://arxiv.org/abs/2510.01953)
*Harry Buhrman,Niklas Galke,Konstantinos Meichanetzidis*

Main category: quant-ph

TL;DR: 该论文定义了计算问题的“queasy实例”，即那些在量子计算机上易于解决但在经典计算机上难以解决的问题实例，以此来量化量子计算优势。


<details>
  <summary>Details</summary>
Motivation: 受量子启发式和平均情况而非最坏情况算法分析的启发，本文旨在为量子计算优势提供一种新的定义和分析框架。

Method: 本文定义了量子实例复杂性，并以此为基础定义了“queasy实例”。通过将因子分解问题（Factoring）约简到可满足性问题（Satisfiability），证明了存在queasy的Satisfiability实例。

Result: 证明了存在queasy的Satisfiability实例，这些实例在量子计算机上易于解决，但在经典计算机上难以解决。此外，还表明queasiness在量子算法中具有指数级的算法效用。

Conclusion: 本文提出的量子计算优势的框架，通过关注单个实例，为寻找实际中的量子优势提供了指导，并可能催生新的量子算法设计方法。

Abstract: Motivated by notions of quantum heuristics and by average-case rather than
worst-case algorithmic analysis, we define quantum computational advantage in
terms of individual problem instances. Inspired by the classical notions of
Kolmogorov complexity and instance complexity, we define their quantum
versions. This allows us to define queasy instances of computational problems,
like e.g. Satisfiability and Factoring, as those whose quantum instance
complexity is significantly smaller than their classical instance complexity.
These instances indicate quantum advantage: they are easy to solve on a quantum
computer, but classical algorithms struggle (they feel queasy). Via a reduction
from Factoring, we prove the existence of queasy Satisfiability instances;
specifically, these instances are maximally queasy (under reasonable
complexity-theoretic assumptions). Further, we show that there is exponential
algorithmic utility in the queasiness of a quantum algorithm. This formal
framework serves as a beacon that guides the hunt for quantum advantage in
practice, and moreover, because its focus lies on single instances, it can lead
to new ways of designing quantum algorithms.

</details>


### [294] [Digital quantum simulation of many-body localization crossover in a disordered kicked Ising model](https://arxiv.org/abs/2510.01983)
*Tomoya Hayata,Kazuhiro Seki,Seiji Yunoki*

Main category: quant-ph

TL;DR: 使用嘈杂的量子设备模拟哈密顿量演化具有挑战性，但可以使用 Trotter 分解和较大的 Trotter 步长来模拟 Floquet 演化。本研究提出了在无序 Floquet 多体系统中模拟多体局域化交叉。


<details>
  <summary>Details</summary>
Motivation: 模拟无序 Floquet 多体系统中的多体局域化（MBL）交叉，将其作为一种适用于当前或近期量子设备的非平衡问题。

Method: 在 ibm_fez 量子设备上，使用 60 个量子比特模拟无序踢摆伊辛模型中的 MBL 交叉。通过计算四点关联函数（OTOC）来指示 MBL 交叉。

Result: 通过 OTOC 的晚期行为，根据无序强度确定了量子混沌和 MBL 状态。

Conclusion: 通过两种独立的错误缓解方法（算子重整化和零噪声外推）的比较，验证了结果的有效性。

Abstract: Simulating nonequilibrium dynamics of quantum many-body systems is one of the
most promising applications of quantum computers. However, a faithful digital
quantum simulation of the Hamiltonian evolution is very challenging in the
present noisy quantum devices. Instead, nonequilibrium dynamics under the
Floquet evolution realized by the Trotter decomposition of the Hamiltonian
evolution with a large Trotter step size is considered to be a suitable problem
for simulating in the present or near-term quantum devices. In this work, we
propose simulating the many-body localization crossover as such a
nonequilibrium problem in the disordered Floquet many-body systems. As a
demonstration, we simulate the many-body localization crossover in a disordered
kicked Ising model on a heavy-hex lattice using $60$ qubits from $156$ qubits
available in the IBM Heron r2 superconducting qubit device named ibm\_fez. We
compute out-of-time-ordered correlators as an indicator of the many-body
localization crossover. From the late-time behavior of out-of-time-ordered
correlators, we locate the quantum chaotic and many-body localized regimes as a
function of the disorder strength. The validity of the results is confirmed by
comparing two independent error mitigation methods, that is, the operator
renormalization method and zero-noise extrapolation.

</details>


### [295] [HIV-1 protease cleavage sites detection with a Quantum convolutional neural network algorithm](https://arxiv.org/abs/2510.01993)
*Junggu Choi,Junho Lee,Kyle L. Jung,Jae U. Jung*

Main category: quant-ph

TL;DR: 本研究提出了一个基于量子卷积神经网络（QCNN）和神经量子嵌入（NQE）的框架，用于预测HIV-1蛋白酶在蛋白质序列中的切割位点。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种能够有效预测HIV-1蛋白酶切割位点的量子计算框架，并评估其在嘈杂量子硬件环境下的鲁棒性。

Method: 提出了一种结合QCNN和NQE的框架，并与经典神经网络进行了比较，在无噪声和有噪声模拟下评估了分类性能。

Result: 在有噪声和无噪声模拟下，结合角度和幅度编码NQE的QCNN在可训练参数规模相似和不同数量的量子比特下，其分类性能均优于经典神经网络。QCNN在量子硬件噪声下表现稳定。

Conclusion: 本研究首次将NQE增强的QCNN应用于HIV-1切割位点分类，证明了该框架在生物医学数据分析中，尤其是在NISQ硬件上的应用潜力，并为可扩展、抗噪声的量子机器学习在生物医学领域的应用提供了新的见解。

Abstract: In this study, we propose a quantum convolutional neural network (QCNN)-based
framework with the neural quantum embedding (NQE) to predict HIV-1 protease
cleavage sites in amino acid sequences from viral and human proteins. To assess
the effectiveness and robustness of our framework, we compared the
classification performance against classical neural networks under both
noiseless and noisy simulations. Among experimental conditions, the QCNN with
the angle and amplitude encoding NQE conditions consistently outperformed
classical counterparts in both the similar trainable parameter scale and the
different number of qubits (the averaged performance of the 4-qubits and
8-qubits QCNN: 0.9146 and 0.8929 / the averaged performance of the classical
neural network: 0.6125 and 0.8278). The QCNN with the NQE showed stable
performance under the quantum hardware noise, confirming its applicability to
biomedical data analysis with the noise intermediate-scale quantum (NISQ)
hardware. This study presents the first application of NQE-augmented QCNNs for
HIV-1 cleavage site classification, providing new insights into scalable and
noise-resilient quantum machine learning for biomedical data.

</details>


### [296] [Fiber-integrated NV Magnetometer with Microcontroller-based Software Lock-in Technique](https://arxiv.org/abs/2510.01996)
*Qilong Wu,Xuan-Ming Shen,Yuan Zhang,Ying-Geng Shan,Hui-Hui Yu,Jing-Hao Zhang,Jiahui Chen,Yan Wang,Xun Yang,Yong-Zhi Tian,Lijun Wang,Chong-Xin Shan*

Main category: quant-ph

TL;DR: 本文开发了一种低成本、基于微控制器的软件锁相技术，并将其应用于光纤集成氮-空位（NV）磁力计，实现了与昂贵设备相当的磁场检测灵敏度（93 nT/Hz^{1/2}），并验证了实时检测能力（标准差488 nT），有望加速NV磁力计的工业应用。


<details>
  <summary>Details</summary>
Motivation: 现有的光纤集成NV磁力计虽然灵敏度高、集成性好，但其配套的电子设备昂贵、笨重，阻碍了其工业化应用。

Method: 开发了一种低成本、基于微控制器的软件锁相技术。该技术利用微控制器协调微波源芯片和模数转换器，并通过程序模拟锁相放大器功能，实现了微波频率调制的光学泵浦磁共振（OPMR）。

Result: 成功实现了对弱磁场的检测，磁场灵敏度达到93 nT/Hz^{1/2}，与使用笨重、专业的设备相当。此外，还实现了实时磁场检测，标准差为488 nT。

Conclusion: 所提出的低成本、微控制器为基础的软件锁相技术，能够实现电子设备的小型化，有望加速NV磁力计在工业领域的应用。

Abstract: Fiber-integrated nitrogen-vacancy (NV) magnetometers possess high
sensitivity, integration, and flexibility, and thus have been explored
extensively for industrial applications. While most studies have focused on the
optimization of the quantum sensing head, less attention has been paid to the
frequently employed professional, expensive, and bulky electronics, which
hinder their practical applications. In this article, we fabricate a
fiber-integrated NV magnetometer and develop a low-cost microcontroller-based
software lock-in technique. In this technique, a microcontroller coordinates
efficiently a microwave source chip and an analog-to-digital converter, and a
program mimicking the lock-in mechanism realizes microwave frequency-modulated
optically detected magnetic resonance of NV centers. As a result, with our
setup and technique, we have realized the detection of weak magnetic field with
a sensitivity of 93 nT/Hz^{1/2}, which is comparable to what obtained with
bulky and professional devices. Furthermore, we demonstrated real-time magnetic
field detection, achieving a standard deviation of 488 nT. Our work provides a
novel and cost-effective technique for electronic miniaturization, thereby
potentially accelerating the industrial application of NV magnetometers.

</details>


### [297] [Understanding Quantum Imaginary Time Evolution and its Variational form](https://arxiv.org/abs/2510.02015)
*Anglés-Castillo Andreu,Ion Luca,Pandit Tanmoy,Gomez-Lurbe Rafael,Martínez Rodrigo,Garcia-March Miguel Angel*

Main category: quant-ph

TL;DR: QITE算法用于求解量子哈密顿量基态，本文回顾了其原始算法、程序及变分版本。


<details>
  <summary>Details</summary>
Motivation: 许多计算难题可以编码为量子哈密顿量，其解为哈密顿量的基态。

Method: 回顾了量子虚时演化（QITE）算法，包括其原始算法、用于量子硬件的酉演化近似方法以及变分版本。

Result: 对QITE算法及其变体进行了全面的计算机程序实现。

Conclusion: 本文对QITE算法及其变体进行了回顾和总结。

Abstract: Many computationally hard problems can be encoded in quantum Hamiltonians.
The solution to these problems is given by the ground states of these
Hamiltonians. A state-of-the-art algorithm for finding the ground state of a
Hamiltonian is the so-called Quantum Imaginary Time Evolution (QITE) which
approximates imaginary time evolution by a unitary evolution that can be
implemented in quantum hardware. In this paper, we review the original
algorithm together with a comprehensive computer program, as well as, the
variational version of it.

</details>


### [298] [Critical Quantum Sensing: a tutorial on parameter estimation near quantum phase transitions](https://arxiv.org/abs/2510.02035)
*George Mihailescu,Uesli Alushi,Roberto Di Candia,Simone Felicetti,Karol Gietka*

Main category: quant-ph

TL;DR: 量子相变附近的量子现象可用于提高测量精度，但面临可扩展性、环境噪声和实际集成挑战。 本教程介绍了利用量子相变附近的临界量子计量学，重点介绍了优化估计精度和实际应用。 


<details>
  <summary>Details</summary>
Motivation: 量子现象在测量精度方面提供了超越经典极限的可能性，但面临可扩展性、环境噪声和实际集成等挑战，因此有必要探索新的方法。临界量子计量学利用量子相变附近的超敏度和非经典关联来实现量子增强精度，是一种新兴的解决方案。 

Method: 本教程通过由浅入深的示例，介绍临界量子计量学的关键概念，并概述利用临界现象的量子传感策略，以及研究其在不同临界系统中应用。教程还将重点介绍估计精度与基本资源之间的最优关系，并讨论临界量子计量学在开放和耗散系统中的应用，以及未来量子技术面临的挑战和机遇。 

Result: 教程通过示例详细介绍了临界量子传感协议在不同临界系统中的应用，并着重讨论了估计精度与资源之间的最优关系。 

Conclusion: 临界量子计量学从理想模型扩展到实际的开放系统和耗散系统，为未来的量子技术带来了挑战和机遇。 

Abstract: Quantum phenomena offer the possibility of measuring physical quantities with
precision beyond classical limits. However, current progress is constrained by
scalability, environmental noise, and challenges in practical integration. This
highlights the necessity for novel approaches. An emerging paradigm in this
direction is \emph{critical quantum metrology} -- which harnesses the enhanced
susceptibility and nonclassical correlations naturally occurring near quantum
phase transitions as resources for quantum-enhanced precision. This tutorial
provides a pedagogical introduction to key concepts and a detailed overview of
prominent quantum sensing strategies that exploit critical phenomena in
metrology. Through examples of increasing complexity, the reader is guided
through various critical quantum sensing protocols applied to different
critical systems. Special emphasis is placed on the optimal scaling of
estimation precision with respect to fundamental resources. Finally, we discuss
how critical quantum metrology extends from idealized models to realistic
open-system and dissipative regimes, and outline both the challenges and
opportunities for future quantum technologies.

</details>


### [299] [Improving neural network performance for solving quantum sign structure](https://arxiv.org/abs/2510.02051)
*Xiaowei Ou,Tianshu Huang,Vidvuds Ozolins*

Main category: quant-ph

TL;DR: 本研究提出了一种改进的随机重构方法，通过使用不同的虚时间步长来分别优化量子态的幅度和相位，实现了幅度和相位神经网络的同步高效训练，解决了现有方法依赖先验知识或需要预训练相位网络的局限性，并在 Heisenberg J_1-J_2 模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经量子态方法在处理非绝挫哈密顿量的基态研究时，常常需要预先了解符号结构或依赖预训练的相位网络，限制了其应用和效率。本研究旨在开发一种更通用的、能够同时高效训练幅度和相位网络的神经量子态方法。

Method: 提出一种改进的随机重构方法，该方法使用不同的虚时间步长来分别演化量子态的幅度和相位。具体来说，对相位优化采用更大的时间步长，以实现幅度和相位神经网络的同步和高效训练。

Result: 所提出的方法能够同时高效地训练幅度和相位神经网络，并在 Heisenberg J_1-J_2 模型上成功展示了其有效性，表明该方法在处理非绝挫哈密顿量方面具有优势。

Conclusion: 本研究提出的改进随机重构方法能够同时有效地训练神经网络的幅度和相位，克服了现有方法的局限性，为使用神经量子态研究非绝挫哈密顿量提供了一种更有效和通用的途径。

Abstract: Neural quantum states have emerged as a widely used approach to the numerical
study of the ground states of non-stoquastic Hamiltonians. However, existing
approaches often rely on a priori knowledge of the sign structure or require a
separately pre-trained phase network. We introduce a modified stochastic
reconfiguration method that effectively uses differing imaginary time steps to
evolve the amplitude and phase. Using a larger time step for phase
optimization, this method enables a simultaneous and efficient training of
phase and amplitude neural networks. The efficacy of our method is demonstrated
on the Heisenberg J_1-J_2 model.

</details>


### [300] [Resource theory of asymmetric distinguishability with partial information](https://arxiv.org/abs/2510.02071)
*Siqi Yao,Kun Fang*

Main category: quant-ph

TL;DR: 本研究将量子散度与非对称可区分性资源理论联系起来，在部分信息设置中对量子态集合进行区分，并使用（平滑）单次散度和正则化散度来表征资源蒸馏和稀释的最优速率，最后展示了资源相互转换的可逆性。


<details>
  <summary>Details</summary>
Motivation: 研究量子散度与非对称可区分性资源理论之间的关系，并将该理论扩展到部分信息设置，以区分量子态集合。

Method: 在部分信息设置中，利用（平滑）单次散度和正则化散度来表征资源蒸馏和稀释的最优速率，并证明了资源相互转换的可逆性。

Result: 研究结果表明，在部分信息设置中，可以使用（平滑）单次散度和正则化散度来表征资源蒸馏和稀释的最优速率，并且资源相互转换可以在由正则化散度决定的速率下无损耗地实现。

Conclusion: 本研究为量子态集合之间的散度提供了广泛的操作解释，并将现有的资源理论推广到包含不完全信息的情况。

Abstract: Recent studies have introduced the worst-case quantum divergence as a key
measure in quantum information. Here we show that such divergences can be
understood from the perspective of the resource theory of asymmetric
distinguishability, which utilizes the asymmetric distinguishability between a
pair of quantum states as resource. In our work, we extend this framework to
settings with partial information, where the goal is to distinguish between
sets of quantum states rather than individual states. Within this setting, we
characterize optimal rates for resource distillation and dilution using
(smoothed) one-shot divergences and regularized divergences. Our framework
further exhibits a reversibility property: resource interconversions can be
achieved without loss at rates determined entirely by regularized divergences.
These results offer a broad operational interpretation of divergences between
state sets and generalize existing resource theories to encompass incomplete
information scenarios.

</details>


### [301] [Optimizing fermionic Hamiltonians with classical interactions](https://arxiv.org/abs/2510.02122)
*Maarten Stroeks,Barbara M. Terhal,Yaroslav Herasymenko*

Main category: quant-ph

TL;DR: 我们将研究具有经典相互作用的费米子哈密顿量的优化问题（基态能量搜索），这是一个 QMA 难题，其动机在于库仑电子-电子相互作用在位置基中是对角的，这是量子化学和凝聚态物理学中电子结构哈密顿量的基本事实。我们证明了费米子高斯态对于这类哈密顿量至少能达到 1/3 的近似比，且与稀疏性无关。这表明经典相互作用足以防止 SYK 型模型中出现的近似比消失问题。我们还为几类无迹和半正定的经典相互作用哈密顿量的“高斯近似”提供了有效的半定规划算法，并能够强制执行固定的粒子数。我们成果的技术核心是“高斯混合”的概念，即通过协方差矩阵的混合来构建高斯态。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机源于库仑电子-电子相互作用在位置基中是对角的事实，这是量子化学和凝聚态物理学中电子结构哈密顿量的基本事实，引发了对具有经典相互作用的费米子哈密顿量优化问题的关注。

Method: 证明了费米子高斯态对于具有经典相互作用的费米子哈密顿量至少能达到 1/3 的近似比，并且与稀疏性无关。开发了能够强制执行固定粒子数的半定规划算法，用于几类无迹和半正定的经典相互作用哈密顿量的“高斯近似”。技术核心在于利用“高斯混合”方法，即通过混合协方差矩阵来构造高斯态。

Result: 费米子高斯态对于具有经典相互作用的费米子哈密顿量可以达到至少 1/3 的近似比，且该近似比不依赖于稀疏性。这与 SYK 型模型中出现的近似比消失现象形成对比。此外，还提供了针对特定哈密顿量类别的有效半定规划算法。

Conclusion: 经典相互作用足以防止 SYK 型模型中近似比消失的问题，并且费米子高斯态能为具有经典相互作用的费米子哈密顿量提供一个不依赖于稀疏性的近似界。通过“高斯混合”方法，可以有效地构造高斯态并用于近似计算。

Abstract: We consider the optimization problem (ground energy search) for fermionic
Hamiltonians with classical interactions. This QMA-hard problem is motivated by
the Coulomb electron-electron interaction being diagonal in the position basis,
a fundamental fact that underpins electronic-structure Hamiltonians in quantum
chemistry and condensed matter. We prove that fermionic Gaussian states achieve
an approximation ratio of at least 1/3 for such Hamiltonians, independent of
sparsity. This shows that classical interactions are sufficient to prevent the
vanishing Gaussian approximation ratio observed in SYK-type models. We also
give efficient semi-definite programming algorithms for Gaussian approximations
to several families of traceless and positive-semidefinite classically
interacting Hamiltonians, with the ability to enforce a fixed particle number.
The technical core of our results is the concept of a Gaussian blend, a
construction for Gaussian states via mixtures of covariance matrices.

</details>


### [302] [Realism and the Inequivalence of the Two Quantum Pictures](https://arxiv.org/abs/2510.02138)
*Charles Alexandre Bédard*

Main category: quant-ph

TL;DR: 量子力学的薛定谔绘景和海森堡绘景在科学实在论下并不等价，海森堡绘景的描述符为量子现象提供了更定域的解释。


<details>
  <summary>Details</summary>
Motivation: 探讨量子力学两种绘景（薛定谔绘景和海森堡绘景）在科学实在论世界观下的等价性问题，并论证海森堡绘景在解释某些量子现象时具有更优的定域性解释。

Method: 通过分析薛定谔绘景和海森堡绘景的数学结构（态函数和描述符），论证两者在科学实在论下（非仪器主义）并非同构，进而得出它们在哲学层面并不等价的结论。

Result: 在科学实在论下，海森堡绘景的描述符比薛定谔绘景的波函数更能提供对超密编码、量子传输、分支和贝尔不等式检验等现象的真正定域解释。

Conclusion: 量子力学的两种绘景在科学实在论下并不等价。海森堡绘景的描述符为量子理论提供了一个更丰富、可分离的本体论基础，并能在定域性方面提供比薛定谔绘景更优的解释。

Abstract: The standard claim that the Schr\"odinger and Heisenberg pictures of quantum
mechanics are equivalent rests on the fact that they yield identical empirical
predictions. This equivalence therefore assumes the instrumentalist worldview
in which theories serve only as tools for prediction. Under scientific realism,
by contrast, theories aim to describe reality. Whereas the Schr\"odinger
picture posits a time-evolving wave function, the Heisenberg picture posits
so-called descriptors, time-evolving generators of the algebra of observables.
These two structures are non-isomorphic: descriptors surject onto but do not
reduce to the Schr\"odinger state. Hence, under realism, the pictures are
inequivalent. I argue that this inequivalence marks an opening toward a richer,
separable ontology for quantum theory. On explanatory grounds, descriptors
provide genuinely local accounts of superdense coding, teleportation,
branching, and Bell inequality violations -- phenomena that the Schr\"odinger
framework does not explain fully locally.

</details>


### [303] [Quantum speed-up for solving the one-dimensional Hubbard model using quantum annealing](https://arxiv.org/abs/2510.02141)
*Kunal Vyas,Fengping Jin,Hans De Raedt,Kristel Michielsen*

Main category: quant-ph

TL;DR: 本研究使用量子算法模拟一维 Hubbard 模型，并在系统规模扩展时观察到量子加速。


<details>
  <summary>Details</summary>
Motivation: 研究 Hubbard 模型在量子计算中的应用，特别是利用量子算法解决多体问题。

Method: 使用基于门控的量子计算机模拟量子退火过程来求解 Hubbard 模型，并研究了系统规模（最多 40 个量子比特）对所需退火时间的影响。

Result: 在考虑的半填充情况下，与基于 Bethe 假设方程的算法相比，量子退火算法在获得基态方面表现出显著的量子加速。

Conclusion: 量子算法，特别是量子退火，在求解一维 Hubbard 模型方面，相对于经典算法具有显著的优势，并且这种优势随着系统规模的增大而更加明显。

Abstract: The Hubbard model has occupied the minds of condensed matter physicists for
most part of the last century. This model provides insight into a range of
phenomena in correlated electron systems. We wish to examine the paradigm of
quantum algorithms for solving such many-body problems. The focus of our
current work is on the one-dimensional model which is integrable, meaning that
there exist analytical results for determining its ground state. In particular,
we demonstrate how to perform a gate-based quantum computer simulation of
quantum annealing for the Hubbard Hamiltonian. We perform simulations for
systems with up to 40 qubits to study the scaling of required annealing time
for obtaining the ground state. We find that for the half-filled cases
considered, there is a substantial quantum speed-up over algorithms based on
the Bethe-ansatz equations.

</details>


### [304] [Chaotic many-body quantum dynamics, spectral correlations, and energy diffusion](https://arxiv.org/abs/2510.02198)
*J. T. Chalker,Dominik Hahn*

Main category: quant-ph

TL;DR: 该研究提出了一个可分析处理的最小模型来研究具有空间结构和局部相互作用的混沌多体量子动力学。


<details>
  <summary>Details</summary>
Motivation: 研究具有空间结构和局部相互作用的混沌多体量子动力学，重点关注其与量子电路的不同之处，并寻求分析处理的方法。

Method: 在大的局域希尔伯特空间维度和弱的格点间耦合的极限下，推导出能量动力学可以用经典主方程描述且是扩散的。此外，还推导了谱形式因子与该主方程解的精确关系。

Result: 对于一个双格点系统，得到了能量密度双点关联函数和谱形式因子的解析表达式，与数值模拟结果高度一致。对于L个格点的系统，在高时标下，谱形式因子呈现出普遍预期的线性增长。在早期，发现了两种增强谱形式因子的机制：一种与能量扩散相关，持续到Thouless时间（与L^2成正比）；另一种涉及未耦合子系统贡献，在（lnL）^2的时间尺度内被抑制。通过对自旋1/2链进行数值研究，发现其早期谱形式因子增强的特性与可解模型具有相似的定性行为。

Conclusion: 该方法不仅在所考虑的极限下是精确的，而且有望成为处理小局域希尔伯特空间维度和强格点间耦合的自然近似方法。

Abstract: We study chaotic many-body quantum dynamics in a minimal model with spatial
structure and local interactions. It has a time-independent Hamiltonian, in
contrast to much-studied quantum circuits, and is analytically tractable for
large local Hilbert space dimension and weak intersite coupling. In this limit
we show that energy dynamics is described by a classical master equation and is
diffusive. We also show that the spectral form factor can be expressed exactly
in terms of the solution to this master equation. For a two-site system we
obtain closed-form expressions for both the two-point correlator of energy
density and the spectral form factor, in essentially perfect agreement with
numerical simulations. For an $L$-site system we show at late times how a
linear ramp emerges in the spectral form factor, as universally expected from
level repulsion in chaotic quantum systems. Conversely, at earlier times we
identify two distinct mechanisms for an increase of the spectral form factor
above its ramp value. One of these is associated with energy diffusion and is
effective until the Thouless time, which varies as $L^2$. The other involves
contributions like those that would appear if the system were composed of many
uncoupled subsystems: they generate a large enhancement of the spectral form
factor, and are suppressed on a timescale varying as $(\ln L)^2$. Besides being
exact for the limit considered, we believe our approach provides the natural
approximation even for small local Hilbert space dimension and strong intersite
coupling. We present a numerical study of a spin-half chain, finding an
early-time enhancement of the spectral form factor which is qualitatively
similar to that in our solvable model.

</details>


### [305] [Quantum Fisher information matrices from Rényi relative entropies](https://arxiv.org/abs/2510.02218)
*Mark M. Wilde*

Main category: quant-ph

TL;DR: 本文推导了量子Fisher信息矩阵的几种新形式，并证明了它们满足数据处理不等式，同时还研究了参数化热态下的α-z信息矩阵及其在量子玻尔兹曼机学习中的应用。


<details>
  <summary>Details</summary>
Motivation: 量子Fisher信息矩阵在量子信息科学、高能物理、凝聚态物理、量子估计理论、机器学习和优化等领域有重要应用。然而，与经典情况不同，量子Fisher信息矩阵没有唯一形式，这促使研究者去探索不同的量子推广。

Method: 本文使用泰勒展开和分裂差分法推导了源自分割Rényi相对熵（log-Euclidean、α-z和几何）的量子Fisher信息矩阵。具体地，利用分裂差分法计算矩阵导数，并研究了这些信息矩阵的基本性质，特别是它们是否满足数据处理不等式。

Result: 研究发现，log-Euclidean Rényi相对熵对应于Kubo-Mori信息矩阵，几何Rényi相对熵对应于右对数导数Fisher信息矩阵，并且它们在所有非负Rényi参数α下都满足数据处理不等式。此外，还推导了α-z信息矩阵，并给出了参数化热态下α-z信息矩阵的计算公式和混合量子-经典估计算法。

Conclusion: 本文成功推导了多种量子Fisher信息矩阵，并揭示了它们与特定相对熵之间的联系。研究强调了log-Euclidean Rényi相对熵和几何Rényi相对熵产生的矩阵在数据处理不等式上的优越性。最后，将研究成果应用于参数化热态的估计，为量子机器学习等领域提供了新的工具和见解。

Abstract: Quantum generalizations of the Fisher information are important in quantum
information science, with applications in high energy and condensed matter
physics and in quantum estimation theory, machine learning, and optimization.
One can derive a quantum generalization of the Fisher information matrix in a
natural way as the Hessian matrix arising in a Taylor expansion of a smooth
divergence. Such an approach is appealing for quantum information theorists,
given the ubiquity of divergences in quantum information theory. In contrast to
the classical case, there is not a unique quantum generalization of the Fisher
information matrix, similar to how there is not a unique quantum generalization
of the relative entropy or the R\'enyi relative entropy. In this paper, I
derive information matrices arising from the log-Euclidean, $\alpha$-$z$, and
geometric R\'enyi relative entropies, with the main technical tool for doing so
being the method of divided differences for calculating matrix derivatives.
Interestingly, for all non-negative values of the R\'enyi parameter $\alpha$,
the log-Euclidean R\'enyi relative entropy leads to the Kubo-Mori information
matrix, and the geometric R\'enyi relative entropy leads to the
right-logarithmic derivative Fisher information matrix. Thus, the resulting
information matrices obey the data-processing inequality for all non-negative
values of the R\'enyi parameter $\alpha$ even though the original quantities do
not. Additionally, I derive and establish basic properties of $\alpha$-$z$
information matrices resulting from the $\alpha$-$z$ R\'enyi relative
entropies. For parameterized thermal states, I establish formulas for their
$\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for
estimating them, with applications in quantum Boltzmann machine learning.

</details>


### [306] [The (PXP)$^2$ model: long-range quantum scars in optical cavities](https://arxiv.org/abs/2510.02246)
*Hossein Hosseinabadi,Riccardo J. Valencia-Tortora,Aleksandr N. Mikheev,Darrick E. Chang,Johannes Zeiher,Roderich Moessner,Jamir Marino*

Main category: quant-ph

TL;DR: Rydberg-cavity系统结合了光子介导的长程耦合和Rydberg激发产生的短程相互作用，为研究多体物理学中的相互作用层级提供了一个可扩展的模型。该模型在强Rydberg阻塞区进行简化，并引入了一种新的块状铁磁/超辐射相。在非平衡状态下，发现了导致纠缠缓慢增长的长程量子多体伤痕态，其纠缠动力学呈对数增长。


<details>
  <summary>Details</summary>
Motivation: 探索Rydberg-腔系统在量子模拟和量子信息处理中的潜力，特别是如何结合长程和短程相互作用来构建多体相。

Method: 提出一个最小且可扩展的Rydberg-腔系统模型，聚焦于强Rydberg阻塞区，将希尔伯特空间限制在由阻塞决定的子空间内，得到一个在一维空间中的动量受限的长程模型。

Result: 在平衡状态下，发现了顺磁相、Néel序相以及一种新的块状铁磁/超辐射相。在非平衡状态下，识别出长程量子多体伤痕态，它们是非典型的非热态，表现出对数纠缠增长动力学，不同于短程伤痕模型中的线性增长。

Conclusion: 该工作提出了一个最小但通用的Rydberg-腔系统框架，有助于理解长程和短程相互作用的协同效应，并为未来在量子多体物理学前沿平台上的理论和实验研究奠定了基础。

Abstract: Rydberg-cavity systems are emerging as promising platforms for quantum
simulation and quantum information processing. These hybrid architectures
combine two complementary interaction mechanisms: cavity photons mediate
collective long-range couplings, while Rydberg excitations generate strong
short-range interactions. Together, they offer a setting for engineering
many-body phases characterized by a hierarchy of interactions across widely
different length scales. In this work, we introduce a minimal and scalable
model for such systems. Focusing on the strong Rydberg blockade regime, we
restrict the Hilbert space to the subspace enforced by the blockade, yielding a
kinetically constrained long-range model in one spatial dimension. This
approach both captures the physics of Rydberg-cavity experiments in the regime
of strong Rydberg interactions and provides a conceptually transparent
framework for studying the interplay of long-range and short-range
interactions. At equilibrium, in addition to paramagnetic and N\'eel-ordered
phases, the system supports a blockaded ferromagnetic/superradiant phase,
distinct from the conventional superradiant phase. Out of equilibrium, we
identify long-range quantum many-body scars, which are atypical nonthermal
eigenstates that evade the eigenstate thermalization hypothesis, and giving
rise to slow entanglement growth. In contrast to the linear-in-time
entanglement growth characteristic of short-range scarred models, these
long-range scars exhibit logarithmic entanglement dynamics. Our results
establish a minimal yet versatile framework for Rydberg-cavity systems, and
provide a stepping stone for future theoretical and experimental studies of
this frontier platform in quantum many-body physics.

</details>


### [307] [Reproducible Builds for Quantum Computing](https://arxiv.org/abs/2510.02251)
*Iyán Méndez Veiga,Esther Hänggi*

Main category: quant-ph

TL;DR: 该论文将可复现构建的概念推广到量子计算领域，以应对与量子计算相关的供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展需要可复现构建来防御针对用户数据和计算结果的供应链攻击。

Method: 提出量子计算环境中可复现构建的定义，并通过实例展示了如何隐藏经典信息和篡改量子计算结果。

Result: 展示了三种将经典信息隐藏在量子电路中的方法，以及两种通过最小化电路修改导致计算结果错误的方法。

Conclusion: 为量子软件工具链的可复现性框架奠定了初步基础。

Abstract: Reproducible builds are a set of software development practices that
establish an independently verifiable path from source code to binary
artifacts, helping to detect and mitigate certain classes of supply chain
attacks. Although quantum computing is a rapidly evolving field of research, it
can already benefit from adopting reproducible builds. This paper aims to
bridge the gap between the quantum computing and reproducible builds
communities. We propose a generalization of the definition of reproducible
builds in the quantum setting, motivated by two threat models: one targeting
the confidentiality of end users' data during circuit preparation and
submission to a quantum computer, and another compromising the integrity of
quantum computation results. This work presents three examples that show how
classical information can be hidden in transpiled quantum circuits, and two
cases illustrating how even minimal modifications to these circuits can lead to
incorrect quantum computation results. Our work provides initial steps towards
a framework for reproducibility in quantum software toolchains.

</details>


### [308] [Lower bounds on the complexity of preparing mixed states](https://arxiv.org/abs/2510.02275)
*Max McGinley,Samuel J. Garratt*

Main category: quant-ph

TL;DR: 研究表明，多量子比特混合态中的关联性与制备该状态所需最小线路深度之间存在联系。当两个子系统之间的互信息超过其中一个子系统与用于净化系统混合态的环境之间的互信息时，子系统过去的光锥必须相交，这为制备该状态的几何局部酉算符系规定了线路深度的下界。该研究还应用此发现来推导一维量子临界系统（由共形场论描述）热态制备所需的线路深度下界，并指出该深度随温度降低而发散，直至由制备误差设定的截止值。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是探索多量子比特混合态的关联性与制备该状态所需的最小线路深度之间的关系，并为制备过程提供理论指导。

Method: 通过分析多量子比特混合态中子系统间的互信息与系统-环境互信息的关系，推导出子系统过去光锥相交的条件，进而建立线路深度的下界。将此方法应用于一维量子临界系统，推导其热态制备的线路深度下界。

Result: 研究建立了多量子比特混合态的关联性与最小制备线路深度之间的联系，并推导出线路深度的下界。研究还表明，对于一维量子临界系统的热态，其制备线路深度随温度降低而发散，直至由制备误差设定的截止值。

Conclusion: 多量子比特混合态的关联性决定了其最小制备线路深度，并存在可计算的下界。该结论对于理解和设计量子信息处理任务中的状态制备具有重要意义。

Abstract: We establish a relationship between the correlations in a many-qubit mixed
state and the minimum circuit depth needed for its preparation. If the mutual
information between two subsystems exceeds the mutual information between one
of those subsystems and the environment, which purifies the mixed state of the
system, then the past lightcones of the subsystems must intersect one another.
This results in a lower bound on the circuit depth of any ensemble of
geometrically local unitaries that prepares the state to some specified degree
of approximation. As an application, we derive lower bounds on the circuit
depth needed to prepare thermal states of one-dimensional quantum critical
systems described by conformal field theory, showing that the depth diverges as
temperature is decreased up to a cutoff set by the preparation error.

</details>


### [309] [Beyond Belief Propagation: Cluster-Corrected Tensor Network Contraction with Exponential Convergence](https://arxiv.org/abs/2510.02290)
*Siddhant Midha,Yifan F. Zhang*

Main category: quant-ph

TL;DR: BP算法在任意图上的张量网络收缩的精度有限，本文提出了一个基于统计力学的聚类展开方法，可以系统地改进BP近似，并提供了一个有效的算法来计算它。


<details>
  <summary>Details</summary>
Motivation: BP算法在张量网络收缩任务中虽然强大，但其精度限制理解不足，系统性改进方法难以找到。

Method: 提出一个基于统计力学的聚类展开，以系统地改进BP近似。

Result: 聚类展开被证明收敛速度为指数级，并且在二维Ising模型上，该方法显著优于BP和现有的修正算法。

Conclusion: 这项工作为BP在张量网络中的系统理论及其在解码和模拟中的应用打开了新的可能性。

Abstract: Tensor network contraction on arbitrary graphs is a fundamental computational
challenge with applications ranging from quantum simulation to error
correction. While belief propagation (BP) provides a powerful approximation
algorithm for this task, its accuracy limitations are poorly understood and
systematic improvements remain elusive. Here, we develop a rigorous theoretical
framework for BP in tensor networks, leveraging insights from statistical
mechanics to devise a \emph{cluster expansion} that systematically improves the
BP approximation. We prove that the cluster expansion converges exponentially
fast if an object called the \emph{loop contribution} decays sufficiently fast
with the loop size, giving a rigorous error bound on BP. We also provide a
simple and efficient algorithm to compute the cluster expansion to arbitrary
order. We demonstrate the efficacy of our method on the two-dimensional Ising
model, where we find that our method significantly improves upon BP and
existing corrective algorithms such as loop series expansion. Our work opens
the door to a systematic theory of BP for tensor networks and its applications
in decoding classical and quantum error-correcting codes and simulating quantum
systems.

</details>


### [310] [Quantum-Assisted Correlation Clustering](https://arxiv.org/abs/2509.03561)
*Antonio Macaluso,Supreeth Mysore Venkatesh,Diego Arenas,Matthias Klusch,Andreas Dengel*

Main category: quant-ph

TL;DR: 提出了一种混合量子-经典方法来解决图的聚类问题，该方法通过递归划分来最大化集群内的一致性，并使用量子退火来解决二次无约束二元优化问题。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决图的聚类问题，这是一个基于图的无监督学习任务，旨在根据成对的同意和不同意来划分图的节点。

Method: 将 GCS-Q（一种最初为联盟结构生成设计的量子辅助求解器）改编为通过递归划分来最大化带符号图中的集群内一致性。该方法将每个二分步骤编码为二次无约束二元优化问题，并通过量子退火求解。

Result: 在合成带符号图和真实世界高光谱成像数据上的经验评估表明，当针对相关性聚类进行调整时，GCS-Q 在鲁棒性和聚类质量方面优于经典算法，尤其是在真实世界数据和集群大小不平衡的情况下。

Conclusion: 混合量子-经典优化有望推动基于图的无监督学习中可扩展且结构感知的聚类技术的发展。

Abstract: This work introduces a hybrid quantum-classical method to correlation
clustering, a graph-based unsupervised learning task that seeks to partition
the nodes in a graph based on pairwise agreement and disagreement. In
particular, we adapt GCS-Q, a quantum-assisted solver originally designed for
coalition structure generation, to maximize intra-cluster agreement in signed
graphs through recursive divisive partitioning. The proposed method encodes
each bipartitioning step as a quadratic unconstrained binary optimization
problem, solved via quantum annealing. This integration of quantum optimization
within a hierarchical clustering framework enables handling of graphs with
arbitrary correlation structures, including negative edges, without relying on
metric assumptions or a predefined number of clusters. Empirical evaluations on
synthetic signed graphs and real-world hyperspectral imaging data demonstrate
that, when adapted for correlation clustering, GCS-Q outperforms classical
algorithms in robustness and clustering quality on real-world data and in
scenarios with cluster size imbalance. Our results highlight the promise of
hybrid quantum-classical optimization for advancing scalable and
structurally-aware clustering techniques in graph-based unsupervised learning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [311] [JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator](https://arxiv.org/abs/2510.01213)
*Tao Han,Ang Li,Qinyu Chen,Chang Gao*

Main category: eess.SP

TL;DR: JaneEye是一个为XR可穿戴设备设计的、节能的、基于事件的眼动追踪硬件加速器，它使用一个简化的轻量级神经网络，在3ET+数据集上实现了2.45像素误差，并且能效比高。


<details>
  <summary>Details</summary>
Motivation: 传统的基于帧的眼动追踪系统在XR环境下存在精度、延迟和能效方面的不足，而事件相机因其高时间分辨率和低功耗的特性，为解决这些问题提供了新的可能。

Method: 提出了一种名为JaneEye的节能型事件驱动眼动追踪硬件加速器，采用了一个简化的轻量级神经网络（ConvJANET层），并结合了自定义的线性激活函数近似和固定点量化。通过软硬件协同设计，在12nm ASIC上实现了低延迟和高能效。

Result: JaneEye在3ET+数据集上实现了2.45像素误差，参数量仅为17.6K，事件帧率高达1250 Hz。其ASIC实现运行频率为400 MHz，端到端延迟为0.5 ms（2000 FPS），能效为18.9 μJ/frame。

Conclusion: JaneEye在低功耗、高性能眼动追踪方面设定了新的基准，适用于集成到下一代XR可穿戴设备中。

Abstract: Eye tracking has become a key technology for gaze-based interactions in
Extended Reality (XR). However, conventional frame-based eye-tracking systems
often fall short of XR's stringent requirements for high accuracy, low latency,
and energy efficiency. Event cameras present a compelling alternative, offering
ultra-high temporal resolution and low power consumption. In this paper, we
present JaneEye, an energy-efficient event-based eye-tracking hardware
accelerator designed specifically for wearable devices, leveraging sparse,
high-temporal-resolution event data. We introduce an ultra-lightweight neural
network architecture featuring a novel ConvJANET layer, which simplifies the
traditional ConvLSTM by retaining only the forget gate, thereby halving
computational complexity without sacrificing temporal modeling capability. Our
proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+
dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To
further enhance hardware efficiency, we employ custom linear approximations of
activation functions (hardsigmoid and hardtanh) and fixed-point quantization.
Through software-hardware co-design, our 12-nm ASIC implementation operates at
400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames
Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a
new benchmark in low-power, high-performance eye-tracking solutions suitable
for integration into next-generation XR wearables.

</details>


### [312] [Satellite Assignment Policy Learning for Coexistence in LEO Networks](https://arxiv.org/abs/2510.01408)
*Jeong Min Kong,Ian P. Roberts*

Main category: eess.SP

TL;DR: LEO卫星系统的频谱接入存在非独占性分配问题，主要系统优先于次要系统。次要系统需要推断主要系统的卫星分配策略以避免干扰。本文提出了一种基于图结构学习的算法来推断最高仰角的主要卫星分配策略，并将卫星坐标映射到用户分配决策。


<details>
  <summary>Details</summary>
Motivation: 次要LEO卫星系统在实际部署中需要了解主要系统的卫星分配策略，以避免对主要地面用户产生过量干扰。然而，主要系统并未公开其分配策略，因此需要一种方法来推断这些策略。

Method: 提出了一种端到端的基于图结构学习的算法，该算法利用有限的历史数据来推断最高仰角的主要卫星分配策略，直接将主要卫星的坐标映射到用户分配决策。

Result: 仿真结果表明，所提出的算法比现有的最佳基线方法在预测准确性方面提高了约15%。

Conclusion: 所提出的基于图结构学习的算法能够有效地推断LEO卫星系统的主要卫星分配策略，并提高了次要系统的性能。

Abstract: Unlike in terrestrial cellular networks, certain frequency bands for
low-earth orbit (LEO) satellite systems have thus far been allocated on a
non-exclusive basis. In this context, systems that launch their satellites
earlier (referred to as primary systems) are given spectrum access priority
over those that launch later, known as secondary systems. For a secondary
system to function, it is expected to either coordinate with primary systems or
ensure that it does not cause excessive interference to primary ground users.
Reliably meeting this interference constraint requires real-time knowledge of
the receive beams of primary users, which in turn depends on the primary
satellite-to-primary user associations. However, in practice, primary systems
have thus far not publicly disclosed their satellite assignment policies;
therefore, it becomes essential for secondary systems to develop methods to
infer such policies. Assuming there is limited historical data indicating which
primary satellites have served which primary users, we propose an end-to-end
graph structure learning-based algorithm for learning highest elevation primary
satellite assignment policies, that, upon deployment, can directly map the
primary satellite coordinates into assignment decisions for the primary users.
Simulation results show that our method can outperform the best baseline,
achieving approximately a 15% improvement in prediction accuracy.

</details>


### [313] [Delay-Augmented Stacked Intelligent Surfaces: Potential, Challenges, and Opportunities](https://arxiv.org/abs/2510.01411)
*Hibatallah Alwazani,Omran Abbas,Loic Markley,Anas Chaaban*

Main category: eess.SP

TL;DR: 在SIS中引入延迟单元，提出延迟增强SIS（DA-SIS），并探讨其在模拟均衡中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了扩展SIS在全息MIMO和超大规模MIMO等技术中的应用，并实现时间域信号处理。

Method: 提出延迟增强SIS（DA-SIS）的概念，研究延迟单元的可行性，并将其应用于消除多径干扰的模拟均衡器。

Result: 通过比特错误率（BER）展示了DA-SIS在均衡过程中的性能，并与数字均衡器进行了比较。

Conclusion: DA-SIS在模拟均衡中具有潜力，并指出了未来的研究方向。

Abstract: Stacked intelligent surfaces (SIS)s have been proposed recently as an
enabling technology for Holographic Multiple Input Multiple Output (HMIMO) and
Ultra-massive MIMO (umMIMO) technologies. Their utility can extend beyond
spatial wave-domain processing of signals if they are enhanced with
strategically-tuned symbol-duration level delays to enable temporal processing
as well. In this work, we introduce the idea of a delay-augmented SIS (DA-SIS).
We shed light on the feasibility of realizing delay units in an SIS. Then, we
discuss the relevance of the proposed DA-SIS and present a use case that
illustrates its potential, wherein the DA-SIS serves as an analog equalizer
that aids in eliminating multi-path-induced inter-symbol-interference (ISI). We
show how the number of elements affect the equalization process using the bit
error rate (BER) as a metric, and demonstrate the potential of the DA-SIS in
equalization via comparing with digital equalizers as a benchmark. Finally, we
present opportunities and future research directions that can be undertaken to
bring this idea to fruition.

</details>


### [314] [A Drone-mounted Magnetometer System for Automatic Interference Removal and Landmine Detection](https://arxiv.org/abs/2510.01417)
*Alex Paul Hoffmann,Matthew G. Finley,Eftyhia Zesta,Mark B. Moldwin,Lauro V. Ojeda*

Main category: eess.SP

TL;DR: 使用无人机搭载的载有双磁力计的有效载荷，通过小波自适应干扰消除（WAIC-UP）和快速无监督事件检测（RUDE）算法，实现对地雷的高保真、低成本检测。


<details>
  <summary>Details</summary>
Motivation: 无人机常用于检测地雷，但其电子设备会产生磁场干扰，影响检测精度。现有技术面临挑战，需要更优的解决方案。

Method: 提出一种两步法：1. 使用小波自适应干扰消除（WAIC-UP）方法去除无人机自身的磁场干扰。2. 使用快速无监督事件检测（RUDE）算法检测地雷特征。该方法利用了多磁力计和两步自动化处理。

Result: 通过蒙特卡洛模拟，该方法在模拟的地雷和无人机电机干扰下，能够实现高保真度的地雷检测，且计算成本低。在不同无人机高度下均表现出有效性。

Conclusion: 提出的WAIC-UP/RUDE两步法能够有效解决无人机磁场干扰问题，实现高保真、低成本的地雷检测，并简化了磁力探测载荷的设计。

Abstract: Landmines have been extensively used in conflict zones as an indiscriminate
weapon to control military movements, often remaining active long after
hostilities have ended. Their presence poses a persistent danger to civilians,
hindering post-war recovery efforts, causing injuries or death, and restricting
access to essential land for agriculture and infrastructure. Unmanned aerial
vehicles (UAV) equipped with magnetometers are commonly used to detect remnant
hidden landmines but come with significant technical challenges due to magnetic
field interference from UAV electronics such as motors. We propose the use of a
frame-mounted UAV-borne two-magnetometer payload to perform a two-step
automated interference removal and landmine detection analysis. The first step
removes interference via the Wavelet-Adaptive Interference Cancellation for
Underdetermined Platform (WAIC-UP) method designed for spaceflight
magnetometers. The second method uses the Rapid Unsupervised Detection of
Events (RUDE) algorithm to detect landmine signatures. This two-step
WAIC-UP/RUDE approach with multiple magnetometers achieves high-fidelity
ordinance detection at a low computational cost and simplifies the design of
magnetic survey payloads. We validate the method through a Monte Carlo
simulation of randomized landmine placements in a 10 x 10 m square grid and
drone motor interference. Additionally, we assess the efficacy of the algorithm
by varying the drone's altitude, examining its performance at different heights
above the ground.

</details>


### [315] [Meta-Learning-Driven Resource Optimization in Full-Duplex ISAC with Movable Antennas](https://arxiv.org/abs/2510.01437)
*Ali Amhaz,Shreya Khisa,Mohamed Elhattab,Chadi Assi,Sanaa Sharafeddine*

Main category: eess.SP

TL;DR: 本文提出了一种基于可移动天线（MA）的基站（BS）在全双工（FD）模式下，为下行链路（DL）和上行链路（UL）用户提供通信服务，同时支持集成传感与通信（ISAC）技术的传感功能。通过优化发送波束成形向量、接收波束成形向量、UL用户发射功率以及MA位置，最大化回波信噪比（SINR），并满足通信和服务质量（QoS）要求。采用基于梯度的元学习（GML）方法解决非凸和耦合问题。数值结果表明，GML方法可达到最优解的99%，并且MA方案优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 在全双工（FD）通信场景下，基站（BS）不仅要为用户提供通信服务，还需要具备传感功能以支持集成传感与通信（ISAC）技术，同时要处理来自用户的上行链路（UL）信号和向用户发送的下行链路（DL）信号，并接收由另一基站（BS R）捕获的反射回波，以优化整体性能并满足服务质量（QoS）要求。

Method: 提出一个优化问题，旨在最大化捕获回波的信噪比（SINR），通过联合优化FD BS的发射波束成形向量、FD BS和BS R的接收波束成形向量、UL用户的发射功率以及两BS的可移动天线（MA）的位置。针对该非凸和耦合问题，采用基于梯度的元学习（GML）方法进行求解。

Result: 数值结果表明，所提出的GML方法能够达到最优解的99%，并且基于MA的方案在ISAC应用中优于多个基准方法。

Conclusion: 基于可移动天线（MA）的基站（BS）在全双工（FD）模式下，能够有效地支持集成传感与通信（ISAC）技术。所提出的基于梯度的元学习（GML）方法是一种高效的优化技术，可以获得接近最优的性能，并实现了优于基准方法的性能，证明了其在实际ISAC应用中的潜力。

Abstract: This paper investigates a full-duplex (FD) scenario where a base station (BS)
equipped with movable antennas (MAs) simultaneously provides communication
services to a set of downlink (DL) and uplink (UL) users while also enabling
sensing functionalities for target detection, thereby supporting integrated
sensing and communication (ISAC) technology. Additionally, a receiving BS, also
equipped with MAs (denoted as BS R), is responsible for capturing the reflected
echo. To optimize this setup, we formulate an optimization problem aimed at
maximizing the signal-to-noise and interference ratio (SINR) of the captured
echo. This is achieved by jointly optimizing the transmit beamforming vectors
at the FD BS, the receiving beamforming vectors at both the FD BS and BS R, the
UL users' transmit power, and the MAs' positions at both BSs, all while
satisfying the quality-of-service (QoS) requirements for both sensing and
communication. Given the non-convex nature of the problem and the high coupling
between the variables, we employ a gradient-based meta-learning (GML) approach
tailored for large-scale optimization. Numerical results demonstrate the
effectiveness of the proposed meta-learning approach, achieving results within
99% of the optimal solution. Furthermore, the MA-based scheme outperforms
several benchmark approaches, highlighting its advantages in practical ISAC
applications.

</details>


### [316] [The Analysis and Performance of LODC-OFDM Signal in Nonlinear Rydberg Atomic Sensor](https://arxiv.org/abs/2510.01605)
*Hao Wu,Xinyuan Yao,Rui Ni,Chen Gong*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Rydberg atomic sensors have been seen as novel radio frequency (RF)
measurements and the high sensitivity to a large range of frequencies makes it
attractive for communications reception. However, the signal sensing process in
Rydberg system involves sequential transduction from electromagnetic waves to
optical signals and finally to electrical signals. The unipolar characteristic
of the optical interface inherently restricts conventional OFDM reception.
Therefore, adopting unipolar OFDM schemes, inspired by optical communication
systems, becomes essential for compatible signal transmission. In this work, we
investigate the amplitude modulation-to-amplitude modulation (AM-AM)
characteristics of Rydberg atomic sensors, establishing an empirical
approximation function. Building on the direct current-biased optical
orthogonal frequency division multiplexing (DCO-OFDM) framework, we propose a
novel local oscillator direct current-biased OFDM (LODC-OFDM) scheme
specifically optimized for Rydberg-based sensing, effectively addressing the
broadband OFDM reception challenge. Then, we adopt Bussgang theorem to analyze
the nonlinear distortion of LODC-OFDM signals and the results in closed-form
solutions are derived for AM/AM curves approximated by Taylor series expansion
and for the ideal pre-distortion case. In real experiments, the experimental
and theoretical results fit well.

</details>


### [317] [SEP Analysis of 1-Bit Quantized SIMO Systems with QPSK over Fading Channels](https://arxiv.org/abs/2510.01707)
*Amila Ravinath,Minhua Ding,Bikshapathi Gouda,Italo Atzeni,Antti Tölli*

Main category: eess.SP

TL;DR: 本文推导了1位量化单输入多输出（SIMO）系统在瑞利衰落信道和QPSK调制下的符号错误概率（SEP）的精确解析表达式，并确定了SIMO-MRC系统的分集增益和编码增益。此外，还量化了1位量化SIMO-SC系统的分集和编码增益。


<details>
  <summary>Details</summary>
Motivation: 先前研究仅部分表征了选择性合并（SC）的分集增益，本文旨在推导1位量化SIMO系统在QPSK调制和最大比合并（MRC）接收下的精确符号错误概率（SEP）表达式，并确定其分集增益和编码增益，同时扩展和补充了SIMO-SC系统的相关结果。

Method: 利用新颖的解析方法，推导了1位量化SIMO-MRC系统在QPSK调制下的精确解析SEP表达式。同时，确定了该系统的分集增益和编码增益。此外，还量化了1位量化SIMO-SC系统（任意数量接收天线）的分集和编码增益。

Result: 推导了1位量化SIMO-MRC系统在QPSK调制下的精确解析SEP表达式，并确定了其分集增益和编码增益。量化了1位量化SIMO-SC系统（任意数量接收天线）的分集和编码增益。

Conclusion: 本文成功推导了1位量化SIMO-MRC系统在QPSK调制下的精确解析SEP表达式，并确定了其分集增益和编码增益。同时，为1位量化SIMO-SC系统提供了分集和编码增益的量化结果，扩展了现有研究。

Abstract: The average symbol error probability (SEP) of a 1-bit quantized single-input
multiple-output (SIMO) system is analyzed under Rayleigh fading channels and
quadrature phase-shift keying (QPSK) modulation. Previous studies have
partially characterized the diversity gain for selection combining (SC). In
this paper, leveraging a novel analytical method, an exact analytical SEP
expression is derived for a 1-bit quantized SIMO system employing QPSK
modulation at the transmitter and maximum ratio combining (MRC) at the
receiver. The corresponding diversity and coding gains of a SIMO-MRC system are
also determined. Furthermore, the diversity and coding gains of a 1-bit
quantized SIMO-SC system are quantified for an arbitrary number of receive
antennas, thereby extending and complementing prior results.

</details>


### [318] [3D 8-Ary Noise Modulation Using Bayesian- and Kurtosis-based Detectors](https://arxiv.org/abs/2510.01748)
*Hadi Zayyani,Felipe A. P. de Figueiredo,Mohammad Salman,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 提出了一种新的三维8-ary噪声调制方案，通过引入混合高斯（MoG）分布的混合概率作为新维度，并结合均值和方差维度。该方案每个传输符号携带三比特，分配给三个子信道。检测器组合包括：用于均值子信道的阈值检测器；用于方差子信道的最大似然（ML）检测器；以及用于MoG概率子信道的峰度、Jarque-Bera（JB）检验和贝叶斯假设检验（BHT）检测器。仿真结果表明，第三子信道比特的误码率（BEP）与现有二维（2D）方案相当，同时数据速率分别比广义二次噪声调制器和经典二元KLJN噪声调制器提高了1.5倍和3倍。其中，峰度检测器提供了一种低复杂度解决方案，BEP约为0.06。


<details>
  <summary>Details</summary>
Motivation: 现有二维（2D）噪声调制方案在提高数据速率和降低误码率方面存在局限性，需要新的调制维度来提升性能。

Method: 提出了一种新的三维（3D）8-ary噪声调制方案，引入了混合高斯（MoG）分布的混合概率作为第三个维度，与均值和方差维度结合。设计了多样的检测器，包括阈值检测器、最大似然（ML）检测器、以及基于峰度、Jarque-Bera（JB）检验和贝叶斯假设检验（BHT）的检测器，以分别处理三个子信道。推导了阈值、峰度和BHT检测器的比特误码概率（BEP），并得到了峰度检测器的最优阈值。

Result: 仿真结果表明，所提出的三维8-ary噪声调制方案实现了与现有二维（2D）方案相当的第三子信道比特的误码率（BEP）。同时，与广义二次噪声调制器和经典二元KLJN噪声调制器相比，数据速率分别提高了1.5倍和3倍。此外，峰度检测器作为一种低复杂度解决方案，实现了约0.06的误码率。

Conclusion: 所提出的三维8-ary噪声调制方案通过引入混合概率维度，在保持较低误码率的同时，显著提高了数据传输速率。峰度检测器提供了一种在性能和复杂度之间取得良好平衡的有效检测方法。

Abstract: This paper presents a novel three-dimensional (3D) 8-ary noise modulation
scheme that introduces a new dimension: the mixture probability of a Mixture of
Gaussian (MoG) distribution. This proposed approach utilizes the dimensions of
mean and variance, in addition to the new probability dimension. Within this
framework, each transmitted symbol carries three bits, each corresponding to a
distinct sub-channel. For detection, a combination of specialized detectors is
employed: a simple threshold based detector for the first sub-channel bit
(modulated by the mean), a Maximum-Likelihood (ML) detector for the second
sub-channel bit (modulated by the variance), a Kurtosis-based, Jarque-Bera (JB)
test, and Bayesian Hypothesis (BHT)-based detectors for the third bit
(modulated by the MoG probability). The Kurtosis- and JB-based detectors
specifically distinguish between Gaussian (or near-Gaussian) and non-Gaussian
MoG distributions by leveraging higher-order statistical measures. The Bit
Error Probabilities (BEPs) are derived for the threshold-, Kurtosis-, and
BHT-based detectors. The optimum threshold for the Kurtosis-based detector is
also derived in a tractable manner. Simulation results demonstrate that a
comparably low BEP is achieved for the third sub-channel bit relative to
existing two-dimensional (2D) schemes. Simultaneously, the proposed scheme
increases the data rate by a factor of 1.5 and 3 compared to the Generalized
Quadratic noise modulator and the classical binary KLJN noise modulator,
respectively. Furthermore, the Kurtosis-based detector offers a low-complexity
solution, achieving an acceptable BEP of approximately 0.06.

</details>


### [319] [Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large](https://arxiv.org/abs/2510.01763)
*Xiao Ding,Enbin Song,Dunbiao Niu,Zhujun Cao,Qingjiang Shi*

Main category: eess.SP

TL;DR: 该论文研究了带有加性噪声的线性测量模型在Wasserstein距离约束下的鲁棒估计问题，将其转化为一个无限维非凸极大极小问题。


<details>
  <summary>Details</summary>
Motivation: 证明了该问题鞍点的存在性等价于一个有限维极大极小问题，并给出了一个反例说明鞍点可能不存在。

Method: 提出一个可验证的充要条件，其参数可从凸问题及其对偶中导出。此外，还引入了一个简化的充分条件，表明当Wasserstein半径足够小时，鞍点总是存在的。在不存在鞍点的情况下，通过将估计器限制为线性来解决一个有限维非凸极大极小问题。

Result: 该有限维非凸极大极小问题的最优值给出了鲁棒估计问题的上界，其最优解则提供了一个鲁棒的线性估计器。

Conclusion: 数值实验验证了理论结果。

Abstract: This paper primarily considers the robust estimation problem under
Wasserstein distance constraints on the parameter and noise distributions in
the linear measurement model with additive noise, which can be formulated as an
infinite-dimensional nonconvex minimax problem. We prove that the existence of
a saddle point for this problem is equivalent to that for a finite-dimensional
minimax problem, and give a counterexample demonstrating that the saddle point
may not exist. Motivated by this observation, we present a verifiable necessary
and sufficient condition whose parameters can be derived from a convex problem
and its dual. Additionally, we also introduce a simplified sufficient
condition, which intuitively indicates that when the Wasserstein radii are
small enough, the saddle point always exists. In the absence of the saddle
point, we solve an finite-dimensional nonconvex minimax problem, obtained by
restricting the estimator to be linear. Its optimal value establishes an upper
bound on the robust estimation problem, while its optimal solution yields a
robust linear estimator. Numerical experiments are also provided to validate
our theoretical results.

</details>


### [320] [Composite Generalized Quadratic Noise Modulation via Signal Addition: Towards Higher Dimensional Noise Modulations](https://arxiv.org/abs/2510.01776)
*Hadi Zayyani,Mohammad Salman,Felipe A. P. de Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 本文提出了一种通过叠加两个广义二次噪声调制器（GQNM）来构建16元噪声调制器的方法，该调制器在信息传输中类似于经典的QAM调制器，通过调制四个不同的均值和四个不同的方差来传输信息比特。通过选择满足理论可区分性条件的参数，该调制器在比特错误概率（BEP）方面优于KLJN调制器和单个GQNM调制器，但增加了调制器、发送器和接收器检测器的复杂度。


<details>
  <summary>Details</summary>
Motivation: 通过叠加两个GQNM来创造一种新的高阶调制器，以期在信息传输中达到更好的性能。

Method: 提出通过简单地将两个GQNM的输出来叠加，构建一个16元噪声调制器。详细阐述了信息比特如何在四个不同的均值和四个不同的方差上进行调制。此外，还指出了通过叠加更多GQNM可以实现更高阶的调制，但这部分留作未来工作。最后，通过选择满足理论可区分性条件的参数，并进行仿真验证。

Result: 与KLJN调制器和GQNM调制器相比，所提出的16元噪声调制器在比特错误概率（BEP）方面取得了更好的性能（即更低的BEP）。

Conclusion: 所提出的通过叠加两个GQNM来构建16元噪声调制器的方法是可行的，并且在性能上优于现有的KLJN和GQNM调制器。然而，这种性能的提升是以增加调制器、发送器和接收器检测器的复杂度为代价的。

Abstract: This letter proposes superposing two Generalized Quadratic Noise Modulators
(GQNM) by simply adding their outputs. It creates a 16-ary noise modulator that
resembles QAM modulators in classical communication. It modulates the
information bits on four different means and four different variances. It could
also be applied to reach higher-order modulations than 16-ary schemes by adding
the outputs of more than two modulators, which is not discussed in detail in
this letter and left for future work. By selecting the parameters necessary for
satisfying the theoretical distinguishability conditions provided in the paper,
we can reach better performances in comparison to the Kirchhoff-Law Johnson
Noise (KLJN) modulator and the GQNM modulator, which is verified by the
simulations. The better result in terms of smaller Bit Error Probability (BEP)
is achieved by increasing the complexity in the modulator, the transmitter, and
the detectors in the receiver.

</details>


### [321] [Closed-form Single UAV-aided Emitter Localization and Trajectory Design Using Doppler and TOA Measurements](https://arxiv.org/abs/2510.01778)
*Samaneh Motie,Hadi Zayyani,Mohammad Salman,Hasan Abu Hilal*

Main category: eess.SP

TL;DR: 本文提出了一种结合多普勒和到达时间（ToA）测量的单架无人机（UAV）辅助定位算法，该算法利用ToA测量将基于多普勒的成本函数转化为二次凸函数，并通过约束最小二乘优化得到发射器位置的闭式解，同时还提供了一个具有闭式解的无人机轨迹设计。


<details>
  <summary>Details</summary>
Motivation: 与基于非凸函数的传统多普勒定位算法不同，本文旨在利用到达时间（ToA）测量来简化和优化定位过程，以获得更精确和高效的解决方案。

Method: 提出了一种单架无人机（UAV）辅助定位算法，该算法结合了多普勒和到达时间（ToA）测量。利用ToA测量将基于多普勒的成本函数转化为二次凸函数，然后通过约束最小二乘优化得到发射器位置的闭式解。此外，还设计了具有闭式解的无人机轨迹。

Result: 仿真实验表明，与文献中的其他算法相比，该算法具有更高的有效性。

Conclusion: 所提出的结合多普勒和ToA测量的单架无人机辅助定位算法，通过将成本函数转化为凸函数并获得闭式解，在定位精度和计算效率方面均优于现有算法。

Abstract: In this paper, a single Unmanned-Aerial-Vehicle (UAV)-aided localization
algorithm which uses both Doppler and Time of Arrival (ToA) measurements is
presented. In contrast to Doppler-based localization algorithms which are based
on non-convex functions, exploiting ToA measurements in a Least-Square (LS)
Doppler-based cost function, leads to a quadratic convex function whose
minimizer lies on a line. Utilizing the ToA measurements in addition to the
linear equation of minimizer, a closed form solution is obtained for the
emitter location using a constrained LS optimization. In addition, a trajectory
design of the UAV is provided which has also closed-form solution. Simulation
experiments demonstrate the effectiveness of the proposed algorithm in
comparison to some others in the literature.

</details>


### [322] [Performance Optimization for Movable Antenna Enhanced MISO-OFDM Systems](https://arxiv.org/abs/2510.01789)
*Ruixi Feng,Weidong Mei,Lele Lu,Xin Wei,Zhi Chen,Zhen Gao,Boyu Ning*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Movable antenna (MA) technology offers a flexible approach to enhancing
wireless channel conditions by adjusting antenna positions within a designated
region. While most existing works focus on narrowband MA systems, this paper
investigates MA position optimization for an MA-enhanced multiple-input
single-output (MISO) orthogonal frequency-division multiplexing (OFDM) system.
This problem appears to be particularly challenging due to the frequency-flat
nature of MA positioning, which should accommodate the channel conditions
across different subcarriers. To overcome this challenge, we discretize the
movement region into a multitude of sampling points, thereby converting the
continuous position optimization problem into a discrete point selection
problem. Although this problem is combinatorial, we develop an efficient
partial enumeration algorithm to find the optimal solution using a
branch-and-bound framework, where a graph-theoretic method is incorporated to
effectively prune suboptimal solutions. In the low signal-to-noise ratio (SNR)
regime, a simplified graph-based algorithm is also proposed to obtain the
optimal MA positions without the need for enumeration. Simulation results
reveal that the proposed algorithm outperforms conventional fixed-position
antennas (FPAs), while narrowband-based antenna position optimization can
achieve near-optimal performance.

</details>


### [323] [NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications](https://arxiv.org/abs/2510.01850)
*Ying-Ren Chien,Po-Heng Chou,You-Jie Peng,Chun-Yuan Huang,Hen-Wai Tsao,Yu Tsao*

Main category: eess.SP

TL;DR: 提出了一种名为NGGAN的生成对抗网络，用于生成更符合实际噪声特征的NB-PLC通信噪声，以提升噪声处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的数学噪声生成模型无法完全捕捉NB-PLC通信中非周期性异步冲激噪声的复杂特征，需要更精确的噪声模型来增强噪声处理效果。

Method: 利用实际测量的NB-PLC噪声数据构建数据集，设计了能够拟合输入信号长度的NGGAN模型以生成周期平稳噪声，并使用Wasserstein距离作为损失函数来提高生成噪声与训练数据集的相似度和样本多样性。通过定量和定性分析，比较了基于数学模型和实际测量数据集训练的GAN模型的相似性。

Result: 所提出的NGGAN模型，采用波形特征进行训练，能够生成更接近实际测量数据集的噪声，证明了其在生成噪声质量方面的优越性。

Conclusion: NGGAN模型能够有效学习并生成复杂的NB-PLC通信噪声特征，为数据增强和通信系统性能提升提供了有效的解决方案。

Abstract: Capturing comprehensive statistics of nonperiodic asynchronous impulsive
noise is a critical issue in enhancing impulse noise processing for narrowband
powerline communication (NB-PLC) transceivers. However, existing mathematical
noise generative models capture only some of the characteristics of additive
noise. Therefore, we propose a generative adversarial network (GAN), called the
noise-generation GAN (NGGAN), that learns the complicated characteristics of
practically measured noise samples for data augmentation. To closely match the
statistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise
via the analog coupling and bandpass filtering circuits of a commercial NB-PLC
modem to build a realistic dataset. Specifically, the NGGAN design approaches
based on the practically measured dataset are as follows: (i) we design the
length of input signals that the NGGAN model can fit to facilitate
cyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss
function to enhance the similarity between the generated noise and the training
dataset and ensure that the sample diversity is sufficient for various
applications. (iii) To measure the similarity performance of the GAN-based
models based on mathematical and practically measured datasets, we perform
quantitative and qualitative analyses. The training datasets include (1) a
piecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a
frequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC
systems. Simulation results demonstrate that the proposed NGGAN trained using
waveform characteristics is closer to the practically measured dataset in terms
of the quality of the generated noise.

</details>


### [324] [Wearable and Ultra-Low-Power Fusion of EMG and A-Mode US for Hand-Wrist Kinematic Tracking](https://arxiv.org/abs/2510.02000)
*Giusy Spacone,Sebastian Frey,Mattia Orlandi,Pierangelo Maria Rapa,Victor Kartsch,Simone Benatti,Luca Benini,Andrea Cossettini*

Main category: eess.SP

TL;DR: 本研究提出了一种超低功耗（低于50毫瓦）的系统，能够同时采集8通道肌电信号（EMG）和4通道A模式超声信号（US），并集成两种先进平台，实现全可穿戴、干接触臂带。该系统能够连续追踪23个自由度（手部20个，腕部3个），并采用轻量级编码器-解码器架构和多任务学习来同时估计手部和腕部关节角度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于生物信号的手势识别方法虽然有潜力，但现有方案依赖高功耗平台，不适合多日使用，并且仅限于离散手势分类。本研究旨在克服这些限制，提出一种低功耗、连续追踪手势的方法。

Method: 该方法集成了EMG和US传感器，构建了全可穿戴的干接触臂带，能够同时采集8通道EMG和4通道US信号。使用轻量级编码器-解码器架构和多任务学习来同时估计手部和腕部关节的角度，并利用运动捕捉手套进行地面真实标记。

Result: 在实际传感器重新定位的条件下，EMG-US融合方法的均方根误差为$10.6^\circ\pm2.0^\circ$，R$^2$得分为$0.61\pm0.1$。相比之下，单独使用EMG的均方根误差为$12.0^\circ\pm1^\circ$，R$^2$得分为$0.54\pm0.03$；单独使用US的均方根误差为$13.1^\circ\pm2.6^\circ$，R$^2$得分为$0.38\pm0.20$。

Conclusion: EMG-US融合方法在连续手势追踪方面优于单独使用EMG或US，实现了更低的误差和更高的准确性，并提供了超低功耗和全可穿戴的解决方案。

Abstract: Hand gesture recognition based on biosignals has shown strong potential for
developing intuitive human-machine interaction strategies that closely mimic
natural human behavior. In particular, sensor fusion approaches have gained
attention for combining complementary information and overcoming the
limitations of individual sensing modalities, thereby enabling more robust and
reliable systems. Among them, the fusion of surface electromyography (EMG) and
A-mode ultrasound (US) is very promising. However, prior solutions rely on
power-hungry platforms unsuitable for multi-day use and are limited to discrete
gesture classification. In this work, we present an ultra-low-power (sub-50 mW)
system for concurrent acquisition of 8-channel EMG and 4-channel A-mode US
signals, integrating two state-of-the-art platforms into fully wearable,
dry-contact armbands. We propose a framework for continuous tracking of 23
degrees of freedom (DoFs), 20 for the hand and 3 for the wrist, using a
kinematic glove for ground-truth labeling. Our method employs lightweight
encoder-decoder architectures with multi-task learning to simultaneously
estimate hand and wrist joint angles. Experimental results under realistic
sensor repositioning conditions demonstrate that EMG-US fusion achieves a root
mean squared error of $10.6^\circ\pm2.0^\circ$, compared to
$12.0^\circ\pm1^\circ$ for EMG and $13.1^\circ\pm2.6^\circ$ for US, and a R$^2$
score of $0.61\pm0.1$, with $0.54\pm0.03$ for EMG and $0.38\pm0.20$ for US.

</details>


### [325] [Computing on Dirty Paper: Interference-Free Integrated Communication and Computing](https://arxiv.org/abs/2510.02012)
*Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,David González G.,Carlo Fischione*

Main category: eess.SP

TL;DR: 该研究提出了一种名为“脏纸计算”（Computing on Dirty Paper）的新型通信与计算融合（ICC）方案，能够同时在多址接入信道上传输离散数据符号和执行非图函数（nomographic functions）的空计算（AirComp）。


<details>
  <summary>Details</summary>
Motivation: 在通信和计算融合（ICC）领域，特别是在多址接入信道中，如何高效地同时传输数据和执行计算是一个关键挑战。本研究旨在解决这一问题，提出一种能够消除计算干扰并提高性能的新方法。

Method: 该方案借鉴了脏纸编码（DPC）的原理，在发射端（TXs）通过预先抵消计算符号的方式，实现了通信和计算的融合，使其在渐近意义上实现无干扰。研究还包括在单输入多输出（SIMO）设置下，通过仿真评估该方案的数据检测性能和函数计算的均方误差（MSE）性能。

Result: 仿真结果表明，该方案在比特错误率（BER）和均方误差（MSE）方面均显著优于现有最先进的ICC方案，验证了所提方法的有效性。

Conclusion: 所提出的“脏纸计算”方案成功地将通信与计算融合，并通过DPC原理有效地消除了计算干扰，在SIMO设置下取得了优于现有技术的性能，证明了其在ICC领域的潜力。

Abstract: Inspired by Costa's pioneering work on dirty paper coding (DPC), this paper
proposes a novel scheme for integrated communication and computing (ICC), named
Computing on Dirty Paper, whereby the transmission of discrete data symbols for
communication, and over-the-air computation (AirComp) of nomographic functions
can be achieved simultaneously over common multiple-access channels. In
particular, the proposed scheme allows for the integration of communication and
computation in a manner that is asymptotically interference-free, by
precanceling the computing symbols at the transmitters (TXs) using DPC
principles. A simulation-based assessment of the proposed ICC scheme under a
single-input multiple-output (SIMO) setup is also offered, including the
evaluation of performance for data detection, and of mean-squared-error (MSE)
performance for function computation, over a block of symbols. The results
validate the proposed method and demonstrate its ability to significantly
outperform state-of-the-art (SotA) ICC schemes in terms of both bit error rate
(BER) and MSE.

</details>


### [326] [Joint Jammer Mitigation and Data Detection](https://arxiv.org/abs/2510.02021)
*Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: 联合干扰器和小区探测（JMD）是一种新颖的多输入多输出（MIMO）干扰器抑制方法，它通过联合估计和消除干扰器干扰子空间以及在多个时间段内检测合法传输数据，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于空间信号特征估计的干扰器抑制方法存在通信速率降低（因训练阶段无法传输数据）和易被智能或多天线干扰器规避（因其在训练阶段不传输或动态改变子空间）的缺点。本研究旨在克服这些缺点。

Method: 提出了一种名为联合干扰器和小区探测（JMD）的新范式，通过在多个时间段内联合估计和消除干扰器的干扰子空间，同时检测合法传输数据。在此基础上，提出了两种 JMD 算法：SANDMAN 和 MAED，它们在估计合法发射器信道的方法上有所不同，从而实现了不同的复杂度-性能权衡。

Result: 通过广泛的仿真验证了 JMD 方法在干扰器抑制方面的有效性。

Conclusion: JMD 方法能够有效抑制智能和动态多天线干扰器，并且无需专门的、会降低速率的训练周期。

Abstract: Multi-antenna (or MIMO) processing is a promising solution to the problem of
jammer mitigation. Existing methods mitigate the jammer based on an estimate of
its spatial signature that is acquired through a dedicated training phase. This
strategy has two main drawbacks: (i) it reduces the communication rate since no
data can be transmitted during the training phase and (ii) it can be evaded by
smart or multi-antenna jammers that do not transmit during the training phase
or that dynamically change their subspace through time-varying beamforming. To
address these drawbacks, we propose Joint jammer Mitigation and data Detection
(JMD), a novel paradigm for MIMO jammer mitigation. The core idea of JMD is to
estimate and remove the jammer interference subspace jointly with detecting the
legitimate transmit data over multiple time slots. Doing so removes the need
for a dedicated and rate-reducing training period while being able to mitigate
smart and dynamic multi-antenna jammers. We provide two JMD-type algorithms,
SANDMAN and MAED, that differ in the way they estimate the channels of the
legitimate transmitters and achieve different complexity-performance tradeoffs.
Extensive simulations demonstrate the efficacy of JMD for jammer mitigation.

</details>


### [327] [A Secure Affine Frequency Division Multiplexing for Wireless Communication Systems](https://arxiv.org/abs/2510.02023)
*Ping Wang,Zulin Wang,Yuanhan Ni,Qu Luo,Yuanfang Ma,Xiaosi Tian,Pei Xiao*

Main category: eess.SP

TL;DR: 该论文提出了一种名为SE-AFDM的新型安全频分复用系统，通过动态调整预啁啾参数来增强物理层安全，并提出了一种同步框架来解决时变信道下的参数同步问题。


<details>
  <summary>Details</summary>
Motivation: 为了在高速移动场景下提高AFDM的性能并增强物理层安全，引入了动态调整预啁啾参数的方法。

Method: 通过一个由长周期伪噪声序列控制的码本动态生成预啁啾参数，并提出了一种参数域扩频方法和同步框架。

Result: 理论推导证明了未同步窃听者无法消除时变参数的非线性影响，并为码本设计提供了指导。仿真结果验证了SE-AFDM系统的安全优势，硬件原型验证了同步框架的有效性。

Conclusion: SE-AFDM系统通过动态调整预啁啾参数和参数域扩频，在保证可靠性和高频谱效率的同时，增强了物理层安全，并且提出的同步框架能有效解决时变信道下的同步问题。

Abstract: Affine frequency division multiplexing (AFDM) has garnered significant
attention due to its superior performance in high-mobility scenarios, coupled
with multiple waveform parameters that provide greater degrees of freedom for
system design. This paper introduces a novel secure affine frequency division
multiplexing (SE-AFDM) system, which advances prior designs by dynamically
varying an AFDM pre-chirp parameter to enhance physical-layer security. In the
SE-AFDM system, the pre-chirp parameter is dynamically generated from a
codebook controlled by a long-period pseudo-noise (LPPN) sequence. Instead of
applying spreading in the data domain, our parameter-domain spreading approach
provides additional security while maintaining reliability and high spectrum
efficiency. We also propose a synchronization framework to solve the problem of
reliably and rapidly synchronizing the time-varying parameter in fast
time-varying channels. The theoretical derivations prove that unsynchronized
eavesdroppers cannot eliminate the nonlinear impact of the time-varying
parameter and further provide useful guidance for codebook design. Simulation
results demonstrate the security advantages of the proposed SE-AFDM system in
high-mobility scenarios, while our hardware prototype validates the
effectiveness of the proposed synchronization framework.

</details>


### [328] [Joint DOA and Attitude Sensing Based on Tri-Polarized Continuous Aperture Array](https://arxiv.org/abs/2510.02029)
*Haonan Si,Zhaolin Wang,Xiansheng Guo,Jin Zhang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文提出一种利用三极化连续孔径阵列（CAPA）进行联合到达角（DOA）和姿态感知的方法，通过电磁信息论模型化信号，并结合连续-离散变换和三极化协方差构建三极化频谱，实现了高精度DOA和姿态估计。


<details>
  <summary>Details</summary>
Motivation: 研究目标是利用三极化连续孔径阵列（CAPA）实现联合到达角（DOA）和姿态感知，并提出相应的估计方法。

Method: 首先，利用电磁信息论对三极化CAPA中的空间连续接收信号进行建模，实现DOA和姿态估计。然后，开发一种等效的连续-离散变换技术，便于对连续算子进行子空间分解。最后，利用三极化信号的自协方差和互协方差构建三极化频谱，以提高DOA估计性能。针对姿态估计，提出一种无需先验知识即可估计部分姿态信息的方法，以及一种在有先验快拍知识的情况下实现完整姿态估计的方法。

Result: 数值结果证明了所提出框架的可行性和优越性，能够实现高精度的DOA和姿态估计。

Conclusion: 本文提出的联合DOA和姿态感知框架，利用三极化CAPA和先进的信号处理技术，在理论和仿真上均验证了其有效性，特别是在利用三极化频谱和结合先验信息进行姿态估计方面取得了显著的性能提升。

Abstract: This paper investigates joint direction-of-arrival (DOA) and attitude sensing
using tri-polarized continuous aperture arrays (CAPAs). By employing
electromagnetic (EM) information theory, the spatially continuous received
signals in tri-polarized CAPA are modeled, thereby enabling accurate DOA and
attitude estimation. To facilitate subspace decomposition for continuous
operators, an equivalent continuous-discrete transformation technique is
developed. Moreover, both self- and cross-covariances of tri-polarized signals
are exploited to construct a tri-polarized spectrum, significantly enhancing
DOA estimation performance. Theoretical analyses reveal that the
identifiability of attitude information fundamentally depends on the
availability of prior target snapshots. Accordingly, two attitude estimation
algorithms are proposed: one capable of estimating partial attitude information
without prior knowledge, and the other achieving full attitude estimation when
such knowledge is available. Numerical results demonstrate the feasibility and
superiority of the proposed framework.

</details>


### [329] [Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing Unauthorized Sensing](https://arxiv.org/abs/2510.02103)
*Kawon Han,Kaitao Meng,Christos Masouros*

Main category: eess.SP

TL;DR: ISAC系统面临安全风险，提出一种通过引入人工瑕疵和优化OFDM信号结构来增强感知安全性的框架，以误导窃听者并保护合法用户。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC系统在通信和感知方面存在安全漏洞，窃听者可以利用目标反射信号获取感知信息。

Method: 通过在模糊度函数（AF）中引入人工瑕疵，制造虚假目标，增加窃听者（Eve）的距离估计模糊性。合法接收者（Alice）可以通过失配滤波抑制这些瑕疵，但会牺牲信噪比。利用OFDM信号，设计子载波功率分配方案来塑造自相关函数（ACF），插入周期性峰值来干扰Eve的距离估计和目标检测。

Result: 提出的感知安全ISAC信号能够有效降低Eve的目标估计性能，同时保持Alice的性能。通过数值结果验证了该方案的有效性。

Conclusion: 该研究提出了一种新颖的感知安全ISAC框架，通过引入信号的模糊性来解决ISAC系统的安全问题，并在通信、合法感知和感知安全之间实现了有效的权衡。

Abstract: The deployment of integrated sensing and communication (ISAC) brings along
unprecedented vulnerabilities to authorized sensing, necessitating the
development of secure solutions. Sensing parameters are embedded within the
target-reflected signal leaked to unauthorized passive radar sensing
eavesdroppers (Eve), implying that they can silently extract sensory
information without prior knowledge of the information data. To overcome this
limitation, we propose a sensing-secure ISAC framework that ensures secure
target detection and estimation for the legitimate system, while obfuscating
unauthorized sensing without requiring any prior knowledge of Eve. By
introducing artificial imperfections into the ambiguity function (AF) of ISAC
signals, we introduce artificial targets into Eve's range profile which
increase its range estimation ambiguity. In contrast, the legitimate sensing
receiver (Alice) can suppress these AF artifacts using mismatched filtering,
albeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM
signal, a structured subcarrier power allocation scheme is designed to shape
the secure autocorrelation function (ACF), inserting periodic peaks to mislead
Eve's range estimation and degrade target detection performance. To quantify
the sensing security, we introduce peak sidelobe level (PSL) and integrated
sidelobe level (ISL) as key performance metrics. Then, we analyze the three-way
trade-offs between communication, legitimate sensing, and sensing security,
highlighting the impact of the proposed sensing-secure ISAC signaling on system
performance. We formulate a convex optimization problem to maximize ISAC
performance while guaranteeing a certain sensing security level. Numerical
results validate the effectiveness of the proposed sensing-secure ISAC
signaling, demonstrating its ability to degrade Eve's target estimation while
preserving Alice's performance.

</details>


### [330] [Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant Neural Network](https://arxiv.org/abs/2510.02108)
*Jinshuo Zhang,Yafei Wang,Xinping Yi,Wenjin Wang,Shi Jin,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 通过端到端深度学习框架解决符号级预编码（SLP）的高复杂度问题，实现性能提升和显著的加速。与传统方法相比，该框架实现了大约 80 倍的加速，同时保持了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有符号级预编码（SLP）方法计算复杂度高的问题。

Method: 提出一个端到端的深度学习（DL）框架，利用最优SLP解的闭式形式和张量等变性（TE），结合基于模型的公式和线性预编码（LP）的分析，构建映射并证明其 TE 性，从而设计出具有参数共享模式的网络。该框架包含一个基于注意力的 TE 模块，实现了线性计算复杂度，并扩展到不完美的 CSI 场景。

Result: 所提出的深度学习框架在性能上取得了最优 SLP 的显著提升，同时实现了大约 80 倍的加速，并且在用户数量和符号块长度上表现出良好的泛化能力。

Conclusion: 该深度学习框架能够有效地解决 SLP 的高复杂度问题，并在性能和效率上取得显著的改进，同时保持了良好的泛化性，并且适用于不完美的 CSI 场景。

Abstract: Although symbol-level precoding (SLP) based on constructive interference (CI)
exploitation offers performance gains, its high complexity remains a
bottleneck. This paper addresses this challenge with an end-to-end deep
learning (DL) framework with low inference complexity that leverages the
structure of the optimal SLP solution in the closed-form and its inherent
tensor equivariance (TE), where TE denotes that a permutation of the input
induces the corresponding permutation of the output. Building upon the
computationally efficient model-based formulations, as well as their known
closed-form solutions, we analyze their relationship with linear precoding (LP)
and investigate the corresponding optimality condition. We then construct a
mapping from the problem formulation to the solution and prove its TE, based on
which the designed networks reveal a specific parameter-sharing pattern that
delivers low computational complexity and strong generalization. Leveraging
these, we propose the backbone of the framework with an attention-based TE
module, achieving linear computational complexity. Furthermore, we demonstrate
that such a framework is also applicable to imperfect CSI scenarios, where we
design a TE-based network to map the CSI, statistics, and symbols to auxiliary
variables. Simulation results show that the proposed framework captures
substantial performance gains of optimal SLP, while achieving an approximately
80-times speedup over conventional methods and maintaining strong
generalization across user numbers and symbol block lengths.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [331] [The Steiner Path Aggregation Problem](https://arxiv.org/abs/2510.01392)
*Da Qi Chen,Daniel Hathcock,D Ellis Hershkowitz,R. Ravi*

Main category: cs.DS

TL;DR: 该论文提出了一种解决斯特纳路径聚合问题（Steiner Path Aggregation Problem）的算法，该问题旨在将有向网络中的多条路径聚合为一棵单向树（arborescence），同时尽量减少路径颜色的切换次数。


<details>
  <summary>Details</summary>
Motivation: 在斯特纳路径聚合问题中，目标是将有向网络中的多条路径聚合为一棵单向树，同时尽量减少对路径的干扰。具体来说，给定一个带有彩色弧的有向多重图、一个根节点和 k 个终端节点，每个终端节点都有一条单色路径通向根节点。目标是找到一棵单向树，使得每个终端节点都有路径通向根节点，并且该路径的颜色切换次数尽可能少。

Method: 提出了一种有效的算法，可以找到满足条件的单向树，并且颜色切换次数不超过 $2ightbrace$log_{4/3}k$ 次。

Result: 算法找到的解决方案的颜色切换次数最多为 $2ightbrace$log_{4/3}k$ 次。

Conclusion: 该算法在颜色切换次数上达到了接近最优的上界，因为存在一些图需要至少 $ightbrace$log_2 k$ 次颜色切换。

Abstract: In the Steiner Path Aggregation Problem, our goal is to aggregate paths in a
directed network into a single arborescence without significantly disrupting
the paths. In particular, we are given a directed multigraph with colored arcs,
a root, and $k$ terminals, each of which has a monochromatic path to the root.
Our goal is to find an arborescence in which every terminal has a path to the
root, and its path does not switch colors too many times. We give an efficient
algorithm that finds such a solution with at most $2\log_{4/3}k$ color
switches. Up to constant factors this is the best possible universal bound, as
there are graphs requiring at least $\log_2 k$ color switches.

</details>


### [332] [Foremost, Fastest, Shortest: Temporal Graph Realization under Various Path Metrics](https://arxiv.org/abs/2510.01702)
*Justine Cauvi,Nils Morawietz,Laurent Viennot*

Main category: cs.DS

TL;DR: 该研究关注如何根据给定的时间关系矩阵（持续时间、长度或最早到达时间）来构建时间图。研究人员针对不同的时间路径定义（严格/非严格路径，周期性/非周期性时间图）和边缘标签数量限制，分析了这些问题的可计算性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索时间图中不同时间路径度量的实现问题，特别是给定任意的时间关系矩阵，判断是否存在一个时间图满足这些关系。

Method: 研究方法包括分析不同时间路径度量（如最早到达时间、最快路径、最短路径）在不同类型的时间图（周期性/非周期性、严格/非严格路径）中的实现问题。利用了图论和算法复杂性理论的工具，包括NP-hard性分析和参数化复杂度（FPT算法）。

Result: 对于最早到达时间问题，在周期性和非周期性时间图以及严格和非严格路径的设置下，研究人员提出了多项式时间算法。然而，当矩阵中的条目包含一组或一个允许值的范围时，该问题会变成NP难，但当具有多个可能值的条目数量较少时，可以通过FPT算法解决。对于最快路径问题，研究人员获得了新的硬度结果，解决了关于参数化复杂性的一个开放性问题，并显著改进了先前关于反馈顶点集数硬度的结果。对于最短路径问题，周期性版本是多项式时间可解的，而非周期性版本则是NP难的。

Conclusion: 研究为时间图实现问题提供了新的见解和算法。最早到达时间问题在大多数情况下是高效可解的，而最短路径和最快路径问题则表现出更强的计算复杂性，尤其是在非周期性设置和特定参数化下。

Abstract: In this work, we follow the current trend on temporal graph realization,
where one is given a property P and the goal is to determine whether there is a
temporal graph, that is, a graph where the edge set changes over time, with
property P . We consider the problems where as property P , we are given a
prescribed matrix for the duration, length, or earliest arrival time of
pairwise temporal paths. That is, we are given a matrix D and ask whether there
is a temporal graph such that for any ordered pair of vertices (s, t), Ds,t
equals the duration (length, or earliest arrival time, respectively) of any
temporal path from s to t minimizing that specific temporal path metric. For
shortest and earliest arrival temporal paths, we are the first to consider
these problems as far as we know. We analyze these problems for many settings
like: strict and non-strict paths, periodic and non-periodic temporal graphs,
and limited number of labels per edge (that is, limited occurrence number per
edge over time). In contrast to all other path metrics, we show that for the
earliest arrival times, we can achieve polynomial-time algorithms in periodic
and non-periodic temporal graphs and for strict and and non-strict paths.
However, the problem becomes NP-hard when the matrix does not contain a single
integer but a set or range of possible allowed values. As we show, the problem
can still be solved efficiently in this scenario, when the number of entries
with more than one value is small, that is, we develop an FPT-algorithm for the
number of such entries. For the setting of fastest paths, we achieve new
hardness results that answers an open question by Klobas, Mertzios, Molter, and
Spirakis [Theor. Comput. Sci. '25] about the parameterized complexity of the
problem with respect to the vertex cover number and significantly improves over
a previous hardness result for the feedback vertex set number. When considering
shortest paths, we show that the periodic versions are polynomial-time solvable
whereas the non-periodic versions become NP-hard.

</details>


### [333] [Improved $\ell_{p}$ Regression via Iteratively Reweighted Least Squares](https://arxiv.org/abs/2510.01729)
*Alina Ene,Ta Duy Nguyen,Adrian Vladu*

Main category: cs.DS

TL;DR: 本文提出了一种新的、更快的迭代重加权最小二乘（IRLS）算法来解决 L_p 回归问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 L_p 回归算法在理论和实践之间存在差距，本文旨在缩小这一差距。

Method: 本文提出了一种新的基于原始-对偶框架的 IRLS 算法，该算法可以从对偶目标的逆子式中自然地推导出来。

Result: 与 Adil-Peng-Sachdeva 的 IRLS 算法和 MATLAB/CVX 实现相比，本文提出的算法在经验上表现更好。

Conclusion: 本文提出的 IRLS 算法在 L_p 回归问题上实现了最先进的迭代复杂度，并提供了理论与实践之间的桥梁。

Abstract: We introduce fast algorithms for solving $\ell_{p}$ regression problems using
the iteratively reweighted least squares (IRLS) method. Our approach achieves
state-of-the-art iteration complexity, outperforming the IRLS algorithm by
Adil-Peng-Sachdeva (NeurIPS 2019) and matching the theoretical bounds
established by the complex algorithm of Adil-Kyng-Peng-Sachdeva (SODA 2019, J.
ACM 2024) via a simpler lightweight iterative scheme. This bridges the existing
gap between theoretical and practical algorithms for $\ell_{p}$ regression. Our
algorithms depart from prior approaches, using a primal-dual framework, in
which the update rule can be naturally derived from an invariant maintained for
the dual objective. Empirically, we show that our algorithms significantly
outperform both the IRLS algorithm by Adil-Peng-Sachdeva and MATLAB/CVX
implementations.

</details>


### [334] [Short circuit walks in fixed dimension](https://arxiv.org/abs/2510.01916)
*Alexander E. Black,Christian Nöbel,Raphael Steiner*

Main category: cs.DS

TL;DR: Circuit augmentation schemes for linear programming are generalized by monotone circuit walks. While a 'Circuit Diameter Conjecture' suggests short walks exist, efficiently approximating these walks is proven to be NP-hard, with recent results showing approximation factors of $O(rac{\log m}{\log \log m})$ are intractable. This paper strengthens these hardness results, demonstrating that approximating shortest monotone circuit walks within a factor of $O(m^{1-\varepsilon})$ for polygons with $m$ edges is NP-hard, which is near-optimal.


<details>
  <summary>Details</summary>
Motivation: The motivation is to significantly strengthen the existing hardness results for approximating shortest monotone circuit walks in linear programming, which generalize the simplex method. Previous work established NP-hardness for a 2-approximation and intractability for $O(rac{\log m}{\log \log m})$ approximation factors. This paper aims to push the boundaries of this hardness by investigating a tighter approximation factor related to the number of edges/inequalities.

Method: The paper strengthens hardness results by showing that approximating the shortest monotone circuit walk problem within a factor of $O(m^{1-\varepsilon})$ for LPs represented by polygons with $m$ edges is NP-hard. This is achieved by building upon previous reductions from degenerate polytopes and extending them to achieve a stronger lower bound on the approximation factor.

Result: The paper proves that for every fixed $\varepsilon > 0$, approximating the shortest monotone circuit walk for LPs on polygons with $m$ edges to within a factor of $O(m^{1-\varepsilon})$ is NP-hard. This result is essentially best-possible, as it cannot be improved beyond $o(m)$, and it implies hardness for simple polytopes and fixed-dimension LPs.

Conclusion: The paper establishes a near-optimal hardness result for approximating shortest monotone circuit walks in linear programming. It demonstrates that achieving an approximation factor better than $O(m^{1-\varepsilon})$ is NP-hard, significantly strengthening previous findings and providing strong evidence against the efficient approximability of this problem.

Abstract: Circuit augmentation schemes are a family of combinatorial algorithms for
linear programming that generalize the simplex method. To solve the linear
program, they construct a so-called monotone circuit walk: They start at an
initial vertex of the feasible region and traverse a discrete sequence of
points on the boundary, while moving along certain allowed directions
(circuits) and improving the objective function at each step until reaching an
optimum. Since the existence of short circuit walks has been conjectured
(Circuit Diameter Conjecture), several works have investigated how well one can
efficiently approximate shortest monotone circuit walks towards an optimum. A
first result addressing this question was given by De Loera, Kafer, and
Sanit\`a [SIAM J. Opt., 2022], who showed that given as input an LP and the
starting vertex, finding a $2$-approximation for this problem is NP-hard.
Cardinal and the third author [Math. Prog. 2023] gave a stronger lower bound
assuming the exponential time hypothesis, showing that even an approximation
factor of $O(\frac{\log m}{\log \log m})$ is intractable for LPs defined by $m$
inequalities. Both of these results were based on reductions from highly
degenerate polytopes in combinatorial optimization with high dimension.
  In this paper, we significantly strengthen the aforementioned hardness
results by showing that for every fixed $\varepsilon>0$ approximating the
problem on polygons with $m$ edges to within a factor of $O(m^{1-\varepsilon})$
is NP-hard. This result is essentially best-possible, as it cannot be improved
beyond $o(m)$. In particular, this implies hardness for simple polytopes and in
fixed dimension.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [335] [Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge](https://arxiv.org/abs/2510.01348)
*Michal Werner,David Čapek,Tomáš Musil,Ondřej Franěk,Tomáš Báča,Martin Saska*

Main category: cs.RO

TL;DR: 开发了一个可在无GPS环境下进行长距离导航的无人机系统，通过结合激光雷达、先验高程图和粒子滤波器来修正航位推算漂移，并成功应用于真实比赛。


<details>
  <summary>Details</summary>
Motivation: 在无GPS环境下实现无人机可靠的长距离飞行，克服航位推算漂移、无法进行回环检测以及计算平台算力限制等挑战。

Method: 提出了一种集成感知、建图、规划和控制的完全机载无人机系统，采用轻量级的漂移校正方法，通过梯度模板匹配将激光雷达局部高程图与先验地理高程图进行匹配，并结合航位推算信息在聚类粒子滤波器中进行融合。

Result: 该系统在比赛中成功执行了跨越城市、森林和开阔地带的数公里飞行任务，与原始航位推算相比显著减少了漂移，并且能够在纯CPU硬件上实时运行。

Conclusion: 报告了实际部署中的实用见解，为未来无GPS环境下的无人机自主导航设计提供了参考。

Abstract: Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied
environments is challenging: integrating odometry leads to drift, loop closures
are unavailable in previously unseen areas and embedded platforms provide
limited computational power. We present a fully onboard UAV system developed
for the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km
long-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS
or prior dense mapping. The system integrates perception, mapping, planning,
and control with a lightweight drift-correction method that matches
LiDAR-derived local heightmaps to a prior geo-data heightmap via
gradient-template matching and fuses the evidence with odometry in a clustered
particle filter. Deployed during the competition, the system executed
kilometer-scale flights across urban, forest, and open-field terrain and
reduced drift substantially relative to raw odometry, while running in real
time on CPU-only hardware. We describe the system architecture, the
localization pipeline, and the competition evaluation, and we report practical
insights from field deployment that inform the design of GNSS-denied UAV
autonomy.

</details>


### [336] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia,Wei Xiao,Wei Wang,Alejandro Astudillo,Wilm Decré,Jan Swevers,Carlo Ratti,Daniela Rus*

Main category: cs.RO

TL;DR: 该研究提出了一种结合模型预测控制（MPC）和控制障碍函数（CBFs）的安全运动规划策略，用于自主船舶在狭窄内陆航道中的导航。


<details>
  <summary>Details</summary>
Motivation: 传统的运动规划方法在狭窄水域中计算成本高或过于保守，需要更有效的安全导航方法。

Method: 提出了一种时间变化膨胀椭圆障碍物表示方法，并结合MPC和高阶CBFs来确保安全。膨胀半径根据船舶与障碍物的相对位置和姿态进行调整，以减少保守性。

Result: 仿真和真实世界实验证明，该策略能够实时、安全地实现全驱动自主机器人的狭窄空间导航，并解决潜在的死锁问题。

Conclusion: 该研究提出的结合MPC和CBFs的自适应膨胀策略，能有效提升自主船舶在复杂环境下的安全导航能力。

Abstract: Safe motion planning is essential for autonomous vessel operations,
especially in challenging spaces such as narrow inland waterways. However,
conventional motion planning approaches are often computationally intensive or
overly conservative. This paper proposes a safe motion planning strategy
combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).
We introduce a time-varying inflated ellipse obstacle representation, where the
inflation radius is adjusted depending on the relative position and attitude
between the vessel and the obstacle. The proposed adaptive inflation reduces
the conservativeness of the controller compared to traditional fixed-ellipsoid
obstacle formulations. The MPC solution provides an approximate motion plan,
and high-order CBFs ensure the vessel's safety using the varying inflation
radius. Simulation and real-world experiments demonstrate that the proposed
strategy enables the fully-actuated autonomous robot vessel to navigate through
narrow spaces in real time and resolve potential deadlocks, all while ensuring
safety.

</details>


### [337] [A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots](https://arxiv.org/abs/2510.01381)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本研究提出了一种用于连续体机器人的连续时间随机状态估计框架，该框架基于因子图优化，能够适应未建模的外部力和数据丢失，并具有线性复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有的连续体机器人状态估计方法依赖于复杂的动力学模型、简化的形状近似或准静态方法，这些方法容易受到未建模干扰的影响。

Method: 提出一种基于连续时间运动学的因子图优化框架，并将其应用于连续体机器人状态估计。该方法使用简单的机器人模型和高频传感器，并用高斯过程（GP）白噪声模拟不确定性。

Result: 该框架能够估计机器人姿态、速度和应变，并提供其均值和协方差。该方法能够处理未建模的外部力和数据丢失，并且在估计过程中可以包含未显式估计的状态。由于固有的稀疏性，该方法的时间复杂度为线性，插值查询的复杂度为常数。

Conclusion: 本研究提出的基于因子图优化的连续时间随机状态估计框架，为连续体机器人提供了灵活且高效的状态估计解决方案，能够处理现实世界系统中的各种挑战。

Abstract: State estimation techniques for continuum robots (CRs) typically involve
using computationally complex dynamic models, simplistic shape approximations,
or are limited to quasi-static methods. These limitations can be sensitive to
unmodelled disturbances acting on the robot. Inspired by a factor-graph
optimization paradigm, this work introduces a continuous-time stochastic state
estimation framework for continuum robots. We introduce factors based on
continuous-time kinematics that are corrupted by a white noise Gaussian process
(GP). By using a simple robot model paired with high-rate sensing, we show
adaptability to unmodelled external forces and data dropout. The result
contains an estimate of the mean and covariance for the robot's pose, velocity,
and strain, each of which can be interpolated continuously in time or space.
This same interpolation scheme can be used during estimation, allowing for
inclusion of measurements on states that are not explicitly estimated. Our
method's inherent sparsity leads to a linear solve complexity with respect to
time and interpolation queries in constant time. We demonstrate our method on a
CR with gyroscope and pose sensors, highlighting its versatility in real-world
systems.

</details>


### [338] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一个视觉-语言导航系统，通过微调图像扩散模型来生成导航路径，并使用行为克隆策略将其转换为可执行轨迹，在现实世界评估中显著优于现有方法，并表现出泛化到未见任务组合的潜力。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在多样化的人类指令下进行适应，并在非结构化、开放世界的环境中安全运行。现有的视觉-语言模型（VLMs）虽然在语言和感知基础方面表现强大，但由于动作空间和预训练目标的差异，难以用于机器人导航任务。

Method: VENTURA通过微调互联网预训练的图像扩散模型来生成路径掩码（视觉规划），而不是直接预测低级动作。然后，一个轻量级的行为克隆策略将这些视觉计划转换为可执行轨迹。为了扩展训练，该系统使用来自自监督跟踪模型和VLM增强的标题的路径掩码进行监督。

Result: 在现实世界的广泛评估中，VENTURA在物体到达、避障和地形偏好任务上，成功率提高了33%，碰撞次数减少了54%，并且能够泛化到未见的任务组合。

Conclusion: VENTURA通过利用图像扩散模型生成视觉导航计划，并结合行为克隆策略，成功解决了现有VLMs在机器人导航中的局限性，在实际应用中取得了优越的性能，并展现了出色的泛化和组合能力。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [339] [INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models](https://arxiv.org/abs/2510.01389)
*Ulas Berk Karli,Ziyao Shangguan,Tesca FItzgerald*

Main category: cs.RO

TL;DR: INSIGHT框架利用token级别的 경계信号来预测VLA何时需要帮助，通过分析熵、对数概率和Dirichlet不确定性估计，并训练Transformer分类器来触发帮助请求。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型缺乏在遇到困难时预测失败并向人类寻求帮助的内省机制。

Method: 使用$'{\pi_0}$-FAST模型提取每token的熵、对数概率以及基于Dirichlet的 aleatoric 和 epistemic 不确定性估计，并训练紧凑型Transformer分类器将这些序列映射到帮助触发器。

Result: 强标签能识别细粒度的不确定性动态以实现可靠的帮助检测；弱标签在训练和评估一致时也能提供有竞争力的内省能力；Transformer对token级别不确定性信号的时间演变建模比静态序列级别分数具有更强的预测能力。

Conclusion: 该研究首次系统地评估了VLA中基于不确定性的内省机制，为主动学习和通过选择性人工干预进行实时错误缓解开辟了新途径。

Abstract: Recent Vision-Language-Action (VLA) models show strong generalization
capabilities, yet they lack introspective mechanisms for anticipating failures
and requesting help from a human supervisor. We present \textbf{INSIGHT}, a
learning framework for leveraging token-level uncertainty signals to predict
when a VLA should request help. Using $\pi_0$-FAST as the underlying model, we
extract per-token \emph{entropy}, \emph{log-probability}, and Dirichlet-based
estimates of \emph{aleatoric and epistemic uncertainty}, and train compact
transformer classifiers to map these sequences to help triggers. We explore
supervision regimes for strong or weak supervision, and extensively compare
them across in-distribution and out-of-distribution tasks. Our results show a
trade-off: strong labels enable models to capture fine-grained uncertainty
dynamics for reliable help detection, while weak labels, though noisier, still
support competitive introspection when training and evaluation are aligned,
offering a scalable path when dense annotation is impractical. Crucially, we
find that modeling the temporal evolution of token-level uncertainty signals
with transformers provides far greater predictive power than static
sequence-level scores. This study provides the first systematic evaluation of
uncertainty-based introspection in VLAs, opening future avenues for active
learning and for real-time error mitigation through selective human
intervention.

</details>


### [340] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari,Roberto Rubinacci,Marco Lovera*

Main category: cs.RO

TL;DR: TACOS框架利用大型语言模型实现多无人机系统的自然语言控制，支持不同级别的自主性。


<details>
  <summary>Details</summary>
Motivation: 单飞行员管理多无人机系统时，任务需求不同级别的自主性，需要支持多种共享自主模式的框架。大型语言模型在推理和规划方面的改进使其成为一个天然的基础，可以通过基于语言的接口实现高级任务委派，从而降低飞行员的工作负荷。

Method: 提出TACOS（任务无关的多无人机系统协调器）框架，该框架通过大型语言模型实现多无人机系统的统一高级自然语言控制。TACOS将一个一对多的自然语言接口、一个将用户意图转化为结构化任务计划的智能协调器以及一个执行计划并与现实世界交互的自主代理集成到一个单一架构中。TACOS允许大型语言模型与可执行API库进行交互，从而将语义推理与实时多机器人协调结合起来。

Result: 在真实的多无人机系统中演示了该系统，并进行了消融研究以评估每个模块的贡献。

Conclusion: TACOS框架通过集成自然语言接口、智能协调器和自主代理，实现了多无人机系统的高级自然语言控制，并能与API库交互，将语义推理与多机器人协调相结合。

Abstract: When a single pilot is responsible for managing a multi-drone system, the
task demands varying levels of autonomy, from direct control of individual
UAVs, to group-level coordination, to fully autonomous swarm behaviors for
accomplishing high-level tasks. Enabling such flexible interaction requires a
framework that supports multiple modes of shared autonomy. As language models
continue to improve in reasoning and planning, they provide a natural
foundation for such systems, reducing pilot workload by enabling high-level
task delegation through intuitive, language-based interfaces. In this paper we
present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified
framework that enables high-level natural language control of multi-UAV systems
through Large Language Models (LLMs). TACOS integrates three key capabilities
into a single architecture: a one-to-many natural language interface for
intuitive user interaction, an intelligent coordinator for translating user
intent into structured task plans, and an autonomous agent that executes plans
interacting with the real-world. TACOS allows a LLM to interact with a library
of executable APIs, bridging semantic reasoning with real-time multi-robot
coordination. We demonstrate the system in real-world multi-drone system and
conduct an ablation study to assess the contribution of each module.

</details>


### [341] [Beyond Collision Cones: Dynamic Obstacle Avoidance for Nonholonomic Robots via Dynamic Parabolic Control Barrier Functions](https://arxiv.org/abs/2510.01402)
*Hun Kuk Park,Taekyung Kim,Dimitra Panagou*

Main category: cs.RO

TL;DR: 动态抛物面控制障碍函数（DPCBF）通过动态调整安全集边界，克服了传统基于碰撞锥或速度障碍的方法在非完整机器人避障中的保守性问题，提高了导航成功率和二次规划可行性。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制障碍函数（CBF）的方法在处理非完整机器人于复杂动态环境中的避障问题时，由于其约束（如碰撞锥或速度障碍）过于保守，常常导致二次规划（QP）问题不可行，尤其是在障碍物密集的场景中。

Method: 提出了一种动态抛物面控制障碍函数（DPCBF），该函数使用抛物面边界来定义安全集。抛物面的顶点和曲率会根据与障碍物的距离以及相对速度的大小进行动态调整，从而生成更宽松的安全约束。并证明了该DPCBF对于受输入约束的运动学自行车模型是有效的。

Result: 与基线方法相比，DPCBF显著提高了导航成功率和QP可行性。该方法能够成功地在包含多达100个动态障碍物的密集环境中进行导航，而基于碰撞锥的方法在此类场景中会因不可行而失败。

Conclusion: DPCBF为非完整机器人提供了一种更有效的避障方法，尤其是在障碍物密集且动态的环境中，能够有效克服传统方法的保守性限制，提高导航的鲁棒性和成功率。

Abstract: Control Barrier Functions (CBFs) are a powerful tool for ensuring the safety
of autonomous systems, yet applying them to nonholonomic robots in cluttered,
dynamic environments remains an open challenge. State-of-the-art methods often
rely on collision-cone or velocity-obstacle constraints which, by only
considering the angle of the relative velocity, are inherently conservative and
can render the CBF-based quadratic program infeasible, particularly in dense
scenarios. To address this issue, we propose a Dynamic Parabolic Control
Barrier Function (DPCBF) that defines the safe set using a parabolic boundary.
The parabola's vertex and curvature dynamically adapt based on both the
distance to an obstacle and the magnitude of the relative velocity, creating a
less restrictive safety constraint. We prove that the proposed DPCBF is valid
for a kinematic bicycle model subject to input constraints. Extensive
comparative simulations demonstrate that our DPCBF-based controller
significantly enhances navigation success rates and QP feasibility compared to
baseline methods. Our approach successfully navigates through dense
environments with up to 100 dynamic obstacles, scenarios where collision
cone-based methods fail due to infeasibility.

</details>


### [342] [How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?](https://arxiv.org/abs/2510.01404)
*Lexi Foland,Thomas Cohn,Adam Wei,Nicholas Pfaff,Boyuan Chen,Russ Tedrake*

Main category: cs.RO

TL;DR: 该研究分析了扩散策略在机器人模仿学习中学习运动学约束的能力，发现它们能学习到约束的粗略近似，但性能受数据集大小和质量影响，而约束曲率的影响不确定。


<details>
  <summary>Details</summary>
Motivation: 为了评估扩散策略在机器人模仿学习中满足运动学等式约束的能力，而不仅仅是任务性能。

Method: 通过一个涉及双臂抓取和放置任务的案例研究，分析了数据集大小、数据集质量和约束流形曲率这三个因素对训练好的策略的影响。

Result: 实验表明，扩散策略学习到约束流形的粗略近似，并且性能会受到数据集大小和质量下降的负面影响。约束流形的曲率与约束满足和任务成功之间的相关性不确定。

Conclusion: 扩散策略可以学习到运动学约束的粗略近似，但其性能会受到训练数据因素的影响。该研究结果通过硬件评估得到验证，表明其在现实世界中具有应用潜力。

Abstract: Diffusion policies have shown impressive results in robot imitation learning,
even for tasks that require satisfaction of kinematic equality constraints.
However, task performance alone is not a reliable indicator of the policy's
ability to precisely learn constraints in the training data. To investigate, we
analyze how well diffusion policies discover these manifolds with a case study
on a bimanual pick-and-place task that encourages fulfillment of a kinematic
constraint for success. We study how three factors affect trained policies:
dataset size, dataset quality, and manifold curvature. Our experiments show
diffusion policies learn a coarse approximation of the constraint manifold with
learning affected negatively by decreases in both dataset size and quality. On
the other hand, the curvature of the constraint manifold showed inconclusive
correlations with both constraint satisfaction and task success. A hardware
evaluation verifies the applicability of our results in the real world. Project
website with additional results and visuals:
https://diffusion-learns-kinematic.github.io

</details>


### [343] [AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation](https://arxiv.org/abs/2510.01433)
*Anukriti Singh,Kasra Torshizi,Khuzema Habib,Kelin Yu,Ruohan Gao,Pratap Tokekar*

Main category: cs.RO

TL;DR: AFFORD2ACT是一个基于视觉的机器人学习框架，它使用文本提示和单个图像来提取最少的语义关键点，用于实时机器人操作。该框架通过过滤、构建和学习策略来提高数据效率，并在各种实际操作任务中表现出色，成功率达到82%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的机器人学习方法计算量大，并且会包含不相关的背景特征。手动确定的关键点方法可扩展性和语义理解能力有限。

Method: AFFORD2ACT是一个三阶段框架：亲和力过滤、类别级关键点构建和基于Transformer的策略学习，其中嵌入了门控机制，用于推理最相关的关键点。

Result: AFFORD2ACT产生了一个紧凑的38维状态策略，可以在15分钟内进行训练，并且无需本体感觉或密集表示即可进行实时操作。在各种实际操作任务中，AFFORD2ACT在未见过的物体、新颖的类别、背景和干扰物上取得了82%的成功率。

Conclusion: AFFORD2ACT通过从文本提示和单个图像中提取最小的语义关键点，提高了数据效率和可扩展性，并在各种机器人操作任务中实现了高性能。

Abstract: Vision-based robot learning often relies on dense image or point-cloud
inputs, which are computationally heavy and entangle irrelevant background
features. Existing keypoint-based approaches can focus on manipulation-centric
features and be lightweight, but either depend on manual heuristics or
task-coupled selection, limiting scalability and semantic understanding. To
address this, we propose AFFORD2ACT, an affordance-guided framework that
distills a minimal set of semantic 2D keypoints from a text prompt and a single
image. AFFORD2ACT follows a three-stage pipeline: affordance filtering,
category-level keypoint construction, and transformer-based policy learning
with embedded gating to reason about the most relevant keypoints, yielding a
compact 38-dimensional state policy that can be trained in 15 minutes, which
performs well in real-time without proprioception or dense representations.
Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves
data efficiency, achieving an 82% success rate on unseen objects, novel
categories, backgrounds, and distractors.

</details>


### [344] [Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation](https://arxiv.org/abs/2510.01438)
*Minglun Wei,Xintong Yang,Yu-Kun Lai,Ze Ji*

Main category: cs.RO

TL;DR: 提出一种用于粉末运输的轨迹优化框架，通过可微分物理模拟、低维技能空间参数化和基于课程的学习策略，实现对机器人轨迹的端到端优化，提高了任务成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人自动化在实验室中精确操控粉末（特别是运输任务）的挑战，该任务要求高精度和稳定性。

Method: 提出一种轨迹优化框架，该框架集成了可微分物理模拟、低维技能空间参数化和基于课程的学习策略，以实现对粉末运输任务的端到端优化。

Result: 实验结果表明，与强化学习基线方法相比，该方法实现了更高的任务成功率和稳定性。

Conclusion: 所提出的框架能够有效地优化接触丰富的机器人轨迹，同时保持稳定性和收敛效率，为解决实验室自动化中的粉末操控问题提供了有效方案。

Abstract: Robotic automation is accelerating scientific discovery by reducing manual
effort in laboratory workflows. However, precise manipulation of powders
remains challenging, particularly in tasks such as transport that demand
accuracy and stability. We propose a trajectory optimisation framework for
powder transport in laboratory settings, which integrates differentiable
physics simulation for accurate modelling of granular dynamics, low-dimensional
skill-space parameterisation to reduce optimisation complexity, and a
curriculum-based strategy that progressively refines task competence over long
horizons. This formulation enables end-to-end optimisation of contact-rich
robot trajectories while maintaining stability and convergence efficiency.
Experimental results demonstrate that the proposed method achieves superior
task success rates and stability compared to the reinforcement learning
baseline.

</details>


### [345] [Touching the tumor boundary: A pilot study on ultrasound based virtual fixtures for breast-conserving surgery](https://arxiv.org/abs/2510.01452)
*Laura Connolly,Tamas Ungi,Adnan Munawar,Anton Deguet,Chris Yeung,Russell H. Taylor,Parvin Mousavi,Gabor Fichtinger Keyvan Hashtrudi-Zaad*

Main category: cs.RO

TL;DR: 本研究介绍了一种结合触觉反馈的合作机器人引导系统，用于在模拟的乳腺癌保乳手术中更精确地确定肿瘤边界，并评估其临床应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 在保乳手术中，肿瘤边界的精确勾画面临挑战，因为肿瘤通常具有高度可移动性、不易触及且边界不规则。本研究旨在开发一种合作机器人引导系统，利用触觉反馈来解决这些问题，并评估其在乳腺癌治疗中的整合潜力。

Method: 研究人员将一个小型触觉机器人改装了电灼刀，作为一种协同控制的手术工具。系统利用超声和电磁导航来识别肿瘤边界和位置，并在手术工具触及肿瘤边界时施加虚拟边界约束。通过对比用户在有无触觉引导下从乳腺模型中切除肿瘤的表现，对结果进行了定性和定量评估。

Result: 研究结果表明，虚拟边界约束提高了切除边缘的精确度。在有触觉反馈的情况下，用户的精神负担、挫败感和体力消耗均有所减少。此外，研究还揭示了对现有手术流程的意外影响，为后续系统设计和培训方案的调整提供了依据。

Conclusion: 研究结果表明，虚拟边界约束技术有望提高模拟保乳手术中肿瘤边界的定位精度。未来的研究计划将包括一项更广泛的用户研究，以进一步验证这些发现并优化引导系统。

Abstract: Purpose: Delineating tumor boundaries during breast-conserving surgery is
challenging as tumors are often highly mobile, non-palpable, and have
irregularly shaped borders. To address these challenges, we introduce a
cooperative robotic guidance system that applies haptic feedback for tumor
localization. In this pilot study, we aim to assess if and how this system can
be successfully integrated into breast cancer care.
  Methods: A small haptic robot is retrofitted with an electrocautery blade to
operate as a cooperatively controlled surgical tool. Ultrasound and
electromagnetic navigation are used to identify the tumor boundaries and
position. A forbidden region virtual fixture is imposed when the surgical tool
collides with the tumor boundary. We conducted a study where users were asked
to resect tumors from breast simulants both with and without the haptic
guidance. We then assess the results of these simulated resections both
qualitatively and quantitatively.
  Results: Virtual fixture guidance is shown to improve resection margins. On
average, users find the task to be less mentally demanding, frustrating, and
effort intensive when haptic feedback is available. We also discovered some
unanticipated impacts on surgical workflow that will guide design adjustments
and training protocol moving forward.
  Conclusion: Our results suggest that virtual fixtures can help localize tumor
boundaries in simulated breast-conserving surgery. Future work will include an
extensive user study to further validate these results and fine-tune our
guidance system.

</details>


### [346] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa,Svetlana Lukina,Timur Akhtyamov,Arthur Nigmatzyanov,Dmitrii Nalberskii,Sergey Zagoruyko,Gonzalo Ferrer*

Main category: cs.RO

TL;DR: VL-KnG是一个视觉场景理解系统，通过构建时空知识图谱和高效查询处理来解决机器人导航中视觉语言模型（VLM）面临的持久场景记忆、空间推理和可扩展性限制。该系统能保持物体身份的持久性，并通过可查询的图结构实现可解释的空间推理。我们还引入了WalkieKnowledge基准，并在真实机器人上进行了部署，证明了其在实时导航、定位和规划等任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLM）在机器人导航方面存在局限性，包括缺乏持久场景记忆、有限的空间推理能力以及在视频时长增加时可扩展性差，影响了实时应用。

Method: VL-KnG系统通过分块处理视频序列，利用现代VLM构建持久的、能够保持物体身份随时间变化的知识图谱。此外，它还实现了可解释的空间推理，并通过查询图结构来完成。该方法还引入了WalkieKnowledge基准，包含手动标注的问题和多样化的视频数据，用于比较结构化方法和通用VLM。

Result: 在真实的差速驱动机器人上的部署表明，VL-KnG的成功率为77.27%，答案准确率为76.92%，性能与Gemini 2.5 Pro相当。该方法还提供了由知识图谱支持的可解释推理，并具有计算效率，适用于实时部署，可用于本地化、导航和规划等不同任务。

Conclusion: VL-KnG通过构建时空知识图谱克服了现有VLM在机器人导航中的关键挑战，实现了持久场景记忆、增强的空间推理和高效的可扩展性。该系统在实际机器人部署中表现出强大的性能和可解释性，为机器人导航和规划开辟了新的可能性。WalkieKnowledge基准的引入为未来的研究提供了宝贵的资源。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but
encounter fundamental limitations: they lack persistent scene memory, offer
limited spatial reasoning, and do not scale effectively with video duration for
real-time application. We present VL-KnG, a Visual Scene Understanding system
that tackles these challenges using spatiotemporal knowledge graph construction
and computationally efficient query processing for navigation goal
identification. Our approach processes video sequences in chunks utilizing
modern VLMs, creates persistent knowledge graphs that maintain object identity
over time, and enables explainable spatial reasoning through queryable graph
structures. We also introduce WalkieKnowledge, a new benchmark with about 200
manually annotated questions across 8 diverse trajectories spanning
approximately 100 minutes of video data, enabling fair comparison between
structured approaches and general-purpose VLMs. Real-world deployment on a
differential drive robot demonstrates practical applicability, with our method
achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5
Pro performance while providing explainable reasoning supported by the
knowledge graph, computational efficiency for real-time deployment across
different tasks, such as localization, navigation and planning. Code and
dataset will be released after acceptance.

</details>


### [347] [Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot](https://arxiv.org/abs/2510.01485)
*Nicholas B. Andrews,Yanhao Yang,Sofya Akhetova,Kristi A. Morgansen,Ross L. Hatton*

Main category: cs.RO

TL;DR: “这项工作展示了对一个自由漂浮的、受生物启发的、具有未驱动关节、用于控制的连杆安装推进器和每个连杆上单个陀螺仪的多连杆机器人的姿态（位置和形状）进行估计，从而得到一个欠驱动、传感最少化的平台。通过概念验证硬件实验和离线卡尔曼滤波器分析，我们证明了该机器人的姿态可以被可靠地估计。状态估计是使用增强了高斯过程残差学习的无迹卡尔曼滤波器来执行的，以补偿非零均值、非高斯噪声。我们进一步表明，在多步态数据集（前进、后退、左转、右转和原地转弯）上训练的滤波器，在应用于相同的正向步态测试轨迹时，其性能与在较大的仅正向步态数据集上训练的滤波器相当。这些结果揭示了步态输入空间的重叠，可以利用这种重叠来减少训练数据需求，同时增强滤波器在多种步态上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 证明一个具有未驱动关节、连杆安装推进器和每个连杆单个陀螺仪的多连杆机器人姿态估计是可行的，并提出一种利用高斯过程残差学习补偿噪声的方法，同时探索减少训练数据需求和提高泛化能力的方法。

Method: 使用增强了高斯过程残差学习的无迹卡尔曼滤波器进行状态估计，并通过硬件实验和离线分析进行验证。

Result: 通过硬件实验和卡尔曼滤波器分析，证明了该机器人的姿态可以被可靠地估计。发现在多步态数据集上训练的滤波器与在仅正向步态数据集上训练的滤波器性能相当，表明存在步态输入空间的重叠。

Conclusion: 该研究成功地展示了欠驱动、传感最少化的多连杆机器人的姿态估计，并提出了一种通过高斯过程残差学习和利用步态输入空间重叠来提高滤波器性能和泛化能力的方法，从而减少了对训练数据的需求。

Abstract: This work demonstrates pose (position and shape) estimation for a
free-floating, bioinspired multi-link robot with unactuated joints,
link-mounted thrusters for control, and a single gyroscope per link, resulting
in an underactuated, minimally sensed platform. Through a proof-of-concept
hardware experiment and offline Kalman filter analysis, we show that the
robot's pose can be reliably estimated. State estimation is performed using an
unscented Kalman filter augmented with Gaussian process residual learning to
compensate for non-zero-mean, non-Gaussian noise. We further show that a filter
trained on a multi-gait dataset (forward, backward, left, right, and turning)
performs comparably to one trained on a larger forward-gait-only dataset when
both are evaluated on the same forward-gait test trajectory. These results
reveal overlap in the gait input space, which can be exploited to reduce
training data requirements while enhancing the filter's generalizability across
multiple gaits.

</details>


### [348] [Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments](https://arxiv.org/abs/2510.01519)
*Wei Han Chen,Yuchen Liu,Alexiy Buynitsky,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ANTFields通过局部观测学习成本函数，但存在频谱偏差和灾难性遗忘问题。本文提出一种分层方法，结合全局连通性图和基于神经场和Eikonal PDE的局部规划器，以克服这些挑战，并在大型环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决机器人室内导航中的复杂性、未知性以及现有方法（如采样和模仿学习）的局限性，特别是ANTFields方法在处理频谱偏差和灾难性遗忘方面的挑战。

Method: 将规划问题分解为分层结构：高层使用稀疏图捕捉全局连通性，低层使用基于神经场并求解Eikonal PDE的规划器来处理局部障碍物。

Result: 该方法在大型环境中得到了验证，展示了比以往方法更强的适应性和精度，克服了频谱偏差和神经场拟合困难等问题，提供了平滑精确的成本景观表示。

Conclusion: 所提出的分层方法能有效克服ANTFields的局限性，在大型室内环境中实现高效、精确的机器人导航，并具有在线探索、建图和实际导航的潜力。

Abstract: Robot navigation in large, complex, and unknown indoor environments is a
challenging problem. The existing approaches, such as traditional
sampling-based methods, struggle with resolution control and scalability, while
imitation learning-based methods require a large amount of demonstration data.
Active Neural Time Fields (ANTFields) have recently emerged as a promising
solution by using local observations to learn cost-to-go functions without
relying on demonstrations. Despite their potential, these methods are hampered
by challenges such as spectral bias and catastrophic forgetting, which diminish
their effectiveness in complex scenarios. To address these issues, our approach
decomposes the planning problem into a hierarchical structure. At the high
level, a sparse graph captures the environment's global connectivity, while at
the low level, a planner based on neural fields navigates local obstacles by
solving the Eikonal PDE. This physics-informed strategy overcomes common
pitfalls like spectral bias and neural field fitting difficulties, resulting in
a smooth and precise representation of the cost landscape. We validate our
framework in large-scale environments, demonstrating its enhanced adaptability
and precision compared to previous methods, and highlighting its potential for
online exploration, mapping, and real-world navigation.

</details>


### [349] [Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion](https://arxiv.org/abs/2510.01592)
*Shun Niijima,Ryoichi Tsuzaki,Noriaki Takasugi,Masaya Kinoshita*

Main category: cs.RO

TL;DR: 提出一种基于GPU加速的高分辨率3D体素映射的实时多平面分割方法，用于腿式机器人运动。现有方法在精度和效率之间存在权衡，该方法通过结合顶点连接组件标记、随机采样一致性平面检测和凸包，并利用GPU并行计算，解决了这些限制，实现了高速率、高精度的3D多平面分割，并验证了其在模拟和真实机器人平台上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有在线平面建图方法在精度和计算效率之间难以平衡：直接分割深度图像的图像分割方法时间整合性差；基于高度图的方法无法表示悬垂等复杂3D结构；基于体素的平面分割方法在实时应用中尚未被探索。

Method: 开发了一个新的框架，该框架整合了基于顶点的连接组件标记、基于随机采样一致性的平面检测和凸包，并利用GPU并行计算，从高分辨率3D体素图中累积的点云中快速提取平面区域。

Result: 实验结果表明，该方法在0.01米的分辨率下，即使在30赫兹以上的更新速率下，也能实现快速、准确的3D多平面分割，从而能够实时利用检测到的平面进行运动任务。

Conclusion: 所提出的方法通过结合多种技术并利用GPU并行计算，实现了高效、准确的实时3D多平面分割，解决了现有方法的局限性，并成功应用于腿式机器人的运动控制。

Abstract: This paper proposes a real-time multi-plane segmentation method based on
GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion.
Existing online planar mapping approaches struggle to balance accuracy and
computational efficiency: direct depth image segmentation from specific sensors
suffers from poor temporal integration, height map-based methods cannot
represent complex 3D structures like overhangs, and voxel-based plane
segmentation remains unexplored for real-time applications. To address these
limitations, we develop a novel framework that integrates vertex-based
connected component labeling with random sample consensus based plane detection
and convex hull, leveraging GPU parallel computing to rapidly extract planar
regions from point clouds accumulated in high-resolution 3D voxel maps.
Experimental results demonstrate that the proposed method achieves fast and
accurate 3D multi-plane segmentation at over 30 Hz update rate even at a
resolution of 0.01 m, enabling the detected planes to be utilized in real time
for locomotion tasks. Furthermore, we validate the effectiveness of our
approach through experiments in both simulated environments and physical legged
robot platforms, confirming robust locomotion performance when considering 3D
planar structures.

</details>


### [350] [MiniBEE: A New Form Factor for Compact Bimanual Dexterity](https://arxiv.org/abs/2510.01603)
*Sharfin Islam,Zewen Chen,Zhanpeng He,Swapneel Bhatt,Andres Permuy,Brock Taylor,James Vickery,Pedro Piacenza,Cheng Zhang,Matei Ciocarlie*

Main category: cs.RO

TL;DR: bimanual robot manipulators typically require two full six- or seven-DOF arms, increasing complexity and limiting workspace. This paper introduces the MiniBEE, a compact system with two reduced-mobility arms (3+ DOF each) coupled into a kinematic chain. It preserves relative gripper positioning, maximizes dexterous workspace using a novel kinematic dexterity metric, and supports wearable data collection or deployment on standard robot arms. The system enables training imitation learning policies for robust bimanual manipulation through wearable demonstrations.


<details>
  <summary>Details</summary>
Motivation: Traditional bimanual robot manipulators are complex and have limited dexterous workspaces. The goal is to design a more compact and versatile system that maximizes dexterity and enables effective bimanual manipulation through learned policies.

Method: The paper introduces the MiniBEE (Miniature Bimanual End-effector), a system with two reduced-mobility arms (3+ DOF each) coupled into a kinematic chain. It utilizes a novel kinematic dexterity metric to enlarge the dexterous workspace and optimize the design for a lightweight and wearable mechanism. The system supports two modes: wearable kinesthetic data collection and deployment on a standard robot arm. An end-to-end pipeline is demonstrated for training imitation learning policies using wearable demonstrations.

Result: The MiniBEE system successfully maximizes dexterous range through kinematic analysis and design optimization. It enables robust, real-world bimanual manipulation by training imitation learning policies using wearable demonstrations. The system is shown to be deployable on standard robot arms, extending their dexterity across the entire workspace.

Conclusion: The MiniBEE system offers a novel solution for enhancing bimanual robot manipulation by providing a compact, lightweight, and highly dexterous end-effector. Its ability to leverage wearable demonstrations for training imitation learning policies opens up new possibilities for real-world applications and extends the capabilities of existing robotic platforms.

Abstract: Bimanual robot manipulators can achieve impressive dexterity, but typically
rely on two full six- or seven- degree-of-freedom arms so that paired grippers
can coordinate effectively. This traditional framework increases system
complexity while only exploiting a fraction of the overall workspace for
dexterous interaction. We introduce the MiniBEE (Miniature Bimanual
End-effector), a compact system in which two reduced-mobility arms (3+ DOF
each) are coupled into a kinematic chain that preserves full relative
positioning between grippers. To guide our design, we formulate a kinematic
dexterity metric that enlarges the dexterous workspace while keeping the
mechanism lightweight and wearable. The resulting system supports two
complementary modes: (i) wearable kinesthetic data collection with self-tracked
gripper poses, and (ii) deployment on a standard robot arm, extending dexterity
across its entire workspace. We present kinematic analysis and design
optimization methods for maximizing dexterous range, and demonstrate an
end-to-end pipeline in which wearable demonstrations train imitation learning
policies that perform robust, real-world bimanual manipulation.

</details>


### [351] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个用于机器人复杂双臂操作的数据收集系统，结合了VR遥操作和传感器控制，通过姿态对齐实现人机运动映射。系统具有便携性，采用3D渲染、可穿戴计算机和高效校准。其特色是捕捉主动的、以操作者为中心的感知，通过记录头部运动学习视觉注意与操作的联系。在六项双臂任务上的评估显示，仅用ActiveUMI数据训练的策略在分布内任务上成功率达70%，在新物体和新环境中测试时仍有56%的成功率，证明了该系统在创建可泛化、高效的机器人策略方面的有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 收集用于训练能够执行复杂双臂操作的机器人的在用人类演示数据，并解决数据收集的便携性和数据质量问题。

Method: 开发了一个名为ActiveUMI的框架，该框架结合了便携式VR遥操作套件和传感器控制器，以匹配机器人的末端执行器。通过记录操作员的头部运动来捕捉主动的、以操作者为中心的感知，并将此与机器人操作联系起来。使用了沉浸式3D模型渲染、独立式可穿戴计算机和高效校准方法来确保移动性和数据质量。

Result: 在六项具有挑战性的双臂任务上评估了ActiveUMI。仅使用ActiveUMI数据训练的策略在分布内任务上的平均成功率为70%，在新物体和新环境中的测试成功率为56%，展示了良好的泛化能力。

Conclusion: 便携式数据收集系统结合主动学习的感知能力，为创建可泛化、高效的现实世界机器人策略提供了一条有效且可扩展的途径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [352] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin,Jiafei Duan,Haoquan Fang,Dieter Fox,Ranjay Krishna,Cheston Tan,Bihan Wen*

Main category: cs.RO

TL;DR: FailSafe是一个新的故障生成和恢复系统，可为机器人操作生成多样化的故障案例和可执行的恢复动作，旨在提高VLA模型在处理不可预测的故障时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作数据集缺乏故障恢复数据，使得机器人无法从意外故障中恢复。此外，现有的故障检测数据集通常只提供文本解释，难以直接用于VLA模型。

Method: FailSafe是一个新颖的故障生成和恢复系统，能够自动生成配有可执行恢复动作的故障案例。该系统可以应用于任何模拟器中的任何操作任务，可扩展地创建故障-动作数据。研究人员使用FailSafe来微调LLaVa-OV-7B模型，构建了FailSafe-VLM。

Result: FailSafe-VLM成功地帮助机械臂检测并从潜在故障中恢复，在Maniskill的几项任务中，与三个最先进的VLA模型（pi0-FAST, OpenVLA, OpenVLA-OFT）相比，平均性能提高了22.6%。FailSafe-VLM还能泛化到不同的空间配置、摄像机视角和机器人实体。

Conclusion: FailSafe系统能够有效地生成故障恢复数据，显著提高了VLA模型在处理机器人操作中的故障时的性能和泛化能力。研究计划将FailSafe代码开源。

Abstract: Recent advances in robotic manipulation have integrated low-level robotic
control into Vision-Language Models (VLMs), extending them into
Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve
strong performance in downstream robotic applications, supported by large-scale
crowd-sourced robot training data, they still inevitably encounter failures
during execution. Enabling robots to reason about and recover from
unpredictable and abrupt failures remains a critical challenge. Existing
robotic manipulation datasets, collected in either simulation or the real
world, primarily provide only ground-truth trajectories, leaving robots unable
to recover once failures occur. Moreover, the few datasets that address failure
detection typically offer only textual explanations, which are difficult to
utilize directly in VLA models. To address this gap, we introduce FailSafe, a
novel failure generation and recovery system that automatically produces
diverse failure cases paired with executable recovery actions. FailSafe can be
seamlessly applied to any manipulation task in any simulator, enabling scalable
creation of failure-action data. To demonstrate its effectiveness, we fine-tune
LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results
show that FailSafe-VLM successfully helps robotic arm detect and recover from
potential failures, improving the performance of three state-of-the-art VLA
models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several
tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different
spatial configurations, camera viewpoints, and robotic embodiments. We plan to
release the FailSafe code to the community.

</details>


### [353] [Statistical Uncertainty Learning for Robust Visual-Inertial State Estimation](https://arxiv.org/abs/2510.01648)
*Seungwon Choi,Donggyu Park,Seo-Yeon Hwang,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 该研究提出了一种在线学习测量可靠性评估的统计框架，以提高视觉-惯性里程计（VIO）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的VIO方法通常假设传感器测量具有静态、统一的不确定性，这限制了其在真实世界数据中捕捉动态误差特性的能力。

Method: 提出了一种利用多视图几何一致性进行自我监督的统计框架，该框架能够在线学习测量可靠性，并自适应地调整优化过程中视觉测量的权重。

Result: 在EuRoC数据集上的评估显示，与基线方法相比，该方法在跟踪精度上有所提高，翻译误差平均减少了约24%，旋转误差平均减少了约42%。

Conclusion: 该框架能够实时运行，同时提高了精度和鲁棒性，并且源代码将公开以促进研究。

Abstract: A fundamental challenge in robust visual-inertial odometry (VIO) is to
dynamically assess the reliability of sensor measurements. This assessment is
crucial for properly weighting the contribution of each measurement to the
state estimate. Conventional methods often simplify this by assuming a static,
uniform uncertainty for all measurements. This heuristic, however, may be
limited in its ability to capture the dynamic error characteristics inherent in
real-world data. To improve this limitation, we present a statistical framework
that learns measurement reliability assessment online, directly from sensor
data and optimization results. Our approach leverages multi-view geometric
consistency as a form of self-supervision. This enables the system to infer
landmark uncertainty and adaptively weight visual measurements during
optimization. We evaluated our method on the public EuRoC dataset,
demonstrating improvements in tracking accuracy with average reductions of
approximately 24\% in translation error and 42\% in rotation error compared to
baseline methods with fixed uncertainty parameters. The resulting framework
operates in real time while showing enhanced accuracy and robustness. To
facilitate reproducibility and encourage further research, the source code will
be made publicly available.

</details>


### [354] [Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation](https://arxiv.org/abs/2510.01661)
*Yifei Simon Shao,Yuchen Zheng,Sunan Sun,Pratik Chaudhari,Vijay Kumar,Nadia Figueroa*

Main category: cs.RO

TL;DR: SymSkill框架结合了模仿学习(IL)和任务与运动规划(TAMP)的优点，实现了实时、组合泛化和故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中进行多步操作具有挑战性，现有的模仿学习方法缺乏组合泛化能力，而任务与运动规划方法规划延迟高，无法实时恢复。

Method: SymSkill框架离线学习谓词、算子和技能，并在执行时使用符号规划器组合和重新排序技能，以实现符号目标，同时在运动和符号层面进行实时恢复。

Result: 在RoboCasa模拟中，SymSkill实现了12个单步任务的85%成功率，并能将这些技能组合成多步计划，在出现执行故障时能够稳健地恢复。在真实的Franka机器人上，SymSkill仅用了5分钟的非结构化、无标签数据即可执行多个任务。

Conclusion: SymSkill框架通过结合模仿学习和任务与运动规划的优势，解决了多步操作的挑战，实现了实时、组合泛化和故障恢复能力。

Abstract: Multi-step manipulation in dynamic environments remains challenging. Two
major families of methods fail in distinct ways: (i) imitation learning (IL) is
reactive but lacks compositional generalization, as monolithic policies do not
decide which skill to reuse when scenes change; (ii) classical task-and-motion
planning (TAMP) offers compositionality but has prohibitive planning latency,
preventing real-time failure recovery. We introduce SymSkill, a unified
learning framework that combines the benefits of IL and TAMP, allowing
compositional generalization and failure recovery in real-time. Offline,
SymSkill jointly learns predicates, operators, and skills directly from
unlabeled and unsegmented demonstrations. At execution time, upon specifying a
conjunction of one or more learned predicates, SymSkill uses a symbolic planner
to compose and reorder learned skills to achieve the symbolic goals, while
performing recovery at both the motion and symbolic levels in real time.
Coupled with a compliant controller, SymSkill enables safe and uninterrupted
execution under human and environmental disturbances. In RoboCasa simulation,
SymSkill can execute 12 single-step tasks with 85% success rate. Without
additional data, it composes these skills into multi-step plans requiring up to
6 skill recompositions, recovering robustly from execution failures. On a real
Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented
and unlabeled play data, is capable of performing multiple tasks simply by goal
specifications. The source code and additional analysis can be found on
https://sites.google.com/view/symskill.

</details>


### [355] [Geometric Backstepping Control of Omnidirectional Tiltrotors Incorporating Servo-Rotor Dynamics for Robustness against Sudden Disturbances](https://arxiv.org/abs/2510.01675)
*Jaewoo Lee,Dongjae Lee,Jinwoo Lee,Hyungyu Lee,Yeonjoon Kim,H. Jin Kim*

Main category: cs.RO

TL;DR: 本研究提出了一种考虑了伺服电机和旋翼动力学的几何反步控制器，用于可变倾角的全向多旋翼无人机。


<details>
  <summary>Details</summary>
Motivation: 考虑到执行器动力学对于更有效和可靠的运行至关重要，特别是在剧烈飞行或从突然干扰中恢复时。先前的使用线性关系的方法无法捕捉可变倾角引起的非线性。

Method: 利用多旋翼刚体动力学及其非线性执行器动力学之间的级联结构，设计了几何反步控制器，并建立了整体系统的指数稳定性。

Result: 与未考虑执行器动力学的基线控制器相比，所提出的方法在快速平移跟踪、快速旋转跟踪和从突然干扰中恢复这三种实验场景中始终实现了更好的跟踪性能。在最快的平移轨迹跟踪和恢复实验中，基线控制器发散并崩溃，而所提出的控制器保持了稳定并成功完成了任务。

Conclusion: 所提出的考虑执行器动力学的几何反步控制器对于可变倾角全向多旋翼无人机是有效且鲁棒的，能够实现更好的跟踪性能并保持稳定性。

Abstract: This work presents a geometric backstepping controller for a variable-tilt
omnidirectional multirotor that explicitly accounts for both servo and rotor
dynamics. Considering actuator dynamics is essential for more effective and
reliable operation, particularly during aggressive flight maneuvers or recovery
from sudden disturbances. While prior studies have investigated actuator-aware
control for conventional and fixed-tilt multirotors, these approaches rely on
linear relationships between actuator input and wrench, which cannot capture
the nonlinearities induced by variable tilt angles. In this work, we exploit
the cascade structure between the rigid-body dynamics of the multirotor and its
nonlinear actuator dynamics to design the proposed backstepping controller and
establish exponential stability of the overall system. Furthermore, we reveal
parametric uncertainty in the actuator model through experiments, and we
demonstrate that the proposed controller remains robust against such
uncertainty. The controller was compared against a baseline that does not
account for actuator dynamics across three experimental scenarios: fast
translational tracking, rapid rotational tracking, and recovery from sudden
disturbance. The proposed method consistently achieved better tracking
performance, and notably, while the baseline diverged and crashed during the
fastest translational trajectory tracking and the recovery experiment, the
proposed controller maintained stability and successfully completed the tasks,
thereby demonstrating its effectiveness.

</details>


### [356] [PolySim: Bridging the Sim-to-Real Gap for Humanoid Control via Multi-Simulator Dynamics Randomization](https://arxiv.org/abs/2510.01708)
*Zixing Lei,Zibo Zhou,Sheng Yin,Yueru Chen,Qingyao Xu,Weixin Li,Yunhong Wang,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: To address the sim-to-real gap in humanoid whole-body control (WBC) caused by simulator inductive bias, this paper proposes PolySim, a training platform that integrates multiple heterogeneous simulators. By training policies jointly across these simulators, PolySim encourages generalization beyond single-simulator assumptions. The paper theoretically shows PolySim reduces inductive bias and experimentally demonstrates substantial improvements in sim-to-sim evaluations and successful zero-shot deployment on a real robot.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap in humanoid whole-body control (WBC) policies trained in simulation is a significant problem, stemming from the inherent assumptions and limitations (inductive bias) of any single simulator. This gap leads to discrepancies between different simulators and between simulation and the real world.

Method: The paper introduces PolySim, a WBC training platform that integrates multiple heterogeneous simulators. This platform allows for the simultaneous launch of parallel environments from different simulation engines within a single training run. This approach enables dynamics-level domain randomization, aiming to train policies that generalize beyond the inductive bias of any individual simulator.

Result: In sim-to-sim evaluations, PolySim substantially reduces motion-tracking error, achieving a 52.8% improvement in execution success on MuJoCo compared to an IsaacSim baseline. Crucially, PolySim enables zero-shot deployment on a real Unitree G1 robot without any additional fine-tuning, demonstrating effective transfer from simulation to the real world.

Conclusion: Training humanoid whole-body control policies jointly across multiple simulators using the proposed PolySim platform effectively mitigates the sim-to-real gap. PolySim's dynamics-level domain randomization leads to policies that generalize better, significantly improving performance in both sim-to-sim evaluations and enabling successful zero-shot transfer to real-world robots.

Abstract: Humanoid whole-body control (WBC) policies trained in simulation often suffer
from the sim-to-real gap, which fundamentally arises from simulator inductive
bias, the inherent assumptions and limitations of any single simulator. These
biases lead to nontrivial discrepancies both across simulators and between
simulation and the real world. To mitigate the effect of simulator inductive
bias, the key idea is to train policies jointly across multiple simulators,
encouraging the learned controller to capture dynamics that generalize beyond
any single simulator's assumptions. We thus introduce PolySim, a WBC training
platform that integrates multiple heterogeneous simulators. PolySim can launch
parallel environments from different engines simultaneously within a single
training run, thereby realizing dynamics-level domain randomization.
Theoretically, we show that PolySim yields a tighter upper bound on simulator
inductive bias than single-simulator training. In experiments, PolySim
substantially reduces motion-tracking error in sim-to-sim evaluations; for
example, on MuJoCo, it improves execution success by 52.8 over an IsaacSim
baseline. PolySim further enables zero-shot deployment on a real Unitree G1
without additional fine-tuning, showing effective transfer from simulation to
the real world. We will release the PolySim code upon acceptance of this work.

</details>


### [357] [Contrastive Representation Regularization for Vision-Language-Action Models](https://arxiv.org/abs/2510.01711)
*Taeyoung Kim,Jimin Lee,Myungkyu Koo,Dongyoung Kim,Kyungmin Lee,Changyeon Kim,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: 该研究提出了一种名为机器人状态感知对比损失（RS-CL）的方法，用于增强视觉-语言-动作（VLA）模型在机器人操作中的表现，通过使模型更好地理解机器人本体感受状态，从而提高了抓取和放置的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型虽然利用了预训练的视觉-语言模型（VLMs），但在机器人信号（如控制动作和本体感受状态）的敏感性方面表示仍有不足，未能充分利用机器人特有的信息。

Method: 提出了一种名为机器人状态感知对比损失（RS-CL）的表示正则化方法，旨在缩小VLM表示与机器人信号之间的差距。RS-CL通过利用本体感受状态之间的相对距离作为软监督，使表示更紧密地对齐机器人的本体感受状态。该方法可以与原始的动作预测目标互补，并且能够轻量级地集成到标准的VLA训练流程中。

Result: RS-CL显著提高了最先进的VLA模型在机器人操作任务上的表现。在RoboCasa-Kitchen数据集的抓取和放置任务上，性能从30.8%提升到41.5%，主要归功于抓取和放置过程中更精确的定位。在真实的机器人操作任务上，成功率从45.0%提升到58.3%。

Conclusion: RS-CL是一种有效的方法，可以增强VLA模型对机器人本体感受状态的理解，从而显著提高机器人在各种操作任务中的性能，并且易于集成到现有VLA模型训练框架中。

Abstract: Vision-Language-Action (VLA) models have shown its capabilities in robot
manipulation by leveraging rich representations from pre-trained
Vision-Language Models (VLMs). However, their representations arguably remain
suboptimal, lacking sensitivity to robotic signals such as control actions and
proprioceptive states. To address the issue, we introduce Robot State-aware
Contrastive Loss (RS-CL), a simple and effective representation regularization
for VLA models, designed to bridge the gap between VLM representations and
robotic signals. In particular, RS-CL aligns the representations more closely
with the robot's proprioceptive states, by using relative distances between the
states as soft supervision. Complementing the original action prediction
objective, RS-CL effectively enhances control-relevant representation learning,
while being lightweight and fully compatible with standard VLA training
pipeline. Our empirical results demonstrate that RS-CL substantially improves
the manipulation performance of state-of-the-art VLA models; it pushes the
prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen,
through more accurate positioning during grasping and placing, and boosts
success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.

</details>


### [358] [Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery](https://arxiv.org/abs/2510.01761)
*Wendu Zhang,Heng Wang,Shuangyi Wang,Yuanrui Huang*

Main category: cs.RO

TL;DR: 本文提出一种新型磁性连续体机器人，通过径向嵌入永磁体实现独立弯曲和扭转，并结合扭转驱动的药物释放机制，在体内实验中验证了其在靶向给药方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有轴向磁化磁性连续体机器人仅能弯曲的局限，扩展其变形能力，实现更复杂的临床操作。

Method: 通过径向嵌入永磁体于管壁，利用外部磁场实现独立弯曲和扭转。通过有限元分析建立驱动原理，并通过实验验证解耦控制。设计了一种结合内外层结构的扭转驱动药物释放装置。

Result: 实现了对磁性连续体机器人的弯曲和扭转的解耦控制，并通过实验验证了在实际磁场下的控制精度。成功设计并验证了基于扭转的药物释放机制。在体模实验中，机器人能够先通过弯曲导航至目标位置，再通过扭转激活释放药物。

Conclusion: 所提出的紧凑、无缆平台结合了多样的变形能力和精确的载药输送能力，预示着其在下一代靶向治疗中具有巨大潜力。

Abstract: Magnetic continuum robots (MCRs) enable minimally invasive navigation through
tortuous anatomical channels, yet axially magnetized designs have largely been
limited to bending-only motion. To expand deformation capabilities, this paper
presents a simple assembly that embeds permanent magnets radially within the
catheter wall, allowing a single externally steered permanent magnet to
independently induce either bending or torsion. A physics-based formulation
together with finite-element analysis establishes the actuation principles, and
benchtop experiments validate decoupled mode control under practical fields.
Building on this, a dual-layer blockage mechanism consisting of outer grooves
and inner plates leverages torsional shear to achieve on-demand drug release.
Finally, an in-phantom intervention experiment demonstrates end-to-end
operation: lumen following by bending for target approach, followed by
twist-activated release at the site. The resulting compact, cable-free platform
combines versatile deformation with precise payload delivery, indicating strong
potential for next-generation, site-specific therapies.

</details>


### [359] [An Anytime, Scalable and Complete Algorithm for Embedding a Manufacturing Procedure in a Smart Factory](https://arxiv.org/abs/2510.01770)
*Christopher Leet,Aidan Sciortino,Sven Koenig*

Main category: cs.RO

TL;DR: TS-ACES是一个可扩展的SFE求解器，可以处理大型智能工厂。


<details>
  <summary>Details</summary>
Motivation: 现有的SFE求解器难以扩展到包含大量机器的现代智能工厂，需要更具可扩展性的解决方案。

Method: 提出了一种名为TS-ACES（基于交通系统的任何周期嵌入求解器）的新型求解器，用于解决SFE问题。

Result: TS-ACES被证明是完整的，并且能够扩展到包含一百多台机器的基于真实工业场景的SFE实例。

Conclusion: TS-ACES解决了现有SFE求解器的可扩展性限制，为大型智能工厂提供了高效的解决方案。

Abstract: Modern automated factories increasingly run manufacturing procedures using a
matrix of programmable machines, such as 3D printers, interconnected by a
programmable transport system, such as a fleet of tabletop robots. To embed a
manufacturing procedure into a smart factory, an operator must: (a) assign each
of its processes to a machine and (b) specify how agents should transport parts
between machines. The problem of embedding a manufacturing process into a smart
factory is termed the Smart Factory Embedding (SFE) problem. State-of-the-art
SFE solvers can only scale to factories containing a couple dozen machines.
Modern smart factories, however, may contain hundreds of machines. We fill this
hole by introducing the first highly scalable solution to the SFE, TS-ACES, the
Traffic System based Anytime Cyclic Embedding Solver. We show that TS-ACES is
complete and can scale to SFE instances based on real industrial scenarios with
more than a hundred machines.

</details>


### [360] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu,Lianming Huang,Xinyu Wang,Yufei Cui,Nan Guan,Chun Jason Xue*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving
for unified perception and reasoning, but high inference latency hinders
real-time deployment. Early-exit reduces latency by terminating inference at
intermediate layers, yet its task-dependent nature limits generalization across
diverse scenarios. We observe that this limitation aligns with autonomous
driving: navigation systems can anticipate upcoming contexts (e.g.,
intersections, traffic lights), indicating which tasks will be required. We
propose Nav-EE, a navigation-guided early-exit framework that precomputes
task-specific exit layers offline and dynamically applies them online based on
navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE
achieves accuracy comparable to full inference while reducing latency by up to
63.9%. Real-vehicle integration with Autoware Universe further demonstrates
reduced inference latency (600ms to 300ms), supporting faster decision-making
in complex scenarios. These results suggest that coupling navigation foresight
with early-exit offers a viable path toward efficient deployment of large
models in autonomous systems. Code and data are available at our anonymous
repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>


### [361] [What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework](https://arxiv.org/abs/2510.01830)
*Hongze Wang,Boyang Sun,Jiaxu Xing,Fan Yang,Marco Hutter,Dhruv Shah,Davide Scaramuzza,Marc Pollefeys*

Main category: cs.RO

TL;DR: 本研究对基于模块化强化学习的物体导航系统进行了大规模实证研究，重点分析了感知、策略和测试时增强三个关键组件。研究发现，感知质量和测试时策略是决定性能的关键因素，而当前策略方法的改进带来的增益很小。基于这些发现，研究提出了实用的设计指南，并展示了一个超越现有最先进（SotA）方法的增强型模块化系统。


<details>
  <summary>Details</summary>
Motivation: 为了在家庭、学校和工作场所等日常、无控制的环境中部署移动机器人，需要解决物体导航（ObjectNav）问题。该问题要求机器人在先前未见过的环境中仅依靠其车载感知来定位目标对象。成功解决该问题需要整合语义理解、空间推理和长时序规划，但这是一个极具挑战性的组合。

Method: 通过对模块化强化学习（RL）的物体导航系统进行大规模实证研究，将其分解为感知、策略和测试时增强三个关键组件。通过广泛的受控实验，分离每个组件的贡献，并揭示了明确的趋势。

Result: 研究发现，感知质量和测试时策略是决定性能的关键因素，而当前策略方法的改进带来的增益很小。研究提出的增强型模块化系统在SPL（Success weighted by Path Length）上比现有SotA方法提高了6.6%，在成功率上提高了2.7%。同时，研究还引入了一个在相同条件下的“人类基线”，其中专家达到了平均98%的成功率，突显了RL智能体与人类导航能力之间的差距。

Conclusion: 本研究不仅设定了SotA性能的新标杆，还为未来物体导航（ObjectNav）的开发和评估提供了原则性的指导。研究强调了感知和测试时策略的重要性，并指出了RL智能体在导航能力上与人类相比仍有较大提升空间。

Abstract: Object-Goal Navigation (ObjectNav) is a critical component toward deploying
mobile robots in everyday, uncontrolled environments such as homes, schools,
and workplaces. In this context, a robot must locate target objects in
previously unseen environments using only its onboard perception. Success
requires the integration of semantic understanding, spatial reasoning, and
long-horizon planning, which is a combination that remains extremely
challenging. While reinforcement learning (RL) has become the dominant
paradigm, progress has spanned a wide range of design choices, yet the field
still lacks a unifying analysis to determine which components truly drive
performance. In this work, we conduct a large-scale empirical study of modular
RL-based ObjectNav systems, decomposing them into three key components:
perception, policy, and test-time enhancement. Through extensive controlled
experiments, we isolate the contribution of each and uncover clear trends:
perception quality and test-time strategies are decisive drivers of
performance, whereas policy improvements with current methods yield only
marginal gains. Building on these insights, we propose practical design
guidelines and demonstrate an enhanced modular system that surpasses
State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We
also introduce a human baseline under identical conditions, where experts
achieve an average 98% success, underscoring the gap between RL agents and
human-level navigation. Our study not only sets the SotA performance but also
provides principled guidance for future ObjectNav development and evaluation.

</details>


### [362] [SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot](https://arxiv.org/abs/2510.01984)
*Yue Wang*

Main category: cs.RO

TL;DR: SPARC是一个紧凑、开源的3自由度脊柱模块，专为四足机器人设计，集成了扭矩控制执行器、控制板和电源，实现了可编程的阻抗控制，可在x、z和theta方向上调整刚度和阻尼。


<details>
  <summary>Details</summary>
Motivation: 为四足机器人开发一个紧凑、开源的脊柱模块，以实现可编程的、类似弹簧-阻尼器特性的运动，从而系统地研究足式运动中的脊柱顺应性。

Method: 采用基于逆动力学（RNEA）的计算加速度控制器，并进行平滑的Stribeck摩擦补偿，以实现弹簧-阻尼器行为，而无需显式进行惯量整形。

Result: 实验结果表明，SPARC在水平刚度控制上表现出线性力-位移特性（300-700 N/m），相对误差小于1.5%。动态测试也证实了其质量-弹簧-阻尼器响应，并且与任务空间PD控制器相比，该方法具有更小的相位偏差和更低的耦合敏感性。

Conclusion: SPARC模块为研究足式运动中的脊柱顺应性提供了一个便携式平台，并将提供完整的硬件和固件资源。

Abstract: We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module
that combines revolute (pitch) and prismatic (axial) motion with programmable
task-space impedance for quadruped robots. The system integrates three
torque-controlled actuators, a custom 1 kHz control board, and a protected
power unit in a 1.26 kg package, enabling closed-loop stiffness and damping
shaping along x, z, and theta. We develop an RNEA-based computed-acceleration
controller with smooth Stribeck friction compensation to render spring-damper
behavior without explicit inertia shaping. Bench experiments validate the
approach. Quasi-static push-pull tests show linear force-displacement
characteristics with commanded horizontal stiffness spanning 300-700 N/m and <=
1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic
displace-and-release trials confirm mass-spring-damper responses over multiple
damping settings, with small, interpretable phase deviations due to
configuration-dependent inertia and low-speed friction effects. A task-space PD
controller produces roughly linear stiffness but with greater variability and
coupling sensitivity. SPARC provides a portable platform for systematic studies
of spine compliance in legged locomotion and will be released with complete
hardware and firmware resources.

</details>


### [363] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li,Ji Ma,Minghao Lu,Peng Lu*

Main category: cs.RO

TL;DR: 该研究将无人机领域成功的时空轨迹规划方法应用于双足机器人系统，以应对人形机器人足球比赛中的稳定性和踢球精确性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人足球比赛在激烈踢球动作中保持系统稳定性和实现精确控球方面存在挑战，现有方法（如基于位置控制和强化学习）存在局限性。虽然模型预测控制（MPC）在四足和双足机器人中应用广泛，但现有研究对其腿部摆动阶段的处理过于简化，限制了足部与环境的交互能力，特别是踢球能力。

Method: 将无人机领域成功的时空轨迹规划方法应用于双足机器人系统，自主生成满足目标踢球位置、速度和加速度约束，并优化摆动阶段持续时间的足部轨迹。

Result: 优化的轨迹能够模仿人类踢球动作中的后摆阶段。仿真和硬件实验表明，该算法的轨迹规划时间小于1毫秒，在目标球门角度为-90至90度时，任务完成准确率接近100%。

Conclusion: 该方法能有效解决人形机器人足球比赛中的稳定性和踢球精确性问题，并能生成接近人类的踢球轨迹。

Abstract: Humanoid robot soccer presents several challenges, particularly in
maintaining system stability during aggressive kicking motions while achieving
precise ball trajectory control. Current solutions, whether traditional
position-based control methods or reinforcement learning (RL) approaches,
exhibit significant limitations. Model predictive control (MPC) is a prevalent
approach for ordinary quadruped and biped robots. While MPC has demonstrated
advantages in legged robots, existing studies often oversimplify the leg swing
progress, relying merely on simple trajectory interpolation methods. This
severely constrains the foot's environmental interaction capability, hindering
tasks such as ball kicking. This study innovatively adapts the spatial-temporal
trajectory planning method, which has been successful in drone applications, to
bipedal robotic systems. The proposed approach autonomously generates foot
trajectories that satisfy constraints on target kicking position, velocity, and
acceleration while simultaneously optimizing swing phase duration. Experimental
results demonstrate that the optimized trajectories closely mimic human kicking
behavior, featuring a backswing motion. Simulation and hardware experiments
confirm the algorithm's efficiency, with trajectory planning times under 1 ms,
and its reliability, achieving nearly 100 % task completion accuracy when the
soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>


### [364] [Reducing Discomfort in Driving Simulators: Motion Cueing for Motion Sickness Mitigation](https://arxiv.org/abs/2510.01986)
*Varun Kotian,Vishrut Jain,Andrea Michelle Rios Lazcano,Daan Marinus Pool,Riender Happee,Barys Shyrokau*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Driving simulators are increasingly used in research and development.
However, simulators often cause motion sickness due to downscaled motion and
unscaled veridical visuals. In this paper, a motion cueing algorithm is
proposed that reduces motion sickness as predicted by the subjective vertical
conflict (SVC) model using model predictive control (MPC). Both sensory
conflict and specific force errors are penalised in the cost function, allowing
the algorithm to jointly optimise fidelity and comfort.
  Human-in-the-loop experiments were conducted to compare four simulator motion
settings: two variations of our MPC-based algorithm, one focused on pure
specific force tracking and the second compromising specific force tracking and
motion sickness minimisation, as well as reference adaptive washout and no
motion cases. The experiments were performed on a hexapod driving simulator
with participants exposed to passive driving.
  Experimental motion sickness results closely matched the sickness model
predictions. As predicted by the model, the no motion condition yielded the
lowest sickness levels. However, it was rated lowest in terms of fidelity. The
compromise solution reduced sickness by over 50% (average MISC level 3 to 1.5)
compared to adaptive washout and the algorithm focusing on specific force
tracking, without any significant reduction in fidelity rating.
  The proposed approach for developing MCA that takes into account both the
simulator dynamics and time evolution of motion sickness offers a significant
advancement in achieving an optimal control of motion sickness and specific
force recreation in driving simulators, supporting broader simulator use.

</details>


### [365] [GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics](https://arxiv.org/abs/2510.01848)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.RO

TL;DR: GreenhouseSplat是一个用于从RGB图像生成逼真温室场景的框架和数据集，支持ROS模拟和机器人任务评估。


<details>
  <summary>Details</summary>
Motivation: 现有温室模拟方法依赖于简化的或合成的资源，限制了模拟到现实的迁移。虽然高斯喷涂等辐射场方法能够实现照片级真实感重建，但仅限于单个植物或实验室条件。

Method: 提出GreenhouseSplat框架，直接从廉价的RGB图像生成照片级真实的温室资产，并将其集成到支持相机和LiDAR渲染的ROS模拟中。

Result: 创建了一个包含82个黄瓜植物的 dataset，并展示了其在机器人评估中的效用，例如使用标点进行定位。

Conclusion: GreenhouseSplat是实现温室规模辐射场模拟的第一步，为未来的农业机器人研究奠定了基础。

Abstract: Simulating greenhouse environments is critical for developing and evaluating
robotic systems for agriculture, yet existing approaches rely on simplistic or
synthetic assets that limit simulation-to-real transfer. Recent advances in
radiance field methods, such as Gaussian splatting, enable photorealistic
reconstruction but have so far been restricted to individual plants or
controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a
framework and dataset for generating photorealistic greenhouse assets directly
from inexpensive RGB images. The resulting assets are integrated into a
ROS-based simulation with support for camera and LiDAR rendering, enabling
tasks such as localization with fiducial markers. We provide a dataset of 82
cucumber plants across multiple row configurations and demonstrate its utility
for robotics evaluation. GreenhouseSplat represents the first step toward
greenhouse-scale radiance-field simulation and offers a foundation for future
research in agricultural robotics.

</details>


### [366] [Product Digital Twin Supporting End-of-life Phase of Electric Vehicle Batteries Utilizing Product-Process-Resource Asset Network](https://arxiv.org/abs/2510.02167)
*Sara Strakosova,Petr Novak,Petr Kadera*

Main category: cs.RO

TL;DR: 数字孪生技术可优化产品拆解过程，以提高可持续性，特别是在电动汽车电池的回收利用方面。


<details>
  <summary>Details</summary>
Motivation: 制造商在产品报废后未提供足够的数据支持再制造或回收过程，影响了循环经济和可持续性。

Method: 提出使用数字孪生技术，特别是双向产品-过程-资源资产网络（Bi-PAN）来建模和优化拆解过程，涵盖制造和再制造/回收两个阶段。

Result: 通过电动汽车电池拆解用例，证明了数字孪生技术能够灵活高效地解决不同类型电池的拆解挑战。

Conclusion: Bi-PAN 作为数字孪生技术的骨干，能够有效连接产品、生产资源和制造/回收过程，为实现循环经济中的可持续性提供支持。

Abstract: In the context of the circular economy, products in their end-of-life phase
should be either remanufactured or recycled. Both of these processes are
crucial for sustainability and environmental conservation. However,
manufacturers often do not support these processes enough by not sharing
relevant data. This paper proposes use of a digital twin technology, which is
capable to help optimizing the disassembly processes to reduce ecological
impact and enhance sustainability. The proposed approach is demonstrated
through a disassembly use-case of the product digital twin of an electric
vehicle battery. By utilizing product digital twins, challenges associated with
the disassembly of electric vehicle batteries can be solved flexibly and
efficiently for various battery types. As a backbone for the product digital
twin representation, the paper uses the paradigm of product-process-resource
asset networks (PAN). Such networks enable to model relevant relationships
across products, production resources, manufacturing processes, and specific
production operations that have to be done in the manufacturing phase of a
product. This paper introduces a Bi-Flow Product-Process-Resource Asset Network
(Bi-PAN) representation, which extends the PAN paradigm to cover not only the
manufacturing, but also the remanufacturing/recycling phase.

</details>


### [367] [EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2510.02080)
*Lingxiang Hu,Naima Ait Oufroukh,Fabien Bonardi,Raymond Ghandour*

Main category: cs.RO

TL;DR: EC3R-SLAM是一个无需标定的单目稠密SLAM框架，通过协同跟踪和重建模块，实现了高精度、低延迟和低内存消耗，并能在资源受限的平台上有效运行。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有单目稠密SLAM方法存在的延迟高、GPU内存占用大和依赖相机标定等问题，提出EC3R-SLAM框架以放宽这些限制。

Method: EC3R-SLAM通过一个跟踪模块（维护稀疏特征点地图）和一个基于前馈3D重建模型（同时估计相机内参）的映射模块协同工作，实现了效率。此外，系统还结合了局部和全局回环检测，以确保中期和长期的数据关联，强制多视图一致性，从而提高整体精度和鲁棒性。

Result: 实验结果表明，EC3R-SLAM在多个基准测试中取得了与最先进方法相当的性能，同时速度更快、内存效率更高。

Conclusion: EC3R-SLAM在资源受限平台（如笔记本电脑和Jetson Orin NX）上也能有效运行，证明了其在实际机器人应用中的潜力。

Abstract: The application of monocular dense Simultaneous Localization and Mapping
(SLAM) is often hindered by high latency, large GPU memory consumption, and
reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM,
a novel calibration-free monocular dense SLAM framework that jointly achieves
high localization and mapping accuracy, low latency, and low GPU memory
consumption. This enables the framework to achieve efficiency through the
coupling of a tracking module, which maintains a sparse map of feature points,
and a mapping module based on a feed-forward 3D reconstruction model that
simultaneously estimates camera intrinsics. In addition, both local and global
loop closures are incorporated to ensure mid-term and long-term data
association, enforcing multi-view consistency and thereby enhancing the overall
accuracy and robustness of the system. Experiments across multiple benchmarks
show that EC3R-SLAM achieves competitive performance compared to
state-of-the-art methods, while being faster and more memory-efficient.
Moreover, it runs effectively even on resource-constrained platforms such as
laptops and Jetson Orin NX, highlighting its potential for real-world robotics
applications.

</details>


### [368] [LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions](https://arxiv.org/abs/2510.02104)
*Yunhan Lin,Wenqi Wu,Zhijie Zhang,Huasong Min*

Main category: cs.RO

TL;DR: LangGrasp是一个集成LLM的机器人抓取框架，可处理含糊不清的指令，实现从对象级到零件级的抓取。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言驱动抓取方法在处理含糊不清且包含隐含意图的指令时遇到的困难。

Method: 集成微调后的大型语言模型（LLMs）以推断隐含意图，并利用2D部件分割引导的点云定位模块实现零件级抓取。

Result: LangGrasp能够准确解析指令中的隐含意图，识别未明示但对任务完成至关重要的操作和目标信息，并整合环境信息动态选择最优抓取姿态，实现从对象级到零件级的高精度抓取。

Conclusion: LangGrasp框架显著提高了机器人在非结构化环境中抓取的适应性和任务执行效率。

Abstract: The existing language-driven grasping methods struggle to fully handle
ambiguous instructions containing implicit intents. To tackle this challenge,
we propose LangGrasp, a novel language-interactive robotic grasping framework.
The framework integrates fine-tuned large language models (LLMs) to leverage
their robust commonsense understanding and environmental perception
capabilities, thereby deducing implicit intents from linguistic instructions
and clarifying task requirements along with target manipulation objects.
Furthermore, our designed point cloud localization module, guided by 2D part
segmentation, enables partial point cloud localization in scenes, thereby
extending grasping operations from coarse-grained object-level to fine-grained
part-level manipulation. Experimental results show that the LangGrasp framework
accurately resolves implicit intents in ambiguous instructions, identifying
critical operations and target information that are unstated yet essential for
task completion. Additionally, it dynamically selects optimal grasping poses by
integrating environmental information. This enables high-precision grasping
from object-level to part-level manipulation, significantly enhancing the
adaptability and task execution efficiency of robots in unstructured
environments. More information and code are available here:
https://github.com/wu467/LangGrasp.

</details>


### [369] [Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control](https://arxiv.org/abs/2510.02129)
*Philip Reichenberg,Tim Laue*

Main category: cs.RO

TL;DR: NAO机器人可以通过特殊的站立动作来解决卡住的肢体问题，并且可以通过补偿关节来处理大的关节位置误差，从而显著提高整体成功率。


<details>
  <summary>Details</summary>
Motivation: 在机器人足球比赛中，机器人站立能力至关重要，否则会被暂时移除比赛资格。本文提出了一种用于NAO机器人的站立方案。

Method: 通过执行特殊的动作来解开卡住的肢体（例如手臂），或通过补偿其他关节来补偿大的关节位置误差。

Result: 成功率显著提高，并且该方案被其他几支标准平台联赛的队伍使用，并且在比赛视频分析中取得了相似的成功率。

Conclusion: 本文提出的站立方案通过解决关节位置误差问题，显著提高了NAO机器人的站立成功率，并且在实际比赛中得到了广泛应用和验证。

Abstract: Stand-up motions are an indispensable part of humanoid robot soccer. A robot
incapable of standing up by itself is removed from the game for some time. In
this paper, we present our stand-up motions for the NAO robot. Our approach
dates back to 2019 and has been evaluated and slightly expanded over the past
six years. We claim that the main reason for failed stand-up attempts are large
errors in the executed joint positions. By addressing such problems by either
executing special motions to free up stuck limbs such as the arms, or by
compensating large errors with other joints, we significantly increased the
overall success rate of our stand-up routine. The motions presented in this
paper are also used by several other teams in the Standard Platform League,
which thereby achieve similar success rates, as shown in an analysis of videos
from multiple tournaments.

</details>


### [370] [SCANS: A Soft Gripper with Curvature and Spectroscopy Sensors for In-Hand Material Differentiation](https://arxiv.org/abs/2510.02164)
*Nathaniel Hanson,Austin Allison,Charles DiMarzio,Taşkın Padır,Kristen L. Dorsey*

Main category: cs.RO

TL;DR: SCANS系统是一种无需电子元件、通过流体驱动的软体机械手，能够评估物体（无论是在手中还是通过预接触围笼）的光谱特性，具有比以往软体机器人更宽的光谱传感能力。


<details>
  <summary>Details</summary>
Motivation: 探索将光学作为软体机器人的多功能传感模式，特别是利用软体材料的优势进行光谱分析。

Method: 开发了一种名为SCANS（软曲率和光谱）的系统，进行材料分析以优化光谱传感的软基材，并评估了预接触和手中操作的性能。使用线性判别分析来确定对区分相似物体至关重要的波长。

Result: 实验证明了SCANS系统在区分不同类别和尺寸的物体（金属、木材、塑料、有机物、纸张、泡沫）方面具有可解释的、统计上的分离能力，物体之间的光谱角度差异很大。研究表明，近红外波长对区分视觉上相似的物体至关重要。

Conclusion: SCANS系统在区分不同物体方面表现出强大的能力，特别是近红外波长在区分视觉上相似的物体方面起着关键作用，这为软体机器人增加了光学传感能力。

Abstract: We introduce the soft curvature and spectroscopy (SCANS) system: a versatile,
electronics-free, fluidically actuated soft manipulator capable of assessing
the spectral properties of objects either in hand or through pre-touch caging.
This platform offers a wider spectral sensing capability than previous soft
robotic counterparts. We perform a material analysis to explore optimal soft
substrates for spectral sensing, and evaluate both pre-touch and in-hand
performance. Experiments demonstrate explainable, statistical separation across
diverse object classes and sizes (metal, wood, plastic, organic, paper, foam),
with large spectral angle differences between items. Through linear
discriminant analysis, we show that sensitivity in the near-infrared
wavelengths is critical to distinguishing visually similar objects. These
capabilities advance the potential of optics as a multi-functional sensory
modality for soft robots. The complete parts list, assembly guidelines, and
processing code for the SCANS gripper are accessible at:
https://parses-lab.github.io/scans/.

</details>


### [371] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout通过解耦和协调物理与语义精炼来改进3D室内布局合成，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D室内布局合成方法在泛化性方面存在不足，基于LLM和VLM的方法虽然语义丰富但精炼能力受限。

Method: 提出DisCo-Layout框架，包含独立精炼的语义精炼工具（SRT）和物理精炼工具（PRT），以及协作精炼的多智能体框架（包括规划器、设计器和评估器）。

Result: 实验证明DisCo-Layout在生成逼真、连贯和可泛化的3D室内布局方面表现出最先进的性能。

Conclusion: DisCo-Layout框架能够有效生成高质量的3D室内布局，克服了现有方法的局限性。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [372] [Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0](https://arxiv.org/abs/2510.02248)
*Yan Miao,Ege Yuceel,Georgios Fainekos,Bardh Hoxha,Hideki Okamoto,Sayan Mitra*

Main category: cs.RO

TL;DR: FalconGym 2.0是一个新的模拟框架，通过其可编辑的API可以快速生成各种轨道。利用这个框架，我们提出了一种性能引导细化（PGR）算法，用于训练视觉策略，以提高其在变化轨道上的泛化能力和鲁棒性。该方法在UAV和四旋翼飞行器实验中表现出色，并成功应用于真实硬件。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉策略在单一轨道上容易过拟合，在轨道几何发生变化时性能会下降。因此，需要一个能够生成多样化轨道并提高视觉策略泛化能力和鲁棒性的方法。

Method: 1. 开发了FalconGym 2.0模拟框架，基于高斯泼溅（GSplat）并包含一个可编程的编辑API，能够快速生成静态和动态轨道。 2. 提出性能引导细化（PGR）算法，在FalconGym 2.0中，将视觉策略的训练集中在具有挑战性的轨道上，并迭代地改进其性能。 3. 在固定翼无人机和四旋翼飞行器两个案例研究中，训练的视觉策略在三个未见过的轨道上实现了100%的成功率，并在门姿扰动下保持了更高的成功率。 4. 将训练好的视觉策略零样本模拟到真实（sim-to-real）迁移到四旋翼硬件上，在30次试验中实现了98.6%的成功率。

Result: 在两个案例研究中，使用PGR在FalconGym 2.0中训练的单一视觉策略，在泛化性和鲁棒性方面优于最先进的方法。该策略在三个未见过的轨道上实现了100%的成功率，无需进行每轨道重新训练。在门姿扰动下，该策略也保持了更高的成功率。此外，该策略成功地零样本模拟到真实，在四旋翼硬件上实现了98.6%的成功率。

Conclusion: FalconGym 2.0及其PGR算法能够有效地训练出泛化能力强、鲁棒性好的视觉策略，解决了现有方法在面对轨道变化时的局限性。该方法在模拟和真实世界中都取得了优异的性能，证明了其在航空导航领域的潜力。

Abstract: Visual policy design is crucial for aerial navigation. However,
state-of-the-art visual policies often overfit to a single track and their
performance degrades when track geometry changes. We develop FalconGym 2.0, a
photorealistic simulation framework built on Gaussian Splatting (GSplat) with
an Edit API that programmatically generates diverse static and dynamic tracks
in milliseconds. Leveraging FalconGym 2.0's editability, we propose a
Performance-Guided Refinement (PGR) algorithm, which concentrates visual
policy's training on challenging tracks while iteratively improving its
performance. Across two case studies (fixed-wing UAVs and quadrotors) with
distinct dynamics and environments, we show that a single visual policy trained
with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in
generalization and robustness: it generalizes to three unseen tracks with 100%
success without per-track retraining and maintains higher success rates under
gate-pose perturbations. Finally, we demonstrate that the visual policy trained
with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a
quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30
trials spanning two three-gate tracks and a moving-gate track.

</details>


### [373] [Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking](https://arxiv.org/abs/2510.02252)
*Joao Pedro Araujo,Yanjie Ze,Pei Xu,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: 现有的模仿学习方法在人形机器人运动追踪中存在“具身鸿沟”问题，我们提出了一种名为GMR的新方法来解决运动重定向中的瑕疵，实验表明GMR在运动追踪性能和运动保真度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动追踪方法在处理人类运动数据到人形机器人本体的转换时，会引入足部滑动、身体穿透等瑕疵，导致机器人运动不准确，并且需要大量奖励工程和域随机化来弥补。本研究旨在系统地评估运动重定向质量对策略性能的影响，并提出一种新的重定向方法GMR来解决现有方法的不足。

Method: 我们首先系统地评估了不同运动重定向方法在去除奖励工程和域随机化的情况下对策略性能的影响。然后，我们提出了一种新的通用运动重定向（GMR）方法，旨在提高运动重定向的质量和保真度。最后，我们使用BeyondMimic进行策略训练，并在LAFAN1数据集的一个子集上评估了GMR与其他两种开源方法（PHC和ProtoMotions）以及一个闭源数据集的性能。

Result: 实验结果表明，重定向数据中的瑕疵会显著降低策略的鲁棒性，尤其是在处理动态或长序列运动时。我们提出的GMR方法在运动追踪性能和对源运动的保真度方面持续优于现有的开源方法，其性能接近于高质量的闭源基线。

Conclusion: 运动重定向的质量对人形机器人的运动追踪策略性能至关重要。我们提出的GMR方法能够显著提高运动追踪的性能和保真度，克服现有方法的局限性，为解决“具身鸿沟”问题提供了新的途径。

Abstract: Humanoid motion tracking policies are central to building teleoperation
pipelines and hierarchical controllers, yet they face a fundamental challenge:
the embodiment gap between humans and humanoid robots. Current approaches
address this gap by retargeting human motion data to humanoid embodiments and
then training reinforcement learning (RL) policies to imitate these reference
trajectories. However, artifacts introduced during retargeting, such as foot
sliding, self-penetration, and physically infeasible motion are often left in
the reference trajectories for the RL policy to correct. While prior work has
demonstrated motion tracking abilities, they often require extensive reward
engineering and domain randomization to succeed. In this paper, we
systematically evaluate how retargeting quality affects policy performance when
excessive reward tuning is suppressed. To address issues that we identify with
existing retargeting methods, we propose a new retargeting method, General
Motion Retargeting (GMR). We evaluate GMR alongside two open-source
retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source
dataset from Unitree. Using BeyondMimic for policy training, we isolate
retargeting effects without reward tuning. Our experiments on a diverse subset
of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts
in retargeted data significantly reduce policy robustness, particularly for
dynamic or long sequences. GMR consistently outperforms existing open-source
methods in both tracking performance and faithfulness to the source motion,
achieving perceptual fidelity and policy success rates close to the
closed-source baseline. Website:
https://jaraujo98.github.io/retargeting_matters. Code:
https://github.com/YanjieZe/GMR.

</details>


### [374] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 通过将相机外参显式地纳入策略条件，研究实现了视点不变的模仿学习，显著提高了跨视点的泛化能力，并引入了新的评估任务来验证策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决模仿学习在不同视点下的泛化能力问题，特别是当策略依赖于视觉线索而非相机外参时，在视点变化时性能会下降。

Method: 提出了一种将相机外参作为条件纳入策略的方法，并使用像素射线作为Plucker嵌入。将此方法应用于ACT、Diffusion Policy和SmolVLA等标准模仿学习策略。在RoboSuite和ManiSkill中引入了新的评估任务，包含固定和随机场景变体，以解耦背景线索和相机位姿。

Result: 实验结果表明，将相机外参作为条件能够显著提高策略在不同视点下的泛化能力。在没有外参的策略中，策略倾向于利用静态背景的视觉线索来推断相机位姿，这种捷径在工作空间几何或相机放置发生变化时会失效。而纳入外参的策略能够恢复性能，实现无需深度信息的RGB控制。

Conclusion: 将相机外参纳入条件是实现鲁棒的视点不变模仿学习的关键，能够有效解决策略在视点变化时的泛化和鲁棒性问题。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


### [375] [ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation](https://arxiv.org/abs/2510.02298)
*Wenye Yu,Jun Lv,Zixi Ying,Yang Jin,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: ARMADA是一个多机器人部署和自适应系统，通过引入名为FLOAT的在线故障检测方法，实现了高效的人类干预式策略学习。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在缺乏领域内数据时表现不佳，而人工收集数据成本高昂且质量参差不齐。现有的人类在环系统需要全程人工监控，限制了可扩展性。

Method: ARMADA系统集成了FLOAT在线故障检测方法，实现了并行的策略回放，仅在必要时请求人工干预，从而减少了对人工监督的依赖。

Result: FLOAT的平均准确率接近95%，比现有方法高20%。ARMADA在策略回放和后训练的多轮过程中，成功率提高了4倍以上，人工干预率降低了2倍以上。

Conclusion: ARMADA通过FLOAT实现了高效的领域内数据获取、可扩展的部署和对新场景的快速自适应，显著优于先前的人类在环学习方法。

Abstract: Imitation learning has shown promise in learning from large-scale real-world
datasets. However, pretrained policies usually perform poorly without
sufficient in-domain data. Besides, human-collected demonstrations entail
substantial labour and tend to encompass mixed-quality data and redundant
information. As a workaround, human-in-the-loop systems gather domain-specific
data for policy post-training, and exploit closed-loop policy feedback to offer
informative guidance, but usually require full-time human surveillance during
policy rollout. In this work, we devise ARMADA, a multi-robot deployment and
adaptation system with human-in-the-loop shared control, featuring an
autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA
enables paralleled policy rollout and requests human intervention only when
necessary, significantly reducing reliance on human supervision. Hence, ARMADA
enables efficient acquisition of in-domain data, and leads to more scalable
deployment and faster adaptation to new scenarios. We evaluate the performance
of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on
average, surpassing prior state-of-the-art failure detection approaches by over
20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and
greater than 2$\times$ reduction in human intervention rate over multiple
rounds of policy rollout and post-training, compared to previous
human-in-the-loop learning methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [376] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti是一个基于DAG的共识协议，通过降低容错率（20%而非33%）来优化区块链的延迟和吞吐量，实现了2轮通信、300毫秒的中值延迟和10000 TPS的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 区块链用户需要高可扩展性，即快速的交易确认和处理。当前的共识协议在这方面存在不足。

Method: Odontoceti协议采用了一种新的基于DAG的共识机制，其特点是：1. 容错率为20%（n=5f+1）。2. 仅需2轮通信即可完成区块确认。3. 提出了一种新的提交区块的决策规则。4. 包含一个优化机制，以应对参与者反应缓慢的情况（优化了崩溃故障场景）。

Result: 与现有协议相比，Odontoceti的延迟改进了20-25%，证明了将通信轮数从3轮减少到2轮可以带来显著的性能提升。在实际网络条件下，实现了300毫秒的中值延迟和10000 TPS的吞吐量。

Conclusion: 通过降低容错率，基于DAG的共识协议可以实现更低的延迟和更高的吞吐量，证明了这种在安全性上做出权衡的协议在区块链中具有实际应用的可行性。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [377] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: Kant是一个高效统一的AI容器集群调度平台，支持训练和推理任务的共调度，通过多种策略提升资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 传统的调度系统难以平衡大规模AI集群的资源利用率、调度效率和服务质量，无法满足LLM训练和推理的需求。

Method: 提出并实现了一个名为Kant的调度平台，定义了一系列关键的AI集群评估指标（GPU分配比、调度占用率、GPU节点碎片率、任务等待时间分布、任务训练时间估计分布），并采用了Backfill和增强型Binpack（E-Binpack）等调度策略。

Result: Kant在数百至数万个GPU的集群中表现出卓越的性能，显著提高了资源利用率和调度效率，有效降低了资源碎片和分布式训练的通信开销。

Conclusion: Kant是一个实用的工程化方法，可用于构建高性能、高可用性的AI原生调度基础设施，已成功部署于多个AI数据中心集群，稳定支持大规模智能计算负载。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [378] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: 该研究提出了IoT-MCP框架，通过在边缘部署服务器来实现模型上下文协议（MCP），以解决大型语言模型（LLM）与物联网（IoT）系统集成中的硬件异构性和控制复杂性问题。他们还创建了IoT-MCP Bench基准，包含114个基础任务和1140个复杂任务，用于评估LLM-IoT系统。实验证明，IoT-MCP在成功率、响应时间和内存占用方面表现优异，并提供了一个开源框架和评估方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）与物联网（IoT）系统的集成面临硬件异构性和控制复杂性的挑战，需要标准化的通信协议来连接LLM与物理设备。

Method: 提出并实现了一个名为IoT-MCP的新框架，该框架通过在边缘部署服务器来实施模型上下文协议（MCP），从而连接LLM和IoT生态系统。此外，还创建了一个名为IoT-MCP Bench的基准，其中包含114个基础任务和1140个复杂任务，用于评估LLM-IoT系统。

Result: IoT-MCP在22种传感器类型和6种微控制器单元上的实验验证显示，该框架在任务成功率（100%）、平均响应时间（205毫秒）和峰值内存占用（74KB）方面表现出色，生成的工具调用完全符合预期并获得完全准确的结果。

Conclusion: 该研究成功地交付了一个开源的IoT-MCP集成框架和一个标准化的评估方法，解决了LLM与IoT系统集成中的关键问题，并为未来的研究和应用奠定了基础。

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [379] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: 现有分布式账本协议要么通信复杂度高，要么依赖于仅适用于大量进程的委员会采样方法，这两种方法均不适用于中等规模的分布式账本。本研究提出QScale协议，旨在解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式账本协议（如PBFT和Algorand）分别适用于少数或大量进程的系统，但不适用于中等规模（几百到几千个进程）的生产环境。需要一种通信复杂度低、延迟低且适用于中等规模系统的分布式账本协议。

Method: 提出QScale协议，该协议旨在实现每个进程的通信复杂度低于亚线性，总通信复杂度低于超二次方，并且区块最终确定性延迟低。

Result: QScale协议的每个区块的通信复杂度期望为$\\

Conclusion: QScale协议是一种适用于中等规模分布式账本的协议，具有较低的通信复杂度和延迟，能够满足生产环境的需求。

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [380] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出一种用于移动边缘设备的低延迟、满足截止时间的深度神经网络卸载解决方案，通过优化的调度算法减少延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决移动边缘设备上低延迟、满足截止时间的深度神经网络卸载问题。

Method: 设计了一种调度算法，该算法具有轻量级的网络状态表示，并考虑了设备可用性、网络链路通信、优先级感知抢占和任务截止时间。该算法通过设计资源可用性表示、网络离散化和动态带宽估计算法来减少延迟。该算法被实现到一个由四个树莓派2组成的系统中。

Result: 在废物分类场景下，与之前的基线方法相比，该调度算法在低延迟和高吞吐量方面表现出更好的性能，尤其是在高负载和资源稀缺的情况下。

Conclusion: 所提出的低延迟抽象模型在处理高流量工作负载时能获得更好的性能，而动态带宽估计算法有助于任务放置，最终在资源稀缺时提高任务吞吐量。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [381] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: RISC-V 加速器难以在科学计算领域推广，因其需要重写 Fortran 代码。


<details>
  <summary>Details</summary>
Motivation: 现有 RISC-V 加速器需要重写 Fortran 代码，这在复杂的科学计算领域不切实际。

Method: 提出一种通过 Fortran 驱动 RISC-V 加速器的方法，无需重写代码。

Result: （摘要未提供）

Conclusion: （摘要未提供）

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [382] [ENLighten: Lighten the Transformer, Enable Efficient Optical Acceleration](https://arxiv.org/abs/2510.01673)
*Hanqing Zhu,Zhican Zhou,Shupeng Ning,Xuhao Wu,Ray Chen,Yating Wan,David Pan*

Main category: cs.ET

TL;DR: 通过软硬件协同设计，提出了一种名为Lighten的压缩流程和ENLighten的光子加速器，以解决光子计算在Transformer模型中的瓶颈问题，实现了能效和吞吐量的提升。


<details>
  <summary>Details</summary>
Motivation: 光子计算在加速AI中的线性代数运算方面具有潜力，但目前在大型Transformer模型上的应用受限于电光转换成本、数据移动开销以及有限的光子资源与模型规模不匹配的问题。

Method: 提出了一种名为Lighten的压缩流程，将Transformer权重矩阵分解为低秩和结构稀疏分量，以适应光子张量核的粒度。同时，提出了一种名为ENLighten的光子加速器，具有动态自适应张量核和全功率门控功能。

Result: 在ImageNet数据集上，Lighten将Vision Transformer的准确率降低了约1%，实现了50%的压缩率。ENLighten光子加速器在能-时积方面比现有技术提升了2.5倍。

Conclusion: 所提出的软硬件协同设计框架有效解决了光子计算在Transformer模型应用中的瓶颈，显著提升了能效和吞吐量。

Abstract: Photonic computing has emerged as a promising substrate for accelerating the
dense linear-algebra operations at the heart of AI, yet adoption for large
Transformer models remains in its infancy. We identify two bottlenecks: (1)
costly electro--optic conversions and data-movement overheads that erode energy
efficiency as model sizes scale; (2) a mismatch between limited on-chip
photonic resources and Transformer scale, which forces frequent reuse of
photonic tensor cores and dilutes throughput gains. To address these
challenges, we introduce a hardware--software co-design framework. First, we
propose \texttt{Lighten}, a PTC-aware compression flow that post-hoc decomposes
each Transformer weight matrix into a low-rank component plus a
structured-sparse component aligned to photonic tensor-core granularity,
without lengthy retraining. Second, we present \texttt{ENLighten}, a
reconfigurable photonic accelerator with dynamically adaptive tensor cores,
driven by broadband light redistribution, enabling fine-grained sparsity
support and full power gating of inactive parts. On ImageNet, \texttt{Lighten}
prunes a Base-scale Vision Transformer by 50\% with $\approx$1\% accuracy drop
after only 3 epochs (about 1 hour) of fine-tuning. Deployed on
\texttt{ENLighten}, it achieves a $2.5\times$ improvement in energy--delay
product over the state-of-the-art photonic Transformer accelerator.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [383] [Tensegrity structures and data-driven analysis for 3d cell mechanics](https://arxiv.org/abs/2510.01604)
*Ziran Zhou,Jacinto Ulloa,Guruswami Ravichandran,Jose E. Andrade*

Main category: physics.app-ph

TL;DR: 该研究提出了一种基于有限元法的3D张力完整性模型，用于模拟细胞力学，并引入了多尺度数据驱动框架以优化计算。


<details>
  <summary>Details</summary>
Motivation: 由于大多数张力完整性单元的低对称性，限制了对真实3D结构的分析，因此该研究旨在提出一种新的3D张力完整性模型来模拟细胞力学。

Method: 提出了一种基于有限元法的3D张力完整性模型，并引入了一个多尺度数据驱动框架。

Result: 该模型能够捕捉单细胞在压痕测试中的非线性行为、单层细胞在拉伸测试中的非线性行为，以及多细胞球体在非均匀预应力设计下的非均匀应力分布。

Conclusion: 该研究提出的3D张力完整性模型和多尺度数据驱动框架为模拟器官等大型细胞组的力学生物学提供了新的途径。

Abstract: The cytoskeleton (CSK) plays an important role in many cell functions. Given
the similarities between the mechanical behavior of tensegrity structures and
the CSK, many studies have proposed different tensegrity-based models for
simulating cell mechanics. However, the low symmetry of most tensegrity units
has hindered the analysis of realistic 3D structures. As a result,
tensegrity-based modeling in cell mechanics has been mainly focused on single
cells or monolayers. In this paper, we propose a 3D tensegrity model based on
the finite element method for simulating 3D cell mechanics. We show that the
proposed model not only captures the nonlinearity of a single cell in an
indentation test and a monolayer in stretch test but also the non-uniform
stress distribution in multicellular spheroids upon non-uniform prestress
design. Furthermore, we introduce a multiscale data-driven framework for
cellular mechanics to optimize the computation, thus paving the way for
modeling the mechanobiology of large cellular assemblies such as organs.

</details>


### [384] [Experimental Investigation of Skew Wind Effects on Vortex-Induced Vibration of Typical Bridge Decks](https://arxiv.org/abs/2510.01772)
*Guangzhong Gao,Pengwei Zhang,Wenkai Du,Yonghui Xie,Pengjie Ren,Xiaofeng Xue*

Main category: physics.app-ph

TL;DR: 偏航风角会影响桥梁甲板的涡激振动（VIV），且独立性原理不适用于此情况。


<details>
  <summary>Details</summary>
Motivation: 研究偏航风对两种典型桥梁（闭合箱梁和双边梁）甲板涡激振动（VIV）的影响。

Method: 通过弹簧悬挂的斜截面模型试验，在不同风偏角和迎角下进行，并引入一种新的数值算法来修正峰值VIV幅值。

Result: VIV幅值和锁定范围随偏航角变化，独立性原理不适用。观察到垂向-扭转耦合现象。闭合箱梁在最不利偏航风条件下，垂向VIV幅值增加约20.1%，扭转VIV幅值增加约179.8%。双边梁垂向VIV幅值增加约3.9%。

Conclusion: 偏航风条件会显著影响桥梁甲板的VIV响应，特别是闭合箱梁的扭转VIV响应。

Abstract: This study explores the skew wind effects on vortex-induced vibration (VIV)
of two typical bridge decks-a closed-box girder and a twin-edge girder-through
spring-suspended oblique section model tests. Experiments were conducted at
various wind yaw angles and angles of attack. Results indicate that VIV
amplitudes and lock-in ranges exhibit a clear variation with yaw angles,
rendering the Independence Principle (IP) unsuitable for these configurations.
A heave-torsion coupling phenomenon was observed in both heaving and torsional
VIV, attributed to the eccentricity of the center of gravity due to asymmetric
end segments. A novel numerical algorithm was introduced to correct peak VIV
amplitudes for variations in structural mass-damping parameters across yaw
angles. The most unfavorable VIV responses occurred under skew wind conditions,
with maximum amplitudes increasing by approximately 20.1 percent for heaving
VIV and 179.8 percent for torsional VIV in the closed-box girder, and by 3.9
percent for heaving VIV in the twin-edge girder, relative to normal wind
conditions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [385] [Sequent Calculi for Data-Aware Modal Logics](https://arxiv.org/abs/2510.01868)
*Carlos Areces,Valentin Cassano,Danae Dutto,Raul Fervari*

Main category: cs.LO

TL;DR: 本文提出了数据感知模态逻辑HXpathD的完备且可靠的Gentzen风格推理演算，并证明了其所有规则均可逆且满足消去割定理。


<details>
  <summary>Details</summary>
Motivation: 提供数据感知模态逻辑的证明理论基础，为分析图结构数据上的查询语言提供工具，并为将证明理论技术扩展到更广泛的模态系统奠定基础。

Method: 提出并证明了HXpathD的Gentzen风格推理演算的可靠性和完备性，并证明了其规则的可逆性和消去割定理。

Result: 提出了一个适用于HXpathD的Gentzen风格推理演算，该演算具有可靠性、完备性、规则可逆性和消去割定理。

Conclusion: 本文的工作为数据感知模态逻辑提供了重要的证明理论基础，并为未来在查询语言和模态系统方面的进一步研究铺平了道路。

Abstract: Data-aware modal logics offer a powerful formalism for reasoning about
semi-structured queries in languages such as DataGL, XPath, and GQL. In brief,
these logics can be viewed as modal systems capable of expressing both
reachability statements and data-aware properties, such as value comparisons.
One particularly expressive logic in this landscape is HXpathD, a hybrid modal
logic that captures not only the navigational core of XPath but also data
comparisons, node labels (keys), and key-based navigation operators. While
previous work on HXpathD has primarily focused on its model-theoretic
properties, in this paper we approach HXpathD from a proof-theoretic
perspective. Concretely, we present a sound and complete Gentzen-style sequent
calculus for HXpathD. Moreover, we show all rules in this calculus are
invertible, and that it enjoys cut elimination. Our work contributes to the
proof-theoretic foundations of data-aware modal logics, and enables a deeper
logical analysis of query languages over graph-structured data. Moreover, our
results lay the groundwork for extending proof-theoretic techniques to a
broader class of modal systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [386] [LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science](https://arxiv.org/abs/2510.01285)
*Alireza Salemi,Mihir Parmar,Palash Goyal,Yiwen Song,Jinsung Yoon,Hamed Zamani,Hamid Palangi,Tomas Pfister*

Main category: cs.MA

TL;DR: 提出一种受黑板架构启发的黑板范式，用于解决大语言模型在数据湖中发现相关数据的挑战，提高了可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理大型异构数据湖中的数据发现问题时存在局限性：单智能体系统易被压垮，主从多智能体系统依赖刚性的中央控制器进行任务分配。

Method: 提出一种新的多智能体通信范式，借鉴了传统人工智能模型的黑板架构。中心智能体将请求发布到共享黑板，自主的下属智能体根据自身能力响应。

Result: 在包含数据发现任务的三个基准测试（KramaBench、DS-Bench 和 DA-Code 的修改版本）上进行了评估。结果显示，该黑板架构在端到端任务成功率上比 RAG 和主从多智能体范式有 13% 到 57% 的相对提升，在数据发现的 F1 分数上比最佳基线有高达 9% 的相对提升。

Conclusion: 黑板范式是一种可扩展且可泛化的通信框架，适用于多智能体系统，能有效解决大语言模型在数据湖中的数据发现问题。

Abstract: The rapid advancement of Large Language Models (LLMs) has opened new
opportunities in data science, yet their practical deployment is often
constrained by the challenge of discovering relevant data within large
heterogeneous data lakes. Existing methods struggle with this: single-agent
systems are quickly overwhelmed by large, heterogeneous files in the large data
lakes, while multi-agent systems designed based on a master-slave paradigm
depend on a rigid central controller for task allocation that requires precise
knowledge of each sub-agent's capabilities. To address these limitations, we
propose a novel multi-agent communication paradigm inspired by the blackboard
architecture for traditional AI models. In this framework, a central agent
posts requests to a shared blackboard, and autonomous subordinate agents --
either responsible for a partition of the data lake or general information
retrieval -- volunteer to respond based on their capabilities. This design
improves scalability and flexibility by eliminating the need for a central
coordinator to have prior knowledge of all sub-agents' expertise. We evaluate
our method on three benchmarks that require explicit data discovery: KramaBench
and modified versions of DS-Bench and DA-Code to incorporate data discovery.
Experimental results demonstrate that the blackboard architecture substantially
outperforms baselines, including RAG and the master-slave multi-agent paradigm,
achieving between 13% to 57% relative improvement in end-to-end task success
and up to a 9% relative gain in F1 score for data discovery over the
best-performing baselines across both proprietary and open-source LLMs. Our
findings establish the blackboard paradigm as a scalable and generalizable
communication framework for multi-agent systems.

</details>


### [387] [SimCity: Multi-Agent Urban Development Simulation with Rich Interactions](https://arxiv.org/abs/2510.01297)
*Yeqi Feng,Yucheng Lu,Hongyu Su,Tianxing He*

Main category: cs.MA

TL;DR: SimCity是一个利用大型语言模型（LLMs）构建可解释的宏观经济模拟的多智能体框架，能够模拟异构智能体及其交互，并重现了包括供需价格弹性、恩格尔定律、奥肯定律、菲利普斯曲线和贝弗里奇曲线在内的经典宏观经济现象。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够进行现实和可解释的宏观经济模拟的系统，该系统能够模拟异构智能体及其交互，并重现经典的宏观经济现象。

Method: SimCity是一个多智能体框架，利用LLMs来模拟一个具有异构智能体和丰富交互的可解释宏观经济系统。它包括四种核心智能体类型：家庭、公司、中央银行和政府。这些智能体在一个有摩擦的劳动力市场、一个异构商品市场和一个金融市场中进行决策和互动。此外，视觉-语言模型（VLM）用于确定新公司的地理位置并渲染虚拟城市，从而能够同时研究宏观经济规律和城市扩张动态。

Result: SimCity能够自然地重现包括供需价格弹性、恩格尔定律、奥肯定律、菲利普斯曲线和贝弗里奇曲线在内的经典宏观经济现象，并且在不同的模拟运行中保持稳健性。

Conclusion: SimCity提供了一个利用LLMs构建可解释宏观经济模拟的新方法，它能够模拟异构智能体及其交互，并重现经典的宏观经济现象。

Abstract: Large Language Models (LLMs) open new possibilities for constructing
realistic and interpretable macroeconomic simulations. We present SimCity, a
multi-agent framework that leverages LLMs to model an interpretable
macroeconomic system with heterogeneous agents and rich interactions. Unlike
classical equilibrium models that limit heterogeneity for tractability, or
traditional agent-based models (ABMs) that rely on hand-crafted decision rules,
SimCity enables flexible, adaptive behavior with transparent natural-language
reasoning. Within SimCity, four core agent types (households, firms, a central
bank, and a government) deliberate and participate in a frictional labor
market, a heterogeneous goods market, and a financial market. Furthermore, a
Vision-Language Model (VLM) determines the geographic placement of new firms
and renders a mapped virtual city, allowing us to study both macroeconomic
regularities and urban expansion dynamics within a unified environment. To
evaluate the framework, we compile a checklist of canonical macroeconomic
phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the
Phillips Curve, and the Beveridge Curve, and show that SimCity naturally
reproduces these empirical patterns while remaining robust across simulation
runs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [388] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: 通过将MD模拟视为时间序列预测问题，并结合物理信息损失，我们提出了一种新型方法，可以比传统基线更准确、更快速地预测原子轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统DFT方法计算成本高昂，限制了长期MD模拟的可行性。

Method: 将MD模拟制定为时间序列预测问题，通过预测原子位移而非绝对位置，并结合基于DFT参数化的莫尔斯势函数的物理信息损失和推理机制。

Result: 所提出的方法在模拟准确性方面持续优于标准基线，并且能够在几分钟内实现数千步的稳定模拟。

Conclusion: 将物理知识整合到原子轨迹预测中可以提高其可靠性和准确性，为替代昂贵的DFT模拟提供了一种可扩展的解决方案。

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [389] [Learning Regularization Functionals for Inverse Problems: A Comparative Study](https://arxiv.org/abs/2510.01755)
*Johannes Hertrich,Hok Shing Wong,Alexander Denker,Stanislas Ducotterd,Zhenghan Fang,Markus Haltmeier,Željko Kereta,Erich Kobler,Oscar Leong,Mohammad Sadegh Salehi,Carola-Bibiane Schönlieb,Johannes Schwab,Zakhar Shumaylov,Jeremias Sulam,German Shâma Wache,Martin Zach,Yasi Zhang,Matthias J. Ehrhardt,Sebastian Neumayer*

Main category: cs.LG

TL;DR: 文章提出了一个统一的框架来比较和分析用于解决成像逆问题的各种学习型正则化方法，解决了现有方法实现不统一导致难以直接比较的问题。


<details>
  <summary>Details</summary>
Motivation: 现有用于解决成像逆问题的学习型正则化方法在模型设计和训练策略上存在差异，导致难以进行直接比较。

Method: 收集并整合了现有的相关代码到一个通用的框架中，以便能够系统地比较不同的方法。

Result: 通过统一的框架，能够更清晰地展示各种方法的优缺点，并为未来的研究提供有价值的见解。此外，还提供了对每种方法的简洁描述和实践指南。

Conclusion: 统一的框架使得对成像逆问题的学习型正则化方法进行系统性比较和分析成为可能，有助于理解它们的优势和局限性，并指导未来的研究方向。

Abstract: In recent years, a variety of learned regularization frameworks for solving
inverse problems in imaging have emerged. These offer flexible modeling
together with mathematical insights. The proposed methods differ in their
architectural design and training strategies, making direct comparison
challenging due to non-modular implementations. We address this gap by
collecting and unifying the available code into a common framework. This
unified view allows us to systematically compare the approaches and highlight
their strengths and limitations, providing valuable insights into their future
potential. We also provide concise descriptions of each method, complemented by
practical guidelines.

</details>


### [390] [Learning Representations Through Contrastive Neural Model Checking](https://arxiv.org/abs/2510.01853)
*Vladimir Krsmanovic,Matthias Cosler,Mohamed Ghanem,Bernd Finkbeiner*

Main category: cs.LG

TL;DR: CNML是一种新的模型检查方法，利用模型检查任务作为学习对齐表示的指导信号，通过自监督对比目标将逻辑规范和系统嵌入共享的潜在空间，在检索任务上显著优于基线方法，并将学到的表示转移到下游任务。


<details>
  <summary>Details</summary>
Motivation: 在形式化验证领域，特别是安全关键系统的验证中，尽管深度学习在视觉和语言领域得到了广泛应用，但表示学习仍未得到充分探索。

Method: CNML是一种新颖的方法，它利用模型检查任务作为学习对齐表示的指导信号。该方法通过自监督对比目标，将逻辑规范和系统联合嵌入到一个共享的潜在空间中。

Result: 在行业启发的检索任务上，CNML在跨模态和模态内设置中均显著优于算法和神经基线。此外，学到的表示能有效地转移到下游任务，并推广到更复杂的公式。

Conclusion: 模型检查可以作为学习形式语言表示的目标。

Abstract: Model checking is a key technique for verifying safety-critical systems
against formal specifications, where recent applications of deep learning have
shown promise. However, while ubiquitous for vision and language domains,
representation learning remains underexplored in formal verification. We
introduce Contrastive Neural Model Checking (CNML), a novel method that
leverages the model checking task as a guiding signal for learning aligned
representations. CNML jointly embeds logical specifications and systems into a
shared latent space through a self-supervised contrastive objective. On
industry-inspired retrieval tasks, CNML considerably outperforms both
algorithmic and neural baselines in cross-modal and intra-modal settings.We
further show that the learned representations effectively transfer to
downstream tasks and generalize to more complex formulas. These findings
demonstrate that model checking can serve as an objective for learning
representations for formal languages.

</details>


### [391] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: 在需要高精度的任务（如数学推理）中，传统的温度采样方法会降低输出质量。本文提出了一种选择性采样方法，该方法根据采样风险动态地在贪婪采样和高温采样之间切换，以提高质量-多样性权衡。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型生成内容的创造力需要多样性指标。然而，像数学推理这样需要高精度的任务，在高温采样时精度会下降。

Method: 提出了一种选择性采样方法，该方法根据采样风险动态地在贪婪采样和高温采样之间切换。采样风险由一个轻量级分类器估计，该分类器在一个可验证问题的子集上进行训练。

Result: 选择性采样提高了在高温设置下数学推理任务的质量-多样性权衡。

Conclusion: 选择性采样是一种有效的方法，可以在保持可接受的推理质量的同时提高生成文本的多样性。

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [392] [Neural non-canonical Hamiltonian dynamics for long-time simulations](https://arxiv.org/abs/2510.01788)
*Clémentine Courtès,Emmanuel Franck,Michael Kraus,Laurent Navoret,Léopold Trémant*

Main category: cs.LG

TL;DR: 本研究旨在从数据中学习非正则哈密顿动力学，其中长期预测需要保持学习模型和数值方案中的结构。以往的研究分别侧重于其中一个方面，但结合两者会产生新的问题。实验中，由于方案的规范依赖性，学习到的模型有时在数值上不稳定，导致无法进行长时间模拟。因此，我们提出了两种不同的训练策略来解决这个问题：直接学习矢量场或通过方案学习时间离散动力学。通过多个数值案例评估了该方法学习复杂物理动力学（如回转动力学等离子体物理中的引导中心）的能力。


<details>
  <summary>Details</summary>
Motivation: 学习非正则哈密顿动力学，并解决长期预测中的结构保持和数值稳定性问题。

Method: 提出两种训练策略：直接学习矢量场或学习时间离散动力学。

Result: 通过数值案例评估了所提出方法在学习复杂物理动力学（如回转动力学等离子体物理中的引导中心）方面的能力。

Conclusion: 所提出的两种训练策略能够有效解决学习非正则哈密顿动力学中的数值不稳定性问题，从而实现可靠的长期预测。

Abstract: This work focuses on learning non-canonical Hamiltonian dynamics from data,
where long-term predictions require the preservation of structure both in the
learned model and in numerical schemes. Previous research focused on either
facet, respectively with a potential-based architecture and with degenerate
variational integrators, but new issues arise when combining both. In
experiments, the learnt model is sometimes numerically unstable due to the
gauge dependency of the scheme, rendering long-time simulations impossible. In
this paper, we identify this problem and propose two different training
strategies to address it, either by directly learning the vector field or by
learning a time-discrete dynamics through the scheme. Several numerical test
cases assess the ability of the methods to learn complex physical dynamics,
like the guiding center from gyrokinetic plasma physics.

</details>


### [393] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: LLM驱动的工作流从约10,000篇科学文献中自动提取了热电和结构性质，创建了最大的LLM策展热电数据集，并发布了一个网络浏览器以方便社区访问。


<details>
  <summary>Details</summary>
Motivation: 现有数据库的局限性，如规模小、手动整理或偏向第一性原理结果，导致实验文献未被充分利用，阻碍了材料的快速发现。

Method: 使用集成了动态令牌分配、零样本多主体提取和条件表解析的LLM驱动工作流，从全文本科学文章中提取热电和结构性质。

Result: 提取了27,822条温度分辨的性质记录，包括品质因数（ZT）、塞贝克系数、电导率、电阻率、功率因子和热导率，以及晶体类别、空间群和掺杂策略等结构属性。GPT-4.1达到了最高的准确性（热电性质F1=0.91，结构字段F1=0.82），而GPT-4.1 Mini的成本效益更高。

Conclusion: 该研究提供了迄今为止最大的LLM策展热电数据集，一个可复制且成本可控的提取流程，并为可扩展的数据驱动材料发现奠定了基础。

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [394] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: RSAVQ通过引入误差方向敏感性引导（EDSG）和权重通道敏感性引导（WCSG）来解决LLM极低比特量化中的未约束方向误差和次优比特分配问题，实验证明在LLaMA-3 8B的2位量化中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM参数量大，在资源受限设备部署困难；现有低比特量化方法（2-4位）存在未约束方向误差和比特分配不优的问题。

Method: 提出RSAVQ框架，包含（1）误差方向敏感性引导（EDSG），利用Fisher信息矩阵（FIM）诱导的黎曼度量将量化误差投影到参数空间中不敏感的方向（沿负自然梯度方向），以抑制误差扩展。（2）权重通道敏感性引导（WCSG），通过FIM曲率分析构建通道级敏感性度量，动态指导比特资源分配，实现最优量化。

Result: 在LLaMA-3 8B的2位量化中，RSAVQ相比VPTQ和QuIP#，困惑度（PPL）降低0.4，零样本准确率提高1.5。

Conclusion: RSAVQ为资源受限环境提供了一个实用的LLM量化解决方案，并连接了信息几何和神经网络量化，推动了高效深度学习的发展。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [395] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度Q网络的防御方法，以应对联邦学习中存在的投毒和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在部分可观察性下容易受到投毒和后门攻击。

Method: 将防御视为一个部分可观察的序贯决策问题，并引入了一个信任感知的深度Q网络（DQN），该网络将多信号证据整合到客户端信任更新中，同时优化长期鲁棒性-准确性目标。

Result: 在CIFAR-10数据集上，该方法相比其他基线方法（包括随机、线性Q和策略梯度控制器）在鲁棒性和准确性之间取得了更好的权衡。实验表明，增加客户端重叠度可以提高准确性并降低攻击成功率（ASR），同时保持稳定的检测。即使在可观察性降低的情况下，序贯信念更新也能有效缓解较弱信号的影响，保持准确性稳定。

Conclusion: 所提出的信任感知DQN在部分可观察的联邦学习环境中，能够有效地防御投毒和后门攻击，并在鲁棒性和准确性之间取得良好的平衡。

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [396] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 本篇论文设计了两种新的多智能体LLM答案聚合算法——最优权重（OW）和逆惊喜度（ISP），以解决标准多数投票法忽略模型异质性和相关性的问题。通过理论分析和在多个数据集上的实证验证，这两种方法在不同场景下均优于多数投票法，为构建鲁棒的多智能体LLM流程提供了实际的性能提升和概念洞见。


<details>
  <summary>Details</summary>
Motivation: 在多智能体大语言模型（LLM）推理快速发展的背景下，如何有效聚合多个LLM的答案是一个根本性挑战。标准多数投票法未能考虑模型间的潜在异质性和相关性，处理方式过于单一。

Method: 设计并提出两种新的聚合算法：最优权重（OW）和逆惊喜度（ISP）。这两种算法均利用了一阶和二阶信息，并通过理论分析证明了它们在温和假设下能够克服多数投票法的固有局限性，从而做出更可靠的集体决策。

Result: 在合成数据集、UltraFeedback和MMLU等LLM微调基准以及真实世界的ARMMAN医疗健康场景中，实验结果均表明OW和ISP算法优于多数投票法，实现了性能上的提升，并为鲁棒的多智能体LLM管道设计提供了概念性见解。

Conclusion: OW和ISP这两种新颖的聚合算法在理论和实践中均证明了其优越性，它们能够有效处理多智能体LLM答案中的异质性和相关性，提供比多数投票法更可靠的集体决策，为多智能体LLM的应用提供了重要的改进方向。

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [397] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: RSTGCN模型用于预测火车站的平均列车晚点时间，并发布了印度铁路网络数据集。


<details>
  <summary>Details</summary>
Motivation: 为了提高铁路运营效率，需要准确预测火车晚点，以支持更高层次的交通管理。

Method: 提出了一种名为铁路中心时空图卷积网络（RSTGCN）的新模型，该模型包含具有频率感知的空间注意力的架构创新和新颖的特征集成，用于预测特定时间段内火车站所有进站列车的平均到达延误。

Result: 在印度铁路网络（IRN）上进行了广泛的实验，与最先进的基线相比，在标准指标上显示出持续的改进。

Conclusion: RSTGCN模型能有效预测大规模铁路网络中的平均延误，并提供了一个开放的数据集以鼓励进一步的研究。

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [398] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: DiT模型生成高质量图像效率低，提出TetriServe系统，通过步进序列并行化和基于回合的调度机制提高GPU利用率和SLO达标率。


<details>
  <summary>Details</summary>
Motivation: DiT模型在生成高质量图像方面表现出色，但在严格的服务水平目标（SLO）下提供服务面临挑战，主要是由于其高昂的计算成本，尤其是在大分辨率下。现有的服务系统采用固定的序列并行度，无法有效处理混合分辨率和截止时间异构的工作负载，导致GPU利用率低和SLO达成率差。

Method: 提出步进序列并行化（step-level sequence parallelism），根据截止时间动态调整单个请求的并行度。提出TetriServe系统，实现该策略以高效生成图像。具体而言，TetriServe引入了一种新颖的基于回合的调度机制：1.将时间离散化为固定回合，使截止感知调度可行；2.在步进级别调整并行度，最大限度地减少GPU小时消耗；3.联合打包请求以最小化延迟完成。

Result: 在最先进的DiT模型上进行的广泛评估表明，TetriServe相比现有解决方案，在不降低图像质量的情况下，SLO达成率最高可提高32%。

Conclusion: TetriServe通过步进序列并行化和基于回合的调度机制，有效解决了DiT模型服务中的效率和SLO达成率问题。

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [399] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: BB是一种新的稀疏化方法，通过为每个单元分配局部通信预算，实现稀疏化、去相关性和精度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的剪枝方法通常基于损失（如幅值或梯度）对参数进行排序，但未能有效平衡选择性和受众。BB旨在通过引入局部通信预算和受众平衡来实现更优的剪枝效果。

Method: BB为每个单元分配一个局部通信预算，即其长期激活率 $a_i$ 和扇出 $k_i$ 的乘积。通过约束熵分析，BB在全局通信预算下最大化编码熵，从而实现选择性-受众平衡：$rac{1-a_i}{a_i}=eta k_i$。BB通过调整扇入（降低活动度）或扇出（减少广播）来强制执行此平衡。

Result: BB在Transformer（用于ASR）、ResNet（用于人脸识别）和3D U-Nets（用于突触预测）上，在保持相同稀疏度的前提下，提高了编码熵和去相关性，并提升了精度，有时甚至超越了密集基线。在电子显微镜图像上，BB达到了最先进的F1和PR-AUC指标。

Conclusion: BB易于集成，能够提高编码熵和去相关性，并在多个任务上提升精度。该方法为学习更多样化、更高效的表示提供了新的方向。

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [400] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 本研究在IsaacLab框架中扩展了对高保真物理模拟中对抗性策略进行可扩展训练的支持，并引入了一套具有异构智能体和不对称目标与能力的对抗性MARL环境，集成了HAPPO算法，以支持高效训练和评估。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要集中在合作环境中，但现实世界中的对抗性交互（如追逐-躲避、安全和竞争性操控）同样重要。

Method: 扩展了IsaacLab框架以支持对抗性策略的可扩展训练，并集成了HAPPO算法，创建了一套包含异构智能体和不对称目标与能力的对抗性MARL环境。

Result: 实验证明，该框架能够为形态各异的多智能体竞争建模和训练鲁棒策略，同时保持高吞吐量和模拟真实性。

Conclusion: 本研究通过扩展IsaacLab框架并引入一套新的环境和算法，为解决对抗性多智能体强化学习问题提供了一个可扩展且高效的平台。

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [401] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: RLP是一种信息驱动的强化预训练目标，通过将链式思考视为一种探索行为，并根据其提供的信息增益计算奖励，从而将强化学习的探索精神融入预训练的最后阶段，鼓励模型提前独立思考，显著提升了数学和科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的预训练范式在预训练阶段主要依赖于下一个词预测损失，而强化学习只在监督微调之后的最后一个阶段引入，这种方式并非最优。本文旨在探索一种更优的训练方法，将强化学习的核心——探索——引入预训练阶段。

Method: 本文提出了一种名为RLP（Reinforcement Pre-training）的信息驱动强化预训练目标。RLP将链式思考（chain-of-thought）视为一种探索行为，并根据链式思考在预测未来词元时提供的信息增益来计算奖励。具体来说，奖励信号衡量了在给定上下文和采样推理链的条件下，下一个词元的对数似然度的增加，与仅在给定上下文的条件下进行比较。这种方法提供了一个无需验证器的密集奖励信号，从而可以在预训练期间对整个文档流进行高效训练。

Result: 在Qwen3-1.7B-Base模型上使用RLP进行预训练，在八个数学和科学基准的综合平均得分上提升了19%。在相同的后续训练下，这种提升效果更加显著，尤其是在AIME25和MMLU-Pro等侧重推理的任务上。将RLP应用于Nemotron-Nano-12B-v2混合模型，可以将综合平均得分从42.81%提高到61.32%，并将科学推理的平均得分提高23%，证明了其在不同架构和模型规模上的可扩展性。

Conclusion: RLP通过将强化学习作为一种面向普通文本的预训练目标，有效地将下一个词元预测与有用的链式思考推理能力的发展联系起来。这种方法能够将探索和独立思考的能力更早地注入到模型中，从而在下游任务中取得显著的性能提升，尤其是在数学和科学推理领域。

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [402] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LQR和RL的混合控制框架，用于解决基于RL的振动控制中的训练安全问题，实现了无模型控制并降低了探索风险。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的控制策略（如LQR）需要精确的系统模型，而基于RL的无模型控制在训练过程中存在潜在风险。本研究旨在解决RL在物理系统上训练时的安全问题，并消除对显式系统模型的依赖。

Method: 提出了一种混合控制框架，结合了LQR和RL控制器。LQR控制器基于随机选择的模型及其参数生成策略，RL控制器则从观察到的结构行为中学习策略。该框架在物理系统上进行训练，以实现真正的无模型控制。

Result: 与单独的RL方法相比，该混合方法在训练过程中引入了LQR的引导，降低了探索风险。即使LQR控制器基于不准确的模型，其性能也优于无控制场景。该方法成功解决了RL在物理系统上训练时的安全问题，是首个提供经验证解决方案的研究。

Conclusion: 所提出的混合LQR-RL控制框架能够安全有效地训练RL控制器，用于结构振动控制，无需依赖精确的系统模型，并解决了RL训练过程中的固有风险。

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [403] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: 本研究提出了一种信息论方法，用于识别和分析循环神经网络（RNN）中的信息中继节点，以提高模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解RNN的内部动态对于提高其可解释性和改进设计至关重要。

Method: 通过量化节点之间输入和输出向量的互信息来识别信息流的关键路径。

Result: 在合成和真实世界的时间序列分类任务中，研究了不同RNN架构（LSTM、GRU）的信息中继模式，并通过节点敲除实验验证了识别出的节点的功能重要性。

Conclusion: 该研究通过阐明特定节点如何影响整体网络行为，增强了对RNN驱动机制的理解，并为设计更鲁棒、更可解释的神经网络提供了工具。

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [404] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: NcPU是一个创新的PU学习框架，通过NoiSNCL损失和PLD方案解决了复杂数据集上的性能瓶颈，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法在复杂数据集上表现不佳，尤其是在没有辅助负样本或预估参数的情况下，主要瓶颈在于不可靠的监督学习下难以学习到区分性表示。

Method: 提出了一种名为NcPU的非对比PU学习框架，结合了NoiSNCL（一种噪声对鲁棒的监督非对比损失）和PLD（幻象标签去模糊）方案。NoiSNCL旨在对齐类内表示，即使在不可靠的监督下，PLD通过基于悔悟的标签更新提供保守的负监督。

Result: 实验表明，NoiSNCL能够使简单的PU方法达到具有竞争力的性能。NcPU在包括灾后建筑损伤测绘在内的各种数据集上，显著优于最先进的PU方法。

Conclusion: NcPU通过结合NoiSNCL和PLD，有效解决了PU学习中的表示学习挑战，在复杂数据集上取得了显著的性能提升，并显示出在实际应用中的潜力。

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [405] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 通过模拟人类的微小眼动（微眼动），提出了一种探测大型语言模型（LLMs）行为的新方法。


<details>
  <summary>Details</summary>
Motivation: 受到微眼动揭示人类感知隐藏动力学的启发，提出一种类似的探测大型语言模型（LLMs）的方法。

Method: 通过轻微的位置编码扰动来诱发潜在信号，以揭示模型的行为不端。

Result: 在事实性、安全性、毒性和后门攻击等多种场景下，该方法无需微调或任务特定监督即可检测到 LLMs 的故障。

Conclusion: 预训练的 LLMs 已经编码了内部证据来标记自身的故障，受微眼动启发的干预措施为检测和减轻不良行为提供了一条途径。

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [406] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV通过自适应量化和淘汰KV缓存中的注意力稀疏性来压缩KV缓存，以提高大型推理模型的长上下文生成能力，并在不损失准确性的情况下实现高达5.8倍的推理吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在生成长上下文时，KV缓存急剧增长，导致GPU内存不足，影响性能。

Method: ThinKV提出了一种基于注意力稀疏性的自适应KV缓存压缩框架，通过混合量化-淘汰策略，根据思考的重要性分配Token精度，并逐步淘汰来自不重要思考的Token。同时，设计了一种扩展PagedAttention的内核，以实现被淘汰Token内存槽的高效复用，消除内存压缩开销。

Result: 实验结果表明，ThinKV在DeepSeek-R1-Distill、GPT-OSS和NVIDIA AceReason模型以及数学和代码基准测试中， KV缓存使用量不到原始的5%，准确率几乎无损，并将推理吞吐量提高了5.8倍。

Conclusion: ThinKV是一种有效的KV缓存压缩框架，能够显著减少内存占用并提高长上下文生成任务的推理性能，同时保持高准确率。

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [407] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: 本研究提出了一个用于跨不同交叉口车辆延误估计的域适应（DA）框架，并引入了一个名为 GBBW 的新颖 DA 模型，通过加权源数据来提高模型在新交叉口数据上的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 准确估计车辆延误对于评估信号交叉口的性能和制定交通管理策略至关重要，但现有机器学习模型在不同交叉口数据上泛化能力不足。

Method: 提出一个域适应（DA）框架，将数据分为源域和目标域，提取交通特征，并使用目标域中的小部分标记数据进行微调。引入了名为 GBBW（Gradient Boosting with Balanced Weighting）的新型 DA 模型，通过重加权源数据来提高适应性。

Result: 在亚利桑那州皮马县 57 个交叉口的数据上进行测试，与八种最先进的机器学习回归模型和七种基于实例的 DA 方法进行比较，结果表明 GBBW 框架提供了更准确、更鲁棒的延误估计。

Conclusion: 所提出的 GBBW 域适应框架能够有效提高车辆延误估计的准确性和鲁棒性，有助于交通信号优化、拥堵管理和基于性能的规划，并促进机器学习在实际交通系统中的广泛应用。

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [408] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 本文对基于深度学习的3D医学图像重建方法进行了综述，重点介绍了点云、网格、形状感知和体积模型四种主要方法，并分析了它们在不同解剖结构中的技术、局限性和应用。


<details>
  <summary>Details</summary>
Motivation: 3D形状重建对于医学疾病诊断、治疗规划和计算建模至关重要，因此需要对相关方法进行综述。

Method: 本文回顾了四种主要的3D MRI重建方法：点云、网格、形状感知和体积模型，并分析了各自的技术、局限性和应用。

Result: 对当前最先进的技术、临床适用性、数据集、计算需求和评估指标进行了广泛的概述，并强调了多模态集成和跨模态框架等新兴研究方向。

Conclusion: 本文旨在为研究人员提供一个结构化的概述，以识别改进深度学习以实现更鲁棒、更通用和临床上更有影响力的解决方案的机会。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [409] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [410] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Montúfar*

Main category: cs.LG

TL;DR: 本论文研究在放宽训练数据和参数的各向同性假设的条件下，两层神经网络训练损失梯度中的低秩结构。


<details>
  <summary>Details</summary>
Motivation: 研究两层神经网络在非各向同性数据和参数下的训练梯度低秩结构。

Method: 分析了在放松各向同性假设、考虑 the spiked data model、以及分析均值场和神经网络核缩放的情况下，输入权重梯度的低秩性质，并识别出主导的秩一分量。

Result: 证明了输入权重梯度近似低秩，并被两个秩一分量主导：一个与数据的本体残差对齐，另一个与输入数据的秩一扰动对齐。论文还阐述了数据特性、缩放机制和激活函数如何影响这两种分量的平衡，并展示了正则化器如何选择性地调节这些分量。

Conclusion: 理论预测得到了合成数据和真实数据的实验结果的证实。

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [411] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: 现有本征维度估计（IDE）方法在复杂流形上的表现不佳，本研究提出了一个名为QuIIEst的新基准来评估这些方法，并发现现有方法在该基准上表现不佳，同时研究了Hofstadter's butterfly的IDE。


<details>
  <summary>Details</summary>
Motivation: 由于现有本征维度估计（IDE）方法的估计值差异很大，因此有必要在比现有基准更复杂的流形上对IDE方法进行基准测试。

Method: 提出了一种名为QuIIEst的量子启发的本征维度估计基准，该基准包含具有已知本征维度的拓扑上非平凡的无限族流形。该基准源于一种嵌入任意齐性空间的量子光学方法，同时允许曲率修改和添加噪声。对IDE方法进行了测试，并对Hofstadter's butterfly进行了IDE。

Result: 在QuIIEst流形上，IDE方法的准确性普遍低于在现有基准上的准确性。随着曲率的增加，性能下降很小。Hofstadter's butterfly的有效维度被确定。

Conclusion: QuIIEst基准揭示了现有IDE方法的局限性，并为未来研究提供了新的方向。

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [412] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*Sébastien Lachapelle*

Main category: cs.LG

TL;DR: LAPO框架下的潜在动作策略学习具有可辨识性。


<details>
  <summary>Details</summary>
Motivation: 研究最近引入的用于从视频数据中发现动作表示的潜在动作策略学习（LAPO）框架的可辨识性。

Method: 形式化描述了此类表示的必要条件、它们的统计优势以及潜在的不可辨识性来源，并证明了熵正则化的LAPO目标在特定条件下可以识别满足这些必要条件的动作表示。

Result: 证明了熵正则化的LAPO目标可以识别满足既定必要条件的动作表示。

Conclusion: 该分析解释了为什么离散动作表示在实践中表现良好。

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [413] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 该论文从信息论角度出发，提出了两种自监督表示学习（SSRL）的训练范式：Self-Distillation MI (SDMI) 和 Joint MI (JMI)，并解释了现有SSRL方法中的结构选择（如预测网络、停止梯度、统计正则化器）的理论依据。


<details>
  <summary>Details</summary>
Motivation: 现有自监督表示学习（SSRL）方法虽然在实践中表现优异，但其基本原理仍未被充分理解。特别是像预测网络、停止梯度操作和统计正则化器等结构元素，通常被视为经验驱动的补充，缺乏理论支撑。本文旨在从第一性原理出发，探讨SSRL的学习目标如何决定其优化策略和模型设计选择。

Method: 本文从变分互信息（MI）下界出发，推导了两种训练范式：Self-Distillation MI (SDMI) 和 Joint MI (JMI)。SDMI 自然地需要交替优化，因此停止梯度操作是理论上必需的；JMI 则可以通过对称结构进行联合优化，无需停止梯度。在SDMI范式中，预测网络成为MI目标的替代；在JMI范式中，统计正则化器成为MI目标的替代。

Result: 论文表明，许多现有的SSRL方法可以被视为SDMI或JMI范式的具体实例或近似。SDMI范式解释了预测网络的作用，而JMI范式解释了统计正则化器的作用，为现有SSRL方法的结构选择提供了理论解释，超越了单纯的启发式便利。

Conclusion: 本文提出了SDMI和JMI两种新的SSRL训练范式，并从信息论角度为预测网络、停止梯度和统计正则化器等结构组件的出现提供了理论依据，解释了它们在不同SSRL方法中的作用，并指出许多现有方法是这两种范式的特例或近似。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [414] [Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study](https://arxiv.org/abs/2510.02142)
*Lena Podina,Christina Humer,Alexandre Duval,Victor Schmidt,Ali Ramlaoui,Shahana Chatterjee,Yoshua Bengio,Alex Hernandez-Garcia,David Rolnick,Félix Therrien*

Main category: cs.LG

TL;DR: 通过利用基于机器学习的预测器，我们开发了一种名为 Catalyst GFlowNet 的生成模型，用于设计高效的析氢反应催化剂，并成功识别出铂作为最优催化剂，为发现新型催化剂提供了有前景的途径。


<details>
  <summary>Details</summary>
Motivation: 为了加速可再生能源的普及和确保能源供应稳定，需要高效且经济的储能技术。电催化剂在氢能储存（HES）中起到关键作用，但开发高性价cc比的催化剂仍然是一个挑战。

Method: 我们引入了 Catalyst GFlowNet，一个利用机器学习预测形成和吸附能的生成模型，来设计作为高效催化剂的晶体表面。

Result: 通过以析氢反应（HES的关键反应）为概念验证应用，我们成功地识别出铂是最高效的已知催化剂。

Conclusion: Catalyst GFlowNet 这一生成模型框架为加速寻找新型高效催化剂提供了一条有前景的途径，未来计划将其应用于析氧反应，并拓展到发现新材料的领域。

Abstract: Efficient and inexpensive energy storage is essential for accelerating the
adoption of renewable energy and ensuring a stable supply, despite fluctuations
in sources such as wind and solar. Electrocatalysts play a key role in hydrogen
energy storage (HES), allowing the energy to be stored as hydrogen. However,
the development of affordable and high-performance catalysts for this process
remains a significant challenge. We introduce Catalyst GFlowNet, a generative
model that leverages machine learning-based predictors of formation and
adsorption energy to design crystal surfaces that act as efficient catalysts.
We demonstrate the performance of the model through a proof-of-concept
application to the hydrogen evolution reaction, a key reaction in HES, for
which we successfully identified platinum as the most efficient known catalyst.
In future work, we aim to extend this approach to the oxygen evolution
reaction, where current optimal catalysts are expensive metal oxides, and open
the search space to discover new materials. This generative modeling framework
offers a promising pathway for accelerating the search for novel and efficient
catalysts.

</details>


### [415] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: Symmetry-aware ML methods assume transformed data is probable; this paper proposes a metric to quantify data anisotropy (symmetry-breaking) to evaluate this assumption. It shows theoretically and empirically that anisotropy can hinder invariant methods, suggesting a need to rethink symmetry biases in data.


<details>
  <summary>Details</summary>
Motivation: The assumption that transformed datapoints are highly probable under the test distribution, which underlies symmetry-aware ML methods like data augmentation and equivariant architectures, needs critical evaluation. Understanding when and why these methods work requires understanding data anisotropy.

Method: A metric to quantify anisotropy (symmetry-breaking) in a dataset is proposed, using a two-sample neural classifier test to distinguish between the original dataset and its randomly augmented version. This metric is validated on synthetic datasets and applied to benchmark point cloud datasets. Theoretical analysis is provided using invariant ridge regression in the infinite feature limit.

Result: The proposed metric reveals surprisingly high degrees of alignment in several benchmark point cloud datasets. Theoretical analysis shows that distributional symmetry-breaking can prevent invariant methods from performing optimally, even with invariant labels. Empirical results indicate that the effectiveness of equivariant methods is dataset-dependent, with some anisotropic datasets benefiting while others do not.

Conclusion: Understanding the effectiveness of symmetry-aware methods requires rethinking symmetry biases in the data, as distributional symmetry-breaking can significantly impact their performance. The proposed metric helps in evaluating the validity of applying these methods to specific datasets.

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [416] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: 本论文提出了一种名为加权行为克隆（Weighted BC）的离线强化学习方法，用于处理含有污染数据的安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 标准的行为克隆（BC）和离线强化学习（RL）方法在处理受污染（如对抗性攻击、系统错误或低质量样本）的数据集时，策略性能会下降。

Method: 加权行为克隆（Weighted BC）使用一个小的、经过验证的干净参考数据集，通过二元判别器估计轨迹级别的密度比。这些密度比被裁剪后用作BC目标中的权重，以优先考虑干净的专家行为，同时降低或剔除受污染的数据。

Result: 理论上，该方法保证能收敛到干净的专家策略，且具有与污染率无关的有限样本界限。实验评估表明，即使在污染率很高的情况下，加权行为克隆（Weighted BC）也能保持接近最优的性能，优于传统的BC、BCQ和BRAC等基线方法。

Conclusion: 加权行为克隆（Weighted BC）是一种处理受污染离线数据集的有效方法，能够鲁棒地学习到干净的专家策略。

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [417] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [418] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: 扩散模型学习选择性欠拟合经验分数，而不是普遍欠拟合，这对于生成新颖样本至关重要。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型在训练过程中实际学习的是哪个分数，以及为什么它们能够生成新颖样本，而不是仅仅复制训练数据。

Method: 提出“选择性欠拟合”的概念，即模型在某些区域更准确地拟合分数，而在其他区域欠拟合，并通过实证研究来验证这一观点。

Result: 发现选择性欠拟合是理解扩散模型泛化和生成性能的关键，并提出了新的可检验的见解。

Conclusion: 选择性欠拟合是理解和改进扩散模型生成能力的核心机制。

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [419] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 利用强化学习（RL）来对抗动态的信道选择和侦测阈值干扰。


<details>
  <summary>Details</summary>
Motivation: 研究如何缓解干扰者动态选择信道和侦测阈值来干扰通信的问题。

Method: 使用Q学习和深度Q网络（DQN）的强化学习方法，使通信方能够自适应调整传输功率、调制方式和信道选择，以规避干扰并优化吞吐量。

Result: 结果表明，强化学习能够快速适应动态频谱，并在信道和干扰策略发生变化时保持高传输速率。

Conclusion: 强化学习是一种有效的对抗动态干扰的策略。

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [420] [Transformers Discover Molecular Structure Without Graph Priors](https://arxiv.org/abs/2510.02259)
*Tobias Kreiman,Yutong Bai,Fadi Atieh,Elizabeth Weaver,Eric Qu,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: Transformer可以直接在笛卡尔坐标上进行训练，以实现有竞争力的分子能量和力预测，并且在扩展性方面优于GNN。


<details>
  <summary>Details</summary>
Motivation: 在分子机器学习中，GNN通常使用预定义的图，这可能限制其表现力和推理速度。本研究旨在探索纯粹的Transformer模型是否能在没有预定义图和物理先验的情况下，仅通过笛卡尔坐标来近似分子能量和力。

Method: 在OMol25数据集上，将Transformer模型与最先进的等变GNN在匹配的训练计算预算下进行比较，评估其在能量和力上的平均绝对误差。

Result: Transformer模型在能量和力预测方面达到了与最先进GNN相当的准确性，并且在训练过程中学会了物理上一致的模式（例如，注意力权重随原子间距离衰减）。Transformer模型还表现出良好的可扩展性，与在其他领域观察到的经验标度定律一致。

Conclusion: 研究结果表明，Transformer模型可以自适应地学习到GNN的许多有利特性，挑战了对硬编码图归纳偏置的必要性，并指出了分子建模中标准化、可扩展架构的可能性。

Abstract: Graph Neural Networks (GNNs) are the dominant architecture for molecular
machine learning, particularly for molecular property prediction and machine
learning interatomic potentials (MLIPs). GNNs perform message passing on
predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor
scheme. While this design aligns with the locality present in many molecular
tasks, a hard-coded graph can limit expressivity due to the fixed receptive
field and slows down inference with sparse graph operations. In this work, we
investigate whether pure, unmodified Transformers trained directly on Cartesian
coordinates$\unicode{x2013}$without predefined graphs or physical
priors$\unicode{x2013}$can approximate molecular energies and forces. As a
starting point for our analysis, we demonstrate how to train a Transformer to
competitive energy and force mean absolute errors under a matched training
compute budget, relative to a state-of-the-art equivariant GNN on the OMol25
dataset. We discover that the Transformer learns physically consistent
patterns$\unicode{x2013}$such as attention weights that decay inversely with
interatomic distance$\unicode{x2013}$and flexibly adapts them across different
molecular environments due to the absence of hard-coded biases. The use of a
standard Transformer also unlocks predictable improvements with respect to
scaling training resources, consistent with empirical scaling laws observed in
other domains. Our results demonstrate that many favorable properties of GNNs
can emerge adaptively in Transformers, challenging the necessity of hard-coded
graph inductive biases and pointing toward standardized, scalable architectures
for molecular modeling.

</details>


### [421] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: PRISM是一种轻量级的、模型无关的插入式再掩蔽方法，用于在推理时对掩蔽扩散模型进行自我修正，它通过学习每个令牌的质量分数来检测低质量令牌，并已在数独、文本和代码等领域得到了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的掩蔽扩散模型（MDMs）在自我修正能力方面有待提高，而现有的方法要么需要改动模型结构或训练过程，要么依赖于不精确的质量代理，这限制了它们的应用。

Method: PRISM通过定义一个自我修正损失函数，在不使用强化学习或验证器的情况下，学习每个令牌的质量分数。这些分数在MDM的同一前向传播过程中计算，用于检测低质量令牌。

Result: PRISM在数独、无条件文本生成（1.7亿参数）和代码生成（LLaDA 80亿参数）等多个领域和规模上提升了MDM的推理能力。

Conclusion: PRISM是一种有效且通用的方法，可以增强现有MDM的自我修正能力，而无需进行额外的模型修改或训练。

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [422] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: 通过引入基于潘多拉魔盒问题的算法，在LLM生成中实现了在保证质量的同时降低推理成本，能在平均减少15-35%的生成次数的情况下，获得与非自适应方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在生成质量和推理成本之间进行权衡，尤其是在使用多次生成的情况下。

Method: 提出了一种新的基于经典潘多拉魔盒问题的推理时间优化框架，开发了在不知道潜在奖励分布的情况下决定何时停止生成的算法，包括一种UCB风格的潘多拉魔盒算法，并适应于LLM设置，通过Bradley-Terry启发的转换来解决跨提示的奖励缩放问题，从而实现自适应推理时间优化。

Result: 在AlpacaFarm和HH-RLHF数据集上，使用多种LLM-奖励模型对进行实验，结果表明所提出的自适应策略在平均需要减少15-35%的生成次数的情况下，可以获得与非自适应的Best-of-N采样相当的性能。

Conclusion: 为最优停时理论和推理时间扩展之间搭建了原则性的桥梁，为LLM部署提供了理论性能界限和实际效率提升。

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [423] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 神经网络代理框架通过从笛卡尔坐标中直接学习集合变量（CV）并利用自动微分提供雅可比矩阵，解决了基于高斯过程回归（GPR）的自由能重构方法在计算复杂CV的雅可比矩阵时的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有的自由能重构方法（如GPR）在计算集合变量（CV）的雅可比矩阵时存在瓶颈，限制了复杂或机器学习CV的应用。本研究旨在提出一种新的框架来解决这个问题。

Method: 提出了一种神经网络代理框架，可以直接从笛卡尔坐标中学习CV，并使用自动微分来提供雅可比矩阵，从而绕过解析形式的计算。

Result: 在MgCl2离子对系统上，该方法对简单距离CV和复杂配位数CV都达到了很高的准确性。此外，雅可比矩阵的误差也呈现近乎高斯分布的特性，适合GPR流程。

Conclusion: 该框架使得基于梯度的自由能方法能够整合复杂的和机器学习的CV，从而扩展了生物化学和材料模拟的应用范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [424] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 通过在向量量化的自动编码器中结合低秩表示，提出了一种新的图像压缩和重建框架，以解决神经压缩中解码器的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前的神经压缩方法虽然压缩率高，但由于基于卷积的解码器计算复杂且成本高，限制了其应用。

Method: 在自动编码器中结合向量量化和低秩表示，并在学习到的图像潜在表示上执行一系列低秩操作来进行数据重建。

Result: 该方法显著降低了神经压缩/重建解码阶段的计算开销，同时保持了高保真度的图像输出。

Conclusion: 该框架通过引入低秩表示，有效解决了神经压缩中解码器的计算瓶颈问题，实现了高效且高质量的图像重建。

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [425] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: Edge AI将智能嵌入网络边缘设备，实现实时处理、提高隐私和降低延迟。本综述通过部署位置、TinyML和联邦学习等处理能力、应用领域和硬件类型等多维度分类，系统地研究了Edge AI的演变、当前格局和未来方向。


<details>
  <summary>Details</summary>
Motivation: Edge AI将智能嵌入网络边缘设备，实现实时处理、提高隐私和降低延迟。

Method: 通过多维度分类（包括部署位置、处理能力（如TinyML和联邦学习）、应用领域和硬件类型）对Edge AI的演变、当前格局和未来方向进行了系统性审查，遵循PRISMA指南。

Result: 该分析追溯了从早期内容分发网络和雾计算到现代设备上智能的领域。探讨了专门的硬件加速器、优化的软件和通信协议等核心使能技术。批判性地评估了资源限制、安全、模型管理、功耗和连接性等挑战。

Conclusion: 重点介绍了神经形态硬件、持续学习算法、边缘-云协作和可信度集成等方面的新兴机遇，为研究人员和从业者提供了全面的框架。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [426] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: DP-SGD的梯度裁剪会损害少数群体的模型公平性，我们提出了SoftAdaClip，一种使用平滑变换替代硬裁剪的方法，显著减少了亚群差异。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）在保护敏感数据方面存在模型性能和公平性（尤其对少数群体）下降的问题，其主要原因是DP-SGD中的梯度裁剪会不成比例地抑制少数群体的学习信号。

Method: 提出SoftAdaClip，一种差分隐私训练方法，用基于tanh的平滑变换替代硬裁剪，以保留相对梯度幅度同时控制敏感性。

Result: 在MIMIC-III、GOSSIS-eICU和Adult Income等数据集上评估SoftAdaClip，结果显示与DP-SGD相比，亚群差异最多减少87%；与Adaptive-DPSGD相比，最多减少48%，且差异减少具有统计显著性。

Conclusion: 将平滑变换与自适应机制相结合对于实现公平和私有的模型训练至关重要。

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [427] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: 提出了一种名为局部线性注意力（LLA）的新型注意力机制，该机制源于非参数统计和测试时回归，在关联记忆方面具有理论优势，并通过内存高效的基元和FlashLLA算法解决了计算挑战，在各种任务中表现出卓越的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在多个领域取得了巨大成功，但对更具表现力且有理论依据的注意力机制的研究（即使计算成本更高）相对不足。本文旨在填补这一空白，提出LLA。

Method: LLA是一种源于非参数统计和测试时回归的新型注意力机制。通过偏差-方差权衡分析，证明了LLA相比于线性注意力和Softmax注意力在关联记忆方面的理论优势。为了解决计算复杂性问题，提出了两个内存高效的基元，并引入了FlashLLA——一种硬件高效的、分块的算法，以实现大规模并行计算。此外，还实现了一个定制的推理内核以减少内存开销。

Result: LLA在测试时回归、上下文回归、关联回忆和状态跟踪任务中得到了实证验证。实验结果表明，LLA能有效适应非平稳性，在测试时训练和上下文学习方面优于强大的基线模型，并为LLA在大规模模型中的可扩展性和适用性提供了有力的证据。

Conclusion: LLA作为一种基于理论的新型注意力机制，通过改进的计算方法和实证验证，在处理各种任务时展现出优于现有方法的性能和可扩展性，尤其是在非平稳和大规模模型场景下。

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [428] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: SCOPED是一种用于扩散模型的快速、通用的OOD检测方法，通过结合分数函数的雅可比行列式迹和平方范数来计算测试统计量，并使用核密度估计来估计SCOPED分数的分布内密度，从而实现高效的OOD检测。


<details>
  <summary>Details</summary>
Motivation: OOD检测对于机器学习系统在视觉、机器人、强化学习等领域的可靠部署至关重要。

Method: SCOPED结合了分数函数的雅可比行列式迹和平方范数，并利用核密度估计来估计OOD分数的分布内密度，从而实现高效的OOD检测。

Result: 在四个视觉基准测试中，SCOPED取得了具有竞争力的或最先进的精度-召回率分数，并且计算成本低。该方法还可以推广到机器人控制任务，识别跨奖励函数和训练机制的分布变化。

Conclusion: SCOPED是一种实用且高效的OOD检测方法，可用于各种实际应用，包括视觉感知伪影、自回归模型中的异常检测、强化学习中的探索以及无监督训练的数据集策划。

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [429] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: MBPO 在 DMC 中表现不佳，原因在于尺度不匹配和目标表示不佳，改进后可超越 SAC。


<details>
  <summary>Details</summary>
Motivation: 研究数据驱动的动态学习方法在强化学习中的作用，特别是 MBPO 在不同环境中的表现差异，以及其失败的原因。

Method: 通过改进 MBPO 中的尺度不匹配和目标表示问题，来提升其在 DMC 中的性能。

Result: 改进后的 MBPO 在七个 DMC 任务中的五个任务上超越了 SAC，同时保持了在 OpenAI Gym 中的表现。

Conclusion: 环境选择对算法泛化能力有重要影响，应建立将 MDP 任务和环境结构与算法失败模式联系起来的分类体系，并寻求统一解决方案，以明确算法泛化的条件。

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [430] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [431] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: 提出一种新的元RLVR算法LSPO，通过动态选择训练数据来提高LLM在推理任务上的学习效率。


<details>
  <summary>Details</summary>
Motivation: 受 LLM 过度思考研究的启发，旨在改进 RLVR 算法以提高 LLM 在推理任务上的表现。

Method: 提出一种名为LSPO的新型元RLVR算法，该算法根据平均响应长度动态选择每一步的训练数据。

Result: LSPO在多个基础模型和数据集上进行评估，证明其能持续提高学习效果。此外，还进行了详细的消融研究，探讨了将长度信号纳入动态采样的替代方法。

Conclusion: LSPO 算法能够有效提高 LLM 在推理任务上的学习效率，并且通过消融研究为未来的研究方向提供了见解。

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [432] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 使用稳定性-塑性原则来指导离线到在线强化学习中的在线微调。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习（RL）在利用离线数据集进行预训练和在线交互进行微调方面已成为一种实用的范式。然而，其经验行为高度不一致：在线微调的设计选择在一个环境中可能效果很好，但在另一个环境中可能完全失败。

Method: 提出稳定性-塑性原则，该原则可以通过在在线微调过程中保留预训练策略或离线数据集的知识（以表现更好者为准），同时保持足够的塑性来解释这种不一致性。这一视角确定了在线微调的三个机制，每个机制都需要不同的稳定性特性。

Result: 通过大规模的经验研究验证了这个框架，发现在 63 个案例中的 45 个案例中，结果与该框架的预测高度一致。

Conclusion: 这项工作提供了一个原则性框架，用于根据离线数据集和预训练策略的相对性能来指导离线到在线 RL 中的设计选择。

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [433] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: 该研究提出了一种利用大型语言模型（LLM）作为代理模型来解决高维黑盒优化问题（特别是具有不规则变量的问题）的新方法，称为 LoRA-VBLL，并通过集成多个模型（ENS-LoRA-VBLL）进一步优化。


<details>
  <summary>Details</summary>
Motivation: 高维、具有不规则变量（如分类、序数变量）的黑盒优化问题难以用传统的基于高斯过程（GP）的贝叶斯优化（BO）解决，因此需要新的代理模型。

Method: 提出了一种名为 LoRA-VBLL 的方法，该方法使用 LLM 作为代理模型，并通过低秩适应（LoRA）和变分贝叶斯最后一层（VBLL）框架对其进行微调。为了自动化超参数选择，还设计了一个加权集成（ENS）的 LoRA-VBLL 模型，支持递归更新。

Result: 所提出的 LoRA-VBLL 和 ENS-LoRA-VBLL 方法在高维基准测试和真实的分子优化任务上都表现出了有竞争力的性能。

Conclusion: LoRA-VBLL 和 ENS-LoRA-VBLL 是处理高维黑盒优化问题（尤其是那些具有不规则变量的问题）的有效且计算效率高的方法。

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [434] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: PEL-NAS通过分区搜索空间、架构提示协同进化和LLM驱动的方法，在降低搜索成本的同时，生成高精度、低延迟的神经网络，解决了传统HW-NAS方法中的探索偏见问题。


<details>
  <summary>Details</summary>
Motivation: 传统硬件感知神经架构搜索（HW-NAS）方法需要联合优化精度和延迟，但计算成本高昂。基于大语言模型（LLM）的方法虽然能提供快速反馈，但存在探索偏见， LLM倾向于重复生成有限搜索空间内的网络设计，无法发现不同延迟范围内的最优架构。

Method: PEL-NAS包含三个关键组件：1）一个复杂性驱动的分区引擎，通过按复杂性划分搜索空间来强制执行多样性并减轻探索偏见；2）一个LLM驱动的架构提示协同进化算子，LLM首先根据上一轮结果更新设计启发式知识库，然后对包含知识库的提示下的架构进行引导式进化；3）一个零成本预测器，避免从头开始训练大量候选模型。

Result: 与基线方法相比，PEL-NAS在HW-NAS-Bench上实现了更高的超体积（HV）和更低的IGD，在精度相似的情况下延迟降低了高达54%。

Conclusion: PEL-NAS通过其创新的方法，显著降低了HW-NAS的搜索成本（从几天减少到几分钟），同时在精度和延迟方面取得了优越的性能。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [435] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 对抗性攻击的迁移性取决于攻击所处的空间（数据空间或表示空间），数据空间攻击可迁移，表示空间攻击不可迁移，除非进行几何对齐。


<details>
  <summary>Details</summary>
Motivation: 解释为何图像越狱攻击在视觉语言模型（VLMs）之间迁移性不如图像分类器或语言模型中的攻击，并提出关于攻击迁移性的根本性区别。

Method: 1. 在两个网络通过不同表示计算相同输入-输出映射的简单场景中，从数学上证明了数据空间攻击和表示空间攻击在迁移性上的区别。2. 针对图像分类器构建了表示空间攻击，其成功率与已知的数据空间攻击相当，但无法迁移。3. 针对语言模型（LMs）构建了表示空间攻击，成功越狱了被攻击的模型，但无法迁移。4. 针对VLMs构建了数据空间攻击，该攻击成功迁移到新的VLMs。同时证明了当VLMs的潜在几何在投影后空间足够对齐时，表示空间攻击也可以迁移。

Result: 数据空间攻击（如针对VLMs的越狱攻击）可以成功迁移，而表示空间攻击（如针对图像分类器和LMs的攻击）则不能迁移，除非在特定条件下（如VLMs的几何对齐）。

Conclusion: 对抗性迁移性并非所有攻击的固有属性，而是取决于其操作域（共享的数据空间或模型独特的表示空间）。这一发现对于构建更鲁棒的模型至关重要。

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [436] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: 该研究提出了一种结合离线保守Q学习和新颖循环建模的方法，用于学习脓毒症休克患者的双血管升压药滴定和控制策略，通过设计动作空间来提高药物剂量的可解释性和临床接受度，并在eICU和MIMIC数据集上取得了超过15%的生存率改善。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持系统中强化学习在药物剂量调整方面面临着实际应用中的质疑，本研究旨在解决这一挑战，为脓毒症休克患者的双血管升压药滴定和控制策略提供一种端到端的方法。

Method: 采用动作空间设计来适应离散、连续和定向的给药策略，并结合离线保守Q学习和新颖的循环建模（在回放缓冲区中）来捕捉ICU时间序列数据的时态依赖性。

Result: 在eICU和MIMIC数据集上的实证结果表明，所设计的动作空间在提高药物剂量的可解释性和临床接受度的同时，能够保持其有效性。与对照组相比，所提出的方法在生存率改善概率方面提高了15%以上，并且符合既定的临床协议。

Conclusion: 动作空间的设计深刻影响了学习到的行为策略，并且所提出的方法不仅提高了患者的生存率，而且还符合现有的临床实践指南。

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [437] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: KGFMs的确定性等变性限制了其区分结构相似但语义不同的关系的能力。我们提出了概率节点-关系等变性，并基于此提出了Flock模型，该模型通过采样随机游走、编码、嵌入和聚合来解决此问题，并在Petals数据集和54个KGs上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的确定性等变性限制了知识图谱基础模型（KGFMs）的表达能力，使其无法区分结构相似但语义不同的关系。

Method: 提出概率节点-关系等变性，并在此基础上构建Flock模型，通过迭代采样随机游走，使用记录协议将其编码为序列，然后用序列模型嵌入，最后通过学习到的池化聚合节点和关系的表示。

Result: Flock完美解决了新的诊断数据集Petals，并在54个不同领域的实体和关系预测任务上取得了最先进的性能。

Conclusion: Flock模型通过概率节点-关系等变性克服了传统KGFMs的局限性，并在各种知识图谱基准测试中表现出色。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [438] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [439] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX是一个利用时间序列基础模型（TSFM）的开源工具，用于计算脱碳任务，如碳强度预测和插补。它克服了现有工具对特定电网数据的依赖，实现了零样本预测的全球覆盖，并提供了不确定性估计，在214个电网上的平均MAPE为15.82%，在13个基准电网上平均MAPE为9.59%，预测区间覆盖率为95%，最长可预测21天。


<details>
  <summary>Details</summary>
Motivation: 现有碳排放计算工具依赖特定电网数据，难以实现全球覆盖，且缺乏不确定性估计，限制了其在碳感知应用中的可靠性。

Method: 利用时间序列基础模型（TSFM）的通用性，开发了一个名为CarbonX的开源工具，仅需历史碳强度数据即可进行碳强度预测和插补等任务。

Result: CarbonX在214个电网的零样本预测中达到15.82%的MAPE，在13个基准电网上的平均MAPE为9.59%，尾部预测MAPE为16.54%，提供95%覆盖率的预测区间，最长可预测21天。在插补任务中，微调后的CarbonX比统计基线提高了1.2-3.9倍。

Conclusion: CarbonX是一个实用的全球性脱碳工具，可以轻松应用于数据有限的任何电网，并提供强大的性能，包括准确的碳强度预测和不确定性估计。

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [440] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 该研究提出了一种名为预测偏好学习（PPL）的方法，通过利用人类干预中的隐式偏好信号来预测未来轨迹，从而解决现有交互式模仿学习方法仅关注当前状态修正而忽略未来状态安全性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习方法主要关注修正当前状态下的 agent 行为错误，但未能调整未来状态下的行为，而未来状态可能存在更危险的隐患。

Method: PPL 的核心思想是将每次人类干预引导至 L 个未来时间步（称为偏好视界），并假设在偏好视界内 agent 遵循相同的动作且人类进行相同的干预。通过对这些未来状态应用偏好优化，将专家的修正传播到 agent 预期探索的安全关键区域。

Result: 通过在自动驾驶和机器人操作基准上的实验评估，PPL 在提高学习效率和减少所需人类演示次数方面表现出有效性和通用性。理论分析表明，选择合适的偏好视界 L 可以在风险状态的覆盖范围和标签的正确性之间取得平衡，从而约束算法的最优性差距。

Conclusion: PPL 通过将人类干预的偏好信号外推到未来状态，有效地将专家的修正传播到安全关键区域，从而显著提高了学习效率并减少了对人类演示的需求。理论分析为选择偏好视界提供了指导。

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [441] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: Binarized neural networks (BNNs) are verified using integer programming (IP). Existing IP formulations suffer from large integrality gaps. This paper introduces a new linear objective for multi-class settings and valid inequalities exploiting the BNN's recursive structure, enabling verification against larger perturbations.


<details>
  <summary>Details</summary>
Motivation: The verification problem for BNNs, which checks for misclassification due to small input perturbations, is crucial for measuring robustness. This problem can be formulated as an integer programming (IP) problem, but existing formulations are often difficult to solve due to large integrality gaps.

Method: This paper proposes two techniques to improve the IP formulation for BNN verification. First, a new method is introduced to obtain a linear objective function suitable for multi-class classification. Second, a novel technique is presented for generating valid inequalities that leverage the recursive structure inherent in BNNs.

Result: The proposed techniques allow for the verification of BNNs against a wider range of input perturbations compared to existing IP-based methods, all within a constrained timeframe.

Conclusion: The improved IP formulation, incorporating a linear objective for multi-class settings and valid inequalities derived from the BNN's recursive structure, significantly enhances the efficiency and effectiveness of BNN verification.

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [442] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: LLMs在计算化学中表现出循环一致性问题， RTRL框架通过引入奖励信号来提高模型的一致性，并能在有监督、无监督和合成数据上提升模型性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在计算化学中存在循环一致性问题，即模型无法从自身生成的文本准确重建原始分子结构，这表明模型只掌握了单向记忆而非灵活的知识。提高循环一致性与模型在主要任务上的性能强相关，因此应将一致性作为模型改进的目标。

Method: 提出了一种名为循环一致性强化学习（RTRL）的新框架，该框架将循环转换的成功作为奖励信号来训练模型，以提高其一致性。此外，还提出了一种迭代变体，其中正向和反向映射在自学循环中交替训练，这种方法数据效率高，并且在化学领域大量的无标签数据上效果显著。

Result: 实验证明，RTRL在有监督、无监督和合成数据环境中，显著提高了模型性能和一致性，优于强大的基线模型。

Conclusion: 循环一致性不仅是一个理想的属性，而且是一个可训练的目标，为构建更强大、更可靠的基础模型提供了新的途径。

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [443] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: LLM 提示防御机制存在漏洞，易受利用资源不对称的新型攻击；建议防御策略从拦截输入转向防止输出。


<details>
  <summary>Details</summary>
Motivation: 提示防御作为一种轻量级机制，被广泛用于过滤 LLM 的恶意查询，但其有效性受到质疑。

Method: 提出了一种利用提示防御与主 LLM 之间资源不对称的新型攻击方法，该方法能解码主 LLM 但防御机制无法解码越狱提示。

Result: 该攻击方法成功绕过了 Google Gemini、DeepSeek Chat、Grok 和 Mistral Le Chat 等生产模型的提示防御，并保持了响应质量。此外，还发现了版权数据提取、训练数据提取和恶意响应泄露等其他对齐问题。

Conclusion: 现有的轻量级提示防御机制存在固有攻击面，防御重点应从拦截恶意输入转向防止生成恶意输出。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [444] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: 6G网络融合AI/ML，提出Python算法到GPU的编译框架，并用CNN实现信道估计，验证了其在数字孪生和实时测试中的有效性。


<details>
  <summary>Details</summary>
Motivation: 6G需要将数字信号处理（DSP）和机器学习（ML）无缝集成到蜂窝网络软件栈中，使网络生命周期更接近AI系统。

Method: 提出一个健壮的框架，将Python算法编译成可在GPU上运行的二进制文件（blobs），以实现效率、灵活性和高性能。

Result: 成功地在数字孪生和实时测试环境中，通过一个在Python中训练的卷积神经网络（CNN）实现了PUSCH接收器中的信道估计功能。

Conclusion: 所提出的基于NVIDIA AI Aerial平台的框架为将AI/ML模型可扩展地集成到下一代蜂窝系统中奠定了基础，是实现原生智能6G网络的关键。

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [445] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: TSci是一个首创的、由LLM驱动的智能体框架，用于通用的时间序列预测，它通过自动化数据预处理、模型选择和集成策略来简化工作流程，并在八个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的时间序列预测领域需要一个通用的、不依赖领域知识的框架，以最大限度地减少人工干预，因为现有的模型通常泛化能力较差，并且在实际应用中，数据预处理、验证和集成等步骤需要大量的人工劳动。

Method: TSci框架包含四个专门的智能体：Curator负责进行由LLM驱动的数据诊断和预处理；Planner利用多模态诊断和自我规划来选择合适的模型；Forecaster负责模型拟合、验证、模型选择和集成策略的确定；Reporter负责生成透明的自然语言报告，总结整个预测过程。

Result: 在八个公开基准测试上，TSci的预测误差平均分别比统计模型和基于LLM的模型低10.4%和38.2%，显示出其优越的预测性能。

Conclusion: TSci通过提供透明的自然语言解释和全面的报告，将时间序列预测工作流转变为一个白盒系统，提高了可解释性和可扩展性，并取得了优于现有基线方法的性能。

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [446] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: 本研究提出了一个名为“可执行反事实”的新框架，用于评估和改进大型语言模型（LLM）的反事实推理能力，该框架通过代码和数学问题来操作化因果推理，并包含推理的三个步骤：溯因、干预和预测。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM反事实推理评估方法常常忽略溯因步骤，导致对LLM能力的过高估计。本研究旨在通过一个包含溯因、干预和预测三个完整步骤的新框架来解决这一问题，以更准确地评估LLM的因果理解能力。

Method: 研究引入了一个名为“可执行反事实”的新框架，该框架利用代码和数学问题来操作化因果推理。该框架明确要求完成反事实推理的三个步骤（溯因、干预、预测），并能够生成不同难度的合成数据，为评估和改进LLM的推理能力提供了新的途径。

Result: 研究结果显示，在先进模型（如o4-mini和Claude-4-Sonnet）上，从仅包含干预推理到包含完整反事实推理（溯因、干预、预测）的准确率下降了25%-40%。通过在反事实代码问题上进行监督微调，可以提高模型在同类任务上的表现，但对模型在不同领域（如反事实数学应用题）上的表现却有负面影响。相比之下，强化学习（RL）能够诱导核心认知行为，并泛化到新的领域，显著提高了模型在代码和数学问题上的表现（分别提高了1.5倍-2倍）。

Conclusion: 强化学习在提高LLM的反事实推理能力方面展现出巨大潜力，其能够诱导核心认知行为并泛化到新的领域，优于监督微调方法。

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [447] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: MIRA是一种无需训练、在推理时即可实现的对齐方法，通过引入图像空间约束来解决扩散模型在生成图像时出现的奖励操纵问题，从而在提高图像评分的同时保持与提示的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本的扩散模型在生成图像时，往往无法满足用户对图像美学分数等标量奖励的具体要求。虽然可以通过微调来解决这个问题，但这种方法计算成本很高。而最近出现的通过噪声优化的推理时对齐方法虽然效率更高，但容易出现奖励操纵问题，即生成的图像虽然得分很高，但却严重偏离了原始提示。因此，有必要开发一种既能提高奖励又能保持提示一致性的方法。

Method: MIRA（Mitigating Reward Hacking）提出了一种在推理时实现的、无需训练的对齐方法。该方法引入了一个图像空间的、基于分数的KL散度替代项，通过冻结的骨干网络对采样轨迹进行正则化，从而约束输出分布，使得在奖励增加的同时不会发生分布外漂移（即奖励操纵）。该方法利用扩散分数推导出了KL散度的可处理近似。此外，还提出了MIRA-DPO，将偏好优化映射到推理时，并使用冻结的骨干网络，从而可以将MIRA扩展到非可微奖励，且无需进行微调。

Result: 在SDv1.5和SDXL模型上，针对多种奖励（包括美学分数、HPSv2、PickScore）以及公开数据集（如Animal-Animal、HPDv2），MIRA相对于现有的强力基线方法，胜率超过60%，同时保持了提示的准确性。通过机制图可以看到，MIRA在增加奖励的同时，漂移几乎为零，而DNO（Diffusion-based Noise Optimization）的漂移则随着计算量的增加而增大。MIRA-DPO成功地将偏好优化扩展到了推理时。

Conclusion: MIRA通过引入图像空间约束，有效解决了扩散模型在推理时对齐中存在的奖励操纵问题，实现了在不牺牲提示准确性的前提下提高生成图像的奖励得分。MIRA-DPO进一步将此方法扩展到了非可微奖励的场景，为扩散模型的应用提供了更灵活的解决方案。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [448] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: RLHF中的KL散度损失在训练中起着稳定作用，但其实现方式存在争议。本文提出了一个统一框架，连接了“KL散度作为奖励系数”和“KL散度作为损失函数”这两种实现方式，并证明了它们之间的等价性。研究发现，PPO中使用的“k1作为奖励系数”是反向KL（RKL）正则化的正确损失函数，而GRPO中使用的“k3作为损失函数”只是一个近似。此外，本文还指出了离策略实现中存在的偏差，并提出了修正方法，为正确实现KL正则化提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: RLHF中KL散度损失的实现方式存在争议，可能导致训练不稳定和效果不佳。需要一个统一的框架来分析和指导KL散度损失的正确实现。

Method: 提出一个统一框架，连接“KL散度作为奖励系数”和“KL散度作为损失函数”两种实现方式，并通过数学推导证明它们之间的等价性。分析不同k_n系数的实现方式，并证明其理论合理性。提出对离策略实现中偏差的修正方法。

Result: 证明了“k_n作为奖励系数”和“k_n作为损失函数”之间的等价性。证明了PPO中的“k1作为奖励系数”是RKL正则化的原则性损失函数。证明了在on-policy条件下，“k2作为损失函数”等价于“k1作为奖励系数”。证明了GRPO中的“k3作为损失函数”是原则性损失函数的近似。指出了off-policy实现中存在的偏差，并提出了修正方法。

Conclusion: 提出了一个统一的框架，为正确选择和实现KL正则化提供了理论依据，有助于构建更鲁棒、更有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [449] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: IBCD是一种利用干预数据的经验贝叶斯因果发现框架，能够提高大规模任务的性能并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 高通量基因扰动筛选的进展启发了利用干预数据来改进因果模型识别的方法，但现有方法在大规模任务上性能较差且无法量化不确定性。

Method: IBCD通过对总因果效应矩阵（可近似为矩阵正态分布）进行建模，而不是对完整数据矩阵进行建模。该方法在边上放置了spike-and-slab马蹄先验，并从观测数据中学习无标度和Erdos-Renyi结构的独立数据驱动权重，将每条边视为潜在变量以实现不确定性感知推理。

Result: 通过广泛的模拟，IBCD与现有基线相比，在结构恢复方面表现更优。将IBCD应用于CRISPR扰动（Perturb-seq）数据（涉及521个基因），结果表明边的后验包含概率能够识别出稳健的图结构。

Conclusion: IBCD通过对总因果效应矩阵进行建模，并使用不确定性感知推理，在大规模因果发现任务中优于现有方法，并能有效识别稳健的因果图结构。

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [450] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [451] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: SPAMP是一个自适应梯度塑造框架，通过动态调整阈值来改善深度网络的稳定性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统的梯度裁剪方法虽然稳定，但缺乏灵活性，并且忽略了梯度分布的动态变化。

Method: SPAMP通过跟踪局部梯度统计数据，动态估计阈值，并应用基于功率的变换来平滑地调整每层的梯度，从而实现可微分的梯度塑造。

Result: 在图像和语言任务上的实验表明，SPAMP相比现有方法能提高稳定性、收敛性和鲁棒性。

Conclusion: SPAMP提供了一种比硬性阈值更灵活、更原则性的梯度调整方法，可以看作是梯度裁剪和预热的统一视角。

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [452] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: TRAAC是一种新的RL方法，通过自适应地调整推理步骤来解决模型在处理不同难度任务时的“推理不足”或“过度推理”问题，从而在提高准确性的同时减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有的思考模型在处理复杂推理任务时，通过增加测试时间的计算量来提升性能，但未能根据任务难度有效分配计算资源。这会导致在简单任务上“过度推理”或在复杂任务上“推理不足”，即“适应性不足”的问题。

Method: 提出TRAAC（Think Right with Adaptive, Attentive Compression）方法，这是一种在线的、训练后RL方法。它利用模型在长推理轨迹中的自注意力机制来识别重要步骤并修剪冗余步骤。同时，TRAAC能够估计任务难度并将其纳入训练奖励，从而学会根据任务难度分配推理预算。

Result: TRAAC在多个任务（AIME, AMC, GPQA-D, BBEH）上，相较于基础模型，准确率平均提高8.4%，推理长度减少36.8%。与现有的RL基线方法相比，准确率提高7.9%，推理长度减少29.4%。TRAAC在数学数据集上训练后，在非数学数据集（GPQA-D, BBEH, OptimalThinkingBench）上也表现出良好的泛化能力，在准确性和效率上均有提升。分析表明，TRAAC能够根据难度进行精细的推理预算调整，并且任务难度校准与基于注意力的压缩相结合是跨任务收益的关键。

Conclusion: TRAAC通过自适应地调整推理步骤，有效解决了模型在处理不同难度任务时的“适应性不足”问题，实现了在提高准确性的同时减少计算量，并在多种任务上展现出优越的性能和良好的泛化能力。

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [453] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: NoRo框架通过构建对比学习的样本对来增强语音特征的噪声鲁棒性，进而提高UPDRS预测的准确性，并提出了一种可定制的噪声注入评估方法。


<details>
  <summary>Details</summary>
Motivation: PD远程监控虽然提高了UPDRS评分的可及性，但会受到患者、环境和数据传输噪声的干扰，增加了预测误差。本研究旨在解决这一问题。

Method: NoRo框架首先将语音特征分组构建对比学习的样本对，然后利用这些样本对训练多层感知机编码器生成噪声鲁棒的特征，最后将这些特征与原始特征连接作为增强特征输入UPDRS预测模型。

Result: NoRo框架能够有效提高UPDRS预测模型在不同噪声环境下的预测准确性，并验证了其在多种下游预测模型上的有效性。

Conclusion: NoRo框架能够有效提高UPDRS预测的噪声鲁棒性，并且通过引入新颖的评估方法和可定制的噪声注入模块，为PD远程监控的准确性提供了保障。

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [454] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: 变分自编码器中的后验坍塌是一种由数据结构和模型超参数共同控制的相变现象，存在一个临界超参数阈值，该阈值区分了有意义的潜在推理和坍塌。


<details>
  <summary>Details</summary>
Motivation: 从统计物理学的角度研究变分自编码器（VAEs）中的后验坍塌现象。

Method: 通过分析与后验坍塌相关的平凡解的稳定性来识别一个临界的超参数阈值，该阈值由近似后验和先验分布之间的KL散度的不连续性表征。

Result: 在合成和真实世界的数据集上验证了相变的存在，并证实了后验坍塌是数据结构和变分约束相互作用产生的相变现象，而不仅仅是优化失败。

Conclusion: 后验坍塌是一种由数据结构和模型超参数共同控制的相变现象，这种观点为理解深度生成模型的训练能力和表示能力提供了新的视角。

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [455] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: SFT得分不能预测RL增益，泛化损失和Pass@large k是更好的代理指标。


<details>
  <summary>Details</summary>
Motivation: SFT得分不能可靠地预测RL增益，甚至可能导致RL表现下降。

Method: 通过大量实验，研究了SFT得分与RL表现之间的关系，并提出了泛化损失和Pass@large k作为预测RL结果的代理指标。

Result: SFT得分与RL增益无显著相关性，泛化损失和Pass@large k可以更准确地预测RL结果，将预测精度提高了约0.5（2倍）。

Conclusion: 泛化损失和Pass@large k是评估LLM推理能力和预测RL训练效果的有效指标，应优先考虑。

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [456] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 综合来看，本文通过大规模实验发现，在LLM预训练中，单独使用改写后的合成数据并不能加速训练，但与2/3的自然网络文本混合使用时，可将达到相同验证损失的速度提高5-10倍。单独使用课本风格的合成数据在许多下游领域会显著增加损失，尤其是在数据预算较小的情况下。改写后的合成数据在混合数据中约占30%时效果最佳，而生成模型的大小并不一定需要超过80亿参数。这些结果揭示了合成数据在预训练中的条件性益处，并为其实际应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 高质量的训练数据对大型语言模型（LLM）的扩展至关重要，但高质量数据供应有限。合成数据技术为克服这些限制提供了一种潜在的途径。

Method: 本文采用统一的协议和扩展定律，进行了大规模的实证研究（>1000个LLM，>100k GPU小时），比较了天然网络数据、多种合成数据类型（改写文本、生成的课本）以及它们的混合数据。

Result: 单独使用改写后的合成数据进行预训练，其速度并不比使用天然网络文本快；然而，当混合2/3的天然网络文本和1/3的改写后的合成数据时，可以在更大的数据预算下将速度提高5-10倍（以达到相同的验证损失）。单独使用课本风格的合成数据进行预训练，在许多下游领域会导致显著更高的损失，尤其是在较小的数据预算下。对于改写后的合成数据，其在训练数据混合物中的“良好”比例取决于模型大小和数据预算，经验上收敛于约30%。生成器模型的大小并不一定能产生比约80亿参数模型更好的预训练数据。这些结果为在大型合成数据上进行大规模单轮（n=1）模型训练时出现的“模型崩溃”提供了混合证据——改写后的合成数据在可预见的规模上没有表现出性能下降，而使用课本风格纯生成合成数据的混合物则显示出“模型崩溃”的预测模式。

Conclusion: 本文揭开了预训练中合成数据的神秘面纱，验证了其条件性益处，并提供了实用的指导。

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


### [457] [CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning](https://arxiv.org/abs/2510.01634)
*Ryan Y. Lin,Siddhartha Ojha,Nicholas Bai*

Main category: cs.LG

TL;DR: CAT通过动态学习跨三种几何注意力分支的每token路由，实现了对具有混合几何特性的数据的自适应几何专业化，在知识图谱补全任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在处理具有非欧几里得结构的数据时效果有限，而早期的扩展（如双曲和球面空间）需要预先指定单一的几何空间，缺乏灵活性。因此，需要一种能够处理具有混合几何特性的数据的模型。

Method: 提出了一种新颖的架构——Curvature-Adaptive Transformer (CAT)，它使用一个轻量级的、可微的门控机制，动态地学习跨三种几何注意力分支（欧几里得、双曲、球面）的每token路由。这使得模型能够根据token的局部关系结构自适应地选择最合适的几何空间。

Result: 在知识图谱补全基准测试（FB15k-237, WN18RR）上，CAT实现了比固定几何基线模型高出约10%的MRR和Hits@10，同时参数量仅增加了5%，推理时间相当。

Conclusion: 学习到的几何适应性优于任何单一的固定几何方法，尤其是在复杂的依赖推理任务中。CAT为跨语言、视觉和多模态领域的混合几何架构提供了一个可扩展且可解释的基础。

Abstract: Transformers achieve strong performance across diverse domains but implicitly
assume Euclidean geometry in their attention mechanisms, limiting their
effectiveness on data with non-Euclidean structure. While recent extensions to
hyperbolic and spherical spaces show promise for hierarchical and cyclical
patterns, respectively, they require committing to a single geometry a priori,
reducing flexibility when data exhibits mixed geometric properties. We
introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that
dynamically learns per-token routing across three geometric attention branches
through a lightweight, differentiable gating mechanism. Unlike fixed-geometry
approaches, CAT enables adaptive geometric specialization, routing tokens to
the appropriate curvature based on their local relational structure. The
routing network provides interpretable curvature preferences while each branch
employs geometry-specific operations optimized for its respective manifold. On
knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves
approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines
with minimal overhead (5% parameter increase, comparable inference time). These
results demonstrate that learned geometric adaptation outperforms any single
fixed geometry for complex relational reasoning, establishing CAT as a scalable
and interpretable foundation for mixture-of-geometry architectures across
language, vision, and multimodal domains.

</details>


### [458] [Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking](https://arxiv.org/abs/2510.01637)
*Liyan Xie,Muhammad Siddeek,Mohamed Seif,Andrea J. Goldsmith,Mengdi Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种基于组合模式的水印框架，用于检测和定位大语言模型（LLM）生成文本中可能存在的后期编辑。


<details>
  <summary>Details</summary>
Motivation: 为了应对大语言模型生成内容在实际应用中可能经历的后期编辑（如人工修改或欺骗攻击），需要一种能够检测和定位这些修改的技术。

Method: 提出了一种基于组合模式的水印框架，该框架将词汇表划分为不相交的子集，并通过在生成过程中强制执行这些子集上的确定性组合模式来嵌入水印。同时，还提出了一种全局统计量用于检测水印，并设计了轻量级的局部统计量来标记和定位潜在的编辑。

Result: 在开源LLM和多种编辑场景下进行了评估，证明了该方法在编辑定位方面具有强大的实证性能，并引入了Type-I错误率和检测准确率这两个任务特定的评估指标。

Conclusion: 所提出的基于组合模式的水印框架能够有效地检测和定位经过后期编辑的大语言模型生成文本中的修改。

Abstract: Watermarking has become a key technique for proprietary language models,
enabling the distinction between AI-generated and human-written text. However,
in many real-world scenarios, LLM-generated content may undergo post-generation
edits, such as human revisions or even spoofing attacks, making it critical to
detect and localize such modifications. In this work, we introduce a new task:
detecting post-generation edits locally made to watermarked LLM outputs. To
this end, we propose a combinatorial pattern-based watermarking framework,
which partitions the vocabulary into disjoint subsets and embeds the watermark
by enforcing a deterministic combinatorial pattern over these subsets during
generation. We accompany the combinatorial watermark with a global statistic
that can be used to detect the watermark. Furthermore, we design lightweight
local statistics to flag and localize potential edits. We introduce two
task-specific evaluation metrics, Type-I error rate and detection accuracy, and
evaluate our method on open-source LLMs across a variety of editing scenarios,
demonstrating strong empirical performance in edit localization.

</details>


### [459] [Support Basis: Fast Attention Beyond Bounded Entries](https://arxiv.org/abs/2510.01643)
*Maryam Aliakbarpour,Vladimir Braverman,Junze Yin,Haochen Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的支持基分解框架，用于在非有界输入的条件下实现高效的 softmax 注意力近似，解决了现有算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 二次方复杂度的 softmax 注意力是扩展大型语言模型的瓶颈，而现有近似算法（如 Alman 和 Song, NeurIPS 2023）的适用性受限于有界输入的假设，该假设在实际中很少成立。

Method: 提出支持基分解框架，利用查询和键矩阵的子高斯分布特性，将大项和小项分开处理，对稀疏项进行精确计算，对稠密项进行多项式近似。进一步扩展到多阈值设置，消除分布假设，并结合 sketching 证明了多项式注意力近似 softmax 注意力的理论基础。

Result: 实现了亚二次方的运行时间，并为多项式注意力（Kacham, Mirrokni, and Zhong, ICML 2024）的经验成功提供了理论依据，表明 softmax 注意力可以通过多个多项式注意力的组合及 sketching 来近似。

Conclusion: 支持基分解框架提供了一种有效的方法来近似 softmax 注意力，克服了现有方法的限制，并为提高大型语言模型的效率提供了理论支持。

Abstract: The quadratic complexity of softmax attention remains a central bottleneck in
scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a
sub-quadratic attention approximation algorithm, but it works only under the
restrictive bounded-entry assumption. Since this assumption rarely holds in
practice, its applicability to modern LLMs is limited.
  In this paper, we introduce support-basis decomposition, a new framework for
efficient attention approximation beyond bounded entries. We empirically
demonstrate that the entries of the query and key matrices exhibit sub-Gaussian
behavior. Our approach uses this property to split large and small entries,
enabling exact computation on sparse components and polynomial approximation on
dense components. We establish rigorous theoretical guarantees, proving a
sub-quadratic runtime, and extend the method to a multi-threshold setting that
eliminates all distributional assumptions. Furthermore, we provide the first
theoretical justification for the empirical success of polynomial attention
[Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be
closely approximated by a combination of multiple polynomial attentions with
sketching.

</details>


### [460] [Source-Free Cross-Domain Continual Learning](https://arxiv.org/abs/2510.01649)
*Muhammad Tanzil Furqon,Mahardhika Pratama,Igor Škrjanc,Lin Liu,Habibullah Habibullah,Kutluyil Dogancay*

Main category: cs.LG

TL;DR: 本研究提出了一种源域无关的跨域持续学习方法REFEREE，通过频率感知提示和不确定性加权策略来解决缺乏源域样本的问题，并利用KLDA克服灾难性遗忘，在数值研究中取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 现有跨域持续学习方法需要使用标注好的源域数据，这在隐私受限环境下不可行。本研究旨在解决源域无关的跨域持续学习问题，即完全禁止使用源域样本。

Method: REFEREE方法结合了源域预训练模型和大型视觉语言模型，采用频率感知提示技术处理域偏移问题，生成鲁棒的频率感知增强样本，并通过不确定性感知加权策略缓解伪标签噪声问题。同时，利用KLDA和冻结骨干网络来克服灾难性遗忘。

Result: 通过严格的数值研究，验证了REFEREE方法的有效性，其性能显著优于那些仍然可以使用源域数据的现有方法。

Conclusion: REFEREE方法成功解决了源域无关的跨域持续学习问题，在没有源域样本的情况下，通过频率感知提示和不确定性加权等技术，实现了优于现有方法的性能。

Abstract: Although existing cross-domain continual learning approaches successfully
address many streaming tasks having domain shifts, they call for a fully
labeled source domain hindering their feasibility in the privacy constrained
environments. This paper goes one step ahead with the problem of source-free
cross-domain continual learning where the use of source-domain samples are
completely prohibited. We propose the idea of rehearsal-free frequency-aware
dynamic prompt collaborations (REFEREE) to cope with the absence of labeled
source-domain samples in realm of cross-domain continual learning. REFEREE is
built upon a synergy between a source-pre-trained model and a large-scale
vision-language model, thus overcoming the problem of sub-optimal
generalizations when relying only on a source pre-trained model. The domain
shift problem between the source domain and the target domain is handled by a
frequency-aware prompting technique encouraging low-frequency components while
suppressing high-frequency components. This strategy generates frequency-aware
augmented samples, robust against noisy pseudo labels. The noisy pseudo-label
problem is further addressed with the uncertainty-aware weighting strategy
where the mean and covariance matrix are weighted by prediction uncertainties,
thus mitigating the adverse effects of the noisy pseudo label. Besides, the
issue of catastrophic forgetting (CF) is overcome by kernel linear discriminant
analysis (KLDA) where the backbone network is frozen while the classification
is performed using the linear discriminant analysis approach guided by the
random kernel method. Our rigorous numerical studies confirm the advantage of
our approach where it beats prior arts having access to source domain samples
with significant margins.

</details>


### [461] [The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM](https://arxiv.org/abs/2510.01650)
*Kwanhee Lee,Hyeondo Jang,Dongyeop Lee,Dan Alistarh,Namhoon Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Neural network pruning is a promising technique to mitigate the excessive
computational and memory requirements of large language models (LLMs). Despite
its promise, however, progress in this area has diminished, as conventional
methods are seemingly unable to surpass moderate sparsity levels (50-60%)
without severely degrading model accuracy. This work breaks through the current
impasse, presenting a principled and effective method called $\texttt{Elsa}$,
which achieves extreme sparsity levels of up to 90% while retaining high model
fidelity. This is done by identifying several limitations in current practice,
all of which can be traced back to their reliance on a surrogate objective
formulation. $\texttt{Elsa}$ tackles this issue directly and effectively via
standard and well-established constrained optimization techniques based on
ADMM. Our extensive experiments across a wide range of models and scales show
that $\texttt{Elsa}$ achieves substantial improvements over existing methods;
e.g., it achieves 7.8$\times$ less perplexity than the best existing method on
LLaMA-2-7B at 90% sparsity. Furthermore, we present
$\texttt{Elsa}_{\text{-L}}$, a quantized variant that scales to extremely large
models (27B), and establish its theoretical convergence guarantees. These
results highlight meaningful progress in advancing the frontier of LLM
sparsity, while promising that significant opportunities for further
advancement may remain in directions that have so far attracted limited
exploration.

</details>


### [462] [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656)
*Jiashun Liu,Johan Obando-Ceron,Han Lu,Yancheng He,Weixun Wang,Wenbo Su,Bo Zheng,Pablo Samuel Castro,Aaron Courville,Ling Pan*

Main category: cs.LG

TL;DR: RL4LLM方法通常避免使用显式 critic，转而使用平均优势基线，因为在 LLM 规模下训练传统价值函数计算成本高且在稀疏奖励和长推理过程中效果不佳。AsyPPO 通过使用轻量级 mini-critics 恢复了 critic 的作用，每个 critic 在不同的 prompt shards 上进行训练，从而鼓励多样性并保持校准，减少价值估计偏差。此外，AsyPPO 利用 critic 间的 
不确定性来改进策略更新，包括在 critics 达成一致时屏蔽优势，以及过滤掉熵正则化中的高分歧状态，以抑制虚假探索。


<details>
  <summary>Details</summary>
Motivation: 大多数 RL4LLM 方法避免使用显式 critic，而是依赖平均优势基线，因为在 LLM 规模下训练传统的价值函数计算成本高，并且在稀疏奖励和长推理过程中往往会失败。因此，作者从架构角度重新审视了这个问题，旨在恢复 critic 的作用，同时保持算法在大模型设置下的效率。

Method: AsyPPO 采用了一组轻量级的 mini-critics，每个 critic 在不相干的 prompt shards 上进行训练。这种设计鼓励了多样性，同时保持了校准，减少了价值估计偏差。此外，AsyPPO 利用 critic 间的 
不确定性来改进策略更新：(i) 在 critics 意见一致且梯度学习信号很小时，屏蔽优势；(ii) 从熵正则化中过滤掉高分歧状态，抑制虚假探索。

Result: 在仅使用 5000 个样本的开源数据上训练后，AsyPPO 在多个基准测试中持续提高了学习稳定性和性能，超越了 GRPO 等强基线。与经典 PPO 相比，在 Qwen3-4b-Base 上实现了超过 6% 的性能提升，在 Qwen3-8b-Base 和 Qwen3-14b-Base 上实现了约 3% 的性能提升，且没有使用额外的技巧。

Conclusion: 这些结果强调了架构创新对于开发可扩展、高效的 RL 算法的重要性。

Abstract: Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing
them with average advantage baselines. This shift is largely pragmatic:
conventional value functions are computationally expensive to train at LLM
scale and often fail under sparse rewards and long reasoning horizons. We
revisit this bottleneck from an architectural perspective and introduce
Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable
framework that restores the critics role while remaining efficient in
large-model settings. AsyPPO employs a set of lightweight mini-critics, each
trained on disjoint prompt shards. This design encourages diversity while
preserving calibration, reducing value-estimation bias. Beyond robust
estimation, AsyPPO leverages inter-critic uncertainty to refine the policy
update: (i) masking advantages in states where critics agree and gradients add
little learning signal, and (ii) filtering high-divergence states from entropy
regularization, suppressing spurious exploration. After training on open-source
data with only 5,000 samples, AsyPPO consistently improves learning stability
and performance across multiple benchmarks over strong baselines, such as GRPO,
achieving performance gains of more than six percent on Qwen3-4b-Base and about
three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without
additional tricks. These results highlight the importance of architectural
innovations for scalable, efficient algorithms.

</details>


### [463] [Learning Time-Series Representations by Hierarchical Uniformity-Tolerance Latent Balancing](https://arxiv.org/abs/2510.01658)
*Amin Jalali,Milad Soltany,Michael Greenspan,Ali Etemad*

Main category: cs.LG

TL;DR: TimeHUT是一种通过分层均匀性-容忍度平衡的对比表示来学习时间序列表示的新方法。


<details>
  <summary>Details</summary>
Motivation: 该方法旨在通过分层均匀性-容忍度平衡的对比表示来学习时间序列表示。

Method: TimeHUT采用分层方法学习实例和时间信息，并结合温度调度器和分层角度裕度损失来平衡嵌入空间的均匀性和容忍度，从而提高正例对的一致性以及与负例的区分度，增强时间序列样本内的时间依赖性捕获能力。

Result: 在UCR和UAE数据集上的分类任务以及Yahoo和KPI数据集上的异常检测任务中，TimeHUT的性能优于现有方法。

Conclusion: TimeHUT在时间序列表示学习方面取得了显著的成果，尤其在分类任务上表现突出，并在异常检测任务上取得了有竞争力的数据。

Abstract: We propose TimeHUT, a novel method for learning time-series representations
by hierarchical uniformity-tolerance balancing of contrastive representations.
Our method uses two distinct losses to learn strong representations with the
aim of striking an effective balance between uniformity and tolerance in the
embedding space. First, TimeHUT uses a hierarchical setup to learn both
instance-wise and temporal information from input time-series. Next, we
integrate a temperature scheduler within the vanilla contrastive loss to
balance the uniformity and tolerance characteristics of the embeddings.
Additionally, a hierarchical angular margin loss enforces instance-wise and
temporal contrast losses, creating geometric margins between positive and
negative pairs of temporal sequences. This approach improves the coherence of
positive pairs and their separation from the negatives, enhancing the capture
of temporal dependencies within a time-series sample. We evaluate our approach
on a wide range of tasks, namely 128 UCR and 30 UAE datasets for univariate and
multivariate classification, as well as Yahoo and KPI datasets for anomaly
detection. The results demonstrate that TimeHUT outperforms prior methods by
considerable margins on classification, while obtaining competitive results for
anomaly detection. Finally, detailed sensitivity and ablation studies are
performed to evaluate different components and hyperparameters of our method.

</details>


### [464] [Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value](https://arxiv.org/abs/2510.01663)
*Wangxuan Fan,Ching Wang,Siqi Li,Nan Liu*

Main category: cs.LG

TL;DR: KANs在保持高性能的同时，可以通过可学习的激活函数来理解特征与结果之间的关系，但其网络剪枝面临挑战。ShapKAN提出了一种基于Shapley值的方法来评估节点重要性，解决了传统方法对坐标偏移敏感的问题，实现了有效的网络压缩和可解释性。


<details>
  <summary>Details</summary>
Motivation: KANs虽然在预测方面表现出色并能提供可解释性，但其网络剪枝方法存在对输入坐标偏移敏感的问题，这阻碍了其在资源受限环境下的部署。

Method: 提出了一种名为ShapKAN的剪枝框架，该框架利用Shapley值来评估节点的重要性，并确保评估结果对于输入的坐标偏移具有不变性。

Result: 实验证明，ShapKAN能够准确地识别出真正重要的节点，并有效地压缩KANs网络，同时不牺牲模型的性能。

Conclusion: ShapKAN通过提供一种对坐标偏移不敏感的节点重要性评估方法，有效解决了KANs网络剪枝的挑战，提高了KANs的可解释性，并使其能够更好地应用于资源受限的环境中。

Abstract: For many real-world applications, understanding feature-outcome relationships
is as crucial as achieving high predictive accuracy. While traditional neural
networks excel at prediction, their black-box nature obscures underlying
functional relationships. Kolmogorov--Arnold Networks (KANs) address this by
employing learnable spline-based activation functions on edges, enabling
recovery of symbolic representations while maintaining competitive performance.
However, KAN's architecture presents unique challenges for network pruning.
Conventional magnitude-based methods become unreliable due to sensitivity to
input coordinate shifts. We propose \textbf{ShapKAN}, a pruning framework using
Shapley value attribution to assess node importance in a shift-invariant
manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's
actual contribution, ensuring consistent importance rankings regardless of
input parameterization. Extensive experiments on synthetic and real-world
datasets demonstrate that ShapKAN preserves true node importance while enabling
effective network compression. Our approach improves KAN's interpretability
advantages, facilitating deployment in resource-constrained environments.

</details>


### [465] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: AGFN通过引入一种基于信息熵和模态重要性的双门控融合机制，能够自适应地调整特征权重，从而提升多模态情感分析的性能，特别是在处理低质量或缺失模态时。


<details>
  <summary>Details</summary>
Motivation: 现有简单融合技术未能充分考虑模态质量的差异（如噪声、缺失或语义冲突），导致在识别细微情感差异时性能不佳。

Method: 提出了一种自适应门控融合网络（AGFN），该网络通过一个双门控融合机制，根据信息熵和模态重要性自适应地调整特征权重，以减轻噪声模态的影响并优先考虑信息丰富的线索。

Result: 在CMU-MOSI和CMU-MOSEI数据集上，AGFN显著优于现有的基线模型，提高了准确性，并能有效识别细微情感，表现出鲁棒的性能。

Conclusion: AGFN通过学习更广泛的特征分布，减少了特征位置与预测误差之间的相关性，从而增强了泛化能力，降低了对特定位置的依赖，创建了更鲁棒的多模态特征表示。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [466] [PASTA: A Unified Framework for Offline Assortment Learning](https://arxiv.org/abs/2510.01693)
*Juncheng Dong,Weibin Mo,Zhengling Qi,Cong Shi,Ethan X. Fang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 该论文提出了一种名为PASTA的新框架，用于解决数据驱动的离线多选优化问题，该框架利用悲观主义原则，在不要求完整数据覆盖的情况下，也能实现最优预期收入，并取得了理论和实践上的显著成果。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的离线多选优化问题中，由于历史客户选择数据可能无法覆盖所有可行选择，导致数据覆盖不足，设计有效的解决方案面临挑战。

Method: 提出了一种名为PASTA（Pessimistic Assortment Optimization）的新框架，该框架利用悲观主义原则来解决多选优化问题。PASTA只需要离线数据分布包含一个最优选择集，而不是对所有可行选择集进行完整覆盖。

Result: 在理论上，该研究首次为多项Logit和嵌套Logit等广泛使用的选择模型建立了离线多选优化的有限样本遗憾界。此外，还推导了一个最小最大遗憾下界，证明了PASTA在样本和模型复杂度方面具有最小最大最优性。在实践中，数值实验表明该方法优于现有的基线方法。

Conclusion: PASTA框架在理论和实践上都取得了显著成果，为解决数据驱动的离线多选优化问题提供了一种有效且具有理论保障的方法。

Abstract: We study a broad class of assortment optimization problems in an offline and
data-driven setting. In such problems, a firm lacks prior knowledge of the
underlying choice model, and aims to determine an optimal assortment based on
historical customer choice data. The combinatorial nature of assortment
optimization often results in insufficient data coverage, posing a significant
challenge in designing provably effective solutions. To address this, we
introduce a novel Pessimistic Assortment Optimization (PASTA) framework that
leverages the principle of pessimism to achieve optimal expected revenue under
general choice models. Notably, PASTA requires only that the offline data
distribution contains an optimal assortment, rather than providing the full
coverage of all feasible assortments. Theoretically, we establish the first
finite-sample regret bounds for offline assortment optimization across several
widely used choice models, including the multinomial logit and nested logit
models. Additionally, we derive a minimax regret lower bound, proving that
PASTA is minimax optimal in terms of sample and model complexity. Numerical
experiments further demonstrate that our method outperforms existing baseline
approaches.

</details>


### [467] [Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport](https://arxiv.org/abs/2510.01706)
*Shaan Shah,Meenakshi Khosla*

Main category: cs.LG

TL;DR: HOT框架通过联合推断软层间耦合和神经元间传输计划，解决了现有表示相似性方法的局限性，实现了全局对齐和对深度不匹配的自然处理，并在各种模型和人类视觉皮层数据上展现出优越的对齐质量和精细的层次结构。


<details>
  <summary>Details</summary>
Motivation: 现有的表示相似性方法在对齐网络层时存在一些局限性，例如结果不对称、缺乏全局对齐分数以及难以处理不同深度的网络。这些问题源于忽略了全局激活结构并限制了层与层之间一对一的对应关系。

Method: 提出了一种名为分层最优传输（HOT）的统一框架，该框架能够联合推断出软的、全局一致的层间耦合以及神经元间传输计划。HOT允许源神经元在最小化总传输成本和满足边缘约束的条件下，将信息分布到多个目标层。这不仅可以得到整个网络比较的单一对齐分数，还可以得到一个软传输计划，从而自然地处理因信息分布而产生的深度不匹配问题。

Result: 在视觉模型、大型语言模型和人类视觉皮层记录的评估中，HOT在对齐质量上与标准的成对匹配方法相当或更优。此外，HOT揭示了平滑、细粒度的层次对应关系：早期层映射到早期层，深层保持相对位置，并且通过在多个层上分布表示来解决深度不匹配问题。这些结构化模式是从全局优化中自然产生的，而不是被强加的，并且是贪婪的逐层方法所缺乏的。

Conclusion: HOT框架能够实现比现有方法更丰富、更可解释的表示比较，尤其是在网络结构或深度不同的情况下。

Abstract: Standard representational similarity methods align each layer of a network to
its best match in another independently, producing asymmetric results, lacking
a global alignment score, and struggling with networks of different depths.
These limitations arise from ignoring global activation structure and
restricting mappings to rigid one-to-one layer correspondences. We propose
Hierarchical Optimal Transport (HOT), a unified framework that jointly infers
soft, globally consistent layer-to-layer couplings and neuron-level transport
plans. HOT allows source neurons to distribute mass across multiple target
layers while minimizing total transport cost under marginal constraints. This
yields both a single alignment score for the entire network comparison and a
soft transport plan that naturally handles depth mismatches through mass
distribution. We evaluate HOT on vision models, large language models, and
human visual cortex recordings. Across all domains, HOT matches or surpasses
standard pairwise matching in alignment quality. Moreover, it reveals smooth,
fine-grained hierarchical correspondences: early layers map to early layers,
deeper layers maintain relative positions, and depth mismatches are resolved by
distributing representations across multiple layers. These structured patterns
emerge naturally from global optimization without being imposed, yet are absent
in greedy layer-wise methods. HOT thus enables richer, more interpretable
comparisons between representations, particularly when networks differ in
architecture or depth.

</details>


### [468] [ActiNet: Activity intensity classification of wrist-worn accelerometers using self-supervised deep learning](https://arxiv.org/abs/2510.01712)
*Aidan Acquah,Shing Chan,Aiden Doherty*

Main category: cs.LG

TL;DR: 与传统的随机森林+HMM相比，ActiNet模型在人类活动识别方面表现出更好的性能，特别是在区分活动强度标签方面，并且在不同年龄和性别亚组中表现一致。


<details>
  <summary>Details</summary>
Motivation: 为了提高大规模流行病学研究中基于腕部加速度计数据的可靠、准确的人类活动识别（HAR）模型，以研究身体活动与健康结果之间的关联。

Method: 使用151名参与者的CAPTURE-24数据，训练了一个名为ActiNet的自监督18层修改版ResNet-V2模型，并结合隐马尔可夫模型（HMM）进行平滑处理，以对活动强度标签进行分类。然后，将该模型的性能与基于相同数据集的基线随机森林（RF）+HMM模型进行比较，并分析了不同年龄和性别亚组的差异。

Result: ActiNet模型在区分活动强度标签方面取得了平均宏观F1分数0.82和平均Cohen's kappa分数0.86，优于平均宏观F1分数0.77和平均Cohen's kappa分数0.81的RF+HMM模型。这些结果在不同年龄和性别亚组中均保持一致。

Conclusion: ActiNet模型在从腕部加速度计数据中提取活动强度标签方面表现出优越的性能，并且在不同亚组中表现一致，这鼓励在未来的流行病学研究中使用ActiNet。

Abstract: The use of reliable and accurate human activity recognition (HAR) models on
passively collected wrist-accelerometer data is essential in large-scale
epidemiological studies that investigate the association between physical
activity and health outcomes. While the use of self-supervised learning has
generated considerable excitement in improving HAR, it remains unknown the
extent to which these models, coupled with hidden Markov models (HMMs), would
make a tangible improvement to classification performance, and the effect this
may have on the predicted daily activity intensity compositions. Using 151
CAPTURE-24 participants' data, we trained the ActiNet model, a self-supervised,
18-layer, modified ResNet-V2 model, followed by hidden Markov model (HMM)
smoothing to classify labels of activity intensity. The performance of this
model, evaluated using 5-fold stratified group cross-validation, was then
compared to a baseline random forest (RF) + HMM, established in existing
literature. Differences in performance and classification outputs were compared
with different subgroups of age and sex within the Capture-24 population. The
ActiNet model was able to distinguish labels of activity intensity with a mean
macro F1 score of 0.82, and mean Cohen's kappa score of 0.86. This exceeded the
performance of the RF + HMM, trained and validated on the same dataset, with
mean scores of 0.77 and 0.81, respectively. These findings were consistent
across subgroups of age and sex. These findings encourage the use of ActiNet
for the extraction of activity intensity labels from wrist-accelerometer data
in future epidemiological studies.

</details>


### [469] [Latency-aware Multimodal Federated Learning over UAV Networks](https://arxiv.org/abs/2510.01717)
*Shaba Shaon,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种无人机辅助的联邦多模态学习（FML）框架，通过联合优化无人机调度、功耗、轨迹、资源分配和基站资源管理来最小化系统延迟，并提供了理论收敛性分析。


<details>
  <summary>Details</summary>
Motivation: 利用无人机和多模态感知克服单模态系统的局限性，提高模型精度、泛化能力，并全面理解环境，同时最小化联邦多模态学习的系统延迟。

Method: 提出一种结合块坐标下降和连续凸近似的迭代优化算法来解决延迟最小化问题，并对非凸损失函数下的收敛性进行了理论分析。

Result: 数值实验表明，该FML框架在系统延迟和模型训练性能方面优于现有方法。

Conclusion: 无人机辅助的联邦多模态学习框架能够有效降低系统延迟并提升模型训练性能。

Abstract: This paper investigates federated multimodal learning (FML) assisted by
unmanned aerial vehicles (UAVs) with a focus on minimizing system latency and
providing convergence analysis. In this framework, UAVs are distributed
throughout the network to collect data, participate in model training, and
collaborate with a base station (BS) to build a global model. By utilizing
multimodal sensing, the UAVs overcome the limitations of unimodal systems,
enhancing model accuracy, generalization, and offering a more comprehensive
understanding of the environment. The primary objective is to optimize FML
system latency in UAV networks by jointly addressing UAV sensing scheduling,
power control, trajectory planning, resource allocation, and BS resource
management. To address the computational complexity of our latency minimization
problem, we propose an efficient iterative optimization algorithm combining
block coordinate descent and successive convex approximation techniques, which
provides high-quality approximate solutions. We also present a theoretical
convergence analysis for the UAV-assisted FML framework under a non-convex loss
function. Numerical experiments demonstrate that our FML framework outperforms
existing approaches in terms of system latency and model training performance
under different data settings.

</details>


### [470] [Accelerating Attention with Basis Decomposition](https://arxiv.org/abs/2510.01718)
*Jialin Zhao*

Main category: cs.LG

TL;DR: BDA是一种无损的注意力机制算法重构，通过矩阵分解将多头投影重组为紧凑形式，实现加速且保持输出精确。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法主要关注I/O感知和系统层面，BDA旨在提供一种数学上保证的、与架构无关的注意力加速方法。

Method: BDA利用了Basis Decomposition（BD）中的矩阵恒等式，将多头注意力投影重构为紧凑形式，实现无损加速。

Result: 在DeepSeek-V2-Lite（16B, FP16）上，BDA仅需4秒的离线准备时间，无需重新训练，即可在现代GPU上实现32%的键/值投影加速和25%的权重减小，同时模型性能仅略有下降（FP16下PPL增加0.02%，FP32下增加0.0004%）。

Conclusion: BDA是首个理论上精确的无损注意力加速方法，可与现有工程级优化互补。

Abstract: Attention is a core operation in large language models (LLMs) and
vision-language models (VLMs). We present BD Attention (BDA), the first
lossless algorithmic reformulation of attention. BDA is enabled by a simple
matrix identity from Basis Decomposition (BD), which restructures multi-head
projections into a compact form while preserving exact outputs. Unlike
I/O-aware system optimizations such as FlashAttention, BDA provides a
mathematically guaranteed acceleration that is architecture-agnostic. On
DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with
no retraining required and, on modern GPUs, achieves 32% faster key/value
projections and 25% smaller weights, while increasing end-to-end perplexity
(PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model
performance. These results position BDA as the first theoretically exact method
for lossless attention acceleration that is complementary to existing
engineering-level optimizations. Our code is available at
https://github.com/abcbdf/basis-decomposition-official.

</details>


### [471] [Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation](https://arxiv.org/abs/2510.01721)
*Saptarshi Mandal,Yashaswini Murthy,R. Srikant*

Main category: cs.LG

TL;DR: 该论文提出了首个具有线性函数逼近的鲁棒性时序差分（TD）学习算法，用于解决分布鲁棒强化学习（DRRL）中的模型不确定性问题，并提供了样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒TD学习算法在模型不确定性下的收敛性保证有限，尤其是在使用函数逼近时存在折扣因子假设的限制。本研究旨在弥合经验上鲁棒RL算法的成功与其对应的非鲁棒算法所享有的非渐进保证之间的差距。

Method: 提出了一种结合双时间尺度随机逼近更新和外循环目标网络更新的鲁棒TD学习算法，该算法能够处理总变差距离和Wasserstein-1距离不确定性集合，并且是无模型的，不需要MDP的生成访问。

Result: 在存在模型不确定性的情况下，该算法实现了$	ilde{O}(1/	ext{epsilon}^2)$的样本复杂度，可以获得$	ext{epsilon}$精度的价值估计。该方法也易于扩展到鲁棒Q学习和函数逼近。

Conclusion: 本研究为分布鲁棒强化学习中的鲁棒TD学习提供了首个具有理论保证的线性函数逼近算法，并有效降低了样本复杂度，为鲁棒RL算法的理论和实践发展做出了贡献。

Abstract: Distributionally robust reinforcement learning (DRRL) focuses on designing
policies that achieve good performance under model uncertainties. In
particular, we are interested in maximizing the worst-case long-term discounted
reward, where the data for RL comes from a nominal model while the deployed
environment can deviate from the nominal model within a prescribed uncertainty
set. Existing convergence guarantees for robust temporal-difference (TD)
learning for policy evaluation are limited to tabular MDPs or are dependent on
restrictive discount-factor assumptions when function approximation is used. We
present the first robust TD learning with linear function approximation, where
robustness is measured with respect to the total-variation distance and
Wasserstein-l distance uncertainty set. Additionally, our algorithm is both
model-free and does not require generative access to the MDP. Our algorithm
combines a two-time-scale stochastic-approximation update with an outer-loop
target-network update. We establish an $\tilde{O}(1/\epsilon^2)$ sample
complexity to obtain an $\epsilon$-accurate value estimate. Our results close a
key gap between the empirical success of robust RL algorithms and the
non-asymptotic guarantees enjoyed by their non-robust counterparts. The key
ideas in the paper also extend in a relatively straightforward fashion to
robust Q-learning with function approximation.

</details>


### [472] [Workplace Location Choice Model based on Deep Neural Network](https://arxiv.org/abs/2510.01723)
*Tanay Rastogi,Anders Karlström*

Main category: cs.LG

TL;DR: 深度神经网络（DNN）在工作场所位置选择建模方面优于传统离散选择模型（DCM），尤其是在较长距离的情况下，但DCM在个体属性对工作场所距离的影响方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择模型（DCM）在准确模拟个体决策过程方面存在挑战，因此需要更有效的方法来分析工作场所位置选择。

Method: 提出并应用一种深度神经网络（DNN）方法来模拟工作场所位置选择，并与传统的DCM进行比较。

Result: DNN在整体上优于DCM，尤其是在较长距离的情况下。然而，在评估个体属性对工作场所距离的影响时，DCM与数据拟合得更好，尤其是在较短距离的情况下。

Conclusion: DNN在工作场所位置选择分析中显示出作为DCM的有力替代品的潜力，但应根据具体应用需求选择合适的模型。

Abstract: Discrete choice models (DCMs) have long been used to analyze workplace
location decisions, but they face challenges in accurately mirroring individual
decision-making processes. This paper presents a deep neural network (DNN)
method for modeling workplace location choices, which aims to better understand
complex decision patterns and provides better results than traditional discrete
choice models (DCMs). The study demonstrates that DNNs show significant
potential as a robust alternative to DCMs in this domain. While both models
effectively replicate the impact of job opportunities on workplace location
choices, the DNN outperforms the DCM in certain aspects. However, the DCM
better aligns with data when assessing the influence of individual attributes
on workplace distance. Notably, DCMs excel at shorter distances, while DNNs
perform comparably to both data and DCMs for longer distances. These findings
underscore the importance of selecting the appropriate model based on specific
application requirements in workplace location choice analysis.

</details>


### [473] [Private and Fair Machine Learning: Revisiting the Disparate Impact of Differentially Private SGD](https://arxiv.org/abs/2510.01744)
*Lea Demelius,Dominik Kowald,Simone Kopeinik,Roman Kern,Andreas Trügler*

Main category: cs.LG

TL;DR: 研究表明，直接在差分私有模型上优化超参数可以改善效用-公平性权衡，但并不能保证消除差分私有随机梯度下降（DPSGD）对公平性的不利影响，并且会带来额外的隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 探讨差分隐私（DP）在保护个人信息方面的应用，特别是DPSGD对模型性能和公平性的影响，以及直接在DPSGD模型上优化超参数是否能解决公平性问题。

Method: 1. 比较DPSGD对不同性能指标的差异化影响；2. 在广泛的超参数设置下进行分析；3. 将DPSGD与非私有模型进行比较；4. 分析DPSGD-Global-Adapt变体的鲁棒性。

Result: DPSGD对不同性能指标的影响存在差异，并非所有指标都受不利影响；直接在DPSGD模型上优化超参数不能可靠地减轻DPSGD对公平性的不利影响，但可以改善效用-公平性权衡；任何超参数调整都会带来额外的隐私泄露；DPSGD-Global-Adapt在超参数选择方面可能不是一个稳健的解决方案。

Conclusion: 直接在DPSGD模型上优化超参数虽然有一定优势，但不能完全解决公平性问题，并且伴随隐私泄露风险，需要在隐私、效用和公平性之间仔细权衡。DPSGD-Global-Adapt并非万能解决方案。

Abstract: Differential privacy (DP) is a prominent method for protecting information
about individuals during data analysis. Training neural networks with
differentially private stochastic gradient descent (DPSGD) influences the
model's learning dynamics and, consequently, its output. This can affect the
model's performance and fairness. While the majority of studies on the topic
report a negative impact on fairness, it has recently been suggested that
fairness levels comparable to non-private models can be achieved by optimizing
hyperparameters for performance directly on differentially private models
(rather than re-using hyperparameters from non-private models, as is common
practice). In this work, we analyze the generalizability of this claim by 1)
comparing the disparate impact of DPSGD on different performance metrics, and
2) analyzing it over a wide range of hyperparameter settings. We highlight that
a disparate impact on one metric does not necessarily imply a disparate impact
on another. Most importantly, we show that while optimizing hyperparameters
directly on differentially private models does not mitigate the disparate
impact of DPSGD reliably, it can still lead to improved utility-fairness
trade-offs compared to re-using hyperparameters from non-private models. We
stress, however, that any form of hyperparameter tuning entails additional
privacy leakage, calling for careful considerations of how to balance privacy,
utility and fairness. Finally, we extend our analyses to DPSGD-Global-Adapt, a
variant of DPSGD designed to mitigate the disparate impact on accuracy, and
conclude that this alternative may not be a robust solution with respect to
hyperparameter choice.

</details>


### [474] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 本研究提出了一种名为动态特征选择（DFS）的无监督方法，用于优化图像的潜在表征，以提高机器学习模型的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型，特别是在视觉任务中，其潜在表征容易受到噪声或无关特征的影响，从而损害模型的性能和泛化能力。

Method: 提出了一种名为动态特征选择（DFS）的无监督方法。该方法为每个实例识别并移除图像中具有误导性或冗余的信息，确保只有最相关的特征被纳入潜在空间。该方法在无监督框架下运行，不依赖于标记数据。

Result: 在图像数据集上的实验表明，采用了无监督DFS的模型在聚类和图像生成等各种任务上都取得了显著的泛化性能提升，同时计算成本仅有微小增加。

Conclusion: 本研究提出的无监督DFS方法能够有效提升潜在表征的质量，显著改善模型的泛化性能，且适用性广泛，计算成本可控。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [475] [Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX](https://arxiv.org/abs/2510.01764)
*Waris Radji,Thomas Michel,Hector Piteau*

Main category: cs.LG

TL;DR: Octax是一个基于JAX的高性能经典街机游戏环境套件，为RL研究提供了GPU替代方案，实现了数量级的加速和大规模实验能力。


<details>
  <summary>Details</summary>
Motivation: 传统的RL研究环境（如现代视频游戏）计算成本高，不适合大规模实验。需要一种计算高效、可扩展的环境。

Method: 使用JAX实现CHIP-8模拟器，创建了包含多种游戏类型（益智、动作、策略）的图像化环境，实现了端到端的GPU加速。

Result: Octax相比传统CPU模拟器实现了数量级的速度提升，同时保持了与原始游戏机制的完全保真度。通过训练RL智能体，展示了在训练速度和可扩展性方面的显著优势。

Conclusion: Octax为RL研究提供了一个理想的大规模实验平台，其模块化设计易于扩展，并能利用LLM生成新环境。

Abstract: Reinforcement learning (RL) research requires diverse, challenging
environments that are both tractable and scalable. While modern video games may
offer rich dynamics, they are computationally expensive and poorly suited for
large-scale experimentation due to their CPU-bound execution. We introduce
Octax, a high-performance suite of classic arcade game environments implemented
in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely
adopted as a benchmark in RL research. Octax provides the JAX community with a
long-awaited end-to-end GPU alternative to the Atari benchmark, offering
image-based environments, spanning puzzle, action, and strategy genres, all
executable at massive scale on modern GPUs. Our JAX-based implementation
achieves orders-of-magnitude speedups over traditional CPU emulators while
maintaining perfect fidelity to the original game mechanics. We demonstrate
Octax's capabilities by training RL agents across multiple games, showing
significant improvements in training speed and scalability compared to existing
solutions. The environment's modular design enables researchers to easily
extend the suite with new games or generate novel environments using large
language models, making it an ideal platform for large-scale RL
experimentation.

</details>


### [476] [Sensitivity, Specificity, and Consistency: A Tripartite Evaluation of Privacy Filters for Synthetic Data Generation](https://arxiv.org/abs/2510.01793)
*Adil Koeken,Alexander Ziller,Moritz Knolle,Daniel Rueckert*

Main category: cs.LG

TL;DR: 现有的过滤技术在保护合成医疗数据（如胸部X光图像）隐私方面效果不佳，可能提供虚假的安全感。


<details>
  <summary>Details</summary>
Motivation: 评估现有后验隐私过滤技术在合成医疗数据（胸部X光图像）上的有效性，以解决其在实际应用中有效性未经充分验证的问题。

Method: 对应用于胸部X光合成数据的过滤流程进行严格评估。

Result: 结果表明，当前的过滤技术特异性和一致性有限，仅对真实图像具有高灵敏度，但无法可靠地检测出由训练数据生成的近乎重复的图像，可能导致患者信息泄露。

Conclusion: 目前的后验过滤技术在保护患者隐私方面存在严重缺陷，在安全部署于敏感应用之前，过滤器的设计需要取得重大进展。

Abstract: The generation of privacy-preserving synthetic datasets is a promising avenue
for overcoming data scarcity in medical AI research. Post-hoc privacy filtering
techniques, designed to remove samples containing personally identifiable
information, have recently been proposed as a solution. However, their
effectiveness remains largely unverified. This work presents a rigorous
evaluation of a filtering pipeline applied to chest X-ray synthesis. Contrary
to claims from the original publications, our results demonstrate that current
filters exhibit limited specificity and consistency, achieving high sensitivity
only for real images while failing to reliably detect near-duplicates generated
from training data. These results demonstrate a critical limitation of post-hoc
filtering: rather than effectively safeguarding patient privacy, these methods
may provide a false sense of security while leaving unacceptable levels of
patient information exposed. We conclude that substantial advances in filter
design are needed before these methods can be confidently deployed in sensitive
applications.

</details>


### [477] [Rethinking the shape convention of an MLP](https://arxiv.org/abs/2510.01796)
*Meng-Hsi Chen,Yu-Ang Lee,Feng-Ting Liao,Da-shan Shiu*

Main category: cs.LG

TL;DR: MLP结构设计创新，提出


<details>
  <summary>Details</summary>
Motivation: 当前MLP架构普遍采用窄-宽-窄设计，作者希望挑战这一传统，提出一种宽-窄-宽（沙漏型）MLP块，以期提升性能。

Method: 提出了一种名为“沙漏型”（Hourglass）的MLP块结构，其特点是：1. 融合（skip connections）在扩展的维度上操作，而残差计算则通过狭窄的瓶颈进行。2. 初始投影层可以固定在随机初始化的状态，无需训练。3. 在图像数据集上进行了生成任务的评估，并通过系统性的架构搜索来表征性能-参数的帕累托前沿。

Result: 与传统设计相比，沙漏型MLP结构在性能-参数帕累托前沿上始终表现更优。随着参数预算的增加，最优的沙漏型结构倾向于更深的网络、更宽的融合路径和更窄的瓶颈，这种扩展模式与传统MLP不同。

Conclusion: 研究表明，沙漏型MLP结构在生成任务上优于传统MLP。调整融合连接和残差计算的位置可以带来性能提升，并且这种宽-窄-宽的结构在扩展时表现出与传统MLP不同的模式。建议重新考虑现代架构中融合连接的放置，并指出其在Transformer和其他残差网络中具有潜在应用价值。

Abstract: Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow
design where skip connections operate at the input/output dimensions while
processing occurs in expanded hidden spaces. We challenge this convention by
proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections
operate at expanded dimensions while residual computation flows through narrow
bottlenecks. This inversion leverages higher-dimensional spaces for incremental
refinement while maintaining computational efficiency through parameter-matched
designs. Implementing Hourglass MLPs requires an initial projection to lift
input signals to expanded dimensions. We propose that this projection can
remain fixed at random initialization throughout training, enabling efficient
training and inference implementations. We evaluate both architectures on
generative tasks over popular image datasets, characterizing
performance-parameter Pareto frontiers through systematic architectural search.
Results show that Hourglass architectures consistently achieve superior Pareto
frontiers compared to conventional designs. As parameter budgets increase,
optimal Hourglass configurations favor deeper networks with wider skip
connections and narrower bottlenecks-a scaling pattern distinct from
conventional MLPs. Our findings suggest reconsidering skip connection placement
in modern architectures, with potential applications extending to Transformers
and other residual networks.

</details>


### [478] [Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction](https://arxiv.org/abs/2510.01817)
*Adam Filipek*

Main category: cs.LG

TL;DR: Transformer 中的多头注意力（MHA）因其计算复杂度与序列长度呈二次方关系而难以扩展。虽然 MQA 和 GQA 通过共享 Key 和 Value 投影解决了内存带宽瓶颈，但它们并未减少注意力分数计算的 FLOPs。本文提出了稀疏查询注意力（SQA），通过减少查询头来直接降低计算复杂度，从而在计算受限的情况下（如模型预训练）可实现高达 3 倍的吞吐量提升，且对模型质量影响极小。


<details>
  <summary>Details</summary>
Motivation: Transformer 中的 MHA 具有二次计算复杂度，限制了其在长序列应用中的扩展性。现有的 MQA 和 GQA 方案主要解决内存带宽瓶颈，但未降低注意力分数计算的 FLOPs，这仍然是训练和全序列处理的关键瓶颈。

Method: 本文提出稀疏查询注意力（SQA）架构，通过减少查询头的数量来直接降低注意力机制的计算复杂度，其计算复杂度的降低与查询头数量的减少成正比。

Result: 在长序列（32k-200k tokens）上的经验基准测试表明，SQA 在计算密集型场景（如模型预训练、微调和基于编码器的任务）中可实现高达 3 倍的吞吐量提升，同时在初步的小规模实验中对模型质量的影响极小。

Conclusion: SQA 是一种新颖的注意力架构，通过减少查询头直接降低计算复杂度，为提高 Transformer 模型的效率和可扩展性提供了一条新的途径，尤其是在计算密集型任务中具有显著优势。

Abstract: The Transformer architecture, underpinned by the Multi-Head Attention (MHA)
mechanism, has become the de facto standard for state-of-the-art models in
artificial intelligence. However, the quadratic computational complexity of MHA
with respect to sequence length presents a significant barrier to scaling,
particularly for applications involving long contexts. Prevailing solutions,
such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have
effectively addressed the memory bandwidth bottleneck that dominates
autoregressive inference latency by sharing Key and Value projections. While
highly successful, these methods do not reduce the fundamental number of
floating-point operations (FLOPs) required for the attention score computation,
which remains a critical bottleneck for training and full-sequence processing.
This paper introduces Sparse Query Attention (SQA), a novel attention
architecture that pursues an alternative and complementary optimization path.
Instead of reducing Key/Value heads, SQA reduces the number of Query heads.
This architectural modification directly decreases the computational complexity
of the attention mechanism by a factor proportional to the reduction in query
heads, thereby lowering the overall FLOPs. This work presents the theoretical
foundation of SQA, its mathematical formulation, and a family of architectural
variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate
that SQA can achieve significant throughput improvements of up to 3x in
computation-bound scenarios such as model pre-training, fine-tuning, and
encoder-based tasks, with only a minimal impact on model quality in preliminary
smallscale experiments. SQA was discovered serendipitously during the
development of the upcoming Reactive Transformer architecture, suggesting its
potential as a powerful tool for building more efficient and scalable models

</details>


### [479] [Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning](https://arxiv.org/abs/2510.01824)
*Olivier Goudet,Quentin Suire,Adrien Goëffon,Frédéric Saubion,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出一种用于黑盒组合优化的不变顺序强化学习框架，通过训练参数化多变量自回归生成模型，使其不受变量顺序影响，从而提高样本效率和搜索空间多样性。


<details>
  <summary>Details</summary>
Motivation: 经典估计分布算法（EDA）依赖于学习显式变量依赖图，成本高且难以捕捉复杂交互，而本研究旨在克服这些限制。

Method: 通过在训练过程中随机采样生成顺序（一种信息保留的dropout形式），使参数化多变量自回归生成模型具有不变性。适应了广义强化策略优化（GRPO）以实现稳定的策略梯度更新。

Result: 在多种基准算法和不同规模的问题实例上，该方法频繁 achieves 最佳性能，并持续避免灾难性故障。

Conclusion: 所提出的不变顺序强化学习框架在黑盒组合优化任务中表现出优越的性能和稳定性。

Abstract: We introduce an order-invariant reinforcement learning framework for
black-box combinatorial optimization. Classical estimation-of-distribution
algorithms (EDAs) often rely on learning explicit variable dependency graphs,
which can be costly and fail to capture complex interactions efficiently. In
contrast, we parameterize a multivariate autoregressive generative model
trained without a fixed variable ordering. By sampling random generation orders
during training - a form of information-preserving dropout - the model is
encouraged to be invariant to variable order, promoting search-space diversity
and shaping the model to focus on the most relevant variable dependencies,
improving sample efficiency. We adapt Generalized Reinforcement Policy
Optimization (GRPO) to this setting, providing stable policy-gradient updates
from scale-invariant advantages. Across a wide range of benchmark algorithms
and problem instances of varying sizes, our method frequently achieves the best
performance and consistently avoids catastrophic failures.

</details>


### [480] [Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model Selection and Benchmarking for Tabular datasets](https://arxiv.org/abs/2510.01842)
*Yannis Belkhiter,Seshu Tirupathi,Giulio Zizzo,Sachin Sharma,John D. Kelleher*

Main category: cs.LG

TL;DR: 本研究提出了一种利用传统模型和大型语言模型 (LLM) 代理来减少 AutoML 搜索空间的方法，通过分析数据集描述和统计信息，在 AWS AutoGluon 数据集上实现了显著的计算开销降低，同时仍能选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 现有的 AutoML 方法通常依赖于对所有模型进行详尽的超参数搜索，计算成本高昂。而预测试（pre-hoc prediction）作为一种有潜力的方法，可以通过智能预选模型来绕过详尽搜索，但目前研究不足。因此，本研究旨在探索 AutoML 与预测试模型选择的结合，以解决现有方法的局限性。

Method: 本研究利用传统模型和大型语言模型（LLM）代理，结合数据集描述和统计信息，来缩小 AutoML 的搜索空间，实现预测试模型选择。

Result: 在 AWS AutoGluon 投资组合数据集（包含 175 个 OpenML 上的表格分类数据集）上，该方法能够显著减少计算开销，并能有效选择出适合给定数据集的最佳模型。

Conclusion: 本研究提出的方法通过结合传统模型和 LLM 代理，利用数据集描述和统计信息来减少 AutoML 的搜索空间，为 AutoML 工作流程提供了一种新的范式，在显著降低计算成本的同时，仍能有效地选择出最佳模型。

Abstract: The field of AutoML has made remarkable progress in post-hoc model selection,
with libraries capable of automatically identifying the most performing models
for a given dataset. Nevertheless, these methods often rely on exhaustive
hyperparameter searches, where methods automatically train and test different
types of models on the target dataset. Contrastingly, pre-hoc prediction
emerges as a promising alternative, capable of bypassing exhaustive search
through intelligent pre-selection of models. Despite its potential, pre-hoc
prediction remains under-explored in the literature. This paper explores the
intersection of AutoML and pre-hoc model selection by leveraging traditional
models and Large Language Model (LLM) agents to reduce the search space of
AutoML libraries. By relying on dataset descriptions and statistical
information, we reduce the AutoML search space. Our methodology is applied to
the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark
containing 175 tabular classification datasets available on OpenML. The
proposed approach offers a shift in AutoML workflows, significantly reducing
computational overhead, while still selecting the best model for the given
dataset.

</details>


### [481] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 通过引入新颖的 Granular-GRPO ($	ext{G}^2$RPO ) 框架，解决现有基于强化学习的生成模型在奖励信号稀疏和狭窄情况下偏好对齐不佳的问题。该框架通过奇异随机采样策略实现对采样方向的精确奖励评估，并通过多粒度优势集成模块聚合多尺度优势，提供更全面鲁棒的评估。实验证明 $	ext{G}^2$RPO 在各种奖励模型上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的生成模型（特别是扩散和流模型）在通过随机微分方程（SDE）进行去噪采样以实现人类偏好对齐时，存在奖励信号稀疏和狭窄导致偏好对齐效果不佳的问题。

Method: 提出 Granular-GRPO ($	ext{G}^2$RPO ) 框架，包含两个关键组件：1. 奇异随机采样（Singular Stochastic Sampling）策略，用于在去噪过程中进行分步随机探索，并强制奖励与注入噪声之间的高度相关性，以实现对每个 SDE 扰动的忠实奖励。2. 多粒度优势集成（Multi-Granularity Advantage Integration）模块，用于消除固定粒度去噪带来的偏差，通过聚合多个扩散尺度下的优势，提供对采样方向更全面、更鲁棒的评估。

Result: 在多种奖励模型（包括域内和域外评估）上的实验表明，$	ext{G}^2$RPO 显著优于现有的基于流的 GRPO 基线方法，证明了其有效性和鲁棒性。

Conclusion: 	ext{G}^2$RPO 框架通过引入奇异随机采样和多粒度优势集成，能够更精确、更全面地评估采样方向，有效解决了现有方法在人类偏好对齐方面存在的奖励信号稀疏和偏差问题，并在实验中展现出优越的性能和鲁棒性。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [482] [Explicit Discovery of Nonlinear Symmetries from Dynamic Data](https://arxiv.org/abs/2510.01855)
*Lexiang Hu,Yikang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: LieNLSD是第一个能够确定非线性生成元的数量及其显式表达式的方法，在顶夸克寻找和动态系统方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对称发现方法大多局限于线性对称，并且在发现非线性对称时无法显式得到李代数子空间。因此，需要一种能够发现非线性对称性并明确其李代数子空间的方法。

Method: LieNLSD通过指定无穷小群作用的函数库，求解其系数矩阵。将数据的中心差分和训练神经网络的雅可比矩阵代入无穷小判据，得到系数矩阵的线性方程组，并使用SVD求解。理论上证明了该方法对于微分方程的延拓公式相对于系数矩阵是线性的。

Result: LieNLSD在顶夸克寻找和一系列动态系统方面，相较于现有方法具有显著优势，并将神经PDE求解器的长期预测精度提高了20%以上，同时还可用于指导数据增强。

Conclusion: LieNLSD是首个能够确定非线性生成元数量及其显式表达式的方法，在对称性发现方面取得了重要进展，并在多个应用场景中展示了其优越性。

Abstract: Symmetry is widely applied in problems such as the design of equivariant
networks and the discovery of governing equations, but in complex scenarios, it
is not known in advance. Most previous symmetry discovery methods are limited
to linear symmetries, and recent attempts to discover nonlinear symmetries fail
to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD,
which is, to our knowledge, the first method capable of determining the number
of infinitesimal generators with nonlinear terms and their explicit
expressions. We specify a function library for the infinitesimal group action
and aim to solve for its coefficient matrix, proving that its prolongation
formula for differential equations, which governs dynamic data, is also linear
with respect to the coefficient matrix. By substituting the central differences
of the data and the Jacobian matrix of the trained neural network into the
infinitesimal criterion, we get a system of linear equations for the
coefficient matrix, which can then be solved using SVD. On top quark tagging
and a series of dynamic systems, LieNLSD shows qualitative advantages over
existing methods and improves the long rollout accuracy of neural PDE solvers
by over 20% while applying to guide data augmentation. Code and data are
available at https://github.com/hulx2002/LieNLSD.

</details>


### [483] [Compositional meta-learning through probabilistic task inference](https://arxiv.org/abs/2510.01858)
*Jacob J. W. Bakermans,Pablo Tano,Reidar Riveland,Charles Findling,Alexandre Pouget*

Main category: cs.LG

TL;DR: A compositional meta-learning model is proposed that represents tasks as structured combinations of reusable computations, enabling rapid learning from minimal experience by transforming the problem into probabilistic inference.


<details>
  <summary>Details</summary>
Motivation: Meta-learning, or the ability to solve new tasks from minimal experience by reusing knowledge from previous tasks, is essential. Compositional solutions are well-suited for this.

Method: The paper proposes a compositional meta-learning model that learns a generative model to represent tasks as structured combinations of reusable computations. This transforms learning into a probabilistic inference problem, allowing solutions to be found without parameter updates via hypothesis testing.

Result: The model successfully recovers ground truth components and statistics in rule learning and motor learning tasks, and demonstrates the ability to quickly infer new solutions from single examples.

Conclusion: The proposed framework combines the expressivity of neural networks with the data-efficiency of probabilistic inference for rapid compositional meta-learning.

Abstract: To solve a new task from minimal experience, it is essential to effectively
reuse knowledge from previous tasks, a problem known as meta-learning.
Compositional solutions, where common elements of computation are flexibly
recombined into new configurations, are particularly well-suited for
meta-learning. Here, we propose a compositional meta-learning model that
explicitly represents tasks as structured combinations of reusable
computations. We achieve this by learning a generative model that captures the
underlying components and their statistics shared across a family of tasks.
This approach transforms learning a new task into a probabilistic inference
problem, which allows for finding solutions without parameter updates through
highly constrained hypothesis testing. Our model successfully recovers ground
truth components and statistics in rule learning and motor learning tasks. We
then demonstrate its ability to quickly infer new solutions from just single
examples. Together, our framework joins the expressivity of neural networks
with the data-efficiency of probabilistic inference to achieve rapid
compositional meta-learning.

</details>


### [484] [Universal Dynamic Regret and Constraint Violation Bounds for Constrained Online Convex Optimization](https://arxiv.org/abs/2510.01867)
*Subhamon Supantha,Abhishek Sinha*

Main category: cs.LG

TL;DR: 本论文提出了两种解决在线对抗约束凸优化的算法，实现了通用的动态遗憾界限和累积约束违反界限。


<details>
  <summary>Details</summary>
Motivation: 解决在线凸优化（OCO）框架的推广问题，其中包含在线对抗约束。

Method: 通过将约束学习问题约简为具有特殊构造的代理成本函数的标准OCO问题来解决。

Result: 提出了两种具有简单模块化结构的算法，实现了通用的动态遗憾界限和累积约束违反界限，优于现有技术水平。

Conclusion: 在成本函数和约束函数均由对手任意选择，且约束函数不一定包含共同可行点的最一般情况下，本论文的方法是有效的。

Abstract: We consider a generalization of the celebrated Online Convex Optimization
(OCO) framework with online adversarial constraints. We present two algorithms
having simple modular structures that yield universal dynamic regret and
cumulative constraint violation bounds, improving upon the state-of-the-art
results. Our results hold in the most general case when both the cost and
constraint functions are chosen arbitrarily by an adversary, and the constraint
functions need not contain any common feasible point. The results are
established by reducing the constrained learning problem to an instance of the
standard OCO problem with specially constructed surrogate cost functions.

</details>


### [485] [Randomized Gradient Subspaces for Efficient Large Language Model Training](https://arxiv.org/abs/2510.01878)
*Sahar Rajabi,Nayeema Nonta,Samanvay Vajpayee,Sirisha Rambhatla*

Main category: cs.LG

TL;DR: 梯度下降的内存瓶颈可以通过提取核心子空间来缓解，但需要新的算法来处理低秩和动态变化的梯度几何。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM训练方法受到内存限制，特别是优化器状态。现有方法试图通过将梯度投影到低维子空间来减轻成本，但没有充分利用梯度空间的全部信息。

Method: 分析了梯度空间及其子空间的动力学，识别出核心子空间和残余部分。提出了一种新的随机算法（GrassWalk 和 GrassJump），利用子空间信息来优化训练。

Result: 在LLaMA-1B和LLaMA-7B预训练任务上实现了最先进的内存节省，并提高了性能。

Conclusion: 梯度空间具有低秩和近乎平坦的曲率特性，可以通过利用子空间信息的随机算法（GrassWalk 和 GrassJump）来解决，从而在LLM训练中实现内存节省和性能提升。

Abstract: Training large language models (LLMs) is often bottlenecked by extreme memory
demands, with optimizer states dominating the footprint. Recent works mitigates
this cost by projecting gradients into low-dimensional subspaces using
sophisticated update strategies. In this paper, we analyze the dynamics of
gradient space and its underlying subspaces. We find that while a small
subspace captures most gradient energy, a significant portion still resides in
the residual bulk; moreover, the influence of the core subspace diminishes over
time and in deeper layers. We also observe that the gradient space exhibits
near-flat curvature, calling for algorithms that explicitly account for this
geometry. Motivated by these insights, we introduce a suite of randomized
algorithms, GrassWalk and GrassJump, which exploit subspace and achieve
state-of-the-art memory savings while improving performance on LLaMA-1B and
LLaMA-7B pretraining.

</details>


### [486] [Multi-marginal temporal Schrödinger Bridge Matching for video generation from unpaired data](https://arxiv.org/abs/2510.01894)
*Thomas Gravier,Thomas Boyer,Auguste Genovesio*

Main category: cs.LG

TL;DR: MMtSBM是一种新颖的视频生成方法，用于从非配对数据中重建动态过程，解决了高维数据和严格假设的限制。


<details>
  <summary>Details</summary>
Motivation: 许多自然动态过程（如细胞分化或疾病进展）只能通过静态样本快照观察，重建其时间演化以破译潜在动态特性具有重要的科学研究价值。现有方法在高维情况下可扩展性差且需要满足严格的假设。

Method: 提出了一种名为MMtSBM（Multi-Marginal temporal Schr"odinger Bridge Matching）的方法，该方法通过推导迭代马尔可夫拟合算法到多个边际，并以新颖的分解方式实现，扩展了扩散Schr"odinger桥匹配的理论保证和经验效率。

Result: MMtSBM在玩具示例中保留了理论性质，在真实世界数据集（如100维的转录组轨迹推断）上实现了最先进的性能，并首次在高维图像设置中恢复了耦合和动力学。

Conclusion: MMtSBM将多边际Schr"odinger桥确立为一种实用且有原则的方法，用于从静态数据中恢复隐藏的动力学。

Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or
disease progression -- can only be observed through the lens of static sample
snapshots. While challenging, reconstructing their temporal evolution to
decipher underlying dynamic properties is of major interest to scientific
research. Existing approaches enable data transport along a temporal axis but
are poorly scalable in high dimension and require restrictive assumptions to be
met. To address these issues, we propose \textit{\textbf{Multi-Marginal
temporal Schr\"odinger Bridge Matching}} (\textbf{MMtSBM}) \textit{for video
generation from unpaired data}, extending the theoretical guarantees and
empirical efficiency of Diffusion Schr\"odinger Bridge Matching
(arXiv:archive/2303.16852) by deriving the Iterative Markovian Fitting
algorithm to multiple marginals in a novel factorized fashion. Experiments show
that MMtSBM retains theoretical properties on toy examples, achieves
state-of-the-art performance on real world datasets such as transcriptomic
trajectory inference in 100 dimensions, and for the first time recovers
couplings and dynamics in very high dimensional image settings. Our work
establishes multi-marginal Schr\"odinger bridges as a practical and principled
approach for recovering hidden dynamics from static data.

</details>


### [487] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 我们提出了一种名为 Anchored Posterior Sampling (APS) 的新方法，用于从噪声测量中恢复图像，并使用预训练的离散扩散基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型在后验采样方面存在一些挑战，例如导数无关的引导产生稀疏信号，连续松弛限制了适用性，以及分裂吉布斯采样器受维度灾难的影响。因此，需要一种新的方法来克服这些限制。

Method: APS 通过两种关键创新解决了这些挑战：在离散嵌入空间中使用量化期望进行类似梯度的引导，以及使用锚定重遮蔽进行自适应解码。

Result: APS 在标准基准的线性和非线性逆问题上，在离散扩散采样器中取得了最先进的性能。此外，APS 在无训练风格化和文本引导编辑方面也显示出优势。

Conclusion: APS 是一种有效且多功能的离散扩散模型后验采样方法，能够处理各种逆问题，并在图像恢复、风格化和编辑等任务中取得优异性能。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [488] [Multimodal Foundation Models for Early Disease Detection](https://arxiv.org/abs/2510.01899)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: cs.LG

TL;DR: 该研究提出了一个多模态基础模型，通过注意力机制Transformer框架整合包括电子健康记录、医学影像、基因组学和可穿戴设备数据在内的多样化患者数据，以克服传统孤立分析数据源的局限性，从而识别对早期疾病诊断至关重要的跨模态关联。


<details>
  <summary>Details</summary>
Motivation: 传统医疗数据分析模型通常孤立地处理电子健康记录、医学影像、基因组学和可穿戴设备数据等多种数据流，这限制了它们识别对早期疾病诊断至关重要的跨模态关联的能力。

Method: 研究提出了一个多模态基础模型，利用注意力机制Transformer框架。首先，使用专门的编码器将每种模态数据映射到共享的潜在空间，然后通过多头注意力和残差归一化进行融合。该模型设计用于多任务预训练，便于在较少工作量下适应新疾病和数据集。实验策略在肿瘤学、心脏病学和神经病学领域的基准数据集上测试了早期检测任务。

Result: 该模型在肿瘤学、心脏病学和神经病学领域的基准数据集上进行了测试，专注于早期检测任务。

Conclusion: 该多模态基础模型通过整合多样化的患者数据并利用Transformer框架，能够提高预测准确性，辅助医生决策，为实现精准诊断的单一基础模型迈出重要一步。此外，该框架还集成了数据治理和模型管理工具，以提高透明度、可靠性和临床可解释性。

Abstract: Healthcare generates diverse streams of data, including electronic health
records (EHR), medical imaging, genetics, and ongoing monitoring from wearable
devices. Traditional diagnostic models frequently analyze these sources in
isolation, which constrains their capacity to identify cross-modal correlations
essential for early disease diagnosis. Our research presents a multimodal
foundation model that consolidates diverse patient data through an
attention-based transformer framework. At first, dedicated encoders put each
modality into a shared latent space. Then, they combine them using multi-head
attention and residual normalization. The architecture is made for pretraining
on many tasks, which makes it easy to adapt to new diseases and datasets with
little extra work. We provide an experimental strategy that uses benchmark
datasets in oncology, cardiology, and neurology, with the goal of testing early
detection tasks. The framework includes data governance and model management
tools in addition to technological performance to improve transparency,
reliability, and clinical interpretability. The suggested method works toward a
single foundation model for precision diagnostics, which could improve the
accuracy of predictions and help doctors make decisions.

</details>


### [489] [A Methodology for Transparent Logic-Based Classification Using a Multi-Task Convolutional Tsetlin Machine](https://arxiv.org/abs/2510.01906)
*Mayur Kishor Shende,Ole-Christoffer Granmo,Runar Helin,Vladimir I. Zadorozhny,Rishad Shafik*

Main category: cs.LG

TL;DR: Tsetlin Machine (TM) 在大规模多通道图像分类任务上表现出色，通过生成局部和全局解释，提高了模型的可解释性，同时在MNIST和CelebA数据集上取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨TM架构在处理大规模多通道（RGB）图像分类任务时的适用性，并提出一种生成局部解释和全局类别表示的方法，以增强模型的可解释性。

Method: 提出一种生成局部解释和全局类别表示的方法，这些解释可以被可视化为图像，用以总结卷积子句所捕获的知识。

Result: 在MNIST数据集上达到98.5%的准确率，在CelebA数据集上达到86.56%的F1分数（相比之下，ResNet50为88.07%）。

Conclusion: TM在复杂的大规模训练环境中，在保持模型可解释性的同时，能够与深度学习模型（如ResNet50）进行竞争。这加深了对TM子句的理解，并为其应用于更复杂、更多样化的数据集提供了见解。

Abstract: The Tsetlin Machine (TM) is a novel machine learning paradigm that employs
finite-state automata for learning and utilizes propositional logic to
represent patterns. Due to its simplistic approach, TMs are inherently more
interpretable than learning algorithms based on Neural Networks. The
Convolutional TM has shown comparable performance on various datasets such as
MNIST, K-MNIST, F-MNIST and CIFAR-2. In this paper, we explore the
applicability of the TM architecture for large-scale multi-channel (RGB) image
classification. We propose a methodology to generate both local interpretations
and global class representations. The local interpretations can be used to
explain the model predictions while the global class representations aggregate
important patterns for each class. These interpretations summarize the
knowledge captured by the convolutional clauses, which can be visualized as
images. We evaluate our methods on MNIST and CelebA datasets, using models that
achieve 98.5\% accuracy on MNIST and 86.56\% F1-score on CelebA (compared to
88.07\% for ResNet50) respectively. We show that the TM performs competitively
to this deep learning model while maintaining its interpretability, even in
large-scale complex training environments. This contributes to a better
understanding of TM clauses and provides insights into how these models can be
applied to more complex and diverse datasets.

</details>


### [490] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS通过选择性地微调与目标概念相关的神经元，在增量设置下实现了高效的个性化扩散模型，解决了灾难性遗忘问题，并提高了存储和处理效率。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，在增量设置下更新扩散模型具有实用价值，但计算成本高昂。

Method: 提出了一种名为概念神经元选择（CNS）的新颖学习策略，该策略能识别并选择性地微调与目标概念相关的神经元，以在持续学习方案中实现个性化，同时保留先前概念的知识，从而解决灾难性遗忘问题并保持零样本文本到图像生成能力。

Result: CNS 在真实数据集的评估中表现出最先进的性能，仅需极少的参数调整，在单概念和多概念个性化方面均优于先前方法。此外，CNS 可实现无融合操作，减少了持续个性化的内存存储和处理时间。

Conclusion: CNS 是一种简单而有效的持续学习策略，可实现扩散模型的个性化，在效率和性能上均优于现有方法。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [491] [Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement](https://arxiv.org/abs/2510.01910)
*Zhaoyan Wang,Zheng Gao,Arogya Kharel,In-Young Ko*

Main category: cs.LG

TL;DR: 该论文首次实证研究了图神经网络（GNNs）和大型语言模型（LLMs）在处理带复合缺陷的图数据时的表现，并提出了名为RoGRAD的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对GNN和LLM在处理复合图缺陷时的系统性理解，并且不清楚LLM增强方法的优越性是否在所有情况下都成立。

Method: 1. 对现有GNN和LLM增强方法在多种图缺陷下的表现进行实证基准测试。2. 提出了一种名为RoGRAD的新框架，采用迭代方式，利用检索增强生成（RAG）技术，通过提供类别一致且多样的增强数据，并结合图对比学习进行迭代优化，实现动态改进。

Result: RoGRAD框架在各项实验中表现优于传统的GNN和LLM增强基线方法，平均提升高达82.43%。

Conclusion: LLM增强并非在所有情况下都优于传统方法，RoGRAD框架通过迭代优化显著提升了图学习的鲁棒性和性能。

Abstract: Graph Neural Networks (GNNs) are widely adopted in Web-related applications,
serving as a core technique for learning from graph-structured data, such as
text-attributed graphs. Yet in real-world scenarios, such graphs exhibit
deficiencies that substantially undermine GNN performance. While prior
GNN-based augmentation studies have explored robustness against individual
imperfections, a systematic understanding of how graph-native and Large
Language Models (LLMs) enhanced methods behave under compound deficiencies is
still missing. Specifically, there has been no comprehensive investigation
comparing conventional approaches and recent LLM-on-graph frameworks, leaving
their merits unclear. To fill this gap, we conduct the first empirical study
that benchmarks these two lines of methods across diverse graph deficiencies,
revealing overlooked vulnerabilities and challenging the assumption that LLM
augmentation is consistently superior. Building on empirical findings, we
propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement
(RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is
the first iterative paradigm that leverages Retrieval-Augmented Generation
(RAG) to inject retrieval-grounded augmentations by supplying class-consistent,
diverse augmentations and enforcing discriminative representations through
iterative graph contrastive learning. It transforms LLM augmentation for graphs
from static signal injection into dynamic refinement. Extensive experiments
demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced
baselines, achieving up to 82.43% average improvement.

</details>


### [492] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: Equilibrium Matching (EqM) is a new generative model that learns an energy landscape's gradient, enabling optimization-based sampling and outperforming diffusion/flow models.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion and flow-based generative models use non-equilibrium, time-conditional dynamics. EqM aims to improve generative modeling by learning the equilibrium gradient of an implicit energy landscape.

Method: EqM learns the equilibrium gradient of an implicit energy landscape. Sampling is performed using an optimization-based process (gradient descent) on this learned landscape, allowing for adjustable step sizes, adaptive optimizers, and adaptive compute.

Result: EqM achieves state-of-the-art generation performance, with an FID of 1.90 on ImageNet 256x256, surpassing diffusion and flow models. It also demonstrates strong performance on tasks like image denoising, OOD detection, and image composition.

Conclusion: EqM provides a theoretically justified framework for learning and sampling from the data manifold. It offers a unified approach to equilibrium landscapes, bridging flow and energy-based models, and enabling optimization-driven inference for various generative tasks.

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


### [493] [StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold](https://arxiv.org/abs/2510.01938)
*Zhizhong Li,Sina Sajadmanesh,Jingtao Li,Lingjuan Lyu*

Main category: cs.LG

TL;DR: LoRA在参数高效的预训练模型微调中被广泛采用，但其性能仍落后于全量微调。本文提出了一种几何感知的LoRA扩展，采用三因子分解USVᵀ，并通过约束U和V位于Stiefel流形上，确保训练过程中的正交性，从而更有效地利用低秩流形下的几何结构。该方法能将任意欧氏优化器转换为黎曼优化器，实现高效的子空间学习，并兼容现有微调流程。实验结果表明，该方法在多项下游任务上优于现有的LoRA变体。


<details>
  <summary>Details</summary>
Motivation: LoRA在预训练模型微调中虽然参数高效，但性能不如全量微调，原因在于未能充分利用低秩流形下的几何结构。

Method: 提出一种几何感知的LoRA扩展，采用三因子分解USVᵀ，并将U和V约束在Stiefel流形上，保证正交性。使用灵活的几何优化设计将欧氏优化器转换为黎曼优化器。

Result: 在常识推理、数学和代码生成、图像分类和图像生成等多种下游任务上，该方法取得了优于现有LoRA变体的性能。

Conclusion: 所提出的几何感知的LoRA扩展方法，通过利用低秩流形的几何结构，显著提升了微调性能，并在多项任务上表现优于最先进的LoRA变体。

Abstract: Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient
technique for fine-tuning large-scale pre-trained models. However, it still
lags behind full fine-tuning in performance, partly due to its insufficient
exploitation of the geometric structure underlying low-rank manifolds. In this
paper, we propose a geometry-aware extension of LoRA that uses a three-factor
decomposition $U\!SV^\top$. Analogous to the structure of singular value
decomposition (SVD), it separates the adapter's input and output subspaces, $V$
and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie
on the Stiefel manifold, ensuring their orthonormality throughout the training.
To optimize on the Stiefel manifold, we employ a flexible and modular geometric
optimization design that converts any Euclidean optimizer to a Riemannian one.
It enables efficient subspace learning while remaining compatible with existing
fine-tuning pipelines. Empirical results across a wide range of downstream
tasks, including commonsense reasoning, math and code generation, image
classification, and image generation, demonstrate the superior performance of
our approach against the recent state-of-the-art variants of LoRA. Code is
available at https://github.com/SonyResearch/stella.

</details>


### [494] [Lower Bounds on Adversarial Robustness for Multiclass Classification with General Loss Functions](https://arxiv.org/abs/2510.01969)
*Camilo Andrés García Trillos,Nicolás García Trillos*

Main category: cs.LG

TL;DR: 该论文研究了多类分类对抗鲁棒性问题，并针对任意损失函数提出了对偶和重心重构方法。


<details>
  <summary>Details</summary>
Motivation: 在多类分类设定下，针对任意损失函数，推导对抗鲁棒分类的对偶和重心重构方法。

Method: 提出并推导了解决对抗鲁棒分类问题的对偶和重心重构方法，并针对交叉熵损失、幂形式损失函数和二次损失等特定情况进行了详细分析。

Result: 得到了比现有0-1损失函数更广泛适用的结果，并揭示了对抗鲁棒性、$\alpha$-公平打包问题和广义重心配分问题之间的联系。同时，通过数值实验验证了在交叉熵损失下可以获得更紧的对抗风险下界。

Conclusion: 该研究为设计超越0-1损失函数的鲁棒分类器提供了新的计算方法和理论基础。

Abstract: We consider adversarially robust classification in a multiclass setting under
arbitrary loss functions and derive dual and barycentric reformulations of the
corresponding learner-agnostic robust risk minimization problem. We provide
explicit characterizations for important cases such as the cross-entropy loss,
loss functions with a power form, and the quadratic loss, extending in this way
available results for the 0-1 loss. These reformulations enable efficient
computation of sharp lower bounds for adversarial risks and facilitate the
design of robust classifiers beyond the 0-1 loss setting. Our paper uncovers
interesting connections between adversarial robustness, $\alpha$-fair packing
problems, and generalized barycenter problems for arbitrary positive measures
where Kullback-Leibler and Tsallis entropies are used as penalties. Our
theoretical results are accompanied with illustrative numerical experiments
where we obtain tighter lower bounds for adversarial risks with the
cross-entropy loss function.

</details>


### [495] [Moon: A Modality Conversion-based Efficient Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.01970)
*Yuanyuan Yao,Yuhan Shi,Lu Chen,Ziquan Fang,Yunjun Gao,Leong Hou U,Yushuai Li,Tianyi Li*

Main category: cs.LG

TL;DR: Moon是一种基于监督模态转换的多元时间序列异常检测框架，通过将时间序列转换为图像表示并结合原始数值数据，提高了检测效率和准确性，并通过SHAP解释器增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多元时间序列异常检测方法在准确性（依赖误差阈值、忽视异常标签）、计算成本和对标签数据的依赖性方面存在挑战。本研究旨在解决这些局限性。

Method: Moon框架提出了一种新颖的多元马尔可夫转移场（MV-MTF）技术将数值时间序列转换为图像表示，并采用多模态CNN融合数值和图像数据，最后使用基于SHAP的异常解释器进行可解释性分析。

Result: 在六个真实世界的时间序列数据集上的实验表明，Moon在效率方面提高了93%，准确率方面提高了4%，解释性方面提高了10.8%，优于六种最先进的方法。

Conclusion: Moon框架通过创新的数据转换和融合方法，显著提高了多元时间序列异常检测的效率、准确性和可解释性，为该领域带来了新的解决方案。

Abstract: Multivariate time series (MTS) anomaly detection identifies abnormal patterns
where each timestamp contains multiple variables. Existing MTS anomaly
detection methods fall into three categories: reconstruction-based,
prediction-based, and classifier-based methods. However, these methods face two
key challenges: (1) Unsupervised learning methods, such as reconstruction-based
and prediction-based methods, rely on error thresholds, which can lead to
inaccuracies; (2) Semi-supervised methods mainly model normal data and often
underuse anomaly labels, limiting detection of subtle anomalies;(3) Supervised
learning methods, such as classifier-based approaches, often fail to capture
local relationships, incur high computational costs, and are constrained by the
scarcity of labeled data. To address these limitations, we propose Moon, a
supervised modality conversion-based multivariate time series anomaly detection
framework. Moon enhances the efficiency and accuracy of anomaly detection while
providing detailed anomaly analysis reports. First, Moon introduces a novel
multivariate Markov Transition Field (MV-MTF) technique to convert numeric time
series data into image representations, capturing relationships across
variables and timestamps. Since numeric data retains unique patterns that
cannot be fully captured by image conversion alone, Moon employs a
Multimodal-CNN to integrate numeric and image data through a feature fusion
model with parameter sharing, enhancing training efficiency. Finally, a
SHAP-based anomaly explainer identifies key variables contributing to
anomalies, improving interpretability. Extensive experiments on six real-world
MTS datasets demonstrate that Moon outperforms six state-of-the-art methods by
up to 93% in efficiency, 4% in accuracy and, 10.8% in interpretation
performance.

</details>


### [496] [Private Federated Multiclass Post-hoc Calibration](https://arxiv.org/abs/2510.01987)
*Samuel Maddock,Graham Cormode,Carsten Maple*

Main category: cs.LG

TL;DR: 本篇论文提出了在联邦学习（FL）环境中进行模型校准的方法，解决了隐私和客户端异构性带来的挑战，并提出了两种主要策略：联邦温度缩放（适用于带差分隐私的FL）和加权分箱（适用于不带差分隐私的FL）。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习（FL）环境中，由于数据分布在多个客户端且不能集中，训练好的模型预测概率往往不能准确反映真实结果频率，这在医疗和金融等领域尤为关键，但现有的联邦私有校准研究却很少。本研究旨在解决这一问题。

Method: 本研究将传统的模型后置校准技术（如直方图分箱和温度缩放）引入联邦学习环境，并提出了适用于客户端异构性的新方法。研究了联邦设置和用户级差分隐私（DP）设置对校准准确性的影响，并提出了相应的缓解策略。

Result: 研究表明，在客户端异构性下，联邦温度缩放方法在有差分隐私的FL环境中效果最佳，而加权分箱方法在不需要差分隐私时效果最好。这两种方法都能有效缓解因异构性导致的校准准确性下降问题。

Conclusion: 本研究成功地将后置校准技术整合到联邦学习中，并针对隐私和客户端异构性提出了有效的解决方案，为在关键领域（如医疗和金融）中实现更可靠的联邦学习模型提供了重要支持。

Abstract: Calibrating machine learning models so that predicted probabilities better
reflect the true outcome frequencies is crucial for reliable decision-making
across many applications. In Federated Learning (FL), the goal is to train a
global model on data which is distributed across multiple clients and cannot be
centralized due to privacy concerns. FL is applied in key areas such as
healthcare and finance where calibration is strongly required, yet federated
private calibration has been largely overlooked. This work introduces the
integration of post-hoc model calibration techniques within FL. Specifically,
we transfer traditional centralized calibration methods such as histogram
binning and temperature scaling into federated environments and define new
methods to operate them under strong client heterogeneity. We study (1) a
federated setting and (2) a user-level Differential Privacy (DP) setting and
demonstrate how both federation and DP impacts calibration accuracy. We propose
strategies to mitigate degradation commonly observed under heterogeneity and
our findings highlight that our federated temperature scaling works best for
DP-FL whereas our weighted binning approach is best when DP is not required.

</details>


### [497] [PepCompass: Navigating peptide embedding spaces using Riemannian Geometry](https://arxiv.org/abs/2510.01988)
*Marcin Możejko,Adam Bielecki,Jurand Prądzyński,Marcin Traskowski,Antoni Janowski,Karol Jurasz,Michał Kucharczyk,Hyun-Su Lee,Marcelo Der Torossian Torres,Cesar de la Fuente-Nunez,Paulina Szymczak,Michał Kmicikiewicz,Ewa Szczurek*

Main category: cs.LG

TL;DR: 生成模型在抗菌肽发现中存在挑战，PepCompass框架通过考虑解码器诱导的几何形状来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽发现因肽空间巨大和活性肽稀少而面临挑战。现有生成模型忽略了解码器诱导的几何形状，并依赖于平坦的欧氏度量，导致探索和优化效率低下。

Method: PepCompass框架的核心是定义了一个$\\kappa$-稳定李群并提出了一种包含二阶黎曼布朗运动高效采样和切空间突变枚举的局部探索方法，并结合了这些方法来执行局部枚举贝叶斯优化（LE-BO）。此外，还提出了一种通过最小化势能的测地线搜索（PoGS）来优化原型嵌入。

Result: 体外验证证实了PepCompass的有效性：PoGS产生了四个新的种子肽，随后通过LE-BO优化发现了25种具有广泛抗菌谱的高活性肽，包括对耐药菌株的活性。

Conclusion: 几何感知探索为抗菌肽设计提供了一种强大的新范例。

Abstract: Antimicrobial peptide discovery is challenged by the astronomical size of
peptide space and the relative scarcity of active peptides. Generative models
provide continuous latent "maps" of peptide space, but conventionally ignore
decoder-induced geometry and rely on flat Euclidean metrics, rendering
exploration and optimization distorted and inefficient. Prior manifold-based
remedies assume fixed intrinsic dimensionality, which critically fails in
practice for peptide data. Here, we introduce PepCompass, a geometry-aware
framework for peptide exploration and optimization. At its core, we define a
Union of $\kappa$-Stable Riemannian Manifolds $\mathbb{M}^{\kappa}$, a family
of decoder-induced manifolds that captures local geometry while ensuring
computational stability. We propose two local exploration methods: Second-Order
Riemannian Brownian Efficient Sampling, which provides a convergent
second-order approximation to Riemannian Brownian motion, and Mutation
Enumeration in Tangent Space, which reinterprets tangent directions as discrete
amino-acid substitutions. Combining these yields Local Enumeration Bayesian
Optimization (LE-BO), an efficient algorithm for local activity optimization.
Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which
interpolates between prototype embeddings along property-enriched geodesics,
biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro
validation confirms the effectiveness of PepCompass: PoGS yields four novel
seeds, and subsequent optimization with LE-BO discovers 25 highly active
peptides with broad-spectrum activity, including against resistant bacterial
strains. These results demonstrate that geometry-informed exploration provides
a powerful new paradigm for antimicrobial peptide design.

</details>


### [498] [Normality Calibration in Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2510.02014)
*Guolei Zeng,Hezhe Qiao,Guoguo Ai,Jinsong Guo,Guansong Pang*

Main category: cs.LG

TL;DR: GraphNC 通过对教师模型的异常分数和节点表示进行校准来改进半监督图异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有半监督图异常检测方法仅依赖标记的正常节点，容易导致过拟合和高错误率。

Method: GraphNC 框架包含两个组件：分数对齐 (ScoreDA) 和基于扰动的规范性正则化 (NormReg)。ScoreDA 通过使模型的分数与教师模型的分数对齐来优化分数，而 NormReg 通过最小化基于扰动的节点表示的一致性损失来规范化表示空间中的图规范性。

Result: ScoreDA 将正常和异常节点的分数推向两个极端，使分数更具可分离性。NormReg 进一步通过使正常节点的表示更紧凑来提高鲁棒性，以应对教师模型中不准确的分数。

Conclusion: GraphNC 通过结合两种方法，显着提高了半监督图异常检测的准确性。

Abstract: Graph anomaly detection (GAD) has attracted growing interest for its crucial
ability to uncover irregular patterns in broad applications. Semi-supervised
GAD, which assumes a subset of annotated normal nodes available during
training, is among the most widely explored application settings. However, the
normality learned by existing semi-supervised GAD methods is limited to the
labeled normal nodes, often inclining to overfitting the given patterns. These
can lead to high detection errors, such as high false positives. To overcome
this limitation, we propose GraphNC , a graph normality calibration framework
that leverages both labeled and unlabeled data to calibrate the normality from
a teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly
score and node representation spaces. GraphNC includes two main components,
anomaly score distribution alignment (ScoreDA) and perturbation-based normality
regularization (NormReg). ScoreDA optimizes the anomaly scores of our model by
aligning them with the score distribution yielded by the teacher model. Due to
accurate scores in most of the normal nodes and part of the anomaly nodes in
the teacher model, the score alignment effectively pulls the anomaly scores of
the normal and abnormal classes toward the two ends, resulting in more
separable anomaly scores. Nevertheless, there are inaccurate scores from the
teacher model. To mitigate the misleading by these scores, NormReg is designed
to regularize the graph normality in the representation space, making the
representations of normal nodes more compact by minimizing a
perturbation-guided consistency loss solely on the labeled nodes.

</details>


### [499] [FairContrast: Enhancing Fairness through Contrastive learning and Customized Augmenting Methods on Tabular Data](https://arxiv.org/abs/2510.02017)
*Aida Tayebi,Ali Khodabandeh Yalabadi,Mehdi Yazdani-Jahromi,Ozlem Ozmen Garibay*

Main category: cs.LG

TL;DR: 本研究提出了一种针对表格数据的对比学习框架，以减少偏差并学习公平的表征。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统日益融入日常生活，开发公平、无偏见的模型变得至关重要。考虑到人工智能系统的社会影响不仅是技术挑战，也是道义上的责任。学习公平且鲁棒的表征是有效消除算法偏差、提高公平性同时保留预测任务所需关键信息的一种有力方法。然而，在表格数据上应用这些方法来学习公平表征的研究尚不充分。

Method: 本研究提出了一种专门用于解决偏差和学习表格数据公平表征的对比学习框架。通过策略性地选择正样本对，并采用监督和自监督对比学习方法。

Result: 与现有的表格数据对比学习模型相比，该方法显著减少了偏差。结果表明，该方法在最小化准确性损失的同时，能够有效地减缓偏差，并能在各种下游任务中利用学习到的公平表征。

Conclusion: 本研究提出的对比学习框架能够有效地学习表格数据的公平表征，在减小偏差的同时，对模型准确性的影响很小。

Abstract: As AI systems become more embedded in everyday life, the development of fair
and unbiased models becomes more critical. Considering the social impact of AI
systems is not merely a technical challenge but a moral imperative. As
evidenced in numerous research studies, learning fair and robust
representations has proven to be a powerful approach to effectively debiasing
algorithms and improving fairness while maintaining essential information for
prediction tasks. Representation learning frameworks, particularly those that
utilize self-supervised and contrastive learning, have demonstrated superior
robustness and generalizability across various domains. Despite the growing
interest in applying these approaches to tabular data, the issue of fairness in
these learned representations remains underexplored. In this study, we
introduce a contrastive learning framework specifically designed to address
bias and learn fair representations in tabular datasets. By strategically
selecting positive pair samples and employing supervised and self-supervised
contrastive learning, we significantly reduce bias compared to existing
state-of-the-art contrastive learning models for tabular data. Our results
demonstrate the efficacy of our approach in mitigating bias with minimum
trade-off in accuracy and leveraging the learned fair representations in
various downstream tasks.

</details>


### [500] [Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning](https://arxiv.org/abs/2510.02049)
*Jinshu Huang,Haibin Su,Xue-Cheng Tai,Chunlin Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In deep learning, dense layer connectivity has become a key design principle
in deep neural networks (DNNs), enabling efficient information flow and strong
performance across a range of applications. In this work, we model densely
connected DNNs mathematically and analyze their learning problems in the
deep-layer limit. For a broad applicability, we present our analysis in a
framework setting of DNNs with densely connected layers and general non-local
feature transformations (with local feature transformations as special cases)
within layers, which is called dense non-local (DNL) framework and includes
standard DenseNets and variants as special examples. In this formulation, the
densely connected networks are modeled as nonlinear integral equations, in
contrast to the ordinary differential equation viewpoint commonly adopted in
prior works. We study the associated training problems from an optimal control
perspective and prove convergence results from the network learning problem to
its continuous-time counterpart. In particular, we show the convergence of
optimal values and the subsequence convergence of minimizers, using a piecewise
linear extension and $\Gamma$-convergence analysis. Our results provide a
mathematical foundation for understanding densely connected DNNs and further
suggest that such architectures can offer stability of training deep models.

</details>


### [501] [Adaptive Heterogeneous Mixtures of Normalising Flows for Robust Variational Inference](https://arxiv.org/abs/2510.02056)
*Benjamin Wiriyapong,Oktay Karakuş,Kirill Sidorov*

Main category: cs.LG

TL;DR: AMF-VI是一种自适应混合流变分推断方法，通过混合多个不同的流（MAF, RealNVP, RBIG）来改进变分推断的性能，尤其是在处理复杂和多样化的后验分布时。


<details>
  <summary>Details</summary>
Motivation: 单一标准化流变分推断方法在处理不同分布时可能表现不一致，AMF-VI旨在解决这一问题，提高变分推断的鲁棒性。

Method: AMF-VI采用两阶段训练：首先分别训练各个流（专家），然后通过似然驱动的更新来估计全局权重，实现自适应组合，无需修改模型结构或引入样本门控。

Result: 在六种不同的后验分布（banana, X-shape, two-moons, rings, bimodal, five-mode mixture）上，AMF-VI的负对数似然度优于单一流基线，并在Wasserstein-2和MDD指标上取得了稳定提升，表明其在不同形状和模态的数据上具有更优越的性能。

Conclusion: AMF-VI通过自适应地混合多种不同的流，提供了一种可靠且高效的变分推断方法，能够处理各种后验分布，并保留每个流的归纳偏置，同时仅带来微小的额外开销。

Abstract: Normalising-flow variational inference (VI) can approximate complex
posteriors, yet single-flow models often behave inconsistently across
qualitatively different distributions. We propose Adaptive Mixture Flow
Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows
(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of
individual flows, and (ii) adaptive global weight estimation via
likelihood-driven updates, without per-sample gating or architectural changes.
Evaluated on six canonical posterior families of banana, X-shape, two-moons,
rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower
negative log-likelihood than each single-flow baseline and delivers stable
gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),
indicating improved robustness across shapes and modalities. The procedure is
efficient and architecture-agnostic, incurring minimal overhead relative to
standard flow training, and demonstrates that adaptive mixtures of diverse
flows provide a reliable route to robust VI across diverse posterior families
whilst preserving each expert's inductive bias.

</details>


### [502] [Inferring Optical Tissue Properties from Photoplethysmography using Hybrid Amortized Inference](https://arxiv.org/abs/2510.02073)
*Jens Behrmann,Maria R. Cervera,Antoine Wehenkel,Andrew C. Miller,Albert Cerussi,Pranay Jain,Vivek Venugopal,Shijie Yan,Guillermo Sapiro,Luca Pegolotti,Jörn-Henrik Jacobsen*

Main category: cs.LG

TL;DR: PPG信号包含丰富的生理信息，但现有深度学习模型缺乏可解释性。本文提出了PPGen（一个生物物理模型）和HAI（一种混合放大推理方法），能够从PPG信号中快速、稳健地估计生理参数，并纠正模型错误。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在利用PPG信号的丰富信息时，存在预测能力、临床可解释性和传感器设计之间的矛盾，因为它们依赖于生理意义不明确的特征。

Method: 提出PPGen（一个生物物理模型）将PPG信号与可解释的生理和光学参数联系起来。在此基础上，提出HAI（混合放大推理）方法，以快速、稳健和可扩展的方式从PPG信号中估计相关生理参数，并纠正模型错误。

Result: 在大量的计算机模拟实验中，证明了HAI在各种噪声和传感器条件下能够准确推断生理参数。

Conclusion: 本文的研究为PPG模型提供了一条途径，可以在保持深度学习所需保真度的同时，支持临床解释和硬件设计的优化。

Abstract: Smart wearables enable continuous tracking of established biomarkers such as
heart rate, heart rate variability, and blood oxygen saturation via
photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer
physiological information, as recent deep learning (DL) studies demonstrate.
However, DL models often rely on features with unclear physiological meaning,
creating a tension between predictive power, clinical interpretability, and
sensor design. We address this gap by introducing PPGen, a biophysical model
that relates PPG signals to interpretable physiological and optical parameters.
Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast,
robust, and scalable estimation of relevant physiological parameters from PPG
signals while correcting for model misspecification. In extensive in-silico
experiments, we show that HAI can accurately infer physiological parameters
under diverse noise and sensor conditions. Our results illustrate a path toward
PPG models that retain the fidelity needed for DL-based features while
supporting clinical interpretation and informed hardware design.

</details>


### [503] [Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions](https://arxiv.org/abs/2510.02081)
*Zhaoyi Li,Jingtao Ding,Yong Li,Shihua Li*

Main category: cs.LG

TL;DR: Flow Matching (FM) 算法在生成任务中表现出色，但存在训练-推理间隙和僵化问题。本文提出通过最大似然估计进行微调，理论分析了训练损失与推理误差的关系，并设计了包含残差微调和收缩性质的架构，实验验证了该方法能可靠提升 FM 的推理性能。


<details>
  <summary>Details</summary>
Motivation: Flow Matching (FM) 算法在生成任务中表现出色，尤其是在机器人操作领域。但它存在训练-推理间隙，即无法在训练阶段评估模型输出，这在需要高精度的任务中尤为明显。此外，FM 过于追求预设的直线路径可能导致系统僵化。因此，需要一种方法来弥补这些不足。

Method: 本文提出通过最大似然估计（Maximum Likelihood Estimation）对 FM 进行微调，利用其平滑的常微分方程（ODE）形式，解决了随机微分方程（SDE）在扩散模型中引入的限制。具体方法包括直接微调和基于残差的微调。基于残差的微调还能通过特定架构引入收缩性质，提高模型的鲁棒性和可解释性。

Result: 通过理论分析，本文揭示了 FM 的训练损失与推理误差之间的关系。实验结果表明，所提出的微调方法，包括直接微调和基于残差的微调（结合收缩性质），在图像生成和机器人操作任务中都可靠地提升了 FM 的推理性能。

Conclusion: 本文提出的通过最大似然估计对 Flow Matching 进行微调的方法，有效解决了训练-推理间隙和系统僵化问题，并通过理论分析和实验验证，证明了其在提升 FM 推理性能方面的有效性，尤其是在机器人操作等高精度应用场景中。

Abstract: Flow Matching (FM) algorithm achieves remarkable results in generative tasks
especially in robotic manipulation. Building upon the foundations of diffusion
models, the simulation-free paradigm of FM enables simple and efficient
training, but inherently introduces a train-inference gap. Specifically, we
cannot assess the model's output during the training phase. In contrast, other
generative models including Variational Autoencoder (VAE), Normalizing Flow and
Generative Adversarial Networks (GANs) directly optimize on the reconstruction
loss. Such a gap is particularly evident in scenarios that demand high
precision, such as robotic manipulation. Moreover, we show that FM's
over-pursuit of straight predefined paths may introduce some serious problems
such as stiffness into the system. These motivate us to fine-tune FM via
Maximum Likelihood Estimation of reconstructions - an approach made feasible by
FM's underlying smooth ODE formulation, in contrast to the stochastic
differential equations (SDEs) used in diffusion models. This paper first
theoretically analyzes the relation between training loss and inference error
in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood
Estimation of reconstructions, which includes both straightforward fine-tuning
and residual-based fine-tuning approaches. Furthermore, through specifically
designed architectures, the residual-based fine-tuning can incorporate the
contraction property into the model, which is crucial for the model's
robustness and interpretability. Experimental results in image generation and
robotic manipulation verify that our method reliably improves the inference
performance of FM.

</details>


### [504] [KAIROS: Unified Training for Universal Non-Autoregressive Time Series Forecasting](https://arxiv.org/abs/2510.02084)
*Kuiye Ding,Fanda Fan,Zheya Wang,Hongxiao Li,Yifan Wang,Lei Wang,Chunjie Luo,Jianfeng Zhan*

Main category: cs.LG

TL;DR: KAIROS是一个用于网络应用的新型非自回归时间序列预测框架，它能直接对分段多峰值分布进行建模，避免了误差累积，实现了即时推理，并超越了现有的非自回归模型，在零样本泛化和推理成本方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 在万维网中，可靠的时间序列预测对于资源规划、缓存放置和异常响应至关重要，而与其它领域相比，Web应用的时间序列预测需要更快的响应速度以支持实时决策。

Method: KAIROS是一个非自回归时间序列预测框架，它直接对分段多峰值分布进行建模。

Result: KAIROS在六个广泛使用的基准测试中表现出强大的零样本泛化能力，其预测性能与同等规模的最新基础模型相当，但推理成本却大大降低。

Conclusion: KAIROS强调了非自回归设计作为时间序列基础模型的可扩展范式的重要性。

Abstract: In the World Wide Web, reliable time series forecasts provide the
forward-looking signals that drive resource planning, cache placement, and
anomaly response, enabling platforms to operate efficiently as user behavior
and content distributions evolve. Compared with other domains, time series
forecasting for Web applications requires much faster responsiveness to support
real-time decision making. We present KAIROS, a non-autoregressive time series
forecasting framework that directly models segment-level multi-peak
distributions. Unlike autoregressive approaches, KAIROS avoids error
accumulation and achieves just-in-time inference, while improving over existing
non-autoregressive models that collapse to over-smoothed predictions. Trained
on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization
on six widely used benchmarks, delivering forecasting performance comparable to
state-of-the-art foundation models with similar scale, at a fraction of their
inference cost. Beyond empirical results, KAIROS highlights the importance of
non-autoregressive design as a scalable paradigm for foundation models in time
series.

</details>


### [505] [Learning Model Representations Using Publicly Available Model Hubs](https://arxiv.org/abs/2510.02096)
*Damian Falk,Konstantin Schürholt,Konstantinos Tzevelekakis,Léo Meynent,Damian Borth*

Main category: cs.LG

TL;DR: 可以通过分析Hugging Face等公开的模型库中的模型权重来学习有意义的权重空间表示，而无需依赖精心构建的大型模型库。


<details>
  <summary>Details</summary>
Motivation: 现有的权重空间学习方法依赖于大型、精心构建的模型库（model zoos），这些模型库的训练过程耗时耗力，且在规模和灵活性上存在限制。

Method: 提出了一种新的权重空间学习骨干网络，可以直接在 Hugging Face 等包含高度异构模型的公开模型库中进行训练，无需对模型进行预先整理和记录。

Result: 在 Hugging Face 模型库上训练的权重空间表示在下游任务上表现优于在传统模型库上训练的模型，并且能够泛化到未见过的数据模态。

Conclusion: 精心构建的模型库并非学习高质量权重空间表示的必需品，公开的模型库也能够满足该领域的需求。

Abstract: The weights of neural networks have emerged as a novel data modality, giving
rise to the field of weight space learning. A central challenge in this area is
that learning meaningful representations of weights typically requires large,
carefully constructed collections of trained models, typically referred to as
model zoos. These model zoos are often trained ad-hoc, requiring large
computational resources, constraining the learned weight space representations
in scale and flexibility. In this work, we drop this requirement by training a
weight space learning backbone on arbitrary models downloaded from large,
unstructured model repositories such as Hugging Face. Unlike curated model
zoos, these repositories contain highly heterogeneous models: they vary in
architecture and dataset, and are largely undocumented. To address the
methodological challenges posed by such heterogeneity, we propose a new weight
space backbone designed to handle unstructured model populations. We
demonstrate that weight space representations trained on models from Hugging
Face achieve strong performance, often outperforming backbones trained on
laboratory-generated model zoos. Finally, we show that the diversity of the
model weights in our training set allows our weight space model to generalize
to unseen data modalities. By demonstrating that high-quality weight space
representations can be learned in the wild, we show that curated model zoos are
not indispensable, thereby overcoming a strong limitation currently faced by
the weight space learning community.

</details>


### [506] [PENEX: AdaBoost-Inspired Neural Network Regularization](https://arxiv.org/abs/2510.02107)
*Klaus-Rudolf Kladny,Bernhard Schölkopf,Michael Muehlebach*

Main category: cs.LG

TL;DR: AdaBoost通过最小化指数损失来顺序拟合弱学习器，该损失比交叉熵等其他损失函数更严厉地惩罚错误标记的数据点。本研究提出了PENEX（惩罚指数损失），一种多类指数损失的新公式，具有理论基础，并且可以通过一阶方法进行优化。PENEX能隐式最大化数据点间隔，并且其梯度增量能隐式参数化提升框架中的弱学习器。在计算机视觉和语言任务中，PENEX的正则化效果优于现有方法，计算成本相似。PENEX可以作为AdaBoost的替代品，用于有效训练和微调深度神经网络。


<details>
  <summary>Details</summary>
Motivation: AdaBoost在实践中具有良好的泛化能力，但其指数损失函数会更严厉地惩罚错误标记的数据点。本研究旨在提出一种新的多类指数损失公式PENEX，该公式具有理论基础，并且易于通过一阶方法进行优化。

Method: 提出了一种名为PENEX（惩罚指数损失）的新公式，用于多类指数损失。通过理论和实验证明PENEX能够隐式最大化数据点间隔，并且其梯度增量能够隐式参数化提升框架中的弱学习器。

Result: PENEX在计算机视觉和语言任务中表现出比现有方法更好的正则化效果，且计算成本相似。理论和实验均表明PENEX能够隐式最大化数据点间隔。

Conclusion: PENEX作为一种AdaBoost的替代方案，在有效训练和微调深度神经网络方面具有潜力。

Abstract: AdaBoost sequentially fits so-called weak learners to minimize an exponential
loss, which penalizes mislabeled data points more severely than other loss
functions like cross-entropy. Paradoxically, AdaBoost generalizes well in
practice as the number of weak learners grows. In the present work, we
introduce Penalized Exponential Loss (PENEX), a new formulation of the
multi-class exponential loss that is theoretically grounded and, in contrast to
the existing formulation, amenable to optimization via first-order methods. We
demonstrate both empirically and theoretically that PENEX implicitly maximizes
margins of data points. Also, we show that gradient increments on PENEX
implicitly parameterize weak learners in the boosting framework. Across
computer vision and language tasks, we show that PENEX exhibits a regularizing
effect often better than established methods with similar computational cost.
Our results highlight PENEX's potential as an AdaBoost-inspired alternative for
effective training and fine-tuning of deep neural networks.

</details>


### [507] [Hybrid Deep Learning Modeling Approach to Predict Natural Gas Consumption of Home Subscribers on Limited Data](https://arxiv.org/abs/2510.02115)
*Milad Firoozeh,Nader Dashti,Mohammad Ali Hatefi*

Main category: cs.LG

TL;DR: 伊朗Zanjan省的居民用气量预测研究表明，混合BiLSTM-XGBoost模型在准确性方面优于单独的LSTM、GRU模型，且在数据量有限时表现稳健，有助于提高资源管理效率。


<details>
  <summary>Details</summary>
Motivation: 伊朗面临人口增长和能源消耗增加导致的冬季供气压力下降和天然气短缺问题，尤其是在居民消费部门。因此，有必要控制和预测居民用气量。

Method: 使用LSTM、GRU和混合BiLSTM-XGBoost机器学习模型，基于2017年至2022年的六年月度燃气消耗和气象数据，对伊朗Zanjan省的居民用气量进行预测和分析。

Result: 混合BiLSTM-XGBoost模型在准确性方面优于其他模型，表现出更低的RMSE、MAPE和MPE值。该模型在数据量有限的情况下也表现出强大的性能。

Conclusion: 机器学习方法，特别是混合模型，可以有效地管理和预测天然气消耗，有助于更有效的资源管理，减少季节性短缺。研究强调了将地理和气候因素纳入预测模型的重要性，因为这些因素对不同地区的天然气使用量有显著影响。

Abstract: Today, natural gas, as a clean fuel and the best alternative to crude oil,
covers a significant part of global demand. Iran is one of the largest
countries with energy resources and in terms of gas is the second-largest
country in the world. But, due to the increase in population and energy
consumption, it faces problems such as pressure drops and gas outages yearly in
cold seasons and therefore it is necessary to control gas consumption,
especially in the residential sector, which has the largest share in Iran. This
study aims to analyze and predict gas consumption for residential customers in
Zanjan province, Iran, using machine learning models, including LSTM, GRU, and
a hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and
meteorology data collected over six years, from 2017 to 2022. The models were
trained and evaluated based on their ability to accurately predict consumption
patterns. The results indicate that the hybrid BiLSTM-XGBoost model
outperformed the other models in terms of accuracy, with lower Root Mean
Squared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean
Percentage Error (MPE). Additionally, the Hybrid model demonstrated robust
performance, particularly in scenarios with limited data. The findings suggest
that machine learning approaches, particularly hybrid models, can be
effectively utilized to manage and predict gas consumption, contributing to
more efficient resource management and reducing seasonal shortages. This study
highlights the importance of incorporating geographical and climatic factors in
predictive modeling, as these significantly influence gas usage across
different regions.

</details>


### [508] [Ensemble Threshold Calibration for Stable Sensitivity Control](https://arxiv.org/abs/2510.02116)
*John N. Daras*

Main category: cs.LG

TL;DR: 该研究提出了一种实现精确召回控制的端到端框架，可用于大规模空间融合和实体匹配任务，能在保证召回率的同时减少人工审核成本。


<details>
  <summary>Details</summary>
Motivation: 在空间融合和实体匹配任务中，精确的召回控制至关重要，漏掉少量匹配项会影响分析结果，而过多人工审核会增加成本。传统方法在召回率估计上存在偏差和高方差问题。

Method: 该框架首先使用等格边界框过滤和压缩稀疏行（CSR）候选表示来减少候选对的数量。然后，使用确定性xxHash bootstrap样本训练一个轻量级神经网络排序器，并通过一次前向传播将其分数应用到所有剩余对，以构建可复现的、分数十分位数分层的校准集。最后，通过逆方差加权聚合四种互补的阈值估计器（包括精确分位数），并通过九个独立子样本进行融合，以降低阈值方差。

Result: 在两个真实的地籍数据集（约6.31M和67.34M对）上进行评估，该方法能够以很小的误差稳定地达到预定的召回率目标，并减少了冗余验证。整个流程可在单个TPU v3核心上端到端运行。

Conclusion: 该研究提出的端到端框架能够以精确的召回率、低方差和高效率（TPU友好）解决了大规模空间融合和实体匹配中的挑战。

Abstract: Precise recall control is critical in large-scale spatial conflation and
entity-matching tasks, where missing even a few true matches can break
downstream analytics, while excessive manual review inflates cost. Classical
confidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds
on recall, but they routinely overshoot the target by several percentage points
and exhibit high run-to-run variance under skewed score distributions. We
present an end-to-end framework that achieves exact recall with sub-percent
variance over tens of millions of geometry pairs, while remaining TPU-friendly.
Our pipeline starts with an equigrid bounding-box filter and compressed sparse
row (CSR) candidate representation, reducing pair enumeration by two orders of
magnitude. A deterministic xxHash bootstrap sample trains a lightweight neural
ranker; its scores are propagated to all remaining pairs via a single forward
pass and used to construct a reproducible, score-decile-stratified calibration
set. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,
Wilson, and an exact quantile - are aggregated via inverse-variance weighting,
then fused across nine independent subsamples. This ensemble reduces threshold
variance compared to any single method. Evaluated on two real cadastral
datasets (approximately 6.31M and 67.34M pairs), our approach consistently hits
a recall target within a small error, decreases redundant verifications
relative to other calibrations, and runs end-to-end on a single TPU v3 core.

</details>


### [509] [DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding](https://arxiv.org/abs/2510.02117)
*Samhita Pal,James O'quinn,Kaveh Aryan,Heather Pua,James P. Long,Amir Asiaee*

Main category: cs.LG

TL;DR: DECOR是一个用于学习具有潜在混淆的线性高斯SEM的联合DAG和相关噪声模型的可微分估计器。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理潜在混淆时存在局限性，DECOR旨在解决此问题。

Method: DECOR是一种基于似然的、完全可微分的估计器，它联合学习DAG和相关噪声模型，并通过平滑-无环图更新和凸噪声更新进行交替。

Result: 在合成基准测试中，DECOR在各种混淆密度、图密度、潜在秩和维度下，与强基线方法相匹配或表现更优，尤其是在非普遍混淆的情况下。

Conclusion: DECOR能够唯一地确定有向结构和噪声，前提是混合图没有弓形且噪声协方差具有均匀的特征值裕度。

Abstract: We study structure learning for linear Gaussian SEMs in the presence of
latent confounding. Existing continuous methods excel when errors are
independent, while deconfounding-first pipelines rely on pervasive factor
structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based
and fully differentiable estimator that jointly learns a DAG and a correlated
noise model. Our theory gives simple sufficient conditions for global parameter
identifiability: if the mixed graph is bow free and the noise covariance has a
uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the
observational covariance is injective, so both the directed structure and the
noise are uniquely determined. The estimator alternates a smooth-acyclic graph
update with a convex noise update and can include a light bow complementarity
penalty or a post hoc reconciliation step. On synthetic benchmarks that vary
confounding density, graph density, latent rank, and dimension with $n<p$,
\textsc{DECOR} matches or outperforms strong baselines and is especially robust
when confounding is non-pervasive, while remaining competitive under
pervasiveness.

</details>


### [510] [StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?](https://arxiv.org/abs/2510.02209)
*Yanxu Chen,Zijun Yao,Yantao Liu,Jin Ye,Jianing Yu,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: StockBench是一个用于评估LLM在真实股票交易环境中表现的基准，它表明LLM在动态金融任务上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 金融领域LLM代理的评估不足，现有基准无法捕捉交易的动态性。

Method: 引入StockBench，一个包含多月股票交易、每日市场信号和金融指标评估的无污染基准。

Result: 大多数LLM代理未能超越简单的买入并持有策略，但一些模型显示出超越基准的潜力。

Conclusion: LLM在金融知识方面的优势不一定能转化为成功的交易策略，需要进一步研究。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
as autonomous agents, showing promise in reasoning, tool use, and sequential
decision-making. While prior benchmarks have evaluated LLM agents in domains
such as software engineering and scientific discovery, the finance domain
remains underexplored, despite its direct relevance to economic value and
high-stakes decision-making. Existing financial benchmarks primarily test
static knowledge through question answering, but they fall short of capturing
the dynamic and iterative nature of trading. To address this gap, we introduce
StockBench, a contamination-free benchmark designed to evaluate LLM agents in
realistic, multi-month stock trading environments. Agents receive daily market
signals -- including prices, fundamentals, and news -- and must make sequential
buy, sell, or hold decisions. Performance is assessed using financial metrics
such as cumulative return, maximum drawdown, and the Sortino ratio. Our
evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and
open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM
agents struggle to outperform the simple buy-and-hold baseline, several models
demonstrate the potential to deliver higher returns and manage risk more
effectively. These findings highlight both the challenges and opportunities in
developing LLM-powered financial agents, showing that excelling at static
financial knowledge tasks does not necessarily translate into successful
trading strategies. We release StockBench as an open-source resource to support
reproducibility and advance future research in this domain.

</details>


### [511] [Policy Gradient Guidance Enables Test Time Control](https://arxiv.org/abs/2510.02148)
*Jianing Qi,Hao Tang,Zhigang Zhu*

Main category: cs.LG

TL;DR: PGG 是一种将 classifier-free guidance 扩展到策略梯度方法的简单方法，可以在不重新训练的情况下控制行为，并在离散和连续控制基准上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 将 classifier-free guidance 从扩散模型扩展到经典策略梯度方法，以实现可控的在线强化学习。

Method: PGG 通过引入一个无条件分支来增强策略梯度，并对条件和无条件分支进行插值，以在测试时控制行为。理论上，当使用优势估计时，额外的归一化项会消失。

Result: 在离散和连续控制基准上，PGG 的效果得到了评估。在简单的离散任务和低样本量下，条件 dropout 有所提高，但在连续控制中会导致不稳定。而使用稍大的引导（γ>1）则能持续提高稳定性和样本效率。

Conclusion: PGG 可以适应标准的在线策略方法，为可控的在线强化学习开辟了新的方向。

Abstract: We introduce Policy Gradient Guidance (PGG), a simple extension of
classifier-free guidance from diffusion models to classical policy gradient
methods. PGG augments the policy gradient with an unconditional branch and
interpolates conditional and unconditional branches, yielding a test-time
control knob that modulates behavior without retraining. We provide a
theoretical derivation showing that the additional normalization term vanishes
under advantage estimation, leading to a clean guided policy gradient update.
Empirically, we evaluate PGG on discrete and continuous control benchmarks. We
find that conditioning dropout-central to diffusion guidance-offers gains in
simple discrete tasks and low sample regimes, but dropout destabilizes
continuous control. Training with modestly larger guidance ($\gamma>1$)
consistently improves stability, sample efficiency, and controllability. Our
results show that guidance, previously confined to diffusion policies, can be
adapted to standard on-policy methods, opening new directions for controllable
online reinforcement learning.

</details>


### [512] [Reinforcement Learning with Action-Triggered Observations](https://arxiv.org/abs/2510.02149)
*Alexander Ryabchenko,Wenlong Mou*

Main category: cs.LG

TL;DR: 本研究提出了动作触发的稀疏可追踪马尔可夫决策过程（ATST-MDPs）框架，用于解决动作随机触发状态观测的强化学习问题，并推导了相应的最优 Bellman 方程和基于动作序列的学习范式。在 线性 MDP 假设下，利用动作序列特征映射的线性结构，提出了具有统计误差保证的离线策略估计器，并引入了 ST-LSVI-UCB 算法，在稀疏观测约束下实现了 $\widetilde O(\sqrt{Kd^3(1-\gamma)^{-3}})$ 的遗憾界限，为在稀疏观测条件下进行有效学习奠定了理论基础。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界的强化学习应用中，状态观测的触发受到动作的随机性影响。现有的强化学习框架未能充分考虑这种“动作触发”的稀疏观测约束。

Method: 1. 提出动作触发的稀疏可追踪马尔可夫决策过程（ATST-MDPs）框架，为动作触发的状态观测设定了概率模型。 2. 推导了 ATST-MDPs 的最优 Bellman 方程。 3. 引入了“动作序列学习”范式，代理人执行一系列动作直到下一次观测。 4. 在线性 MDP 假设下，证明了价值函数可以表示为诱导动作序列特征映射的线性函数。 5. 基于此结构，提出了具有统计误差保证的离线策略估计器。 6. 引入了 ST-LSVI-UCB 算法，这是 LSVI-UCB 算法在动作触发设置下的变体。

Result: ST-LSVI-UCB 算法实现了 $\widetilde O(\sqrt{Kd^3(1-\gamma)^{-3}})$ 的遗憾界限，其中 K 是回合数，d 是特征维度，\gamma 是折扣因子。证明了在动作触发的稀疏观测约束下，仍然可以进行有效的学习。

Conclusion: 本研究为带有稀疏、动作触发观测的强化学习问题提供了坚实的理论基础，并证明了即使存在观测约束，高效学习也是可行的。所提出的 ATST-MDPs 框架和 ST-LSVI-UCB 算法能够有效处理这类问题。

Abstract: We study reinforcement learning problems where state observations are
stochastically triggered by actions, a constraint common in many real-world
applications. This framework is formulated as Action-Triggered Sporadically
Traceable Markov Decision Processes (ATST-MDPs), where each action has a
specified probability of triggering a state observation. We derive tailored
Bellman optimality equations for this framework and introduce the
action-sequence learning paradigm in which agents commit to executing a
sequence of actions until the next observation arrives. Under the linear MDP
assumption, value-functions are shown to admit linear representations in an
induced action-sequence feature map. Leveraging this structure, we propose
off-policy estimators with statistical error guarantees for such feature maps
and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered
settings. ST-LSVI-UCB achieves regret $\widetilde
O(\sqrt{Kd^3(1-\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the
feature dimension, and $\gamma$ the discount factor (per-step episode
non-termination probability). Crucially, this work establishes the theoretical
foundation for learning with sporadic, action-triggered observations while
demonstrating that efficient learning remains feasible under such observation
constraints.

</details>


### [513] [ExGRPO: Learning to Reason from Experience](https://arxiv.org/abs/2510.02245)
*Runzhe Zhan,Yafu Li,Zhi Wang,Xiaoye Qu,Dongrui Liu,Jing Shao,Derek F. Wong,Yu Cheng*

Main category: cs.LG

TL;DR: 通过组织和优先排序有价值的经验，ExGRPO 框架提高了大型语言模型的推理能力，并稳定了训练过程。


<details>
  <summary>Details</summary>
Motivation: 标准的 on-policy RLVR 方法在每次更新后会丢弃经验，导致计算效率低下和训练不稳定。现有 RL 工作虽然强调了重用过去经验的好处，但对于塑造大型推理模型的学习动态的经验特征的研究尚不充分。

Method: 提出 ExGRPO（Experiential Group Relative Policy Optimization）框架，该框架首先识别出正确性和熵作为经验价值的指标，然后组织和优先排序有价值的经验，并采用混合策略目标来平衡探索与经验利用。

Result: 在五个不同参数规模（1.5B-8B）的模型上进行实验，ExGRPO 在数学和通用基准测试上均能持续提高推理性能，平均分别比 on-policy RLVR 提高 3.5/7.6 分。此外，ExGRPO 在 on-policy 方法失败的较强和较弱模型上都能稳定训练。

Conclusion: 原则性的经验管理是 RLVR 高效和可扩展的关键组成部分。

Abstract: Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm
for improving the reasoning ability of large language models. However, standard
on-policy training discards rollout experiences after a single update, leading
to computational inefficiency and instability. While prior work on RL has
highlighted the benefits of reusing past experience, the role of experience
characteristics in shaping learning dynamics of large reasoning models remains
underexplored. In this paper, we are the first to investigate what makes a
reasoning experience valuable and identify rollout correctness and entropy as
effective indicators of experience value. Based on these insights, we propose
ExGRPO (Experiential Group Relative Policy Optimization), a framework that
organizes and prioritizes valuable experiences, and employs a mixed-policy
objective to balance exploration with experience exploitation. Experiments on
five backbone models (1.5B-8B parameters) show that ExGRPO consistently
improves reasoning performance on mathematical/general benchmarks, with an
average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO
stabilizes training on both stronger and weaker models where on-policy methods
fail. These results highlight principled experience management as a key
ingredient for efficient and scalable RLVR.

</details>


### [514] [Flatness-Aware Stochastic Gradient Langevin Dynamics](https://arxiv.org/abs/2510.02174)
*Stefano Bruno,Youngsik Hwang,Jaehyeon An,Sotirios Sabanis,Dong-Young Lim*

Main category: cs.LG

TL;DR: fSGLD 算法通过随机权重扰动（RWP）在损失景观中寻找平坦的最小值，并在理论和实验上证明了其优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的随机梯度下降 Langevin 动力学（SGLD）在寻找损失景观中的平坦最小值方面存在不足，而平坦最小值与深度学习中的泛化能力密切相关。

Method: fSGLD 在每次迭代时，使用在经过高斯噪声扰动的参数上计算出的随机梯度进行优化。这种随机权重扰动（RWP）能隐式地捕获损失函数的曲率信息，并优化一个随机平滑的目标函数。

Result: 理论上，fSGLD 的不变测度接近于一个在全局最小化损失函数的测度，该损失函数由 Hessian 迹正则化。实验上，fSGLD 在泛化能力、鲁棒性以及计算成本方面表现优于或媲美基线算法，并且 Hessian-谱分析证实了 fSGLD 收敛于更平坦的最小值。

Conclusion: fSGLD 是一种有效且具有理论保证的算法，能够在高维非凸优化问题中寻找平坦的最小值，从而提升深度学习模型的泛化能力。

Abstract: Generalization in deep learning is closely tied to the pursuit of flat minima
in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics
(SGLD) offers no mechanism to bias its dynamics toward such low-curvature
solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin
Dynamics (fSGLD), designed to efficiently and provably seek flat minima in
high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses
the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian
noise, commonly referred to as Random Weight Perturbation (RWP), thereby
optimizing a randomized-smoothing objective that implicitly captures curvature
information. Leveraging these properties, we prove that the invariant measure
of fSGLD stays close to a stationary measure concentrated on the global
minimizers of a loss function regularized by the Hessian trace whenever the
inverse temperature and the scale of random weight perturbation are properly
coupled. This result provides a rigorous theoretical explanation for the
benefits of random weight perturbation. In particular, we establish
non-asymptotic convergence guarantees in Wasserstein distance with the best
known rate and derive an excess-risk bound for the Hessian-trace regularized
objective. Extensive experiments on noisy-label and large-scale vision tasks,
in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD
achieves superior or comparable generalization and robustness to baseline
algorithms while maintaining the computational cost of SGD, about half that of
SAM. Hessian-spectrum analysis further confirms that fSGLD converges to
significantly flatter minima.

</details>


### [515] [GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning](https://arxiv.org/abs/2510.02180)
*Silvia Sapora,Devon Hjelm,Alexander Toshev,Omar Attia,Bogdan Mazoure*

Main category: cs.LG

TL;DR: GRACE是一个使用大型语言模型和进化搜索来从专家轨迹中生成可解释的代码奖励函数的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习方法产生的“黑盒”模型难以解释和调试，而GRACE旨在生成可解释的、基于代码的奖励函数。

Method: GRACE利用大型语言模型和进化搜索来直接从专家轨迹中逆向工程可执行的代码奖励函数。

Result: GRACE在BabyAI和AndroidWorld基准测试中学习到了高精度的奖励函数，即使在复杂的多任务环境中也是如此。生成的奖励函数能够训练出强大的策略，表现优于竞争性模仿学习和使用真实奖励的在线强化学习方法。

Conclusion: GRACE能够生成可解释的代码奖励函数，并且在多任务环境中能够构建复杂的奖励API，在BabyAI和AndroidWorld基准测试中表现出了优越性。

Abstract: Inverse Reinforcement Learning aims to recover reward models from expert
demonstrations, but traditional methods yield "black-box" models that are
difficult to interpret and debug. In this work, we introduce GRACE (Generating
Rewards As CodE), a method for using Large Language Models within an
evolutionary search to reverse-engineer an interpretable, code-based reward
function directly from expert trajectories. The resulting reward function is
executable code that can be inspected and verified. We empirically validate
GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns
highly accurate rewards, even in complex, multi-task settings. Further, we
demonstrate that the resulting reward leads to strong policies, compared to
both competitive Imitation Learning and online RL approaches with ground-truth
rewards. Finally, we show that GRACE is able to build complex reward APIs in
multi-task setups.

</details>


### [516] [Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet Challenge 2025](https://arxiv.org/abs/2510.02202)
*Matthew A. Reyna,Zuzana Koscova,Jan Pavlus,Soheil Saghafi,James Weigle,Andoni Elola,Salman Seyedi,Kiersten Campbell,Qiao Li,Ali Bahrami Rad,Antônio H. Ribeiro,Antonio Luiz P. Ribeiro,Reza Sameni,Gari D. Clifford*

Main category: cs.LG

TL;DR: Chagas病主要通过昆虫传播，可导致心血管和消化系统疾病。通过心电图（ECG）识别Chagas病心肌病可优先患者接受检测和治疗。


<details>
  <summary>Details</summary>
Motivation: 现有Chagas病血清学检测能力有限，但Chagas心肌病常在心电图中表现出来，这为优先检测和治疗患者提供了机会。

Method: George B. Moody PhysioNet Challenge 2025 旨在开发从心电图（ECG）中识别Chagas病的算法方法。本次挑战赛提供了包含患者报告和血清学检测标签的数据集，并对数据进行了增强以支持模型的鲁棒性和泛化能力。同时，采用的评估指标考虑了Chagas病当地的血清学检测能力，将机器学习问题构建为一个分诊任务。

Result: 本次挑战赛吸引了来自全球111支队伍的630多名参赛者，提交了1300多项参赛作品，展示了来自学术界和工业界的多种方法。

Conclusion: 本次挑战赛通过提供多样化的数据集、数据增强以及采用分诊任务的评估指标，推动了Chagas病心电图诊断算法的发展，并激发了全球范围内的广泛参与。

Abstract: Objective: Chagas disease is a parasitic infection that is endemic to South
America, Central America, and, more recently, the U.S., primarily transmitted
by insects. Chronic Chagas disease can cause cardiovascular diseases and
digestive problems. Serological testing capacities for Chagas disease are
limited, but Chagas cardiomyopathy often manifests in ECGs, providing an
opportunity to prioritize patients for testing and treatment. Approach: The
George B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic
approaches for identifying Chagas disease from electrocardiograms (ECGs). Main
results: This Challenge provides multiple innovations. First, we leveraged
several datasets with labels from patient reports and serological testing,
provided a large dataset with weak labels and smaller datasets with strong
labels. Second, we augmented the data to support model robustness and
generalizability to unseen data sources. Third, we applied an evaluation metric
that captured the local serological testing capacity for Chagas disease to
frame the machine learning problem as a triage task. Significance: Over 630
participants from 111 teams submitted over 1300 entries during the Challenge,
representing diverse approaches from academia and industry worldwide.

</details>


### [517] [Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks](https://arxiv.org/abs/2510.02286)
*Ruohao Guo,Afshin Oroojlooy,Roshan Sridhar,Miguel Ballesteros,Alan Ritter,Dan Roth*

Main category: cs.LG

TL;DR: 现有方法无法有效发现多轮对话中的大模型安全漏洞，我们提出了DialTree-RPO，一个基于强化学习和树搜索的框架，可以自主发现多轮攻击策略，实验表明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对大模型安全性的研究主要集中在单轮攻击，忽略了多轮交互中攻击者可能利用对话动态和策略性规划进行更复杂的攻击，而这正是大模型更易受攻击的场景。

Method: 提出DialTree-RPO框架，采用在线强化学习并结合树搜索，将对话视为序列决策问题，自主探索多轮攻击策略，无需人工标注数据。

Result: 在10个目标模型上，DialTree-RPO实现了超过25.9%的攻击成功率提升，并发现了新的攻击策略，学习到了最大化多轮攻击成功的最优对话策略。

Conclusion: DialTree-RPO能够有效地自主发现多样化的多轮对话攻击策略，显著提升攻击成功率，为大模型安全研究提供了新的方向。

Abstract: Despite recent rapid progress in AI safety, current large language models
remain vulnerable to adversarial attacks in multi-turn interaction settings,
where attackers strategically adapt their prompts across conversation turns and
pose a more critical yet realistic challenge. Existing approaches that discover
safety vulnerabilities either rely on manual red-teaming with human experts or
employ automated methods using pre-defined templates and human-curated attack
data, with most focusing on single-turn attacks. However, these methods did not
explore the vast space of possible multi-turn attacks, failing to consider
novel attack trajectories that emerge from complex dialogue dynamics and
strategic conversation planning. This gap is particularly critical given recent
findings that LLMs exhibit significantly higher vulnerability to multi-turn
attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy
reinforcement learning framework integrated with tree search that autonomously
discovers diverse multi-turn attack strategies by treating the dialogue as a
sequential decision-making problem, enabling systematic exploration without
manually curated data. Through extensive experiments, our approach not only
achieves more than 25.9% higher ASR across 10 target models compared to
previous state-of-the-art approaches, but also effectively uncovers new attack
strategies by learning optimal dialogue policies that maximize attack success
across multiple turns.

</details>


### [518] [Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling](https://arxiv.org/abs/2510.02206)
*Daniel Gallo Fernández*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sequence-to-sequence models have become central in Artificial Intelligence,
particularly following the introduction of the transformer architecture. While
initially developed for Natural Language Processing, these models have
demonstrated utility across domains, including Computer Vision. Such models
require mechanisms to exchange information along the time dimension, typically
using recurrent or self-attention layers. However, self-attention scales
quadratically with sequence length, limiting its practicality for very long
sequences.
  We introduce Poolformer, a sequence-to-sequence model that replaces
self-attention with recurrent layers and incorporates pooling operations to
reduce sequence length. Poolformer is defined recursively using SkipBlocks,
which contain residual blocks, a down-pooling layer, a nested SkipBlock, an
up-pooling layer, and additional residual blocks. We conduct extensive
experiments to support our architectural choices.
  Our results show that pooling greatly accelerates training, improves
perceptual metrics (FID and IS), and prevents overfitting. Our experiments also
suggest that long-range dependencies are handled by deep layers, while shallow
layers take care of short-term features.
  Evaluated on raw audio, which naturally features long sequence lengths,
Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba.
Future directions include applications to text and vision, as well as
multi-modal scenarios, where a Poolformer-based LLM could effectively process
dense representations of images and videos.

</details>


### [519] [Interactive Training: Feedback-Driven Neural Network Optimization](https://arxiv.org/abs/2510.02297)
*Wentao Zhang,Yang Young Lu,Yuntian Deng*

Main category: cs.LG

TL;DR: 本框架通过引入实时反馈机制，允许人类专家或AI代理干预神经网络的训练过程，从而提高训练稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络训练方法缺乏灵活性，无法应对训练过程中的不稳定性或问题。本研究旨在提供一种能够实时干预训练过程的框架。

Method: 通过控制服务器连接用户/代理与训练过程，实现超参数、训练数据和模型检查点的动态调整。

Result: 通过三个案例研究证明，本框架提高了训练稳定性，降低了对初始超参数的敏感性，并增强了对用户不断变化的需求的适应性。

Conclusion: 本研究提出的交互式训练框架为未来AI代理自主监控、解决不稳定性并优化训练动态的训练范式奠定了基础。

Abstract: Traditional neural network training typically follows fixed, predefined
optimization recipes, lacking the flexibility to dynamically respond to
instabilities or emerging training issues. In this paper, we introduce
Interactive Training, an open-source framework that enables real-time,
feedback-driven intervention during neural network training by human experts or
automated AI agents. At its core, Interactive Training uses a control server to
mediate communication between users or agents and the ongoing training process,
allowing users to dynamically adjust optimizer hyperparameters, training data,
and model checkpoints. Through three case studies, we demonstrate that
Interactive Training achieves superior training stability, reduced sensitivity
to initial hyperparameters, and improved adaptability to evolving user needs,
paving the way toward a future training paradigm where AI agents autonomously
monitor training logs, proactively resolve instabilities, and optimize training
dynamics.

</details>


### [520] [DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning](https://arxiv.org/abs/2510.02212)
*Hanyang Zhao,Dawen Liang,Wenpin Tang,David Yao,Nathan Kallus*

Main category: cs.LG

TL;DR: DiFFPO是一个统一的框架，用于训练掩码扩散大语言模型（dLLMs），通过强化学习（RL）实现更优、更快的推理。


<details>
  <summary>Details</summary>
Motivation: 现有dLLMs训练方法存在局限，DiFFPO旨在通过强化学习提高推理速度和准确性。

Method: DiFFPO提出两种方法：1. 通过离策略RL训练代理策略，实现更准确的两阶段似然近似和重要性采样校正。2. 通过RL联合训练高效采样器/控制器，使模型自适应调整推理阈值，降低函数评估次数（NFEs）。

Result: DiFFPO在数学和规划任务上展现了其有效性，实现了比仅训练模型更好的准确性，并在推理计算的帕累托前沿方面取得了最优性能。

Conclusion: DiFFPO通过强化学习在提高dLLMs推理速度和准确性方面取得了显著成效，并在推理计算效率上达到了最优。

Abstract: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified
framework for training masked diffusion large language models (dLLMs) to reason
not only better (furious), but also faster via reinforcement learning (RL). We
first unify the existing baseline approach such as d1 by proposing to train
surrogate policies via off-policy RL, whose likelihood is much more tractable
as an approximation to the true dLLM policy. This naturally motivates a more
accurate and informative two-stage likelihood approximation combined with
importance sampling correction, which leads to generalized RL algorithms with
better sample efficiency and superior task performance. Second, we propose a
new direction of joint training efficient samplers/controllers of dLLMs policy.
Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by
letting the model learn to adaptively allocate an inference threshold for each
prompt. By jointly training the sampler, we yield better accuracies with lower
number of function evaluations (NFEs) compared to training the model only,
obtaining the best performance in improving the Pareto frontier of the
inference-time compute of dLLMs. We showcase the effectiveness of our pipeline
by training open source large diffusion language models over benchmark math and
planning tasks.

</details>


### [521] [C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale Recommendation Systems](https://arxiv.org/abs/2510.02215)
*Mertcan Cokbas,Ziteng Liu,Zeyi Tao,Chengkai Zhang,Elder Veliz,Qin Huang,Ellie Wen,Huayu Li,Qiang Jin,Murat Duman,Benjamin Au,Guy Lebanon,Sagar Chordia*

Main category: cs.LG

TL;DR: 大规模推荐模型训练中的同质性假设会忽略用户群体中的异质性，导致模型性能下降。本文提出C2AL方法，利用注意力机制和辅助学习来解决此问题，通过分析数据子结构和利用部分冲突的辅助标签来正则化共享表示，从而提升模型对少数群体的关注度并优化整体性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的推荐数据包含具有不同条件分布的异质群体，而单一全局目标训练下的推荐模型会忽略这些异质性，特别是头部和尾部区域的数据，导致模型学习能力受限，出现注意力权重无效或神经元死亡等问题。

Method: 本文首先揭示了注意力机制在因子分解机中用于共享嵌入选择的关键作用，然后提出通过分析数据集中的子结构，并利用部分冲突的辅助标签来正则化共享表示，以解决上述挑战。该方法通过定制注意力层的学习过程，在提升整体性能的同时，保留与少数群体的互信息。

Result: 在包含数十亿数据点的六个SOTA（State-of-the-Art）模型的生产数据集上评估C2AL。实验结果表明，所提出的方法能够通过因子分解机捕捉细粒度的用户-广告交互，整体归一化熵最多可降低0.16%，并在目标少数群体上带来超过0.30%的性能提升。

Conclusion: 所提出的C2AL方法通过分析数据子结构并利用部分冲突的辅助标签来正则化共享表示，有效解决了大规模推荐模型中因用户群体异质性导致的性能下降问题，提升了模型对少数群体的关注度，并优化了整体性能。

Abstract: Training large-scale recommendation models under a single global objective
implicitly assumes homogeneity across user populations. However, real-world
data are composites of heterogeneous cohorts with distinct conditional
distributions. As models increase in scale and complexity and as more data is
used for training, they become dominated by central distribution patterns,
neglecting head and tail regions. This imbalance limits the model's learning
ability and can result in inactive attention weights or dead neurons. In this
paper, we reveal how the attention mechanism can play a key role in
factorization machines for shared embedding selection, and propose to address
this challenge by analyzing the substructures in the dataset and exposing those
with strong distributional contrast through auxiliary learning. Unlike previous
research, which heuristically applies weighted labels or multi-task heads to
mitigate such biases, we leverage partially conflicting auxiliary labels to
regularize the shared representation. This approach customizes the learning
process of attention layers to preserve mutual information with minority
cohorts while improving global performance. We evaluated C2AL on massive
production datasets with billions of data points each for six SOTA models.
Experiments show that the factorization machine is able to capture fine-grained
user-ad interactions using the proposed method, achieving up to a 0.16%
reduction in normalized entropy overall and delivering gains exceeding 0.30% on
targeted minority cohorts.

</details>


### [522] [Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification](https://arxiv.org/abs/2510.02216)
*Zeqi Ye,Minshuo Chen*

Main category: cs.LG

TL;DR: 扩散模型在时间序列填充方面表现出色，但对其理论基础的理解不足。本研究利用Transformer的近似理论，推导了条件扩散模型的样本复杂度界限，用于量化缺失值的统计效率和不确定性，并提出了改进填充性能的混合掩码训练策略。


<details>
  <summary>Details</summary>
Motivation: 填补扩散模型在时间序列填充中对空间和时间依赖性理解的理论空白，并量化其不确定性。

Method: 利用Transformer的近似理论推导条件扩散模型的样本复杂度界限，并构建缺失值的置信区域。

Result: 填充效率和准确性受缺失模式的显著影响，模拟验证了理论推导的有效性，并提出了一种混合掩码训练策略。

Conclusion: 本研究为扩散模型在时间序列填充中的应用提供了理论基础，并提出了一种改进填充性能的实用策略。

Abstract: Imputation methods play a critical role in enhancing the quality of practical
time-series data, which often suffer from pervasive missing values. Recently,
diffusion-based generative imputation methods have demonstrated remarkable
success compared to autoregressive and conventional statistical approaches.
Despite their empirical success, the theoretical understanding of how well
diffusion-based models capture complex spatial and temporal dependencies
between the missing values and observed ones remains limited. Our work
addresses this gap by investigating the statistical efficiency of conditional
diffusion transformers for imputation and quantifying the uncertainty in
missing values. Specifically, we derive statistical sample complexity bounds
based on a novel approximation theory for conditional score functions using
transformers, and, through this, construct tight confidence regions for missing
values. Our findings also reveal that the efficiency and accuracy of imputation
are significantly influenced by the missing patterns. Furthermore, we validate
these theoretical insights through simulation and propose a mixed-masking
training strategy to enhance the imputation performance.

</details>


### [523] [Efficiently Generating Correlated Sample Paths from Multi-step Time Series Foundation Models](https://arxiv.org/abs/2510.02224)
*Ethan Baron,Boris Oreshkin,Ruijun Ma,Hanyu Zhang,Kari Torkkola,Michael W. Mahoney,Andrew Gordon Wilson,Tatiana Konstantinova*

Main category: cs.LG

TL;DR: 该研究提出了一种基于 copula 的方法，可以从现有的多步时间序列基础模型中高效地生成准确、相关的样本路径。


<details>
  <summary>Details</summary>
Motivation: 现有的多步时间序列基础模型在生成多步预测时，只预测每个时间步长的独立边缘分布，而不是完整的联合预测分布，导致生成的样本路径缺乏真实的依赖结构。而传统的自回归采样方法生成样本路径的计算成本极高。

Method: 提出了一种基于 copula 的方法，可以在一次前向传播中，从现有时间序列基础模型生成具有真实相关结构的样本路径。

Result: 该方法生成相关样本路径的速度比自回归采样快几个数量级，并能通过缓解累积误差现象来提高样本路径的质量。

Conclusion: 所提出的 copula 方法能够高效、准确地从多步时间序列基础模型生成具有真实相关结构的样本路径，解决了现有方法的局限性。

Abstract: Many time series applications require access to multi-step forecast
trajectories in the form of sample paths. Recently, time series foundation
models have leveraged multi-step lookahead predictions to improve the quality
and efficiency of multi-step forecasts. However, these models only predict
independent marginal distributions for each time step, rather than a full joint
predictive distribution. To generate forecast sample paths with realistic
correlation structures, one typically resorts to autoregressive sampling, which
can be extremely expensive. In this paper, we present a copula-based approach
to efficiently generate accurate, correlated sample paths from existing
multi-step time series foundation models in one forward pass. Our copula-based
approach generates correlated sample paths orders of magnitude faster than
autoregressive sampling, and it yields improved sample path quality by
mitigating the snowballing error phenomenon.

</details>


### [524] [xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity](https://arxiv.org/abs/2510.02228)
*Maximilian Beck,Kajetan Schweighofer,Sebastian Böck,Sebastian Lehner,Sepp Hochreiter*

Main category: cs.LG

TL;DR: xLSTM在LLM的计算-性能关系上表现优于Transformer，尤其是在长上下文场景下。


<details>
  <summary>Details</summary>
Motivation: LLM的扩展规律在模型性能预测和计算预算规划中起着关键作用。虽然Transformer是主流架构，但xLSTM等替代架构在保持竞争力的同时提供了与上下文长度相关的线性复杂度。

Method: 1. 在计算最优和过训练两种模式下，使用IsoFLOP和参数拟合方法，在80M到7B参数量及2B到2T训练token数量范围内，研究xLSTM的扩展行为。 2. 检查最优模型大小对上下文长度的依赖性。 3. 分析推理时间扩展特性。

Result: xLSTM在典型的LLM训练和推理场景下，扩展性优于Transformer，并且在训练和推理上下文长度增加时，xLSTM的优势更加明显。

Conclusion: xLSTM在LLM的扩展规律方面，尤其是在长上下文场景下，比Transformer更有优势，为未来的模型设计和部署提供了指导。

Abstract: Scaling laws play a central role in the success of Large Language Models
(LLMs), enabling the prediction of model performance relative to compute
budgets prior to training. While Transformers have been the dominant
architecture, recent alternatives such as xLSTM offer linear complexity with
respect to context length while remaining competitive in the billion-parameter
regime. We conduct a comparative investigation on the scaling behavior of
Transformers and xLSTM along the following lines, providing insights to guide
future model design and deployment. First, we study the scaling behavior for
xLSTM in compute-optimal and over-training regimes using both IsoFLOP and
parametric fit approaches on a wide range of model sizes (80M-7B) and number of
training tokens (2B-2T). Second, we examine the dependence of optimal model
sizes on context length, a pivotal aspect that was largely ignored in previous
work. Finally, we analyze inference-time scaling characteristics. Our findings
reveal that in typical LLM training and inference scenarios, xLSTM scales
favorably compared to Transformers. Importantly, xLSTM's advantage widens as
training and inference contexts grow.

</details>


### [525] [PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed Slice Mobility Attacks](https://arxiv.org/abs/2510.02236)
*Ricardo Misael Ayala Molina,Hyame Assem Alameddine,Makan Pourzandi,Chadi Assi*

Main category: cs.LG

TL;DR: 该论文提出了一种名为 PUL-Inter-Slice Defender 的新颖方法，用于检测和防御 5G 网络中的分布式切片迁移 (DSM) 攻击，该攻击利用了网络切片之间的切换功能。


<details>
  <summary>Details</summary>
Motivation: 第五代 (5G) 网络中的网络切片 (NS) 提供了灵活性，但用户设备 (UE) 在多个 NS 之间无缝切换的能力（称为切片间切换，ISS）可能被利用来发起分布式切片迁移 (DSM) 攻击，这是一种分布式拒绝服务 (DDoS) 攻击。因此，需要一种安全解决方案来防御这些攻击。

Method: 该研究提出了一种名为 PUL-Inter-Slice Defender 的异常检测解决方案。它利用正负例学习 (PUL) 方法，并结合了长短期记忆自编码器 (LSTM AE) 和 K 均值聚类。该解决方案使用第三代合作伙伴项目 (3GPP) 的关键绩效指标 (KPI) 和性能测量计数器作为特征，以检测 DSM 攻击。

Result: 在基于 free5GC 和 UERANSIM 的 5G 测试台上进行评估时，PUL-Inter-Slice Defender 在包含 10% 至 40% 攻击污染的训练数据集上实现了超过 98.50% 的 F1 分数。与未受保护的 Inter-Slice Defender 和其他基于 PUL 的解决方案（如结合了 One-Class SVM (OCSVM) 的随机森林和 XGBoost）相比，PUL-Inter-Slice Defender 表现持续优越。

Conclusion: PUL-Inter-Slice Defender 是一种有效的解决方案，可以抵御 5G 网络中的分布式切片迁移 (DSM) 攻击，即使在存在被污染的训练数据的情况下也能保持稳健性。它通过利用 KPI 和性能计数器，并结合 PUL、LSTM AE 和 K 均值聚类，实现了高精度的攻击检测。

Abstract: Network Slices (NSs) are virtual networks operating over a shared physical
infrastructure, each designed to meet specific application requirements while
maintaining consistent Quality of Service (QoS). In Fifth Generation (5G)
networks, User Equipment (UE) can connect to and seamlessly switch between
multiple NSs to access diverse services. However, this flexibility, known as
Inter-Slice Switching (ISS), introduces a potential vulnerability that can be
exploited to launch Distributed Slice Mobility (DSM) attacks, a form of
Distributed Denial of Service (DDoS) attack. To secure 5G networks and their
NSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an
anomaly detection solution that leverages Positive Unlabeled Learning (PUL) and
incorporates a combination of Long Short-Term Memory Autoencoders and K-Means
clustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership
Project (3GPP) key performance indicators and performance measurement counters
as features for its machine learning models to detect DSM attack variants while
maintaining robustness in the presence of contaminated training data. When
evaluated on data collected from our 5G testbed based on the open-source
free5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;
PUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training
datasets with 10% to 40% attack contamination, consistently outperforming its
counterpart Inter-Slice Defender and other PUL based solutions combining
One-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.

</details>


### [526] [Drop-Muon: Update Less, Converge Faster](https://arxiv.org/abs/2510.02239)
*Kaja Gruntkowska,Yassine Maziane,Zheng Qu,Peter Richtárik*

Main category: cs.LG

TL;DR: full-network updates are suboptimal; Drop-Muon updates a subset of layers per step, achieving faster and comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that all layers must be updated at every step in deep learning optimization, as it can be fundamentally suboptimal.

Method: Introduce Drop-Muon, a non-Euclidean Randomized Progressive Training method that updates only a subset of layers per step according to a randomized schedule, combining progressive training efficiency with layer-specific non-Euclidean updates.

Result: Provide rigorous convergence guarantees under layer-wise smoothness and layer-wise (L0, L1)-smoothness for both deterministic and stochastic gradient settings. Cost analysis shows full-network updates are not optimal unless a specific relationship between layer smoothness constants holds. Empirical results show Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to 1.4x faster.

Conclusion: Suggest a shift in efficient large-scale model training, challenging the status quo by offering a highly efficient, theoretically grounded alternative to full-network updates.

Abstract: Conventional wisdom in deep learning optimization dictates updating all
layers at every step-a principle followed by all recent state-of-the-art
optimizers such as Muon. In this work, we challenge this assumption, showing
that full-network updates can be fundamentally suboptimal, both in theory and
in practice. We introduce a non-Euclidean Randomized Progressive Training
method-Drop-Muon-a simple yet powerful framework that updates only a subset of
layers per step according to a randomized schedule, combining the efficiency of
progressive training with layer-specific non-Euclidean updates for top-tier
performance. We provide rigorous convergence guarantees under both layer-wise
smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and
stochastic gradient settings, marking the first such results for progressive
training in the stochastic and non-smooth regime. Our cost analysis further
reveals that full-network updates are not optimal unless a very specific
relationship between layer smoothness constants holds. Through controlled CNN
experiments, we empirically demonstrate that Drop-Muon consistently outperforms
full-network Muon, achieving the same accuracy up to $1.4\times$ faster in
wall-clock time. Together, our results suggest a shift in how large-scale
models can be efficiently trained, challenging the status quo and offering a
highly efficient, theoretically grounded alternative to full-network updates.

</details>


### [527] [Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps](https://arxiv.org/abs/2510.02274)
*Kyoungjun Park,Yifan Yang,Changhan Ge,Lili Qiu,Shiqi Jiang*

Main category: cs.LG

TL;DR: Diffusion^2是一个基于3D点云的扩散模型，用于模拟Wi-Fi到毫米波的射频信号传播，并取得了1.9 dB的误差和27倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 射频信号能提供超越RGB相机能力的宝贵环境洞察，但准确预测其在复杂环境中的传播仍然是一个挑战，因为需要考虑信号与障碍物的相互作用。

Method: 提出了一种名为Diffusion^2的方法，该方法使用3D点云来建模射频信号传播，并引入了RF-3D编码器来提取3D数据中的射频相关特征，然后进行多尺度嵌入以模拟信号传播过程。

Result: 在合成和真实世界测量中，Diffusion^2能够准确估计不同频段和环境条件下的射频信号行为，误差仅为1.9 dB，并且比现有方法快27倍。

Conclusion: Diffusion^2在射频信号建模方面取得了显著进展，能够高效且准确地预测射频信号在复杂环境中的传播。

Abstract: Modeling radio frequency (RF) signal propagation is essential for
understanding the environment, as RF signals offer valuable insights beyond the
capabilities of RGB cameras, which are limited by the visible-light spectrum,
lens coverage, and occlusions. It is also useful for supporting wireless
diagnosis, deployment, and optimization. However, accurately predicting RF
signals in complex environments remains a challenge due to interactions with
obstacles such as absorption and reflection. We introduce Diffusion^2, a
diffusion-based approach that uses 3D point clouds to model the propagation of
RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.
To effectively capture RF-related features from 3D data, we present the RF-3D
Encoder, which encapsulates the complexities of 3D geometry along with
signal-specific details. These features undergo multi-scale embedding to
simulate the actual RF signal dissemination process. Our evaluation, based on
synthetic and real-world measurements, demonstrates that Diffusion^2 accurately
estimates the behavior of RF signals in various frequency bands and
environmental conditions, with an error margin of just 1.9 dB and 27x faster
than existing methods, marking a significant advancement in the field. Refer to
https://rfvision-project.github.io/ for more information.

</details>


### [528] [Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks](https://arxiv.org/abs/2510.02278)
*Fedor Velikonivtsev,Oleg Platonov,Gleb Bazhenov,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 现有的道路交通预测基准数据集存在诸多问题，例如缺乏道路连通性信息、道路属性信息有限以及规模较小等，并且多集中于城际高速公路，未能很好地反映城市复杂交通状况。本文提出了包含中国两个主要城市道路网络的、更全面、更现实、更具挑战性的交通预测新基准数据集，其中最大数据集包含近10万个道路段。这些数据集包含丰富的道路特征以及交通流量和速度的细粒度数据。研究发现，现有的大多数神经时空模型在处理如此大规模的数据集时存在扩展性问题。为此，本文提出了一种不依赖专门时间序列处理模块的图神经网络（GNN）方法，该方法在保持更优预测性能的同时，也展现出更好的可扩展性。本文希望这些数据集和模型洞见能为交通预测研究提供有价值的资源。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测基准数据集存在信息缺失、规模不足、忽视城市复杂路况等问题，难以满足真实世界应用的需求。因此，需要构建更全面、更现实、更具挑战性的数据集，并开发相应的交通预测模型。

Method: 提出包含两个中国主要城市道路网络的、规模更大、信息更丰富的交通预测新基准数据集。针对现有模型在大规模数据上扩展性不足的问题，提出了一种不依赖专门时间序列处理模块的图神经网络（GNN）方法。

Result: 新数据集包含近10万个道路段，提供丰富的道路特征及交通流量和速度的细粒度数据。所提出的GNN方法在扩展性和预测性能上均优于现有的大多数神经时空模型。

Conclusion: 本文提供的数据集和提出的GNN方法为交通预测领域的研究提供了新的资源和有价值的见解，有望推动该领域的发展。

Abstract: Traffic forecasting on road networks is a complex task of significant
practical importance that has recently attracted considerable attention from
the machine learning community, with spatiotemporal graph neural networks
(GNNs) becoming the most popular approach. The proper evaluation of traffic
forecasting methods requires realistic datasets, but current publicly available
benchmarks have significant drawbacks, including the absence of information
about road connectivity for road graph construction, limited information about
road properties, and a relatively small number of road segments that falls
short of real-world applications. Further, current datasets mostly contain
information about intercity highways with sparsely located sensors, while city
road networks arguably present a more challenging forecasting task due to much
denser roads and more complex urban traffic patterns. In this work, we provide
a more complete, realistic, and challenging benchmark for traffic forecasting
by releasing datasets representing the road networks of two major cities, with
the largest containing almost 100,000 road segments (more than a 10-fold
increase relative to existing datasets). Our datasets contain rich road
features and provide fine-grained data about both traffic volume and traffic
speed, allowing for building more holistic traffic forecasting systems. We show
that most current implementations of neural spatiotemporal models for traffic
forecasting have problems scaling to datasets of our size. To overcome this
issue, we propose an alternative approach to neural traffic forecasting that
uses a GNN without a dedicated module for temporal sequence processing, thus
achieving much better scalability, while also demonstrating stronger
forecasting performance. We hope our datasets and modeling insights will serve
as a valuable resource for research in traffic forecasting.

</details>


### [529] [Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation](https://arxiv.org/abs/2510.02279)
*Mykyta Ielanskyi,Kajetan Schweighofer,Lukas Aichberger,Sepp Hochreiter*

Main category: cs.LG

TL;DR: LLM中的幻觉问题，特别是因模型预测不确定性引起的“混淆”，可以通过改进不确定性估计（UE）方法来解决。然而，现有的评估方法存在偏差，导致不同UE方法的效果排名不一致。本文提出使用更鲁棒的风险指标，如多元LLM评估、结构化任务、分布外检测和扰动检测，并引入Elo评级系统来客观评估UE算法在自然语言生成（NLG）中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM幻觉检测方法（特别是针对混淆）在评估不确定性估计（UE）方法时，由于近似正确性函数的分歧，导致对UE算法的评估结果存在偏差和不一致性。

Method: 本文提出使用更鲁棒的风险指标来评估UE算法，包括：1. 在问答（QA）任务中，通过对多个“LLM作为评判者”的变体进行边缘化来减少评估偏差。2. 探索结构化任务、分布外检测任务和扰动检测任务，以提供更可靠和可控的风险指标。3. 提出使用Elo评级系统来综合评估不同UE方法在多种设置下的表现。

Result: 通过采用更鲁棒的风险指标和Elo评级系统，可以更客观、更可靠地评估UE算法在NLG中的性能，减少现有评估方法的偏差，并为UE方法提供一个客观的性能总结。

Conclusion: 为了更准确地评估用于NLG的不确定性估计（UE）算法，需要采用更鲁棒的风险指标，并结合Elo评级系统等综合评估方法，以克服现有评估方法中的偏差和不一致性。

Abstract: Hallucinations are a common issue that undermine the reliability of large
language models (LLMs). Recent studies have identified a specific subset of
hallucinations, known as confabulations, which arise due to predictive
uncertainty of LLMs. To detect confabulations, various methods for estimating
predictive uncertainty in natural language generation (NLG) have been
developed. These methods are typically evaluated by correlating uncertainty
estimates with the correctness of generated text, with question-answering (QA)
datasets serving as the standard benchmark. However, commonly used approximate
correctness functions have substantial disagreement between each other and,
consequently, in the ranking of the uncertainty estimation methods. This allows
one to inflate the apparent performance of uncertainty estimation methods. We
propose using several alternative risk indicators for risk correlation
experiments that improve robustness of empirical assessment of UE algorithms
for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge
variants leads to reducing the evaluation biases. Furthermore, we explore
structured tasks as well as out of distribution and perturbation detection
tasks which provide robust and controllable risk indicators. Finally, we
propose to use an Elo rating of uncertainty estimation methods to give an
objective summarization over extensive evaluation settings.

</details>


### [530] [Knowledge Distillation Detection for Open-weights Models](https://arxiv.org/abs/2510.02302)
*Qin Shi,Amber Yijia Zheng,Qifan Song,Raymond A. Yeh*

Main category: cs.LG

TL;DR: 该论文提出了一种新的“知识蒸馏检测”任务，用于判断一个学生模型是否由给定的教师模型蒸馏而来，其场景是只能获得学生模型权重和教师模型API。


<details>
  <summary>Details</summary>
Motivation: 由于对模型溯源和未经授权的复制日益担忧，因此提出此任务。

Method: 提出了一种模型无关的框架，结合了无数据输入合成和统计分数计算来检测知识蒸馏。

Result: 该方法适用于分类和生成模型，在CIFAR-10、ImageNet和文本到图像生成任务上，相比现有基线方法，检测准确率分别提高了59.6%、71.2%和20.0%。

Conclusion: 提出了一种有效的知识蒸馏检测方法，并验证了其在多种模型和任务上的有效性。

Abstract: We propose the task of knowledge distillation detection, which aims to
determine whether a student model has been distilled from a given teacher,
under a practical setting where only the student's weights and the teacher's
API are available. This problem is motivated by growing concerns about model
provenance and unauthorized replication through distillation. To address this
task, we introduce a model-agnostic framework that combines data-free input
synthesis and statistical score computation for detecting distillation. Our
approach is applicable to both classification and generative models.
Experiments on diverse architectures for image classification and text-to-image
generation show that our method improves detection accuracy over the strongest
baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image
generation. The code is available at
https://github.com/shqii1j/distillation_detection.

</details>


### [531] [Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive](https://arxiv.org/abs/2510.02305)
*Tyler Farghly,Peter Potaptchik,Samuel Howard,George Deligiannidis,Jakiw Pidstrigach*

Main category: cs.LG

TL;DR: 扩散模型因其强大的泛化能力而闻名，但其背后的机制尚不完全清楚。本研究通过研究得分匹配（score matching）的学习问题，为“数据存在低维几何结构”的假说提供了证据。研究表明，通过对经验得分匹配目标函数的最小化器进行平滑处理，可以实现数据流形上的平滑，并且可以通过选择合适的平滑度来控制扩散模型的泛化流形。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在各种领域都取得了最先进的性能，并表现出卓越的泛化能力。然而，支撑这些强大能力的机制仍未得到充分理解。一个主要的猜想，基于流形假说，认为这种成功归因于它们适应数据内在的低维几何结构的能力。本研究旨在为这一猜想提供证据，特别是关注学习问题的制定如何通过得分匹配来实现这种现象。

Method: 本研究通过检查经验得分匹配目标函数的平滑最小化器的效果，来检验隐式正则化的作用。理论和实证结果均表明，平滑得分函数（或等效地，在对数密度域中进行平滑）会在数据流形上产生切向平滑。

Result: 研究证实，平滑得分函数或对数密度域中的平滑，会在数据流形上产生切向平滑。此外，研究还表明，通过选择合适的平滑度，可以控制扩散模型泛化的流形。

Conclusion: 本研究通过理论和实证结果，为扩散模型成功依赖于数据低维几何结构的假说提供了支持。研究发现，通过对得分匹配目标进行平滑处理，可以在数据流形上实现切向平滑，并且可以通过调整平滑度来控制模型的泛化方向。

Abstract: Diffusion models have achieved state-of-the-art performance, demonstrating
remarkable generalisation capabilities across diverse domains. However, the
mechanisms underpinning these strong capabilities remain only partially
understood. A leading conjecture, based on the manifold hypothesis, attributes
this success to their ability to adapt to low-dimensional geometric structure
within the data. This work provides evidence for this conjecture, focusing on
how such phenomena could result from the formulation of the learning problem
through score matching. We inspect the role of implicit regularisation by
investigating the effect of smoothing minimisers of the empirical score
matching objective. Our theoretical and empirical results confirm that
smoothing the score function -- or equivalently, smoothing in the log-density
domain -- produces smoothing tangential to the data manifold. In addition, we
show that the manifold along which the diffusion model generalises can be
controlled by choosing an appropriate smoothing.

</details>


### [532] [Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization](https://arxiv.org/abs/2510.02308)
*Dhruv Kohli,Sawyer J. Robertson,Gal Mishne,Alexander Cloninger*

Main category: cs.LG

TL;DR: LEGO是一种新的谱方法，用于估计数据流形切空间，比LPCA在有噪声的情况下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统的LPCA方法在处理高噪声数据时，由于邻域大小选择的权衡而效果不佳，需要预先了解数据特性，而这通常是不可用的。

Method: LEGO是一种谱方法，通过对图拉普拉斯算子的低频特征向量的梯度进行正交化来估计每个数据点的切空间，从而利用了数据的全局结构，而不是仅仅依赖局部邻域。

Result: LEGO估计出的切空间比LPCA更有鲁棒性，在流形学习、边界检测和局部内在维度估计等下游任务中表现更好。

Conclusion: LEGO利用全局数据结构来指导局部切空间估计，通过微分几何和随机矩阵理论的分析，证明了其在有噪声环境下的优越性。

Abstract: Estimating the tangent spaces of a data manifold is a fundamental problem in
data analysis. The standard approach, Local Principal Component Analysis
(LPCA), struggles in high-noise settings due to a critical trade-off in
choosing the neighborhood size. Selecting an optimal size requires prior
knowledge of the geometric and noise characteristics of the data that are often
unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector
Gradient Orthogonalization (LEGO), that utilizes the global structure of the
data to guide local tangent space estimation. Instead of relying solely on
local neighborhoods, LEGO estimates the tangent space at each data point by
orthogonalizing the gradients of low-frequency eigenvectors of the graph
Laplacian. We provide two theoretical justifications of our method. First, a
differential geometric analysis on a tubular neighborhood of a manifold shows
that gradients of the low-frequency Laplacian eigenfunctions of the tube align
closely with the manifold's tangent bundle, while an eigenfunction with high
gradient in directions orthogonal to the manifold lie deeper in the spectrum.
Second, a random matrix theoretic analysis also demonstrates that low-frequency
eigenvectors are robust to sub-Gaussian noise. Through comprehensive
experiments, we demonstrate that LEGO yields tangent space estimates that are
significantly more robust to noise than those from LPCA, resulting in marked
improvements in downstream tasks such as manifold learning, boundary detection,
and local intrinsic dimension estimation.

</details>


### [533] [KaVa: Latent Reasoning via Compressed KV-Cache Distillation](https://arxiv.org/abs/2510.02312)
*Anna Kuzina,Maciej Pioro,Paul N. Whatmough,Babak Ehteshami Bejnordi*

Main category: cs.LG

TL;DR: KaVa框架通过自蒸馏，将教师模型的压缩KV缓存中的知识迁移到学生模型的潜在推理过程中，解决了潜在推理缺乏监督的问题，提高了复杂自然语言推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的思维链（CoT）方法在处理多步推理问题时计算成本高且存在冗余信息。潜在推理是一种更高效的替代方法，但由于缺乏监督而效果受限。本项目旨在解决潜在推理的监督问题，并结合CoT的准确性和潜在推理的效率。

Method: 提出KaVa框架，利用连续潜在令牌的表示灵活性，通过自蒸馏将教师模型的压缩KV缓存中的知识迁移到学生模型的潜在推理过程中，以对齐逐步KV轨迹。

Result: KaVa框架在性能上持续优于强大的潜在推理基线模型，在从仅方程到自然语言推理的转变中表现出更小的性能下降，并且在保持效率的同时可以扩展到更大的模型。压缩KV缓存蒸馏可以作为一种可扩展的潜在推理监督信号。

Conclusion: KaVa框架成功地将压缩KV缓存蒸馏作为一种可扩展的监督信号，用于潜在推理，结合了CoT教师的准确性和潜在推理的效率与可部署性。

Abstract: Large Language Models (LLMs) excel at multi-step reasoning problems with
explicit chain-of-thought (CoT), but verbose traces incur significant
computational costs and memory overhead, and often carry redundant, stylistic
artifacts. Latent reasoning has emerged as an efficient alternative that
internalizes the thought process, but it suffers from a critical lack of
supervision, limiting its effectiveness on complex, natural-language reasoning
traces. In this work, we propose KaVa, the first framework that bridges this
gap by distilling knowledge directly from a compressed KV-cache of the teacher
into a latent-reasoning student via self-distillation, leveraging the
representational flexibility of continuous latent tokens to align stepwise KV
trajectories. We show that the abstract, unstructured knowledge within
compressed KV-cache, which lacks direct token correspondence, can serve as a
rich supervisory signal for a latent reasoning student. Empirically, the
approach consistently outperforms strong latent baselines, exhibits markedly
smaller degradation from equation-only to natural-language traces, and scales
to larger backbones while preserving efficiency. These results establish
compressed KV-cache distillation as a scalable supervision signal for latent
reasoning, combining the accuracy of CoT-trained teachers with the efficiency
and deployability of latent inference.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [534] [Adversarial Social Influence: Modeling Persuasion in Contested Social Networks](https://arxiv.org/abs/2510.01481)
*Renukanandan Tumu,Cristian Ioan Vasile,Victor Preciado,Rahul Mangharam*

Main category: cs.SI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the Social Influence Game (SIG), a framework for modeling
adversarial persuasion in social networks with an arbitrary number of competing
players. Our goal is to provide a tractable and interpretable model of
contested influence that scales to large systems while capturing the structural
leverage points of networks. Each player allocates influence from a fixed
budget to steer opinions that evolve under DeGroot dynamics, and we prove that
the resulting optimization problem is a difference-of-convex program. To enable
scalability, we develop an Iterated Linear (IL) solver that approximates player
objectives with linear programs. In experiments on random and archetypical
networks, IL achieves solutions within 7% of nonlinear solvers while being over
10x faster, scaling to large social networks. This paper lays a foundation for
asymptotic analysis of contested influence in complex networks.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [535] [Coupling Magnons to an Opto-Electronic Parametric Oscillator](https://arxiv.org/abs/2510.01435)
*Junming Wu,Shihao Zhou,Benedetta Flebus,Wei Zhang*

Main category: cond-mat.mes-hall

TL;DR: The paper demonstrates strong and coherent coupling between an opto-electronic oscillator and a magnonic oscillator using a YIG sphere, creating a new hybrid platform for investigating long-distance coupling in coherent magnonic phenomena.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of local, near-field schemes in current magnonic systems, which restricts their applicability in remotely-coupled, distributed quantum networks, and to leverage opto-electronic architectures for long-haul signal transmission.

Method: Integrated an opto-electronic oscillator with a magnonic oscillator (microwave waveguide and YIG sphere), demonstrating strong and coherent coupling between magnon modes and photon modes via a nonlinear, parametric process seeded by an external pump. Tuned internal cavity and external pump phases to stabilize auto-oscillations.

Result: Achieved strong and coherent coupling between YIG's magnon modes and the opto-electronic oscillator's photon modes, observing the characteristic anti-crossing gap in the spectrum. The photon mode was produced on-demand via a nonlinear, parametric process.

Conclusion: This work presents a new hybrid platform for studying long-distance coupling and nonlinearity in coherent magnonic phenomena, with potential applications in future distributed hybrid magnonic systems.

Abstract: Hybrid magnonic systems have emerged as versatile modular components for
quantum signal transduction and sensing applications owing to their capability
of connecting distinct quantum platforms. To date, the majority of the magnonic
systems have been explored in a local, near-field scheme, due to the close
proximity required for realizing a strong coupling between magnons and other
excitations. This constraint greatly limits the applicability of magnons in
developing remotely-coupled, distributed quantum network systems. On the
contrary, opto-electronic architectures hosting self-sustained oscillations has
been a unique platform for longhaul signal transmission and processing. Here,
we integrated an opto-electronic oscillator with a magnonic oscillator
consisting of a microwave waveguide and a Y3Fe5O12(YIG) sphere, and
demonstrated strong and coherent coupling between YIG's magnon modes and the
opto-electronic oscillator's characteristic photon modes - revealing the
hallmark anti-crossing gap in the measured spectrum. In particular, the photon
mode is produced on-demand via a nonlinear, parametric process as stipulated by
an external seed pump. Both the internal cavity phase and the external pump
phase can be precisely tuned to stabilize either degenerate or nondegenerate
auto-oscillations. Our result lays out a new, hybrid platform for investigating
long-distance coupling and nonlinearity in coherent magnonic phenomena, which
may be find useful in constructing future distributed hybrid magnonic systems.

</details>


### [536] [Even-denominator fractional quantum Hall states with spontaneously broken rotational symmetry](https://arxiv.org/abs/2510.01482)
*Chengyu Wang,A. Gupta,S. K. Singh,C. T. Tai,L. N. Pfeiffer,K. W. Baldwin,R. Winkler,M. Shayegan*

Main category: cond-mat.mes-hall

TL;DR: 在GaAs二维电子气中，作者观察到了在垂直磁场下，即使是偶分母朗道能级填充{
u} = 5/2和7/2处，也存在分数量子霍尔效应（FQHS）和向列序共存的现象，这表明了向列分数量子霍尔态（NFQHS）的存在，并可能包含非阿贝尔准粒子激发。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探索分数量子霍尔效应（FQHS）与向列性（nematicity）之间的相互作用，特别是研究在纯垂直磁场下，在偶分母朗道能级填充 {
u} = 5/2 和 7/2 处，GaAs二维电子气中是否会出现具有高度各向异性纵向电阻的分数量子霍尔态，以及这种现象是否与自发对称性破缺（即向列序）同时存在。

Method: 通过实验在超高品质的GaAs二维电子系统中，在纯垂直磁场下，对朗道能级填充 {
u} = 5/2 和 7/2 处的电子行为进行测量，观察其纵向电阻的各向异性。通过栅极调控电子密度，研究从各向异性、正在形成的FQHS到各向同性的复合费米海之间的相变。此外，还进行了理论计算，探讨混合轨道分量在拓扑序和向列序竞争与相互作用中的作用。

Result: 观察到在纯垂直磁场下，GaAs二维电子系统在 {
u} = 5/2 和 7/2 处出现了高度各向异性的纵向电阻，这表明存在分数量子霍尔效应（FQHS）。同时，在 {
u} = 7/2 处，通过栅极调控电子密度，观察到了从各向异性、正在形成的FQHS到各向同性的复合费米海的相变。理论计算表明，部分占据的朗道能级中的混合轨道分量在拓扑序和向列序的竞争和相互作用中起着关键作用。

Conclusion: 在GaAs二维电子系统中，首次在纯垂直磁场下，在偶分母朗道能级填充 {
u} = 5/2 和 7/2 处观察到了分数量子霍尔效应（FQHS）与自发对称性破缺（向列序）的共存，这标志着向列分数量子霍尔态（NFQHS）的出现，并可能包含非阿贝尔准粒子激发。该研究揭示了混合轨道分量在拓扑序和向列序中的重要作用。

Abstract: The interplay between the fractional quantum Hall effect and nematicity is
intriguing as it links emerging topological order and spontaneous symmetry
breaking. Anisotropic fractional quantum Hall states (FQHSs) have indeed been
reported in GaAs quantum wells but only in tilted magnetic fields, where the
in-plane field explicitly breaks the rotational symmetry. Here we report the
observation of FQHSs with highly anisotropic longitudinal resistances in purely
perpendicular magnetic fields at even-denominator Landau level (LL) fillings
{\nu} = 5/2 and 7/2 in ultrahigh-quality GaAs two-dimensional hole systems. The
coexistence of FQHSs and spontaneous symmetry breaking at half fillings signals
the emergence of nematic FQHSs which also likely harbor non-Abelian
quasiparticle excitations. By gate tuning the hole density, we observe a phase
transition from an anisotropic, developing FQHS to an isotropic composite
fermion Fermi sea at {\nu} = 7/2. Our calculations suggest that the mixed
orbital components in the partially occupied LL play a key role in the
competition and interplay between topological and nematic orders.

</details>


### [537] [Coupling free-surface geometry and localized ion dose for continuum models of radiation-induced nanopatterning](https://arxiv.org/abs/2510.01503)
*Tyler P. Evans,Scott A. Norris*

Main category: cond-mat.mes-hall

TL;DR: 在辐照半导体表面上自组装纳米结构的研究长期存在，但缺乏统一的物理模型。本文提出一个耦合离子剂量和自由界面的模型，以解释图案形成，并表明该模型对实验观察具有重要预测意义。


<details>
  <summary>Details</summary>
Motivation: 解释辐照半导体表面纳米结构自组装现象的统一物理模型长期以来一直难以找到，而理解离子注入引起的碰撞级联的双重影响对于建立这样的模型至关重要。

Method: 开发了几种耦合局部离子剂量和演化自由界面的渐近近似方法。

Result: 理论预测的波纹波长、图案形成起始的临界辐照角度和表面粗糙度等量对耦合细节表现出令人惊讶的敏感性。

Conclusion: 所提出的模型能够解释图案形成中的细节，并为比较实验观察提供了理论基础。

Abstract: A first-principles understanding of the self-organization of highly regular,
nanometer-scale structures atop irradiated semiconductor surfaces has been
sought for decades. While numerous models exist which explain certain aspects
of this phenomenon, a unified, physical model capable of explaining all details
of pattern formation has remained elusive. However, it is increasingly apparent
that such a model will require understanding the dual influence of the
collision cascade initiated by ion implantation: first, as a source of material
transport by sputtering and atomic displacements occurring over short time
scales, and, second, as a source of defects permitting viscous flow within the
thin, amorphous layer that results from sustained irradiation over longer time
scales. To better understand the latter, we develop several asymptotic
approximations for coupling the localized ion dose with an evolving free
interface. We then show how theoretical predictions of quantities commonly used
for comparison with experimental observations -- such as ripple wavelengths,
critical irradiation angle for patterning onset, and surface roughening --
exhibit surprising sensitivity to the details of this coupling.

</details>


### [538] [Electric spin and valley Hall effects](https://arxiv.org/abs/2510.01714)
*W. Zeng*

Main category: cond-mat.mes-hall

TL;DR: 垂直电场可在二维材料中诱导横向自旋和谷流，这是一种新的自旋和谷霍尔效应。


<details>
  <summary>Details</summary>
Motivation: 提出并演示了自旋和谷霍尔效应的一种新版本，称为电自旋霍尔效应和电谷霍尔效应，它们是由垂直电场而非磁场引起的。

Method: 在基于弯曲二维六方材料的一体式隧道结中，通过垂直电场引起的电子后向反射相位来实现横向自旋和谷流。

Result: 在垂直电场作用下，观察到了电谷霍尔效应（奇响应）和电自旋霍尔效应（偶响应）。这两种效应可以实现具有完整自旋谷极化的纯自旋谷锁定态的横向分离，并且具有相等的自旋谷霍尔角，同时保持时间反转对称性。

Conclusion: 该研究提出了一种实现自旋和谷霍尔效应的新机制，并为自旋和谷自由度的全电场操控提供了新途径，在自旋电子学和谷电子学领域具有重要的潜在应用价值。

Abstract: The electric Hall effect (EHE) is a newly identified Hall effect
characterized by a perpendicular electric field inducing a transverse charge
current in two-dimensional (2D) systems. Here, we propose a spin and valley
version of EHE. We demonstrate that the transverse spin and valley currents can
be generated in an all-in-one tunnel junction based on a buckled 2D hexagonal
material in response to a perpendicular electric field, referred to as the
electric spin Hall effect and electric valley Hall effect, respectively. These
effects arise from the perpendicular-electric-field-induced backreflection
phase of electrons in the junction spacer, independent of Berry curvature. The
valley Hall conductance exhibits an odd response to the perpendicular electric
field, whereas the spin Hall conductance shows an even one. The predicted
effects can further enable the transverse separation of a pair of pure
spin-valley-locked states with full spin-valley polarization while preserving
time-reversal symmetry, as manifested by equal spin and valley Hall angles. Our
findings present a new mechanism for realizing the spin and valley Hall effects
and provide a novel route to the full electric-field manipulation of spin and
valley degrees of freedom, with significant potential for future applications
in spintronics and valleytronics.

</details>


### [539] [Orbital Magnetization in Correlated States of Twisted Bilayer Transition Metal Dichalcogenides](https://arxiv.org/abs/2510.01727)
*Xiaoyu Liu,Chong Wang,Xiao-Wei Zhang,Ting Cao,Di Xiao*

Main category: cond-mat.mes-hall

TL;DR: 在量子反常霍尔效应的莫尔二}}.{}.{}.{}体系中，我们观察到了具有显著轨道贡献的相互作用驱动铁磁性。本文将现代轨道磁化理论扩展到Hartree-Fock态，并证明了标准表达式在Hartree-Fock轨道和哈密顿量下仍然有效。将此理论应用于扭曲的MoTe$_2$双层体系，我们发现其轨道磁化强度约为一个Bohr磁元/莫尔单元，且与扭曲角度存在非单调关系。本研究为理解相互作用的莫尔体系中的轨道磁化提供了一个通用理论，并为解释相关实验提供了量化指导。


<details>
  <summary>Details</summary>
Motivation: 解释在量子反常霍尔效应的莫尔体系中观察到的具有显著轨道贡献的相互作用驱动铁磁性。

Method: 将现代轨道磁化理论扩展到Hartree-Fock态，并使用Hartree-Fock轨道和哈密顿量验证其有效性。将此理论应用于扭曲的MoTe$_2$双层体系，并与直接数值计算进行比较。

Result: 理论在Kane-Mele-Hubbard模型弱场情况下与直接数值计算结果一致。对于扭曲的MoTe$_2$双层体系，计算得到轨道磁化强度约为一个Bohr磁元/莫尔单元，且该磁化强度随扭曲角度呈现非单调变化。

Conclusion: 提出了一个适用于相互作用莫尔体系的轨道磁化通用理论，并为解释相关实验提供了量化指导。

Abstract: Recent observations of quantum anomalous Hall effects in moir\'e systems have
revealed the emergence of interaction-driven ferromagnetism with significant
orbital contributions. To capture this physics, we extend the modern theory of
orbital magnetization to Hartree-Fock states and show that the standard
expression remains valid with Hartree-Fock orbitals and Hamiltonians. We then
benchmark our theory against the Kane-Mele-Hubbard model in a weak field, which
yields excellent agreement with direct numerical calculations. Applying our
theory to twisted MoTe$_2$ bilayers, we find orbital magnetization of order one
Bohr magneton per moir\'e cell with a non-monotonic twist-angle dependence. Our
work establishes a general theory of orbital magnetization in interacting
moir\'e systems and provides quantitative guidance for interpreting recent
experiments.

</details>


### [540] [Tunable Wigner Molecules in a Germanium Quantum Dot](https://arxiv.org/abs/2510.01786)
*Chenggang Yang,Jun Lu,Hongzhang Wang,Jian Zeng,Wendong Bian,Zhengshan Guo,Jiankun Li,Yulei Zhang,Junwei Luo,Tian Pei*

Main category: cond-mat.mes-hall

TL;DR: 电子在量子点中形成空穴维格纳分子，其行为可调，并可用于量子信息。


<details>
  <summary>Details</summary>
Motivation: 研究强相互作用物理的微观特征，并探索维格纳分子在量子信息中的应用。

Method: 通过改变施加到量子点器件上的电压来精确调控空穴密度，从而形成或破坏维格纳分子。

Result: 在栅极定义的锗量子点中成功形成了空穴维格纳分子，并观察到了从维格纳分子到费米液体类似物的熔化过程，以及有序和无序结构的中间共存状态。

Conclusion: 栅极定义的锗量子点为研究强相互作用物理和量子信息应用提供了一个新的平台。

Abstract: The interplay between Coulomb interactions and kinetic energy underlies many
exotic phases in condensed matter physics. In a two-dimensional electronic
system, If Coulomb interaction dominates over kinetic energy, electrons
condense into a crystalline phase which is referred as Wigner crystal. This
ordered state manifests as Wigner molecule for few electrons at the microscopic
scale. Observation of Wigner molecules has been reported in quantum dot and
moire superlattice systems. Here we demonstrate hole Wigner molecules can be
formed in a gate-defined germanium quantum dot with high tunability. By varying
voltages applied to the quantum dot device, we can precisely tune the hole
density by either changing the hole occupancy or the quantum dot size. For
densities smaller than a certain critical value, Coulomb interaction localizes
individual holes into ordered lattice sites, forming a Wigner molecule. By
increasing the densities, melting process from a Wigner molecule to Fermi
liquid-like particles is observed. An intermediate configuration which
indicates the coexistence of ordered structure and disordered structure can be
formed within a narrow effective density range. Our results provide a new
platform for further exploration of the microscopic feature of strong
correlated physics and open an avenue to exploit the application of Wigner
molecules for quantum information in a very promising spin qubit platform.

</details>


### [541] [Intermediate diffusive-ballistic electron conduction around mesoscopic defects in graphene](https://arxiv.org/abs/2510.01821)
*Toni Markovic,Wei Huang,William S. Huxter,Pietro Gambardella,Sebastian Stepanow*

Main category: cond-mat.mes-hall

TL;DR: Graphene 中的电荷传输在设备尺寸与电子平均自由路径相当时变得很重要。通过扫描隧道电位测量法研究了石墨烯中介观缺陷周围的电荷传输。该方法可区分散射偶极子的扩散和弹道分量。


<details>
  <summary>Details</summary>
Motivation: 当器件尺寸和特征与电子平均自由路径相当时，电荷传输中的非扩散效应变得相关。

Method: 通过扫描隧道电位测量法研究了石墨烯中介观缺陷周围的电荷传输，同时测量局部电化学势和石墨烯层的形貌。

Result: 发现电荷传输处于扩散和弹道极限之间的中间状态，并且扩散模型严重低估了缺陷周围的电化学势。此外，即使在大于平均自由路径的特征尺寸下，也存在弹道输运的证据，并且在较小的尺寸下会迅速增加。

Conclusion: 石墨烯中缺陷周围的电荷传输不能用纯粹的扩散或弹道模型来解释。需要考虑介于两者之间的中间输运机制。实验和模拟都强调了缺陷尺寸与平均自由路径之比在介观尺度上的重要性。

Abstract: Non-diffusive effects in charge transport become relevant as device sizes and
features become comparable to the electronic mean free path. As a model system,
we investigate the electric transport around mesoscopic defects in graphene
with scanning tunneling potentiometry. Diffusive and ballistic contributions to
the scattering dipole are probed by simultaneously resolving the nanoscale
topography of pits in the graphene layer and measuring the local
electrochemical potential in the surrounding area. We find evidence of
transport in the intermediate regime between the diffusive and ballistic
limits, such that the magnitude of the electrochemical potential around the
defects is substantially underestimated by diffusive models. Our experiments
and modeling are supported by lattice Boltzmann simulations, which highlight
the importance of the ratio between defect size and mean free path in the
intermediate transport regime. The magnitude of the scattering dipole depends
on the shape of the pits in both the ballistic and diffusive transport modes.
Remarkably, ballistic contributions to the electron transport are found at
feature sizes larger than the mean free path and rapidly increase at lower
sizes, having a noticeable impact already at mesoscopic length scales.

</details>


### [542] [Band Gap Engineering of Nitrogen-Doped Monolayer WSe$_2$ Superlattice and its application to Field Effect Transistor](https://arxiv.org/abs/2510.01917)
*Yi-Cheng Lo,Liao Jia Wang,Yu-Chang Chen*

Main category: cond-mat.mes-hall

TL;DR: 通过对氮掺杂的WSe2进行周期性取代，实现能带工程。


<details>
  <summary>Details</summary>
Motivation: 探索氮掺杂WSe2在电子和光电应用中的潜力。

Method: 系统研究了掺杂和未掺杂WSe2的电子结构，并评估了其场效应晶体管（FET）性能。

Result: 发现掺杂会调节能带隙，且器件性能对温度不敏感，但4排结构不适合实际应用，原始结构阈值电压过高，6排和8排结构是较优选择。

Conclusion: 氮掺杂WSe2为未来的FET集成提供了有前景的候选者。

Abstract: We systematically investigate the electronic structures of pristine monolayer
WSe$_2$ and WSe$_2$ superlattices with periodic nitrogen substitution. Unlike
random doping, which often introduces in-gap impurity states, periodic nitrogen
doping primarily modulates the band gap, thereby facilitating effective band
gap engineering for electronic and optoelectronic applications. The gap narrows
monotonically with increasing dopant density (pristine $>$ 8-row $>$ 6-row $>$
4-row), directly influencing device switching. We also evaluate the FET
performance of nanojunctions created by these configurations by examining the
contour plot of current density as a function of temperature and gate voltage,
which quantifies how bandgap engineering affects switching characteristics. Our
calculations clarify the classical-quantum crossover in sub-10 nm 2D FETs: as
$T$ rises, $J$ approaches the thermionic current; as $T$ falls, quantum
tunneling dominates, and the steep energy dependence of $\tau(E)$ may break the
classical limit of subthreshold swing imposed by the Boltzmann tyranny. The
optimal gating range ($V_g^\mathrm{ON}$, $V_g^\mathrm{OFF}$) is investigated
for each temperature, insensitive to temperature in the high-temperature
regime, confirming the good thermal stability of the FET devices. A comparison
study demonstrates that the 4-row structure, with large $J_\mathrm{OFF}$ and
restricted operation range, is inappropriate for realistic FET applications.
The pristine structure has a high $V_g^\mathrm{OFF}$ ($\sim$1.1 V) makes it
less practical, since such a large threshold voltage may promote time-dependent
dielectric breakdown (TDDB) of the oxide layer, reducing device dependability.
The 6-row and 8-row structures exhibit more favorable $V_g^\mathrm{OFF}$ values
($\sim$0.75 V), achieving compromise, making them more promising candidates for
future FET integration.

</details>


### [543] [Electrically tunable ultrafast dynamics and interactions of hybrid excitons in a 2D semiconductor bilayer](https://arxiv.org/abs/2510.01921)
*Edoardo Lopriore,Charalambos Louca,Armando Genco,Irantzu Landa,Daniel Erkensten,Charles J. Sayers,Samuel Brem,Raul Perea-Causin,Kenji Watanabe,Takashi Taniguchi,Christoph Gadermaier,Ermin Malic,Giulio Cerullo,Stefano Dal Conte,Andras Kis*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Extended efforts have been devoted to the study of strongly-interacting
excitons and their dynamics, towards macroscopic quantum states of matter such
as Bose-Einstein condensates of excitons and polaritons. Momentum-direct
layer-hybridized excitons in transition metal dichalcogenides have attracted
considerable attention due to their high oscillator strength and dipolar
nature. However, the tunability of their interactions and dynamics remains
unexplored. Here, we achieve an unprecedented control over the nonlinear
properties of dipolar layer-hybridized excitons in an electrically gated van
der Waals homobilayer monitored by transient optical spectroscopy. By applying
a vertical electric field, we reveal strong Coulomb interactions of dipolar
hybrid excitons, leading to opposite density-dependent energy shifts of the two
main hybrid species based on their dipolar orientation, together with a
strongly enhanced optical saturation of their absorption. Furthermore, by
electrically tuning the interlayer tunneling between the hybridized carriers,
we significantly extend the formation time of hybrid excitons, while
simultaneously increasing their decay times. Our findings have implications for
the search on quantum blockade and condensation of excitons and dipolaritons in
two-dimensional materials.

</details>


### [544] [Measuring the measurement problem: controlling decoherence with measurement duration in molecular MCB junctions](https://arxiv.org/abs/2510.01945)
*C. J. Muller*

Main category: cond-mat.mes-hall

TL;DR: 测量时长影响分子结量子相干性，可调控以探测量子行为。


<details>
  <summary>Details</summary>
Motivation: 研究测量持续时间如何影响分子机械可控断裂结中的量子相干性，以及这些系统在室温下具有长退相干时间（1-20毫秒）的特性。

Method: 通过调节电流-电压（IV）特性测量积分时间，并观察其与退相干时间的相对关系，研究量子干涉图样向经典行为的转变。

Result: 观察到测量时长可引起量子干涉图样（表现为结构化的数据点带）向经典行为（表现为单一平均响应）的转变。

Conclusion: 测量时长是探测分子结中量子行为的可控参数，为理解量子力学中的退相干动力学提供了新见解。

Abstract: We investigate the influence of the measurement duration on quantum coherence
in molecular mechanically controlled break junctions operating in a
tetrahydrofuran (THF) partially wet phase. These systems represent a distinct
class of enclosed open quantum systems with unusually long decoherence times at
ambient conditions, on the order of 1-20 ms. By tuning the integration time of
the current measurement in current-voltage (IV) characteristics, relative to
the decoherence time, we observe a transition from quantum interference
patterns, manifested as structured bands of data points, to classical behavior
characterized by a single averaged response. This demonstrates that the
duration of a measurement acts as a controllable parameter for probing quantum
behavior in molecular junctions, offering new insights into decoherence
dynamics in quantum mechanics.

</details>


### [545] [Pulsed-laser induced gold microparticle fragmentation by thermal strain](https://arxiv.org/abs/2510.02011)
*Yogesh Pokhrel,Meike Tack,Sven Reichenberger,Matteo Levantino,Anton Plech*

Main category: cond-mat.mes-hall

TL;DR: 激光碎片化是激光在液體烧蚀（LAL）的替代方法，但其结构基础尚不清楚。本研究使用超快X射线散射和皮秒激光激发金微粒悬浮液，研究碎片化后的热动力学和结构演变。实验结果与双温模型模拟结果一致，发现超过750 J/m$^2$的能量阈值，微粒会在纳秒内碎裂成大块，主要原因是严重不均匀的热分布导致应变和超快加热导致应力限制。2700 J/m$^2$的能量下，有限数量的小团簇的形成归因于微粒前侧的光热分解。


<details>
  <summary>Details</summary>
Motivation: 理解激光碎片化金微粒悬浮液的过程的结构基础，包括热动力学和结构演变。

Method: 使用超快X射线散射和皮秒激光激发金微粒悬浮液，并辅以双温模型进行模拟。

Result: 在能量阈值750 J/m$^2$以上，微粒在纳秒内碎裂成大块，这是由不均匀热分布引起的应变和超快加热引起的应力限制所驱动。在2700 J/m$^2$下，观察到有限数量的小团簇形成，这归因于微粒前侧的光热分解。

Conclusion: 研究阐明了激光碎片化金微粒悬浮液的结构基础，确定了导致碎片化的关键因素，并解释了不同能量密度下观察到的现象。

Abstract: Laser fragmentation of suspended microparticles is an upcoming alternative to
laser ablation in liquid (LAL) that allows to streamline the the delivery
process and optimize the irradiation conditions for best efficiency. Yet, the
structural basis of this process is not well understood to date. Herein we
employed ultrafast x-ray scattering upon picosecond laser excitation of a gold
microparticle suspension in order to understand the thermal kinetics as well as
structure evolution after fragmentation. The experiments are complemented by
simulations according to the two-temperature model to verify the spatiotemporal
temperature distribution. It is found that above a fluence threshold of 750
J/m$^2$ the microparticles are fragmented within a nanosecond into several
large pieces where the driving force is the strain due to a strongly
inhomogenous heat distribution on the one hand and stress confinement due to
the ultrafast heating compared to stress propagation on the other hand. The
additional limited formation of small clusters is attributed to photothermal
decomposition on the front side of the microparticles at the fluence of 2700
J/m$^2$.

</details>


### [546] [Phonon Spin Selective One-Way Axial Phonon Transport in Chiral Nanohelix](https://arxiv.org/abs/2510.02221)
*Jia Li,Yu-Tao Tan,Yizhou Liu,Jie Ren*

Main category: cond-mat.mes-hall

TL;DR: 通过操纵声子自旋角动量，在手性纳米材料中实现纳米级单向声子激发和路由。


<details>
  <summary>Details</summary>
Motivation: 选择性地激发和操纵纳米声子在现代纳米能源控制和信息传感中越来越重要，但仍然具有挑战性。

Method: 通过将圆偏振红外吸收等自旋多物理场耦合到声子自旋角动量，实现手性纳米材料中轴向声子的单向激发和路由。

Result: 在最小的手性碳纳米管中，整流率接近100%，实现了理想的单向声子路由器，并通过分子动力学模拟得到验证。

Conclusion: 研究结果为通过声子自旋自由度进行灵活的声子操纵提供了新思路，为未来的自旋声子学铺平了道路。

Abstract: Selectively exciting and manipulating phonons at nanoscale becomes more and
more important but still remains challenging in modern nano-energy control and
information sensing. Here, we show that the phonon spin angular momentum
provides an extra degree of freedom to achieve versatile manipulation of axial
phonons in nanomaterials via coupling to spinful multi-physical fields, such as
circularly polarized infrared absorption. In particular, we demonstrate the
nanoscale one-way axial phonon excitation and routing in chiral nanomaterials,
by converting the photon spin in circularly polarized optical fields into the
collective interference phonon spin. As exemplified in the smallest chiral
carbon nanotube, we show that the rectification rate can reach nearly 100\%,
achieving an ideal one-way phonon router, which is verified by molecular
dynamics simulations. Our results shed new light on the flexible phonon
manipulation via phonon spin degree of freedom, paving the way for future spin
phononics.

</details>


### [547] [Emergent Hierarchy in Localized States of Organic Quantum Chains](https://arxiv.org/abs/2510.02231)
*L. L. Lage,A. B. Félix,D. S. Gomes,M. L. Pereira Jr.,A. Latgé*

Main category: cond-mat.mes-hall

TL;DR: OQCs的电子和输运性质研究


<details>
  <summary>Details</summary>
Motivation: 研究有机量子链（OQCs）的电子和输运性质，特别是其准一维特性带来的非常规电子和输运现象。

Method: 结合分子动力学松弛和密度泛函理论（DFT）评估结构稳定性；使用指数衰减的跳跃参数化进行紧束缚模型计算，以重现DFT结果；分析分层态的局域化行为；分析一维OQC与碳栏相互作用的输运响应。

Result: 发现OQCs具有稳健且接近恒定的能隙，与实验数据一致；识别出具有不同局域化行为的分层态；分析了在耦合到碳栏时的不同输运响应。

Conclusion: OQCs的计算结果与实验数据吻合，并且其独特的电子和输运性质表明其在碳基纳米器件中具有应用潜力。

Abstract: Organic Quantum Chains (OQCs) represent a newly synthesized class of
carbon-based nanostructures whose quasi-one-dimensional nature gives rise to
unconventional electronic and transport phenomena. Here we investigate the
electronic and transport properties of recently synthesized OQCs [Nature
Communications, 12, 5895 (2021)]. Structural stability was first assessed
through molecular dynamics relaxation combined with density functional theory
(DFT). The optimized coordinates are then used in a tight-binding model with
exponentially decaying hopping parameterization, which reproduces the DFT
results with high accuracy. Our calculations reveal a robust and nearly
constant energy gap across several OQC configurations, in agreement with
experimental data. We also identify emergent hierarchical states, characterized
by distinct localization behaviors within sets of localized bands. Finally, we
analyze different transport responses in scenarios involving the
one-dimensional OQC coupled to carbon corrals, as observed in the experimental
data, highlighting their potential as promising systems for application in
carbon nanodevices.

</details>


### [548] [Quantum gates in coupled quantum dots controlled by coupling modulation](https://arxiv.org/abs/2510.02267)
*Alejandro D. Bendersky,Sergio S. Gomez,Rodolfo H. Romero*

Main category: cond-mat.mes-hall

TL;DR: 通过调制量子比特间的隧穿耦合来控制单比特门，并通过调制双量子比特间的交换耦合来生成多比特门，从而实现量子计算。


<details>
  <summary>Details</summary>
Motivation: 研究了在静态磁场和时间依赖的相互作用耦合调制下，一对单电子双量子点（DQD）的动力学行为，旨在为量子计算提供可控的量子门操作。

Method: 提出了通过调制量子比特间的隧穿耦合来实现单比特门，并通过调制双量子比特间的交换耦合来实现多比特门，并发展了解析近似方法来控制量子比特，最后通过数值计算验证了模型的准确性和鲁棒性。

Result: 数值计算结果表明，即使在偏离理想条件的情况下，双电子量子比特的幺正演化也能成功执行设计的操作。

Conclusion: 研究表明，通过调制量子比特的耦合方式，可以有效地实现量子计算所需的操作，并且该方法具有一定的鲁棒性，能够在非理想条件下稳定运行。

Abstract: We studied the dynamics of a pair of single-electron double quantum dots
(DQD) under longitudinal and transverse static magnetic fields and
time-dependent harmonic modulation of their interaction couplings. We propose
to modulate the tunnel coupling between the QDs to produce one-qubit gates and
the exchange coupling between DQDs to generate entangling gates, the set of
operations required for quantum computing. We developed analytical
approximations to set the conditions to control the qubits and applied them to
numerical calculations to test the accuracy and robustness of the analytical
model. The results shows that the unitary evolution of the two-electron state
performs the designed operations even under conditions shifted from the ideal
ones.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [549] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 该论文提出了一种基于分布式算术（DA）的内存计算方法，用于加速神经网络（NN）中的向量-矩阵乘法（VMM），以克服传统内存VMM中ADC/DAC的功耗和面积瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统的内存VMM依赖ADC/DAC，功耗和面积开销大。

Method: 将DA技术扩展到输入向量与常数矩阵的乘法，通过在ReRAM内存外围使用移位和加法电路来实现VMM，并将权重和存储在内存中。

Result: 与比特切片相比，该方法延迟降低4.5倍，能耗降低12倍，并且消除了对ADC的需求。

Conclusion: 提出的DA方法通过利用移位和加法电路，以及能量高效的传感和细粒度流水线，显著提高了内存VMM的效率，减少了功耗和面积。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>


### [550] [Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis](https://arxiv.org/abs/2510.01730)
*Ashiyana Abdul Majeed,Mahmoud Meribout,Safa Mohammed Sali*

Main category: cs.AR

TL;DR: 该论文提出了一种利用 NVIDIA 边缘 GPU 加速的软硬件协同方法，实现了医学图像（MRI 和 CT）的同步重建与诊断，在 NVIDIA Jetson 平台上实现了接近 150 FPS 的吞吐量，并提高了 5% 的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的 AI 医学影像诊断方法效率不高，尤其是在多模型系统和硬件加速方面缺乏优化，而边缘计算设备（如 NVIDIA Jetson）的出现使得高效利用其硬件加速器（GPU 和 DLA）变得至关重要。

Method: 提出了一种硬件加速方法，利用 NVIDIA 边缘 GPU（Jetson AGX Xavier 和 Orin）的硬件引擎和调度技术，实现了多个人工智能模型的并行重建和诊断。通过精细调整 AI 模型（特别是 GAN 模型），使其能够在 GPU 上高效运行，无需回退到 CPU，同时不牺牲准确性。对模型进行了硬件感知（hardware-aware）的优化和分层分配，以减少硬件引擎之间的时间延迟。

Result: 实现了近 150 FPS 的吞吐量，比原始模型将性能提高了一倍。所提出的硬件感知 AI 模型在准确性方面比原始模型提高了 5%。

Conclusion: 硬件感知的多模型并行处理是提高医学影像分析和诊断效率的有效途径，尤其是在资源受限的边缘计算环境中。

Abstract: Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [551] [Microscaling Floating Point Formats for Large Language Models](https://arxiv.org/abs/2510.01863)
*Marco Cococcioni,Dario Pagani,Federico Rossi*

Main category: cs.NE

TL;DR: 该论文提出了一种名为“微缩标度”（microscaling）的新型浮点格式，用于优化大型语言模型（LLM）的计算和内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）日益增长的计算和内存需求，迫切需要创新的方法来优化资源利用，同时不损害性能。

Method: 通过使用微缩标度技术，这是一种旨在通过减少LLM中数值表示相关的存储和计算开销的新技术。与为每个值分配专用标度的传统浮点表示不同，微缩标度跨一组值共享标度，从而能够实现紧凑的单字节浮点表示，同时保持扩展的动态范围。我们探索了在8位浮点格式的背景下应用微缩标度，以显著减少内存占用和计算成本。

Result: 在GPT-2 LLM架构中测试了微缩标度浮点格式的几种配置，证明微缩标度数据格式在训练和推理期间可以实现具有竞争力的准确性。

Conclusion: 微缩标度是一种有效的、资源高效的替代方案，可用于大规模部署LLM。

Abstract: The increasing computational and memory demands of large language models
(LLMs) necessitate innovative approaches to optimize resource usage without
compromising performance. This paper leverages microscaling floating-point
formats, a novel technique designed to address these challenges by reducing the
storage and computational overhead associated with numerical representations in
LLMs. Unlike traditional floating-point representations that allocate a
dedicated scale for each value, microscaling employs a shared scale across a
block of values, enabling compact one-byte floating-point representations while
maintaining an extended dynamic range. We explore the application of
microscaling in the context of 8-bit floating-point formats to significantly
reduce memory footprint and computational costs. We tested several
configurations of microscaling floats within the GPT-2 LLM architecture,
demonstrating that microscaling data formats can achieve competitive accuracy
during training and inference, proving its efficacy as a resource-efficient
alternative for deploying LLMs at scale. The source code is publicly available
at: https://github.com/unipi-dii-compressedarith/llm.c-sve

</details>


### [552] [VarCoNet: A variability-aware self-supervised framework for functional connectome extraction from resting-state fMRI](https://arxiv.org/abs/2510.02120)
*Charalampos Lamprou,Aamna Alshehhi,Leontios J. Hadjileontiadis,Mohamed L. Seghier*

Main category: cs.NE

TL;DR: VarCoNet是一个基于自监督对比学习的框架，用于从静息态fMRI数据中提取功能连接组(FC)，并通过考虑个体功能变异性来提高稳健性、可解释性和泛化能力，在受试者识别和自闭症谱系障碍分类任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在精确医疗领域，考虑个体间大脑功能变异性至关重要，应将这种变异性视为有意义的数据而非噪声。

Method: VarCoNet采用自监督对比学习，利用内在的功能个体变异性作为大脑功能编码器，生成可用于下游任务的FC嵌入。该方法的核心是1D-CNN-Transformer编码器，并结合了新颖的基于信号分割的数据增强策略和贝叶斯超参数优化。

Result: VarCoNet在人类连接组计划(HCP)的受试者识别任务以及ABIDE I和ABIDE II数据集的自闭症谱系障碍(ASD)分类任务中进行了评估，并与包括13种深度学习方法在内的最先进方法进行了广泛比较，证明了其优越性、稳健性、可解释性和泛化能力。

Conclusion: VarCoNet为静息态fMRI的功能连接组分析提供了一个通用且强大的框架。

Abstract: Accounting for inter-individual variability in brain function is key to
precision medicine. Here, by considering functional inter-individual
variability as meaningful data rather than noise, we introduce VarCoNet, an
enhanced self-supervised framework for robust functional connectome (FC)
extraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs
self-supervised contrastive learning to exploit inherent functional
inter-individual variability, serving as a brain function encoder that
generates FC embeddings readily applicable to downstream tasks even in the
absence of labeled data. Contrastive learning is facilitated by a novel
augmentation strategy based on segmenting rs-fMRI signals. At its core,
VarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series
processing, enhanced with a robust Bayesian hyperparameter optimization. Our
VarCoNet framework is evaluated on two downstream tasks: (i) subject
fingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)
autism spectrum disorder (ASD) classification, using rs-fMRI data from the
ABIDE I and ABIDE II datasets. Using different brain parcellations, our
extensive testing against state-of-the-art methods, including 13 deep learning
methods, demonstrates VarCoNet's superiority, robustness, interpretability, and
generalizability. Overall, VarCoNet provides a versatile and robust framework
for FC analysis in rs-fMRI.

</details>
