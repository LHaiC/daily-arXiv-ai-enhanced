<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [cs.CL](#cs.CL) [Total: 54]
- [cs.LG](#cs.LG) [Total: 25]
- [cs.RO](#cs.RO) [Total: 12]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 12]
- [quant-ph](#quant-ph) [Total: 36]
- [eess.SP](#eess.SP) [Total: 7]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.SI](#cs.SI) [Total: 4]
- [eess.SY](#eess.SY) [Total: 7]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 16]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CuriosAI Submission to the EgoExo4D Proficiency Estimation Challenge 2025](https://arxiv.org/abs/2507.08022)
*Hayato Tanoue,Hiroki Nishihara,Yuma Suzuki,Takayuki Hori,Hiroki Takushima,Aiswariya Manojkumar,Yuki Shibata,Mitsuru Takeda,Fumika Beppu,Zhao Hengwei,Yuto Kanda,Daichi Yamaga*

Main category: cs.CV

TL;DR: CuriosAI团队在CVPR 2025 EgoExo4D技能熟练度估计挑战赛中，提出了两种多视图技能评估方法：基于Sapiens-2B的多任务学习框架（准确率43.6%）和结合零样本场景识别与VideoMAE分类器的两阶段管道（准确率47.8%）。后者性能更优，证明了场景条件建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估多视图技能熟练度。

Method: 1. 多任务学习框架：使用Sapiens-2B联合预测熟练度和场景标签。 2. 两阶段管道：结合零样本场景识别和特定视图VideoMAE分类器。

Result: 两阶段方法展现出优于多任务学习框架的性能，表明了场景条件建模在熟练度估计中的有效性。

Conclusion: 该报告提出了CuriosAI团队在CVPR 2025 EgoExo4D技能熟练度估计挑战赛中的提交内容。我们的多视图技能评估方法包括一个多任务学习框架，该框架使用Sapiens-2B来联合预测熟练度和场景标签（准确率为43.6%），以及一个结合零样本场景识别和特定视图VideoMAE分类器的两阶段管道（准确率为47.8%）。

Abstract: This report presents the CuriosAI team's submission to the EgoExo4D
Proficiency Estimation Challenge at CVPR 2025. We propose two methods for
multi-view skill assessment: (1) a multi-task learning framework using
Sapiens-2B that jointly predicts proficiency and scenario labels (43.6 %
accuracy), and (2) a two-stage pipeline combining zero-shot scenario
recognition with view-specific VideoMAE classifiers (47.8 % accuracy). The
superior performance of the two-stage approach demonstrates the effectiveness
of scenario-conditioned modeling for proficiency estimation.

</details>


### [2] [Self-Consistency in Vision-Language Models for Precision Agriculture: Multi-Response Consensus for Crop Disease Management](https://arxiv.org/abs/2507.08024)
*Mihir Gupta,Abhay Mangla,Ross Greer,Pratik Desai*

Main category: cs.CV

TL;DR: 提出了一种结合专家评估和自洽机制的农业图像分析框架，提高了VLM在玉米叶病害识别中的准确性、症状分析和治疗建议，并支持移动设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在专业的农业领域表现不佳，而精准农业严重依赖准确的图像分析来进行作物病害识别和治疗建议。

Method: 提出了一种领域感知框架，结合了基于提示的专家评估和自洽机制来增强VLM在精准农业应用中的可靠性。具体创新包括：1. 基于提示的评估协议，将语言模型配置为专家植物病理学家，对图像分析输出进行可扩展评估；2. 余弦一致性自投票机制，通过生成多个候选响应并利用领域自适应嵌入来选择最语义一致的诊断。

Result: 在玉米叶病害识别任务中，该方法将诊断准确性从82.2%提高到87.8%，症状分析从38.9%提高到52.2%，治疗建议从27.8%提高到43.3%，优于标准的贪婪解码。系统紧凑，支持移动设备部署和实时决策。

Conclusion: 该方法在玉米叶病害诊断方面显著提高了准确性、症状分析和治疗建议的质量，并且模型足够紧凑，可以在移动设备上部署，支持资源受限环境下的实时农业决策，展示了人工智能驱动的精准农业工具的巨大潜力。

Abstract: Precision agriculture relies heavily on accurate image analysis for crop
disease identification and treatment recommendation, yet existing
vision-language models (VLMs) often underperform in specialized agricultural
domains. This work presents a domain-aware framework for agricultural image
processing that combines prompt-based expert evaluation with self-consistency
mechanisms to enhance VLM reliability in precision agriculture applications. We
introduce two key innovations: (1) a prompt-based evaluation protocol that
configures a language model as an expert plant pathologist for scalable
assessment of image analysis outputs, and (2) a cosine-consistency self-voting
mechanism that generates multiple candidate responses from agricultural images
and selects the most semantically coherent diagnosis using domain-adapted
embeddings. Applied to maize leaf disease identification from field images
using a fine-tuned PaliGemma model, our approach improves diagnostic accuracy
from 82.2\% to 87.8\%, symptom analysis from 38.9\% to 52.2\%, and treatment
recommendation from 27.8\% to 43.3\% compared to standard greedy decoding. The
system remains compact enough for deployment on mobile devices, supporting
real-time agricultural decision-making in resource-constrained environments.
These results demonstrate significant potential for AI-driven precision
agriculture tools that can operate reliably in diverse field conditions.

</details>


### [3] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 通过剪枝优化深度学习模型，降低模型复杂度，提高可解释性，适用于医学图像分析。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析领域取得了显著进展，但模型的巨大尺寸和缺乏透明性限制了其在临床实践中的应用。可解释性技术的发展使得评估神经网络中各个组件的贡献成为可能。

Method: 提出了一种受可解释性指导的剪枝框架，通过选择性地保留每个图层中最相关的部分来降低模型复杂度，同时保持预测性能和可解释性。

Result: 实验证明，该方法在多个医学图像分类基准测试中实现了高压缩率和极小的准确性损失，有望实现轻量级、可解释的模型。

Conclusion: 该方法通过选择性地保留每个图层最相关的部分，实现了高压缩率和可接受的准确性损失，适用于医疗保健领域中轻量级、可解释模型的实际部署。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [4] [Development of a Canada-Wide Morphology Map for the ITU-R P. 1411 Propagation Model](https://arxiv.org/abs/2507.08026)
*Jennifer P. T. Nguyen*

Main category: cs.CV

TL;DR: 本研究开发了一个加拿大范围的形态地图，使用机器学习自动分类环境类型，以提高300 MHz至100 GHz频率范围内室外短距离传播的路径损耗估计精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决ITU-R P.1411-12传播模型指南中环境类型描述符的定性性质问题，并提高室外短距离传播路径损耗估计的准确性。

Method: 采用机器学习方法自动进行环境类型分类，以解决ITU-R P.1411-12传播模型指南中环境类型描述符的定性性质问题。

Result: 生成了一个加拿大范围的形态地图，将区域划分为住宅、城市低层和城市高层环境。

Conclusion: 开发了一个加拿大范围的形态地图，将区域划分为住宅、城市低层和城市高层环境，遵循ITU-R P.1411-12传播模型指南。该地图通过机器学习方法自动分类环境类型，解决了该建议书中环境类型描述符的定性性质问题。通过广泛的实验优化了分类精度，确保了在300 MHz至100 GHz频率范围内进行室外短距离传播时更精确的路径损耗估计。

Abstract: This paper outlines the development of a Canada-wide morphology map
classifying regions into residential, urban low-rise, and urban high-rise
environments, following the ITU-R P.1411-12 propagation model guidelines. To
address the qualitative nature of the environment-type descriptors found in the
Recommendation, a machine learning approach is employed to automate the
classification process. Extensive experimentation optimized classification
accuracy, resulting in a Canada-wide morphology map that ensures more accurate
path loss estimations for outdoor short-range propagation at frequencies
ranging from 300 MHz to 100 GHz.

</details>


### [5] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本研究提出了一种评估文本到图像模型遵循提示的能力的框架和数据集，发现即使是先进的模型也很难生成符合简单提示（如形状和位置）的图像，并且在遵循数据分布方面也存在问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和多模态模型（如文本到图像模型）的快速发展，对其可靠性和有效性的评估变得至关重要。然而，与文本模型相比，多模态模型的研究相对较少，特别是评估其性能和鲁棒性的方法。本研究旨在解决这一差距，通过建立一个全面的评估框架来评估文本到图像模型的性能，重点关注它们对文本提示的遵循程度。

Method: 本研究提出了一种评估文本到图像模型（特别是其对提示的遵循程度）的框架。该框架使用gpt-4o模型生成文本描述以创建地面真实图像，然后将这些描述输入到文本到图像模型以生成人工图像。接着，再次使用gpt-4o模型处理生成的人工图像，并与原始文本描述进行比较，以评估模型在遵循提示方面的鲁棒性。研究评估了Stable Diffusion和Janus模型的几个变体，并使用预训练的VAEs在创建的数据集上进行了训练，以分析模型对输入数据集分布的遵循情况。

Result: 研究结果表明，文本到图像模型在根据包含简单二元因素（如形状和位置）的文本提示生成图像方面存在困难。此外，研究还发现这些模型在生成符合特定数据集分布的图像方面也存在不足，这在利用预训练的VAEs在研究创建的数据集上进行训练时得到了证实。

Conclusion: 该研究提出的评估框架和数据集为评估文本到图像模型（特别是其对提示的遵循程度）提供了一个标准化的方法。研究结果表明，即使是像Stable Diffusion和Janus这样先进的模型，在根据简单的文本描述生成具有特定属性（如形状和位置）的图像方面也存在挑战，并且在遵循特定数据集分布方面也存在不足。

Abstract: The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [6] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

TL;DR: CNTLoRA improves LoRA fine-tuning by initializing weights using a data-driven method based on domain shift and activation constraints, outperforming existing methods in various vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA weight matrices are typically randomly initialized with a fixed rank across all attachment points, which can be improved for convergence and final performance during fine-tuning.

Method: CNTLoRA uses a data-driven weight initialization method by expressing LoRA initialization as a domain shift problem, utilizing multiple constraints relating pre-training and fine-tuning activations. This allows for a closed-form estimate of LoRA weights, decomposed into up and down matrices with variable ranks, requiring no training during initialization.

Result: CNTLoRA outperforms standard and data-driven weight initialization methods in image generation, image classification, and image understanding tasks, as demonstrated by both quantitative and qualitative results.

Conclusion: CNTLoRA

Abstract: Foundation models are pre-trained on large-scale datasets and subsequently
fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)
techniques like low-rank adapters (LoRA). In most previous works, LoRA weight
matrices are randomly initialized with a fixed rank across all attachment
points. In this paper, we improve convergence and final performance of LoRA
fine-tuning, using our proposed data-driven weight initialization method,
ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift
problem where we use multiple constraints relating the pre-training and
fine-tuning activations. By reformulating these constraints, we obtain a
closed-form estimate of LoRA weights that depends on pre-training weights and
fine-tuning activation vectors and hence requires no training during
initialization. This weight estimate is decomposed to initialize the up and
down matrices with proposed flexibility of variable ranks. With the proposed
initialization method, we fine-tune on downstream tasks such as image
generation, image classification and image understanding. Both quantitative and
qualitative results demonstrate that CNTLoRA outperforms standard and
data-driven weight initialization methods. Extensive analyses and ablations
further elucidate the design choices of our framework, providing an optimal
recipe for faster convergence and enhanced performance.

</details>


### [7] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

TL;DR: 提出了一种结合 ELM-AE 和区间II型模糊逻辑的混合多层极限学习机 (HML-ELM)，用于无人机图像分类，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了提高自然信号分类的有效性，特别是在图像分类领域，并将其应用于无人机（UAV）的主动图像分类任务。

Method: 提出了一种混合多层极限学习机 (HML-ELM)，该方法结合了基于 ELM 的自动编码器 (ELM-AE) 和区间II型模糊逻辑理论。该方法采用分层 ELM 学习框架，包括无监督的特征提取（通过堆叠 ELM-AE 实现）和有监督的特征分类（使用简化的区间II型模糊 ELM (SIT2-FELM) 和基于 SC 算法的输出约简层）。

Result: HML-ELM 在基准图像分类问题和无人机主动图像分类的实际实验中均表现出优越的效率，能够成功进行图像分类和物体传输任务。

Conclusion: HML-ELM 在图像分类任务上展现出优越的性能，优于 ML-ELM、ML-FELM 和 ELM 等其他方法。

Abstract: Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [8] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: CNN模型在云和云阴影掩蔽方面表现出色，特别是轻量级CNN模型在精度、存储和速度方面取得了良好平衡，适合卫星部署。


<details>
  <summary>Details</summary>
Motivation: 云和云阴影掩蔽是高光谱卫星成像中的关键预处理步骤，可实现高质量、可分析数据的提取。

Method: 评估了包括XGBoost、LightGBM等梯度提升方法以及卷积神经网络（CNN）在内的多种机器学习方法。

Result: 所有提升和CNN模型均达到93%以上的准确率。其中，经过特征约减的CNN模型最为高效，在CPU和GPU上都表现出高准确率、低存储需求和快速推理时间的良好平衡。该模型的变体（仅包含多达597个可训练参数）在可部署性、准确性和计算效率方面取得了最佳的权衡。

Conclusion: 轻量级人工智能（AI）模型在实时高光谱图像处理方面具有巨大潜力，有望支持卫星AI系统在太空应用中的发展。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [9] [The relative importance of being Gaussian](https://arxiv.org/abs/2507.08059)
*F. Alberto Grünbaum,Tondgi Xu*

Main category: cs.CV

TL;DR: 即使扩散模型去噪算法在高斯噪声下表现优异，但将其应用于非高斯噪声（如均匀分布、Beta 分布或混合高斯噪声）时，其性能可能会受到影响。本研究在小型计算机和最小图像尺寸下进行了实验，以评估其在不同噪声下的鲁棒性，并指出在不同实验设置下验证这些观察结果是一个有趣的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型去噪算法的理论基础是高斯噪声的性质，本研究旨在探索当噪声分布偏离高斯分布时，这些算法的鲁棒性和性能。

Method: 通过在小型笔记本电脑上使用最小图像尺寸进行实验，来评估算法在非高斯噪声下的表现，并与原始算法在高斯噪声下的表现进行对比。

Result: 文章指出现有算法是基于高斯噪声的性质（如均值、方差决定分布，高斯分布之和仍为高斯分布）推导出来的，并提出疑问：如果将噪声替换为均匀分布噪声、Beta 分布噪声或两个方差差异很大的高斯噪声的随机叠加，算法性能会如何？研究者选择不修改算法以适应新的噪声分布，而是直接测试算法在这些非高斯噪声下的表现。

Conclusion: 该笔记探讨了在不改变现有算法的情况下，使用非高斯噪声（例如均匀分布噪声、Beta 分布噪声或不同方差的高斯噪声叠加）代替高斯噪声对扩散模型在计算机视觉去噪任务中性能的影响。

Abstract: The remarkable results for denoising in computer vision using diffusion
models given in \cite{SDWMG,HJA,HHG} yield a robust mathematical justification
for algorithms based on crucial properties of a sequence of Gaussian
independent $N(0,1)$ random variables. In particular the derivations use the
fact that a Gaussian distribution is determined by its mean and variance and
that the sum of two Gaussians is another Gaussian.
  \bigskip
  The issue raised in this short note is the following: suppose we use the
algorithm without any changes but replace the nature of the noise and use, for
instance, uniformly distributed noise or noise with a Beta distribution, or
noise which is a random superposition of two Gaussians with very different
variances. One could, of course, try to modify the algorithm keeping in mind
the nature of the noise, but this is not what we do. Instead we study the
performance of the algorithm when used with noise that is very far in nature
from the Gaussian case, where it is designed to work well.
  Usually these algorithms are implemented on very powerful computers. Our
experiments are all carried out on a small laptop and for the smallest possible
image size. Exploring how our observations are confirmed or changed when
dealing in different situations remains an interesting challenge.

</details>


### [10] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的方法，利用SAR图像从单景数据中自动估计建筑物高度，并在跨城市和跨大陆的数据集上取得了有希望的结果，尤其是在欧洲城市表现突出。


<details>
  <summary>Details</summary>
Motivation: 为了提高在各种城市应用中使用甚高分辨率（VHR）合成孔径雷达（SAR）图像准确估计建筑物高度的能力。

Method: 提出了一种基于深度学习（DL）的目标检测（边界框检测）和回归方法，用于从单景甚高分辨率（VHR）COSMO-SkyMed图像自动估计建筑物高度。

Result: 在跨大陆数据集上，特别是在欧洲城市，该模型表现出非常有希望的性能，平均绝对误差（MAE）约为一个建筑楼层（慕尼黑为2.20米），在类似的分布外（OOD）场景中显著优于现有的最先进方法。

Conclusion: 该研究强调了深度学习在单景甚高分辨率合成孔径雷达（SAR）数据中进行建筑物高度估计的巨大潜力，实现了稳健的跨城市和跨大陆迁移学习。

Abstract: Accurate estimation of building heights using very high resolution (VHR)
synthetic aperture radar (SAR) imagery is crucial for various urban
applications. This paper introduces a Deep Learning (DL)-based methodology for
automated building height estimation from single VHR COSMO-SkyMed images: an
object-based regression approach based on bounding box detection followed by
height estimation. This model was trained and evaluated on a unique
multi-continental dataset comprising eight geographically diverse cities across
Europe, North and South America, and Asia, employing a cross-validation
strategy to explicitly assess out-of-distribution (OOD) generalization. The
results demonstrate highly promising performance, particularly on European
cities where the model achieves a Mean Absolute Error (MAE) of approximately
one building story (2.20 m in Munich), significantly outperforming recent
state-of-the-art methods in similar OOD scenarios. Despite the increased
variability observed when generalizing to cities in other continents,
particularly in Asia with its distinct urban typologies and prevalence of
high-rise structures, this study underscores the significant potential of DL
for robust cross-city and cross-continental transfer learning in building
height estimation from single VHR SAR data.

</details>


### [11] [RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration](https://arxiv.org/abs/2507.08136)
*Chong Cheng,Yu Hu,Sicheng Yu,Beizhen Zhao,Zijian Wang,Hao Wang*

Main category: cs.CV

TL;DR: RegGS is a framework for reconstructing unposed sparse views using 3D Gaussian Splatting by registering local 3D Gaussians into a globally consistent representation. It uses an entropy-regularized Sinkhorn algorithm for optimal transport alignment and a joint registration module for accurate pose and scene alignment, outperforming existing methods in experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by optimization-based 3DGS methods with sparse views due to limited prior knowledge, and the constraints of feed-forward Gaussian approaches that struggle to incorporate more input views.

Method: RegGS utilizes an entropy-regularized Sinkhorn algorithm to compute the optimal transport Mixture 2-Wasserstein (MW2) distance for aligning Gaussian mixture models (GMMs) in Sim(3) space. It incorporates a joint 3DGS registration module that integrates MW2 distance, photometric consistency, and depth geometry for a coarse-to-fine registration process, enabling accurate camera pose estimation and scene alignment.

Result: RegGS effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis.

Conclusion: RegGS in experiments effectively registers local Gaussians with high fidelity, achieving precise pose estimation and high-quality novel-view synthesis on the RE10K and ACID datasets.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing
scenes from unposed images. However, optimization-based 3DGS methods struggle
with sparse views due to limited prior knowledge. Meanwhile, feed-forward
Gaussian approaches are constrained by input formats, making it challenging to
incorporate more input views. To address these challenges, we propose RegGS, a
3D Gaussian registration-based framework for reconstructing unposed sparse
views. RegGS aligns local 3D Gaussians generated by a feed-forward network into
a globally consistent 3D Gaussian representation. Technically, we implement an
entropy-regularized Sinkhorn algorithm to efficiently solve the optimal
transport Mixture 2-Wasserstein $(\text{MW}_2)$ distance, which serves as an
alignment metric for Gaussian mixture models (GMMs) in $\mathrm{Sim}(3)$ space.
Furthermore, we design a joint 3DGS registration module that integrates the
$\text{MW}_2$ distance, photometric consistency, and depth geometry. This
enables a coarse-to-fine registration process while accurately estimating
camera poses and aligning the scene. Experiments on the RE10K and ACID datasets
demonstrate that RegGS effectively registers local Gaussians with high
fidelity, achieving precise pose estimation and high-quality novel-view
synthesis. Project page: https://3dagentworld.github.io/reggs/.

</details>


### [12] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 一种用于从单眼视频重建动态人与物交互的新型框架，通过非模态完成和时间上下文来解决遮挡和时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服从单眼视频重建动态人与物交互时出现的遮挡和时间不一致性问题。

Method: 该框架利用非模态完成来推断部分遮挡区域的完整结构，并整合时间上下文以跨视频序列强制执行连贯性，从而逐步优化和稳定重建。该方法是无模板的，可以适应不同的条件，而无需预定义的模型。

Result: 在具有挑战性的单眼视频上使用 3D 高斯泼溅进行验证，证明了其在处理遮挡和保持时间稳定性方面优于现有技术。

Conclusion: 该框架在处理遮挡和保持时间稳定性方面优于现有技术。

Abstract: We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [13] [Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment](https://arxiv.org/abs/2507.08367)
*Yuki Yoshihara,Linjing Jiang,Nihan Karatas,Hitoshi Kanamori,Asuka Harada,Takahiro Tanaka*

Main category: cs.CV

TL;DR: ChatGPT-4o在理解交通场景方面展现出潜力，尤其在结合人类标注和优化提示词策略后，在评估交通密度、交叉口可见性和停车标志识别方面表现出提升，尤其在需要情境推理的任务上，但召回率和对模糊场景的理解仍有待提高。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索多模态大型语言模型（LLM），特别是ChatGPT-4o，在理解交通场景方面的能力，以评估老年驾驶员的驾驶行为，这些任务需要情境推理而非简单的对象检测。

Method: 本研究采用ChatGPT-4o模型，利用零样本、少样本和多样本提示策略，对静态行车记录仪图像进行交通场景解读，并评估模型在交通密度评估、交叉口可见性评估和停车标志识别任务上的表现，以人类标注作为参照标准。

Result: 结果表明，提示词设计显著影响模型表现，交叉口可见性的召回率从21.7%提升至57.0%，交通密度的准确率从53.5%提升至67.6%。停车标志识别方面，模型精度最高可达86.3%，召回率约为76.7%，显示出保守的识别倾向。模型解释性文本与其预测结果一致，且在结构模糊场景下，模型与人类均面临理解困难。

Conclusion: 通过精心设计的提示词，大型语言模型有潜力作为支持性工具，用于驾驶风险评估，尤其是针对老年驾驶员的场景理解能力。

Abstract: This study investigates the potential of a multimodal large language model
(LLM), specifically ChatGPT-4o, to perform human-like interpretations of
traffic scenes using static dashcam images. Herein, we focus on three judgment
tasks relevant to elderly driver assessments: evaluating traffic density,
assessing intersection visibility, and recognizing stop signs recognition.
These tasks require contextual reasoning rather than simple object detection.
Using zero-shot, few-shot, and multi-shot prompting strategies, we evaluated
the performance of the model with human annotations serving as the reference
standard. Evaluation metrics included precision, recall, and F1-score. Results
indicate that prompt design considerably affects performance, with recall for
intersection visibility increasing from 21.7% (zero-shot) to 57.0%
(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In
stop-sign detection, the model demonstrated high precision (up to 86.3%) but a
lower recall (approximately 76.7%), indicating a conservative response
tendency. Output stability analysis revealed that humans and the model faced
difficulties interpreting structurally ambiguous scenes. However, the model's
explanatory texts corresponded with its predictions, enhancing
interpretability. These findings suggest that, with well-designed prompts, LLMs
hold promise as supportive tools for scene-level driving risk assessments.
Future studies should explore scalability using larger datasets, diverse
annotators, and next-generation model architectures for elderly driver
assessments.

</details>


### [14] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: 一种新的自适应扩散去噪平滑方法，通过将扩散模型视为差分隐私机制，提高了视觉模型在对抗性样本下的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在对抗性样本的攻击下，验证视觉模型的预测能力，并使其能够适应输入。

Method: 该方法将引导式去噪扩散模型重新解释为一系列自适应高斯差分隐私（GDP）机制，将纯噪声样本提炼成图像。通过GDP隐私过滤器组合这些自适应机制，可以分析引导式去噪过程的端到端鲁棒性，从而提供可证明的认证，并扩展了自适应随机平滑分析。

Result: 在特定引导策略下，该方法在ImageNet上针对l2威胁模型，在认证准确率和标准准确率方面均有所提升。

Conclusion: 提出了一种名为自适应扩散去噪平滑的新方法，用于验证视觉模型在对抗性样本下的预测能力，并能适应输入。

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [15] [An Embedded Real-time Object Alert System for Visually Impaired: A Monocular Depth Estimation based Approach through Computer Vision](https://arxiv.org/abs/2507.08165)
*Jareen Anjom,Rashik Iram Chowdhury,Tarbia Hasan,Md. Ishan Arefin Hossain*

Main category: cs.CV

TL;DR: A new system uses AI to detect objects and estimate distance, making it safer for visually impaired people to walk in busy cities. It's lightweight and works in real-time.


<details>
  <summary>Details</summary>
Motivation: To address the significant challenges faced by visually impaired people in urban commutes due to numerous obstructions and daily road accidents, a system is needed to alert them to nearby objects.

Method: The research proposes a novel alert system utilizing transfer learning for depth estimation and object detection, optimized with quantization techniques for deployment on embedded systems.

Result: The developed system achieved a lightweight real-time depth estimation and object detection model with an mAP50 of 0.801.

Conclusion: The proposed system offers a lightweight, real-time solution for depth estimation and object detection, achieving an mAP50 of 0.801, to assist visually impaired individuals in urban environments.

Abstract: Visually impaired people face significant challenges in their day-to-day
commutes in the urban cities of Bangladesh due to the vast number of
obstructions on every path. With many injuries taking place through road
accidents on a daily basis, it is paramount for a system to be developed that
can alert the visually impaired of objects at close distance beforehand. To
overcome this issue, a novel alert system is proposed in this research to
assist the visually impaired in commuting through these busy streets without
colliding with any objects. The proposed system can alert the individual to
objects that are present at a close distance. It utilizes transfer learning to
train models for depth estimation and object detection, and combines both
models to introduce a novel system. The models are optimized through the
utilization of quantization techniques to make them lightweight and efficient,
allowing them to be easily deployed on embedded systems. The proposed solution
achieved a lightweight real-time depth estimation and object detection model
with an mAP50 of 0.801.

</details>


### [16] [HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation](https://arxiv.org/abs/2507.08205)
*Ken C. L. Wong,Hongzhi Wang,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: HNOSeg-XS 是一種創新的醫學影像分割架構，透過在頻域中使用哈特利轉換和偏微分方程，克服了現有模型的限制，實現了優異的解析度魯棒性、速度、記憶體效率和參數效率。


<details>
  <summary>Details</summary>
Motivation: 為了解決現有醫學影像分割模型（CNN 和 Transformer）在處理長距離空間關聯時面臨的計算成本高、記憶體佔用大以及解析度敏感性等問題，提出了一種新的架構。

Method: HNOSeg-XS 架構透過可學習的偏微分方程和傅立葉神經算子來進行影像分割，並利用哈特利轉換取代傅立葉轉換，在頻域中重新構建問題，使其具有零樣本超解析度特性，從而實現解析度魯棒性、速度、記憶體效率和參數效率。

Result: HNOSeg-XS 架構透過利用哈特利轉換和頻域處理，成功實現了醫學影像分割的解析度魯棒性、速度、記憶體效率和參數效率，並在多個基準測試中取得了優異的效能。

Conclusion: HNOSeg-XS 在 BraTS'23、KiTS'23 和 MVSeg'23 資料集上展現了優越的解析度魯棒性、更少的模型參數（少於 34.7k）、最快的推理時間（< 0.24 秒）和記憶體效率（< 1.8 GiB），優於經過測試的 CNN 和 Transformer 模型。

Abstract: In medical image segmentation, convolutional neural networks (CNNs) and
transformers are dominant. For CNNs, given the local receptive fields of
convolutional layers, long-range spatial correlations are captured through
consecutive convolutions and pooling. However, as the computational cost and
memory footprint can be prohibitively large, 3D models can only afford fewer
layers than 2D models with reduced receptive fields and abstract levels. For
transformers, although long-range correlations can be captured by multi-head
attention, its quadratic complexity with respect to input size is
computationally demanding. Therefore, either model may require input size
reduction to allow more filters and layers for better segmentation.
Nevertheless, given their discrete nature, models trained with patch-wise
training or image downsampling may produce suboptimal results when applied on
higher resolutions. To address this issue, here we propose the
resolution-robust HNOSeg-XS architecture. We model image segmentation by
learnable partial differential equations through the Fourier neural operator
which has the zero-shot super-resolution property. By replacing the Fourier
transform by the Hartley transform and reformulating the problem in the
frequency domain, we created the HNOSeg-XS model, which is resolution robust,
fast, memory efficient, and extremely parameter efficient. When tested on the
BraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS
showed its superior resolution robustness with fewer than 34.7k model
parameters. It also achieved the overall best inference time (< 0.24 s) and
memory efficiency (< 1.8 GiB) compared to the tested CNN and transformer
models.

</details>


### [17] [SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches](https://arxiv.org/abs/2507.08223)
*Jackson Borchardt,Saul Kato*

Main category: cs.CV

TL;DR: SurfDist is a new 3D instance segmentation model that uses smooth surface patches, outperforming StarDist-3D by avoiding voxelization artifacts and offering more compact representations for blob-shaped objects.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of StarDist-3D, specifically its coupling of instance parameterization dimension and instance voxel resolution, and to enable higher-resolution predictions without artifacts for blob-shaped instances common in biomedical imaging.

Method: SurfDist, a convolutional neural network architecture for three-dimensional volumetric instance segmentation, which enables prediction of instances represented as closed surfaces composed of smooth parametric surface patches (bicubic B-spline triangles). It modifies StarDist-3D by decoupling instance parameterization dimension and instance voxel resolution, allowing predictions to be upsampled to arbitrarily high resolutions without voxelization artifacts.

Result: SurfDist outperforms StarDist-3D on both a synthetic and a real-world dataset, particularly for blob-shaped instances, by providing more compact parameterizations and avoiding voxelization artifacts upon upsampling.

Conclusion: SurfDist's interpretable instance surface model can be learned effectively alongside instance membership, outperforming StarDist-3D on datasets with blob-shaped instances.

Abstract: We present SurfDist, a convolutional neural network architecture for
three-dimensional volumetric instance segmentation. SurfDist enables prediction
of instances represented as closed surfaces composed of smooth parametric
surface patches, specifically bicubic B\'ezier triangles. SurfDist is a
modification of the popular model architecture StarDist-3D which breaks
StarDist-3D's coupling of instance parameterization dimension and instance
voxel resolution, and it produces predictions which may be upsampled to
arbitrarily high resolutions without introduction of voxelization artifacts.
  For datasets with blob-shaped instances, common in biomedical imaging,
SurfDist can outperform StarDist-3D with more compact instance
parameterizations. We detail SurfDist's technical implementation and show one
synthetic and one real-world dataset for which it outperforms StarDist-3D.
These results demonstrate that interpretable instance surface models can be
learned effectively alongside instance membership.

</details>


### [18] [Car Object Counting and Position Estimation via Extension of the CLIP-EBC Framework](https://arxiv.org/abs/2507.08240)
*Seoik Jung,Taekyung Song*

Main category: cs.CV

TL;DR: CLIP-EBC框架在车辆计数任务中表现良好，并有潜力扩展到定位任务。


<details>
  <summary>Details</summary>
Motivation: 为了探究CLIP-EBC框架在车辆计数任务上的适用性。

Method: 应用CLIP-EBC框架于CARPK数据集进行车辆计数，并提出K-means加权聚类方法估计物体位置。

Result: 模型在CARPK数据集上达到了第二名性能，并验证了结合K-means加权聚类方法在定位任务上的扩展潜力。

Conclusion: CLIP-EBC框架在CARPK数据集上应用于车辆计数任务，取得了除最佳方法外的第二名成绩，显示了其在该领域的潜力。

Abstract: In this paper, we investigate the applicability of the CLIP-EBC framework,
originally designed for crowd counting, to car object counting using the CARPK
dataset. Experimental results show that our model achieves second-best
performance compared to existing methods. In addition, we propose a K-means
weighted clustering method to estimate object positions based on predicted
density maps, indicating the framework's potential extension to localization
tasks.

</details>


### [19] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于Vision Transformer和数据增强的真菌物种识别方法，并在FungiCLEF 2025竞赛中取得了良好成绩，但仍有在元数据选择和多模态学习方面进一步改进的空间。


<details>
  <summary>Details</summary>
Motivation: 准确识别真菌物种在计算机视觉领域面临独特挑战，主要是由于物种间的细微差别和物种内部的显著变异性。

Method: 该研究采用了包括Vision Transformer模型、数据增强、加权采样以及整合文本信息在内的多种方法，并探索了使用生成式AI模型进行零样本分类。

Result: 最终模型在FungiCLEF 2025竞赛中表现优于基线模型，证明了领域特定预训练和平衡采样策略的有效性。

Conclusion: 该方法在FungiCLEF 2025竞赛中取得了第35/74名的成绩，表明在元数据选择和域适应多模态学习方面仍有改进空间。

Abstract: Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [20] [Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone](https://arxiv.org/abs/2507.08268)
*J. D. Peiffer,Kunal Shah,Irina Djuraskovic,Shawana Anarwala,Kayan Abdou,Rujvee Patel,Prakash Jayabalan,Brenton Pennicooke,R. James Cotton*

Main category: cs.CV

TL;DR: 这项研究提出了一种名为PBL的便携式生物力学实验室，它使用智能手机应用程序和新颖的算法来客观测量运动。该系统已在临床环境中得到广泛验证，证明其准确、可靠且对临床差异敏感，有望改善运动障碍的监测。


<details>
  <summary>Details</summary>
Motivation: 为了解决临床实践中缺乏可及且经过验证的客观测量运动的方法，以及生物力学测量无法广泛用于实践的差距。

Method: 提出了一种便携式生物力学实验室（PBL），包括一个安全、支持云的智能手机应用程序进行数据收集和一个用于将生物力学模型拟合到该数据的新颖算法。

Result: PBL在神经外科和运动医学诊所进行了可用性和效用测试。在神经损伤、假肢使用者、儿科住院患者和对照组参与者中，关节角度误差在3度以内。PBL计算出的步态指标显示出高可靠性，并且能够区分临床差异。特别是，在接受颈椎脊髓病减压手术的个体中，PBL步态指标与mJOA评分相关，并显示出比患者报告结果更强的对手术干预的反应性。

Conclusion: 研究表明，使用手持智能手机视频作为一种可扩展、低负担的工具来捕获具有临床意义的生物力学数据，为可及地监测运动障碍提供了一条有前景的途径。

Abstract: The way a person moves is a direct reflection of their neurological and
musculoskeletal health, yet it remains one of the most underutilized vital
signs in clinical practice. Although clinicians visually observe movement
impairments, they lack accessible and validated methods to objectively measure
movement in routine care. This gap prevents wider use of biomechanical
measurements in practice, which could enable more sensitive outcome measures or
earlier identification of impairment. We present our Portable Biomechanics
Laboratory (PBL), which includes a secure, cloud-enabled smartphone app for
data collection and a novel algorithm for fitting biomechanical models to this
data. We extensively validated PBL's biomechanical measures using a large,
clinically representative dataset. Next, we tested the usability and utility of
our system in neurosurgery and sports medicine clinics. We found joint angle
errors within 3 degrees across participants with neurological injury,
lower-limb prosthesis users, pediatric inpatients, and controls. In addition to
being easy to use, gait metrics computed from the PBL showed high reliability
and were sensitive to clinical differences. For example, in individuals
undergoing decompression surgery for cervical myelopathy, the mJOA score is a
common patient-reported outcome measure; we found that PBL gait metrics
correlated with mJOA scores and demonstrated greater responsiveness to surgical
intervention than the patient-reported outcomes. These findings support the use
of handheld smartphone video as a scalable, low-burden tool for capturing
clinically meaningful biomechanical data, offering a promising path toward
accessible monitoring of mobility impairments. We release the first clinically
validated method for measuring whole-body kinematics from handheld smartphone
video at
https://intelligentsensingandrehabilitation.github.io/MonocularBiomechanics/ .

</details>


### [21] [Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment](https://arxiv.org/abs/2507.08290)
*Jiang Qin,Bin Zou,Haolin Li,Lamei Zhang*

Main category: cs.CV

TL;DR: CR-Net是一种新颖的SAR目标检测方法，通过结合结构先验和证据学习理论，解决了高分辨率SAR图像的跨分辨率域适应性问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率SAR图像在城市监测和目标检测等领域具有显著优势，但分辨率的提高也带来了散射特性的差异，对目标检测模型的泛化能力提出了挑战。现有的域适应技术在处理分辨率差异时存在盲目特征适应和不可靠语义传播的问题，导致性能下降。

Method: CR-Net模型整合了结构先验和证据学习理论，通过SHFA模块建立结构相关性以实现结构感知特征自适应，并通过RSAA模块利用安全邻接集增强可靠语义对齐，从而进行可靠的跨分辨率域自适应。

Result: 实验结果表明，CR-Net在不同分辨率的数据集上显著提高了跨分辨率自适应能力，同时保留了训练集中的结构并增强了可区分性。

Conclusion: CR-Net通过保留训练集中的结构来提高模型的可区分性，从而显著提升跨分辨率自适应能力，在跨分辨率SAR目标检测方面取得了最先进的性能。

Abstract: In recent years, continuous improvements in SAR resolution have significantly
benefited applications such as urban monitoring and target detection. However,
the improvement in resolution leads to increased discrepancies in scattering
characteristics, posing challenges to the generalization ability of target
detection models. While domain adaptation technology is a potential solution,
the inevitable discrepancies caused by resolution differences often lead to
blind feature adaptation and unreliable semantic propagation, ultimately
degrading the domain adaptation performance. To address these challenges, this
paper proposes a novel SAR target detection method (termed CR-Net), that
incorporates structure priors and evidential learning theory into the detection
model, enabling reliable domain adaptation for cross-resolution detection. To
be specific, CR-Net integrates Structure-induced Hierarchical Feature
Adaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA
module is introduced to establish structural correlations between targets and
achieve structure-aware feature adaptation, thereby enhancing the
interpretability of the feature adaptation process. Afterwards, the RSAA module
is proposed to enhance reliable semantic alignment, by leveraging the secure
adjacency set to transfer valuable discriminative knowledge from the source
domain to the target domain. This further improves the discriminability of the
detection model in the target domain. Based on experimental results from
different-resolution datasets,the proposed CR-Net significantly enhances
cross-resolution adaptation by preserving intra-domain structures and improving
discriminability. It achieves state-of-the-art (SOTA) performance in
cross-resolution SAR target detection.

</details>


### [22] [M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation](https://arxiv.org/abs/2507.08307)
*Kui Jiang,Shiyu Liu,Junjun Jiang,Xin Yang,Hongxun Yang,Xiaopeng Fan*

Main category: cs.CV

TL;DR: M2DAO-Talker是一种新的音频驱动说话头像生成方法，通过多粒度运动解耦和交替优化解决了现有3D方法的局限性，实现了更逼真的视频生成和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D方法在表示稳定的细粒度运动场方面存在局限性，导致渲染伪影，如运动模糊、时间抖动和局部穿透。因此，需要一种新的方法来改进音频驱动的说话头像生成。

Method: M2DAO-Talker通过多粒度运动解耦和交替优化来解决现有3D方法在表示稳定、细粒度运动场方面的局限性。具体来说，通过2D肖像预处理流水线提取逐帧变形控制条件（运动区域分割掩码和相机参数）来促进运动表示。通过多粒度运动解耦策略，独立地对非刚性（口腔和面部）和刚性（头部）运动进行建模，以提高重建精度。同时，开发了运动一致性约束来确保头身运动学一致性，从而减轻由运动混叠引起的穿透伪影。此外，设计了交替优化策略来迭代地优化面部和口腔运动参数，以实现更逼真的视频生成。

Result: M2DAO-Talker实现了最先进的性能，在生成质量方面提高了2.43 dB的PSNR，在用户评估的视频真实度方面比TalkingGaussian提高了0.64，同时推理速度为150 FPS。

Conclusion: M2DAO-Talker在多个数据集的实验表明，其在生成质量方面实现了最先进的性能，PSNR提高了2.43 dB，视频真实度评分比TalkingGaussian提高了0.64，同时推理速度达到150 FPS。

Abstract: Audio-driven talking head generation holds significant potential for film
production. While existing 3D methods have advanced motion modeling and content
synthesis, they often produce rendering artifacts, such as motion blur,
temporal jitter, and local penetration, due to limitations in representing
stable, fine-grained motion fields. Through systematic analysis, we reformulate
talking head generation into a unified framework comprising three steps: video
preprocessing, motion representation, and rendering reconstruction. This
framework underpins our proposed M2DAO-Talker, which addresses current
limitations via multi-granular motion decoupling and alternating
optimization.Specifically, we devise a novel 2D portrait preprocessing pipeline
to extract frame-wise deformation control conditions (motion region
segmentation masks, and camera parameters) to facilitate motion representation.
To ameliorate motion modeling, we elaborate a multi-granular motion decoupling
strategy, which independently models non-rigid (oral and facial) and rigid
(head) motions for improved reconstruction accuracy.Meanwhile, a motion
consistency constraint is developed to ensure head-torso kinematic consistency,
thereby mitigating penetration artifacts caused by motion aliasing. In
addition, an alternating optimization strategy is designed to iteratively
refine facial and oral motion parameters, enabling more realistic video
generation.Experiments across multiple datasets show that M2DAO-Talker achieves
state-of-the-art performance, with the 2.43 dB PSNR improvement in generation
quality and 0.64 gain in user-evaluated video realness versus TalkingGaussian
while with 150 FPS inference speed. Our project homepage is
https://m2dao-talker.github.io/M2DAO-Talk.github.io

</details>


### [23] [Cross-Domain Identity Representation for Skull to Face Matching with Benchmark DataSet](https://arxiv.org/abs/2507.08329)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出一种使用卷积孪生网络从颅骨X光片识别个人身份的方法，并构建了一个包含40名志愿者颅骨X光片和面部照片的数据集进行训练和测试，实验结果令人满意。


<details>
  <summary>Details</summary>
Motivation: 为了在法医科学中通过颅骨X射线图像识别犯罪和灾难受害者，利用计算机视觉和深度学习的最新进展，实现颅骨与已知身份的人脸图像之间的匹配。

Method: 提出了一种使用卷积孪生网络（Siamese networks）的框架，该网络通过共享相同的架构来学习特征空间，将相似的样本（颅骨和对应的面部图像）拉近，将不相似的样本推远。通过最小化相似样本对之间的欧氏距离并最大化不相似样本对之间的欧氏距离来进行训练。

Result: 实验结果表明，所提出的基于卷积孪生网络的框架在从颅骨X射线图像识别个人方面取得了令人满意的效果。

Conclusion: 该研究成功地提出了一种使用卷积孪生网络从颅骨X射线图像识别个人的框架，并在自建的跨域数据集上进行了实验验证，取得了令人满意的结果。

Abstract: Craniofacial reconstruction in forensic science is crucial for the
identification of the victims of crimes and disasters. The objective is to map
a given skull to its corresponding face in a corpus of faces with known
identities using recent advancements in computer vision, such as deep learning.
In this paper, we presented a framework for the identification of a person
given the X-ray image of a skull using convolutional Siamese networks for
cross-domain identity representation. Siamese networks are twin networks that
share the same architecture and can be trained to discover a feature space
where nearby observations that are similar are grouped and dissimilar
observations are moved apart. To do this, the network is exposed to two sets of
comparable and different data. The Euclidean distance is then minimized between
similar pairs and maximized between dissimilar ones. Since getting pairs of
skull and face images are difficult, we prepared our own dataset of 40
volunteers whose front and side skull X-ray images and optical face images were
collected. Experiments were conducted on the collected cross-domain dataset to
train and validate the Siamese networks. The experimental results provide
satisfactory results on the identification of a person from the given skull.

</details>


### [24] [CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models](https://arxiv.org/abs/2507.08334)
*Sangwon Kim,In-su Jang,Pyongkun Kim,Kwang-Ju Kim*

Main category: cs.CV

TL;DR: CoCo-Bot 是一种新的生成模型，通过显式概念进行可控生成，无需辅助视觉线索，并支持后验干预。


<details>
  <summary>Details</summary>
Motivation: 先前的生成模型依赖于瓶颈处的辅助视觉线索，这会损害可解释性和组合性。本研究旨在解决这一问题。

Method: CoCo-Bot 通过利用基于扩散的能量函数，支持跨任意概念的鲁棒的后验干预，例如概念组合和否定。

Result: 实验表明，CoCo-Bot 提高了概念层面的可控性和可解释性，同时保持了具有竞争力的视觉质量。

Conclusion: CoCo-Bot 作为一个后验的、可组合的概念瓶颈生成模型，通过仅通过显式概念传递所有信息，消除了对辅助视觉线索的依赖，从而提高了概念层面的可控性和可解释性，同时保持了具有竞争力的视觉质量。

Abstract: Concept Bottleneck Models (CBMs) provide interpretable and controllable
generative modeling by routing generation through explicit,
human-understandable concepts. However, previous generative CBMs often rely on
auxiliary visual cues at the bottleneck to compensate for information not
captured by the concepts, which undermines interpretability and
compositionality. We propose CoCo-Bot, a post-hoc, composable concept
bottleneck generative model that eliminates the need for auxiliary cues by
transmitting all information solely through explicit concepts. Guided by
diffusion-based energy functions, CoCo-Bot supports robust post-hoc
interventions-such as concept composition and negation-across arbitrary
concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that
CoCo-Bot improves concept-level controllability and interpretability, while
maintaining competitive visual quality.

</details>


### [25] [Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement](https://arxiv.org/abs/2507.08340)
*Jia-Xuan Jiang,Jiashuai Liu,Hongtao Wu,Yifeng Wu,Zhong Wang,Qi Bi,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本研究首次发现多模态预后模型在跨癌症场景下泛化能力不如单模态模型，并提出“跨癌症单域多模态预后泛化”任务。为解决此问题，引入了SDIR和CADE两个模块，分别处理特征退化和多模态整合问题。实验结果表明该方法具有优越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在整合多模态数据进行生存预测方面取得了显著的性能。然而，现有的多模态方法主要集中在单一癌症类型上，忽视了跨癌症泛化的挑战。本研究旨在解决多模态预后模型在跨癌症场景下的泛化能力不如单模态模型的问题，并提出相应的解决方案以提高模型的鲁棒性，满足临床实践的需求。

Method: 为了解决跨癌症单域多模态预后泛化任务中的特征退化和多模态整合无效问题，我们提出了稀疏狄拉克信息再平衡器（SDIR）和癌症感知分布纠缠（CADE）两个模块。SDIR通过应用基于伯努利的稀疏化和狄拉克启发的稳定性来增强较弱模态信号，以减弱强特征的主导地位。CADE旨在合成目标域分布，通过在潜在空间中融合局部形态线索和全局基因表达来实现。

Result: 在四种癌症类型的基准测试上的实验证明了本研究提出的方法具有优越的泛化能力，为实现实际、鲁棒的跨癌症多模态预后奠定了基础。

Conclusion: 本研究首次揭示了多模态预后模型在跨癌症场景下的泛化能力通常弱于单模态模型，尽管这种鲁棒性在临床实践中至关重要。为了解决这个问题，我们提出了一个新的任务：跨癌症单域多模态预后泛化，用于评估在单一癌症类型上训练的模型能否泛化到未知的癌症。我们确定了两个关键挑战：来自较弱模态的特征退化和无效的多模态整合。为了应对这些挑战，我们引入了两个即插即用模块：稀疏狄拉克信息再平衡器（SDIR）和癌症感知分布纠缠（CADE）。SDIR通过应用基于伯努利的稀疏化和受狄拉克启发的稳定性来增强较弱模态信号，从而减弱强特征的主导地位。CADE旨在合成目标域分布，在潜在空间中融合局部形态线索和全局基因表达。在四种癌症类型的基准测试上的实验证明了其优越的泛化能力，为实际、鲁棒的跨癌症多模态预后奠定了基础。

Abstract: Deep learning has shown remarkable performance in integrating multimodal data
for survival prediction. However, existing multimodal methods mainly focus on
single cancer types and overlook the challenge of generalization across
cancers. In this work, we are the first to reveal that multimodal prognosis
models often generalize worse than unimodal ones in cross-cancer scenarios,
despite the critical need for such robustness in clinical practice. To address
this, we propose a new task: Cross-Cancer Single Domain Generalization for
Multimodal Prognosis, which evaluates whether models trained on a single cancer
type can generalize to unseen cancers. We identify two key challenges: degraded
features from weaker modalities and ineffective multimodal integration. To
tackle these, we introduce two plug-and-play modules: Sparse Dirac Information
Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR
mitigates the dominance of strong features by applying Bernoulli-based
sparsification and Dirac-inspired stabilization to enhance weaker modality
signals. CADE, designed to synthesize the target domain distribution, fuses
local morphological cues and global gene expression in latent space.
Experiments on a four-cancer-type benchmark demonstrate superior
generalization, laying the foundation for practical, robust cross-cancer
multimodal prognosis. Code is available at
https://github.com/HopkinsKwong/MCCSDG

</details>


### [26] [Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation](https://arxiv.org/abs/2507.08343)
*Junxue Yang,Xin Liao,Weixuan Tang,Jianhua Yang,Zheng Qin*

Main category: cs.CV

TL;DR: MRAG通过结合卷积和Transformer，并利用多粒度频率信息和特征角度-范数解耦损失，解决了现有深度隐藏方案易被检测的问题，并在颜色JPEG图像深度隐藏方面取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 解决了现有深度隐藏方案因载荷大、仅限于单一范围的纯卷积或纯Transformer算子以及像素级损失约束而易被检测的问题。

Method: 提出了一种名为MRAG的多范围表示驱动的隐写术生成框架，该框架整合了卷积的局部邻域接收特性和Transformer的全局依赖建模，并通过粗粒度和细粒度频率分解引入多粒度信息。此外，还设计了一种特征角度-范数解耦损失来约束生成的隐写图像在分析器的特征角度和范数空间上更接近封面图像。

Result: MRAG能够注入微小但有效的对抗性扰动，以生成既能保持良好的秘密可恢复性，又具有不可感知性的隐写图像。

Conclusion: MRAG在颜色JPEG图像深度隐藏方面实现了最先进的性能。

Abstract: Deep hiding has been exploring the hiding capability of deep learning-based
models, aiming to conceal image-level messages into cover images and reveal
them from generated stego images. Existing schemes are easily detected by
steganalyzers due to their large payloads and their limitation to feature
extraction based solely on either pure convolution or pure transformer
operators within a single range, as well as pixel-level loss constraints. To
address the issue, in this paper, we introduce generation-based adversarial
attacks into color JPEG image deep hiding and propose a multi-range
representations-driven adversarial stego generation framework called MRAG from
a steganalysis perspective. Specifically, we integrate the local-range neighbor
reception characteristic of the convolution and the global-range dependency
modeling of the transformer to construct MRAG. Meanwhile, we use the
transformed images obtained through coarse-grained and fine-grained frequency
decomposition as inputs, introducing multi-grained information. Furthermore, a
features angle-norm disentanglement loss is designed to constrain the generated
stegos closer to covers in the angle and norm space of the steganalyzer's
classified features. Consequently, small yet effective adversarial
perturbations can be injected into the process of generating stegos, ensuring
that stegos maintain favorable secret restorability and imperceptibility.
Extensive experiments demonstrate that MRAG can achieve state-of-the-art
performance.

</details>


### [27] [MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion](https://arxiv.org/abs/2507.08344)
*Jihao Gu,Fei Wang,Kun Li,Yanyan Wei,Zhiliang Wu,Dan Guo*

Main category: cs.CV

TL;DR: MM-Gesture是一个多模态融合框架，在第三届MiGA挑战赛中取得第一名，并在iMiGUE基准测试中达到73.213%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在第三届MiGA挑战赛的微手势分类赛道中取得优异成绩，我们开发了MM-Gesture框架来识别细微且持续时间短的微手势（MG）。

Method: MM-Gesture是一个多模态融合框架，结合了来自关节、肢体、RGB视频、泰勒级数视频、光流视频和深度视频模态的互补线索。它采用了PoseConv3D和视频Swin Transformer架构，并使用了一种新颖的模态加权集成策略。此外，通过在MA-52数据集上进行预训练的迁移学习，进一步提升了RGB模态的性能。

Result: MM-Gesture框架在iMiGUE基准测试中实现了73.213%的top-1准确率，优于先前最先进的方法。

Conclusion: MM-Gesture在iMiGUE基准测试中取得了73.213%的top-1准确率，验证了该方法的有效性。

Abstract: In this paper, we present MM-Gesture, the solution developed by our team
HFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd
MiGA Challenge at IJCAI 2025, achieving superior performance compared to
previous state-of-the-art methods. MM-Gesture is a multimodal fusion framework
designed specifically for recognizing subtle and short-duration micro-gestures
(MGs), integrating complementary cues from joint, limb, RGB video,
Taylor-series video, optical-flow video, and depth video modalities. Utilizing
PoseConv3D and Video Swin Transformer architectures with a novel
modality-weighted ensemble strategy, our method further enhances RGB modality
performance through transfer learning pre-trained on the larger MA-52 dataset.
Extensive experiments on the iMiGUE benchmark, including ablation studies
across different modalities, validate the effectiveness of our proposed
approach, achieving a top-1 accuracy of 73.213%.

</details>


### [28] [Cycle Context Verification for In-Context Medical Image Segmentation](https://arxiv.org/abs/2507.08357)
*Shishuai Hu,Zehui Liao,Liangli Zhen,Huazhu Fu,Yong Xia*

Main category: cs.CV

TL;DR: CCV通过自验证和增强上下文对齐来改进ICL医学图像分割，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在临床场景中，带注释的医学图像稀少，这使得选择最佳的上下文对具有挑战性，并且由于计算成本和灾难性遗忘的风险，在上下文数据上对基础ICL模型进行微调是不可行的。CCV旨在解决ICL在医学图像分割中对查询图像和上下文图像掩码对的对齐敏感性问题。

Method: 提出了一种名为Cycle Context Verification (CCV)的新颖框架，通过实现预测的自验证和相应的增强上下文对齐来增强基于ICL的医学图像分割。CCV采用循环流程：模型首先为查询图像生成分割掩码；然后，交换查询和上下文对的角色，使模型能够通过预测原始上下文图像的掩码来验证其预测；最后，通过改变查询图像并更新查询特定提示来改进度量，从而增强查询和上下文对之间的对齐。

Result: 在七个医学图像分割数据集上使用两个ICL基础模型对CCV进行了评估，证明了其优于现有方法。

Conclusion: CCV能够增强基于ICL的分割能力，使其成为通用医学图像分割的稳健解决方案。

Abstract: In-context learning (ICL) is emerging as a promising technique for achieving
universal medical image segmentation, where a variety of objects of interest
across imaging modalities can be segmented using a single model. Nevertheless,
its performance is highly sensitive to the alignment between the query image
and in-context image-mask pairs. In a clinical scenario, the scarcity of
annotated medical images makes it challenging to select optimal in-context
pairs, and fine-tuning foundation ICL models on contextual data is infeasible
due to computational costs and the risk of catastrophic forgetting. To address
this challenge, we propose Cycle Context Verification (CCV), a novel framework
that enhances ICL-based medical image segmentation by enabling
self-verification of predictions and accordingly enhancing contextual
alignment. Specifically, CCV employs a cyclic pipeline in which the model
initially generates a segmentation mask for the query image. Subsequently, the
roles of the query and an in-context pair are swapped, allowing the model to
validate its prediction by predicting the mask of the original in-context
image. The accuracy of this secondary prediction serves as an implicit measure
of the initial query segmentation. A query-specific prompt is introduced to
alter the query image and updated to improve the measure, thereby enhancing the
alignment between the query and in-context pairs. We evaluated CCV on seven
medical image segmentation datasets using two ICL foundation models,
demonstrating its superiority over existing methods. Our results highlight
CCV's ability to enhance ICL-based segmentation, making it a robust solution
for universal medical image segmentation. The code will be available at
https://github.com/ShishuaiHu/CCV.

</details>


### [29] [Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques](https://arxiv.org/abs/2507.08375)
*Alexandra Malyugina,Yini Li,Joanne Lin,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 本文对无监督视频修复和增强技术进行了全面的回顾，概述了退化、方法、损失函数和未来方向。


<details>
  <summary>Details</summary>
Motivation: 视频修复和增强不仅对于提高视觉质量至关重要，而且是提高各种下游计算机视觉任务性能的关键预处理步骤。

Method: 本文首先概述了常见的视频退化及其根本原因，然后回顾了早期传统和基于深度学习的方法，接着重点介绍了无监督方法，并按其基本方法（包括域翻译、自监督信号设计和基于盲点或噪声的方法）进行了分类。此外，还对无监督视频修复和增强中使用的损失函数进行了分类，并讨论了成对合成数据集在实现客观评估中的作用。

Result: 对视频修复和增强技术，特别是无监督方法进行了全面的回顾，并提供了对该领域挑战和未来方向的见解。

Conclusion: 本文对视频修复和增强技术进行了全面的回顾，特别关注了无监督方法，并讨论了关键挑战和未来研究方向。

Abstract: Video restoration and enhancement are critical not only for improving visual
quality, but also as essential pre-processing steps to boost the performance of
a wide range of downstream computer vision tasks. This survey presents a
comprehensive review of video restoration and enhancement techniques with a
particular focus on unsupervised approaches. We begin by outlining the most
common video degradations and their underlying causes, followed by a review of
early conventional and deep learning methods-based, highlighting their
strengths and limitations. We then present an in-depth overview of unsupervised
methods, categorise by their fundamental approaches, including domain
translation, self-supervision signal design and blind spot or noise-based
methods. We also provide a categorization of loss functions employed in
unsupervised video restoration and enhancement, and discuss the role of paired
synthetic datasets in enabling objective evaluation. Finally, we identify key
challenges and outline promising directions for future research in this field.

</details>


### [30] [From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning](https://arxiv.org/abs/2507.08380)
*Sen Wang,Shao Zeng,Tianjun Gu,Zhizhong Zhang,Ruixin Zhang,Shouhong Ding,Jingyun Zhang,Jun Wang,Xin Tan,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本研究提出了GEFU框架，结合扩散模型和SCUF技术，解决了低光图像增强和理解的挑战，提高了泛化性和可扩展性，并在多项任务上取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 传统低光视觉任务（增强和理解）被孤立处理，低光增强方法泛化性受限且评估侧重图像质量而非下游任务表现；低光视觉理解因缺乏标注数据而扩展性不足。本研究旨在解决这些挑战，建立低光增强和低光理解之间的通用桥梁。

Method: 通过利用预训练的生成扩散模型进行零样本图像优化，并结合语义一致性无监督微调（SCUF）技术，包括引入光照感知图像提示和周期注意力适配器来增强语义表达，同时利用字幕和反射率一致性来学习高层和图像级空间语义。

Result: 实验证明，所提出的方法在图像质量和包括分类、检测、语义分割在内的GEFU任务上均优于现有最先进方法。

Conclusion: 该研究提出了一种名为GEFU的通用框架，连接了低光图像增强和低光视觉理解，实现了泛化性和可扩展性的提升。

Abstract: Low-level enhancement and high-level visual understanding in low-light vision
have traditionally been treated separately. Low-light enhancement improves
image quality for downstream tasks, but existing methods rely on physical or
geometric priors, limiting generalization. Evaluation mainly focuses on visual
quality rather than downstream performance. Low-light visual understanding,
constrained by scarce labeled data, primarily uses task-specific domain
adaptation, which lacks scalability. To address these challenges, we build a
generalized bridge between low-light enhancement and low-light understanding,
which we term Generalized Enhancement For Understanding (GEFU). This paradigm
improves both generalization and scalability. To address the diverse causes of
low-light degradation, we leverage pretrained generative diffusion models to
optimize images, achieving zero-shot generalization performance. Building on
this, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF).
Specifically, to overcome text prompt limitations, we introduce an
illumination-aware image prompt to explicitly guide image generation and
propose a cycle-attention adapter to maximize its semantic potential. To
mitigate semantic degradation in unsupervised training, we propose caption and
reflectance consistency to learn high-level semantics and image-level spatial
semantics. Extensive experiments demonstrate that our proposed method
outperforms current state-of-the-art methods in traditional image quality and
GEFU tasks including classification, detection, and semantic segmentation.

</details>


### [31] [Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset](https://arxiv.org/abs/2507.08384)
*Mathias Zinnen,Prathmesh Madhu,Inger Leemans,Peter Bell,Azhar Hussian,Hang Tran,Ali Hürriyetoğlu,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: ODOR数据集包含38,116个艺术作品对象标注，覆盖139个细粒度类别，旨在提高计算机视觉在人文学科中的鲁棒性，尤其是在处理艺术风格、复杂场景和细微类别差异方面。通过基线分析和次级研究，突显了其对物体检测模型的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的计算机视觉在人文学科中的应用，需要算法能够应对艺术抽象、外围对象以及细粒度目标类别之间的细微差别。现有数据集在艺术作品方面提供实例级标注，但通常偏向图像中心，且在详细对象类别方面存在局限性。

Method: 构建了ODOR数据集，包含38,116个对象级标注和4712张图像，覆盖139个细粒度类别。通过统计分析展示了数据集的挑战性属性，如类别细节丰富、物体密集且重叠、以及在整个图像画布上的空间分布。此外，进行了广泛的物体检测模型基线分析，并通过次级研究突显了数据集的挑战性。

Result: ODOR数据集包含38,116个对象级标注，涵盖139个细粒度类别，并进行了详细的统计分析和基线分析，突显了数据集的挑战性属性，为艺术作品对象检测和视觉文化遗产研究提供了新的资源和方向。

Conclusion: 该ODOR数据集填补了现有数据集在艺术作品对象识别方面的空白，特别是在处理艺术抽象、外围对象和细粒度类别差异方面。数据集包含38,116个对象级标注，涵盖139个细粒度类别，挑战现有模型的鲁棒性，并激发在视觉文化遗产研究中对物体识别和气味感知的交叉探索。

Abstract: Real-world applications of computer vision in the humanities require
algorithms to be robust against artistic abstraction, peripheral objects, and
subtle differences between fine-grained target classes. Existing datasets
provide instance-level annotations on artworks but are generally biased towards
the image centre and limited with regard to detailed object classes. The
proposed ODOR dataset fills this gap, offering 38,116 object-level annotations
across 4712 images, spanning an extensive set of 139 fine-grained categories.
Conducting a statistical analysis, we showcase challenging dataset properties,
such as a detailed set of categories, dense and overlapping objects, and
spatial distribution over the whole image canvas. Furthermore, we provide an
extensive baseline analysis for object detection models and highlight the
challenging properties of the dataset through a set of secondary studies.
Inspiring further research on artwork object detection and broader visual
cultural heritage studies, the dataset challenges researchers to explore the
intersection of object recognition and smell perception.

</details>


### [32] [Subject-Consistent and Pose-Diverse Text-to-Image Generation](https://arxiv.org/abs/2507.08396)
*Zhanxin Gao,Beier Zhu,Liang Yao,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: CoDi 是一种新的文本到图像框架，可以通过身份传输和身份精炼来生成主题一致且姿势多样的图像，解决了现有方法在保持一致性的同时牺牲多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的免训练主题一致性生成（SCG）方法通常以牺牲布局和姿势多样性为代价来实现一致性，这会阻碍富有表现力的视觉叙事。为了解决这一限制，我们提出了 CoDi 框架，它能够在保持主题身份一致性的同时实现姿势和布局的多样性。

Method: CoDi 采用两阶段策略：身份传输（IT）和身份精炼（IR）。IT 在早期去噪步骤中使用最优传输将姿势感知的身份特征转移到目标图像，以保持主题一致性并保留姿势多样性。IR 在后期去噪步骤中，选择最显著的身份特征以进一步精炼主题细节。

Result: 大量的定性和定量结果表明，CoDi 在主题一致性、姿势多样性和提示保真度方面均取得了更好的视觉感知和更强的性能。

Conclusion: CoDi 在主题一致性、姿势多样性和提示保真度方面均优于现有方法，并在视觉感知和整体性能上均取得了更好的效果。

Abstract: Subject-consistent generation (SCG)-aiming to maintain a consistent subject
identity across diverse scenes-remains a challenge for text-to-image (T2I)
models. Existing training-free SCG methods often achieve consistency at the
cost of layout and pose diversity, hindering expressive visual storytelling. To
address the limitation, we propose subject-Consistent and pose-Diverse T2I
framework, dubbed as CoDi, that enables consistent subject generation with
diverse pose and layout. Motivated by the progressive nature of diffusion,
where coarse structures emerge early and fine details are refined later, CoDi
adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement
(IR). IT operates in the early denoising steps, using optimal transport to
transfer identity features to each target image in a pose-aware manner. This
promotes subject consistency while preserving pose diversity. IR is applied in
the later denoising steps, selecting the most salient identity features to
further refine subject details. Extensive qualitative and quantitative results
on subject consistency, pose diversity, and prompt fidelity demonstrate that
CoDi achieves both better visual perception and stronger performance across all
metrics. The code is provided in https://github.com/NJU-PCALab/CoDi.

</details>


### [33] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [34] [Deep Hashing with Semantic Hash Centers for Image Retrieval](https://arxiv.org/abs/2507.08404)
*Li Chen,Rui Liu,Yuxiang Zhou,Xudong Ma,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: SHC通过生成语义哈希中心，利用类间语义相似性来提高深度哈希的图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有逐点监督的深度哈希方法依赖数据无关算法生成哈希中心，忽略了类间语义关系，可能导致检索性能下降。本文提出语义哈希中心，假设语义相关的类的哈希中心距离更近，不相关的类的哈希中心距离更远。

Method: 提出了一种名为SHC的三阶段框架：1.开发分类网络计算类间语义相似度；2.提出优化算法生成语义哈希中心，保持语义相关性并强制最小距离；3.训练深度哈希网络将图像转换为二进制哈希码。

Result: 在大型图像检索任务中，SHC的MAP@100、MAP@1000和MAP@ALL指标分别比现有最先进方法平均提高了7.26%、7.62%和11.71%。

Conclusion: SHC通过生成语义哈希中心，在图像检索任务中显著提高了检索性能，在多个评估指标上均优于现有最先进方法。

Abstract: Deep hashing is an effective approach for large-scale image retrieval.
Current methods are typically classified by their supervision types:
point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,
MDS) have improved retrieval performance by pre-assigning a hash center to each
class, enhancing the discriminability of hash codes across various datasets.
However, these methods rely on data-independent algorithms to generate hash
centers, which neglect the semantic relationships between classes and may
degrade retrieval performance.
  This paper introduces the concept of semantic hash centers, building on the
idea of traditional hash centers. We hypothesize that hash centers of
semantically related classes should have closer Hamming distances, while those
of unrelated classes should be more distant. To this end, we propose a
three-stage framework, SHC, to generate hash codes that preserve semantic
structure.
  First, we develop a classification network to identify semantic similarities
between classes using a data-dependent similarity calculation that adapts to
varying data distributions. Second, we introduce an optimization algorithm to
generate semantic hash centers, preserving semantic relatedness while enforcing
a minimum distance between centers to avoid excessively similar hash codes.
Finally, a deep hashing network is trained using these semantic centers to
convert images into binary hash codes.
  Experimental results on large-scale retrieval tasks across several public
datasets show that SHC significantly improves retrieval performance.
Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%
in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art
methods.

</details>


### [35] [Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.08410)
*Shijun Yang,Xiang Zhang,Wanqing Zhao,Hangzai Luo,Sheng Zhong,Jinye Peng,Jianping Fan*

Main category: cs.CV

TL;DR: MuGCP通过利用MLLMs生成包含丰富语义信息的SCP，并通过AMG模块促进跨模态交互生成VCP，最后通过MPF机制融合SCP和VCP，解决了现有提示学习方法在新类别泛化能力和跨模态对齐方面的不足，并在14个数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的提示学习方法在为未见实例建模类别嵌入分布方面存在不足，导致在新类别上的泛化能力不佳。此外，现有方法主要将跨模态对齐限制在视觉和文本编码器的最终输出层，限制了其保留预训练多模态嵌入空间拓扑一致性的能力。

Method: 提出了一种新颖的条件提示生成范式MuGCP（多模态互导条件提示学习），该范式利用多模态大语言模型（MLLMs）作为条件提示学习器，自适应地生成包含丰富、细粒度的高级语义知识的语义条件提示（SCP）。引入了注意互导（AMG）模块，以促进视觉和语义信息之间的交互，并生成视觉条件提示（VCP）。此外，还提出了一种多提示融合（MPF）机制，将SCP和VCP与上下文提示集成，以增强类别嵌入和实例特定知识的建模。

Result: MuGCP在14个不同数据集上 outperformed 了现有的最先进方法。

Conclusion: MuGCP在14个不同数据集上表现优于现有的最先进方法。

Abstract: Prompt learning facilitates the efficient adaptation of Vision-Language
Models (VLMs) to various downstream tasks. However, it faces two significant
challenges: (1) inadequate modeling of class embedding distributions for unseen
instances, leading to suboptimal generalization on novel classes; (2)
prevailing methodologies predominantly confine cross-modal alignment to the
final output layer of vision and text encoders, which fundamentally limits
their capacity to preserve topological consistency with pre-trained multi-modal
embedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance
Conditional Prompt Learning), a novel paradigm designed for conditional prompt
generation. MuGCP leverages Multi-modal Large Language Models (MLLMs) as
conditional prompt learners to adaptively generate Semantic Conditional Prompts
(SCP) that incorporate rich, fine-grained high-level semantic knowledge for
image instances. To ensure effective alignment and interaction across the
multi-modal space of Vision-Language Models (VLMs), we introduce the Attention
Mutual-Guidance (AMG) module, which facilitates interactions between visual and
semantic information. Through mutual guidance, the AMG module generates Visual
Conditional Prompts (VCP), enhancing the model's performance in multi-modal
tasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that
integrates SCP and VCP with contextual prompts, ensuring seamless coordination
among the different prompts and enhancing the modeling of class embeddings and
instance-specific knowledge. Our MuGCP outperforms existing state-of-the-art
methods on 14 different datasets. The code will be made available after
publication.

</details>


### [36] [InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes](https://arxiv.org/abs/2507.08416)
*Zesong Yang,Bangbang Yang,Wenqi Dong,Chenxuan Cao,Liyuan Cui,Yuewen Ma,Zhaopeng Cui,Hujun Bao*

Main category: cs.CV

TL;DR: InstaScene 是一种新的 3D 感知范例，用于在杂乱环境中分解和重建实例。它使用空间对比学习进行精确分解，并使用原地生成进行完整重建。


<details>
  <summary>Details</summary>
Motivation: 尽管先进的重建技术可以处理杂乱环境中的遮挡物体，但它们将场景视为未分化的整体，无法从部分观测中识别完整物体。本文旨在解决这一挑战，提出一种新的范例来实现复杂场景的整体 3D 感知，其主要目标是将任意实例进行分解，同时确保完整重建。

Method: 提出了一种新的空间对比学习方法，通过追踪每个实例跨视图的光栅化来实现精确分解，并通过原位生成利用有价值的观测和几何线索来克服不完整性问题，有效地指导 3D 生成模型来重建与真实世界无缝对齐的完整实例。

Result: 在场景分解和物体补全方面取得了优越的准确性和几何保真度。

Conclusion: 该方法在复杂现实世界和合成场景中的场景分解和物体补全实验表明，我们的方法在分解精度方面表现优越，同时能生成几何保真且视觉完整的物体。

Abstract: Humans can naturally identify and mentally complete occluded objects in
cluttered environments. However, imparting similar cognitive ability to
robotics remains challenging even with advanced reconstruction techniques,
which models scenes as undifferentiated wholes and fails to recognize complete
object from partial observations. In this paper, we propose InstaScene, a new
paradigm towards holistic 3D perception of complex scenes with a primary goal:
decomposing arbitrary instances while ensuring complete reconstruction. To
achieve precise decomposition, we develop a novel spatial contrastive learning
by tracing rasterization of each instance across views, significantly enhancing
semantic supervision in cluttered scenes. To overcome incompleteness from
limited observations, we introduce in-situ generation that harnesses valuable
observations and geometric cues, effectively guiding 3D generative models to
reconstruct complete instances that seamlessly align with the real world.
Experiments on scene decomposition and object completion across complex
real-world and synthetic scenes demonstrate that our method achieves superior
decomposition accuracy while producing geometrically faithful and visually
intact objects.

</details>


### [37] [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](https://arxiv.org/abs/2507.08422)
*Wongi Jeong,Kyungryeol Lee,Hoigi Seo,Se Young Chun*

Main category: cs.CV

TL;DR: RALU 框架通过混合分辨率采样技术，在空间维度上加速扩散 Transformer 的推理过程，在不损失图像质量的前提下显著提升了生成速度，并能与现有时间加速方法结合使用。


<details>
  <summary>Details</summary>
Motivation: 为了解决扩散 Transformer 模型推理计算量大，难以在实际部署中应用的问题，提出 RALU 框架，通过在空间维度上进行加速来提高效率。

Method: RALU 框架采用训练无关的方法，在空间维度上进行加速。它包括三个阶段：1) 低分辨率潜在扩散，用于捕获全局语义结构；2) 针对易出现伪影的特定区域进行自适应上采样；3) 在全分辨率下进行所有潜在上采样以进行细节精炼。为了稳定跨分辨率生成的质量，该方法利用噪声-时间步重调度来适应不同分辨率下的噪声水平。

Result: RALU 框架在 FLUX 上实现了高达 7.0 倍的加速，在 Stable Diffusion 3 上实现了 3.0 倍的加速，且图像质量损失极小。

Conclusion: RALU框架通过在空间维度上进行混合分辨率采样，能够显著降低扩散 Transformer 的计算量，同时保持图像质量。该方法在 FLUX 上实现了高达 7.0 倍的加速，在 Stable Diffusion 3 上实现了 3.0 倍的加速，且图像质量损失极小。此外，RALU 与现有的时间加速方法（如缓存方法）兼容，可以无缝集成以进一步降低推理延迟，而不会影响生成质量。

Abstract: Diffusion transformers have emerged as an alternative to U-net-based
diffusion models for high-fidelity image and video generation, offering
superior scalability. However, their heavy computation remains a major obstacle
to real-world deployment. Existing acceleration methods primarily exploit the
temporal dimension such as reusing cached features across diffusion timesteps.
Here, we propose Region-Adaptive Latent Upsampling (RALU), a training-free
framework that accelerates inference along spatial dimension. RALU performs
mixed-resolution sampling across three stages: 1) low-resolution denoising
latent diffusion to efficiently capture global semantic structure, 2)
region-adaptive upsampling on specific regions prone to artifacts at
full-resolution, and 3) all latent upsampling at full-resolution for detail
refinement. To stabilize generations across resolution transitions, we leverage
noise-timestep rescheduling to adapt the noise level across varying
resolutions. Our method significantly reduces computation while preserving
image quality by achieving up to 7.0$\times$ speed-up on FLUX and 3.0$\times$
on Stable Diffusion 3 with minimal degradation. Furthermore, RALU is
complementary to existing temporal accelerations such as caching methods, thus
can be seamlessly integrated to further reduce inference latency without
compromising generation quality.

</details>


### [38] [RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting](https://arxiv.org/abs/2507.08434)
*Ji Hyun Seo,Byounhyun Yoo,Gerard Jounghyun Kim*

Main category: cs.CV

TL;DR: 通过参考视图进行3D场景修复，解决视图间不一致问题，提高几何和外观保真度。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景修复方法（如基于图像修复的方法）在处理被移除对象后暴露的遮挡区域时，会产生不同视图间不一致的修复结果，并且基于感知线索的融合方法容易丢失细节或在跨视图感知不一致时失效。

Method: 提出了一种新颖的3D场景修复方法，通过参考视图估计其他视图的修复相似度，并据此调整它们对精确几何的贡献。然后利用该几何将参考视图的修复结果扭曲到其他视图作为伪地面真值，指导优化以匹配参考视图的外观。

Result: 提出的方法在修复复杂场景时，能够生成逼真且在不同视点间保持一致的结果，提高了修复场景的几何保真度和外观一致性。

Conclusion: 该方法通过利用参考视图来生成逼真且在不同视点间保持一致的修复结果，提高了修复场景的几何保真度和外观一致性。

Abstract: Radiance field methods, such as Neural Radiance Field or 3D Gaussian
Splatting, have emerged as seminal 3D representations for synthesizing
realistic novel views. For practical applications, there is ongoing research on
flexible scene editing techniques, among which object removal is a
representative task. However, removing objects exposes occluded regions, often
leading to unnatural appearances. Thus, studies have employed image inpainting
techniques to replace such regions with plausible content - a task referred to
as 3D scene inpainting. However, image inpainting methods produce one of many
plausible completions for each view, leading to inconsistencies between
viewpoints. A widely adopted approach leverages perceptual cues to blend
inpainted views smoothly. However, it is prone to detail loss and can fail when
there are perceptual inconsistencies across views. In this paper, we propose a
novel 3D scene inpainting method that reliably produces realistic and
perceptually consistent results even for complex scenes by leveraging a
reference view. Given the inpainted reference view, we estimate the inpainting
similarity of the other views to adjust their contribution in constructing an
accurate geometry tailored to the reference. This geometry is then used to warp
the reference inpainting to other views as pseudo-ground truth, guiding the
optimization to match the reference appearance. Comparative evaluation studies
have shown that our approach improves both the geometric fidelity and
appearance consistency of inpainted scenes.

</details>


### [39] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文提出了一种名为VFMTok的新型图像分词器，它利用预训练的视觉基础模型，并通过区域自适应量化和语义重建目标进行增强。VFMTok在图像重建、生成质量和令牌效率方面均有显著改进，并在图像生成任务中取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 本文探索了一个新的方向：在预训练的视觉基础模型之上直接构建图像分词器，这是一个很大程度上未被探索的领域。

Method: 本文提出了一种新颖的图像分词器构建方法，该方法直接基于预训练的视觉基础模型。具体来说，本文采用冻结的视觉基础模型作为分词器的编码器，并引入了两个关键组件：1）一个区域自适应量化框架，用于减少二维网格上预训练特征的冗余；2）一个语义重建目标，用于将分词器的输出与基础模型的表示对齐，以保留语义保真度。

Result: VFMTok在图像重建和生成质量方面取得了实质性改进，同时提高了令牌效率。此外，它还改进了自回归（AR）生成，在ImageNet基准测试中实现了2.07的gFID，同时将模型收敛速度提高了三倍，并能够在不需要分类器指导的情况下实现高保真类条件合成。

Conclusion: VFMTok在图像重建和生成质量方面取得了实质性改进，同时提高了令牌效率，并在ImageNet基准测试中实现了2.07的gFID，同时将模型收敛速度提高了三倍，并能够在不需要分类器指导的情况下实现高保真类条件合成。

Abstract: Leveraging the powerful representations of pre-trained vision foundation
models -- traditionally used for visual comprehension -- we explore a novel
direction: building an image tokenizer directly atop such models, a largely
underexplored area. Specifically, we employ a frozen vision foundation model as
the encoder of our tokenizer. To enhance its effectiveness, we introduce two
key components: (1) a region-adaptive quantization framework that reduces
redundancy in the pre-trained features on regular 2D grids, and (2) a semantic
reconstruction objective that aligns the tokenizer's outputs with the
foundation model's representations to preserve semantic fidelity. Based on
these designs, our proposed image tokenizer, VFMTok, achieves substantial
improvements in image reconstruction and generation quality, while also
enhancing token efficiency. It further boosts autoregressive (AR) generation --
achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model
convergence by three times, and enabling high-fidelity class-conditional
synthesis without the need for classifier-free guidance (CFG). The code will be
released publicly to benefit the community.

</details>


### [40] [Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT](https://arxiv.org/abs/2507.08448)
*Wei Zhang,Yihang Wu,Songhua Li,Wenjie Ma,Xin Ma,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 深度学习前馈模型在3D重建领域带来革新，提供更优的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 为了应对传统3D重建方法（如SfM和MVS）在复杂性、计算成本和鲁棒性方面的限制，该研究旨在系统地回顾和分析新兴的基于深度学习的前馈3D重建方法。

Method: 该论文系统地回顾了基于深度学习的前馈3D重建模型，深入分析了它们的Transformer基础对应建模、联合姿态和几何回归机制，以及从两视图扩展到多视图的策略，并与传统方法（如SfM和MVS）和早期学习方法（如MVSNet）进行了对比。

Result: 该论文提供了对新兴的前馈深度学习3D重建方法的全面概述，包括其技术细节、与传统方法的对比、相关数据集和评估指标，并讨论了其应用前景和未来挑战。

Conclusion: 深度学习方法，特别是像DUSt3R这样的前馈模型，正在彻底改变3D重建领域，提供了比传统方法更高效、更鲁棒的解决方案。

Abstract: 3D reconstruction, which aims to recover the dense three-dimensional
structure of a scene, is a cornerstone technology for numerous applications,
including augmented/virtual reality, autonomous driving, and robotics. While
traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo
(MVS) achieve high precision through iterative optimization, they are limited
by complex workflows, high computational cost, and poor robustness in
challenging scenarios like texture-less regions. Recently, deep learning has
catalyzed a paradigm shift in 3D reconstruction. A new family of models,
exemplified by DUSt3R, has pioneered a feed-forward approach. These models
employ a unified deep network to jointly infer camera poses and dense geometry
directly from an Unconstrained set of images in a single forward pass. This
survey provides a systematic review of this emerging domain. We begin by
dissecting the technical framework of these feed-forward models, including
their Transformer-based correspondence modeling, joint pose and geometry
regression mechanisms, and strategies for scaling from two-view to multi-view
scenarios. To highlight the disruptive nature of this new paradigm, we contrast
it with both traditional pipelines and earlier learning-based methods like
MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation
metrics. Finally, we discuss the technology's broad application prospects and
identify key future challenges and opportunities, such as model accuracy and
scalability, and handling dynamic scenes.

</details>


### [41] [A document is worth a structured record: Principled inductive bias design for document recognition](https://arxiv.org/abs/2507.08458)
*Benjamin Meyer,Lukas Tuggener,Sascha Hänzi,Daniel Schmid,Erdal Ayfer,Benjamin F. Grewe,Ahmed Abdulkadir,Thilo Stadelmann*

Main category: cs.CV

TL;DR: 该研究提出了一种将文档识别视为转录任务的新方法，并设计了一种适应不同文档结构的 Transformer 架构，在工程图纸转录等任务上取得了突破性进展。


<details>
  <summary>Details</summary>
Motivation: 现有的文档识别方法将该任务视为计算机视觉问题，忽略了文档类型特有的内在结构属性，导致它们依赖于次优的启发式后处理，并使得许多不常见或更复杂的文档类型无法被现代文档识别技术处理。

Method: 提出了一种将文档识别视为从文档到记录的转录任务的新视角，从而可以根据转录中固有的内在结构对文档进行自然分组。提出了一种为潜在的机器学习端到端文档识别系统设计特定于结构的归纳偏倚的方法以及一个相应的基准 Transformer 架构。

Result: 我们证明了所发现的归纳偏倚在从单音图、图形绘制和简化工程图纸的结构日益复杂的实验中是有效的。

Conclusion: 我们的方法可以为文档识别系统设计特定于结构的归纳偏倚，并提出了一种可以成功适应不同结构的基准 Transformer 架构。我们将非限制性图结构的归纳偏倚整合到模型中，成功训练了首个将工程图纸转录为其固有效接信息端到端的模型。我们的方法为理解不如标准 OCR、OMR 等文档类型那样广泛的文档类型的文档识别系统的设计提供了信息，并为统一未来文档基础模型的设计提供了指南。

Abstract: Many document types use intrinsic, convention-driven structures that serve to
encode precise and structured information, such as the conventions governing
engineering drawings. However, state-of-the-art approaches treat document
recognition as a mere computer vision problem, neglecting these underlying
document-type-specific structural properties, making them dependent on
sub-optimal heuristic post-processing and rendering many less frequent or more
complicated document types inaccessible to modern document recognition. We
suggest a novel perspective that frames document recognition as a transcription
task from a document to a record. This implies a natural grouping of documents
based on the intrinsic structure inherent in their transcription, where related
document types can be treated (and learned) similarly. We propose a method to
design structure-specific inductive biases for the underlying machine-learned
end-to-end document recognition systems, and a respective base transformer
architecture that we successfully adapt to different structures. We demonstrate
the effectiveness of the so-found inductive biases in extensive experiments
with progressively complex record structures from monophonic sheet music, shape
drawings, and simplified engineering drawings. By integrating an inductive bias
for unrestricted graph structures, we train the first-ever successful
end-to-end model to transcribe engineering drawings to their inherently
interlinked information. Our approach is relevant to inform the design of
document recognition systems for document types that are less well understood
than standard OCR, OMR, etc., and serves as a guide to unify the design of
future document foundation models.

</details>


### [42] [F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement](https://arxiv.org/abs/2507.08460)
*Seyedeh Sahar Taheri Otaghsara,Reza Rahmanzadeh*

Main category: cs.CV

TL;DR: F3-Net 是一个强大的基础模型，能够处理缺失的 MRI 序列，并支持多种病理分割，在各种临床医学图像分割任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 克服临床医学图像分割中存在的挑战，包括对完整多模态输入的依赖、有限的泛化能力和狭窄的任务特异性。

Method: F3-Net 采用灵活的合成模态训练，并利用零图像策略来替代缺失的模态，无需依赖显式的合成网络。

Result: 在 BraTS 2021、BraTS 2024 和 ISLES 2022 等多样化数据集上，F3-Net 在处理缺失的 MRI 序列时表现出鲁棒的性能，并且在单一模型中支持多种病理分割，平均 DSC 分数分别为：BraTS-GLI 2024 的 0.94，BraTS-MET 2024 的 0.82，BraTS 2021 的 0.94，以及 ISLES 2022 的 0.79。

Conclusion: F3-Net 具有很强的适应性和可扩展性，能够弥合深度学习研究与实际临床应用之间的差距。

Abstract: F3-Net is a foundation model designed to overcome persistent challenges in
clinical medical image segmentation, including reliance on complete multimodal
inputs, limited generalizability, and narrow task specificity. Through flexible
synthetic modality training, F3-Net maintains robust performance even in the
presence of missing MRI sequences, leveraging a zero-image strategy to
substitute absent modalities without relying on explicit synthesis networks,
thereby enhancing real-world applicability. Its unified architecture supports
multi-pathology segmentation across glioma, metastasis, stroke, and white
matter lesions without retraining, outperforming CNN-based and
transformer-based models that typically require disease-specific fine-tuning.
Evaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,
F3-Net demonstrates strong resilience to domain shifts and clinical
heterogeneity. On the whole pathology dataset, F3-Net achieves average Dice
Similarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET
2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a
versatile, scalable solution bridging the gap between deep learning research
and practical clinical deployment.

</details>


### [43] [Dual Dimensions Geometric Representation Learning Based Document Dewarping](https://arxiv.org/abs/2507.08492)
*Heng Li,Qingcai Chen,Xiangping Wu*

Main category: cs.CV

TL;DR: D2Dewarp是一种用于文档反翘的深度学习模型，通过同时关注水平和垂直维度线条的形变，并结合X、Y坐标进行特征融合，提高了校正精度。同时，研究人员还构建了一个新的大规模训练数据集。


<details>
  <summary>Details</summary>
Motivation: 为了改进文档反翘任务，解决了现有方法仅关注单一水平维度，忽略了文档垂直维度形变的问题。

Method: 提出了一种精细化的形变感知模型D2Dewarp，该模型关注文档的水平-垂直双维度线条，并通过基于X和Y坐标的融合模块来结合和约束双维度特征，以实现特征互补。此外，还提出了一种自动化的精细化标注方法，利用公开的文档纹理图像和渲染引擎构建了一个大规模的训练数据集。

Result: 所提出的方法在中文和英文公开基准测试中，通过定量和定性评估，均显示出比现有最先进方法更优越的校正效果。

Conclusion: 所提出的D2Dewarp模型在中文和英文公开基准测试中均取得了优于现有最先进方法的校正结果。

Abstract: Document image dewarping remains a challenging task in the deep learning era.
While existing methods have improved by leveraging text line awareness, they
typically focus only on a single horizontal dimension. In this paper, we
propose a fine-grained deformation perception model that focuses on Dual
Dimensions of document horizontal-vertical-lines to improve document Dewarping
called D2Dewarp. It can perceive distortion trends in different directions
across document details. To combine the horizontal and vertical granularity
features, an effective fusion module based on X and Y coordinate is designed to
facilitate interaction and constraint between the two dimensions for feature
complementarity. Due to the lack of annotated line features in current public
dewarping datasets, we also propose an automatic fine-grained annotation method
using public document texture images and an automatic rendering engine to build
a new large-scale distortion training dataset. The code and dataset will be
publicly released. On public Chinese and English benchmarks, both quantitative
and qualitative results show that our method achieves better rectification
results compared with the state-of-the-art methods. The dataset will be
publicly available at https://github.com/xiaomore/DocDewarpHV

</details>


### [44] [Unified People Tracking with Graph Neural Networks](https://arxiv.org/abs/2507.08494)
*Martin Engilberge,Ivan Vrkic,Friedrich Wilke Grosche,Julien Pilet,Engin Turetken,Pascal Fua*

Main category: cs.CV

TL;DR: 提出了一种新的多人跟踪模型，该模型使用动态时空图来关联检测并处理遮挡问题，并在新数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 提出了一种统一的、完全可微的多人跟踪模型，该模型学习将检测关联到轨迹，而无需依赖预先计算的轨迹片段。

Method: 提出了一种统一的、完全可微的多人跟踪模型，该模型学习将检测关联到轨迹，而无需依赖预先计算的轨迹片段。该模型构建了一个动态时空图，该图聚合了空间、上下文和时间信息，从而能够跨整个序列进行无缝信息传播。为了提高遮挡处理能力，该图还可以编码场景特定的信息。

Result: 在公开基准和新数据集上均达到了最先进的性能，并且在各种条件下都具有灵活性。

Conclusion: 该模型在公开基准和新数据集上均达到了最先进的性能，并且在各种条件下都具有灵活性。

Abstract: This work presents a unified, fully differentiable model for multi-people
tracking that learns to associate detections into trajectories without relying
on pre-computed tracklets. The model builds a dynamic spatiotemporal graph that
aggregates spatial, contextual, and temporal information, enabling seamless
information propagation across entire sequences. To improve occlusion handling,
the graph can also encode scene-specific information. We also introduce a new
large-scale dataset with 25 partially overlapping views, detailed scene
reconstructions, and extensive occlusions. Experiments show the model achieves
state-of-the-art performance on public benchmarks and the new dataset, with
flexibility across diverse conditions. Both the dataset and approach will be
publicly released to advance research in multi-people tracking.

</details>


### [45] [Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification](https://arxiv.org/abs/2507.08520)
*Yufei Zheng,Wenjun Wang,Wenjun Gan,Jiawei Liu*

Main category: cs.CV

TL;DR: OGFR通过强化知识蒸馏，解决遮挡重识别中的未见遮挡场景和特征污染问题，提升了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的遮挡重识别方法在处理未见过的遮挡场景以及避免整体图像的特征污染方面存在挑战。

Method: OGFR采用师生蒸馏架构，利用可学习的遮挡模式嵌入来明确建模遮挡类型，引导形成对遮挡感知的鲁棒特征表示。通过深度强化学习，特征擦除与纯化模块能够识别并替换掉整体图像分支中包含噪声负信息的低质量块标记，从而避免特征污染并提取与身份相关的判别线索。学生分支通过知识蒸馏有效吸收纯化后的整体知识，以精确学习鲁棒的表示。

Result: OGFR有效减轻了遮挡重识别中的挑战，通过强化知识蒸馏实现了对遮挡场景的鲁棒表示。

Conclusion: 通过强化知识蒸馏的遮挡引导特征纯化学习（OGFR）可以有效解决遮挡重识别中的挑战，例如处理未见过的遮挡场景和避免整体图像的特征污染。

Abstract: Occluded person re-identification aims to retrieve holistic images based on
occluded ones. Existing methods often rely on aligning visible body parts,
applying occlusion augmentation, or complementing missing semantics using
holistic images. However, they face challenges in handling diverse occlusion
scenarios not seen during training and the issue of feature contamination from
holistic images. To address these limitations, we propose Occlusion-Guided
Feature Purification Learning via Reinforced Knowledge Distillation (OGFR),
which simultaneously mitigates these challenges. OGFR adopts a teacher-student
distillation architecture that effectively incorporates diverse occlusion
patterns into feature representation while transferring the purified
discriminative holistic knowledge from the holistic to the occluded branch
through reinforced knowledge distillation. Specifically, an Occlusion-Aware
Vision Transformer is designed to leverage learnable occlusion pattern
embeddings to explicitly model such diverse occlusion types, thereby guiding
occlusion-aware robust feature representation. Moreover, we devise a Feature
Erasing and Purification Module within the holistic branch, in which an agent
is employed to identify low-quality patch tokens of holistic images that
contain noisy negative information via deep reinforcement learning, and
substitute these patch tokens with learnable embedding tokens to avoid feature
contamination and further excavate identity-related discriminative clues.
Afterward, with the assistance of knowledge distillation, the student branch
effectively absorbs the purified holistic knowledge to precisely learn robust
representation regardless of the interference of occlusions.

</details>


### [46] [RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features](https://arxiv.org/abs/2507.08546)
*Inye Na,Nejung Rue,Jiwon Chung,Hyunjin Park*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Medical image retrieval is a valuable field for supporting clinical
decision-making, yet current methods primarily support 2D images and require
fully annotated queries, limiting clinical flexibility. To address this, we
propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging
handcrafted radiomics descriptors with deep learning-based embeddings at the
tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits
volumetric data to leverage richer spatial context in medical images. We employ
a promptable segmentation model (e.g., SAM) to derive tumor-specific image
embeddings, which are aligned with radiomics features extracted from the same
tumor via contrastive learning. These representations are further enriched by
anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables
flexible querying based on shape, location, or partial feature sets. Extensive
experiments on both lung CT and brain MRI public datasets demonstrate that
radiomics features significantly enhance retrieval specificity, while APE
provides global anatomical context essential for location-based searches.
Notably, our framework requires only minimal user prompts (e.g., a single
point), minimizing segmentation overhead and supporting diverse clinical
scenarios. The capability to query using either image embeddings or selected
radiomics attributes highlights its adaptability, potentially benefiting
diagnosis, treatment planning, and research on large-scale medical imaging
repositories. Our code is available at
https://github.com/nainye/RadiomicsRetrieval.

</details>


### [47] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: 本研究提出了一种利用强化学习优化SAM 2模型内存更新的方法，在视频跟踪任务中取得了显著优于现有方法的性能提升，证明了强化学习在处理跟踪挑战方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在处理干扰、遮挡和对象运动时存在的局限性，并探索SAM 2内存库的潜力，提出了一种新的优化内存更新的方法。

Method: 提出了一种基于强化学习的SAM 2内存更新优化方法，将内存控制视为一个序列决策问题，并训练一个独立的智能体来处理每个视频序列。

Result: 在过拟合设置下，所提出的方法相对于SAM 2取得了超过现有启发式方法三倍以上的相对改进。

Conclusion: 提出了一种新颖的基于强化学习的SAM 2内存更新优化方法，显著优于现有的手工设计方法，展示了内存库的巨大潜力，并强调了强化学习在视觉对象跟踪中的应用前景。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [48] [Image Translation with Kernel Prediction Networks for Semantic Segmentation](https://arxiv.org/abs/2507.08554)
*Cristina Mata,Michael S. Ryoo,Henrik Turbell*

Main category: cs.CV

TL;DR: DA-KPN是一种新的图像翻译方法，可以解决语义分割中的域间隙问题，并保证像素级语义匹配，从而提高性能。


<details>
  <summary>Details</summary>
Motivation: 由于真实世界数据的密集像素级标注难以获取，研究人员通常在大型合成数据集上进行训练。然而，这会产生域间隙问题。为了解决这个问题，研究人员使用了无监督图像翻译来生成更真实的训练数据。但现有的基于GAN的无监督图像翻译方法通过循环一致性强制执行像素级语义匹配，但不能保证语义匹配的准确性，这对于对噪声像素标签敏感的语义分割来说是一个问题。

Method: 提出了一种名为DA-KPN（Domain Adversarial Kernel Prediction Network）的新型图像翻译方法，该方法通过估计像素级输入变换参数来约束一个轻量级的翻译函数，并使用多尺度判别器来区分翻译和目标样本，以确保语义匹配。

Result: DA-KPN在Syn2Real基准测试中，在真实图像标签有限的情况下，比先前基于GAN的方法在语义分割方面表现更好，并在人脸解析方面取得了相当的性能。

Conclusion: DA-KPN通过估计像素级输入变换参数，并利用多尺度判别器区分翻译和目标样本，确保了合成标签和翻译之间的语义匹配，从而在真实世界语义分割的Syn2Real基准测试中优于现有的基于GAN的方法，并在人脸解析任务上取得了可比的性能。

Abstract: Semantic segmentation relies on many dense pixel-wise annotations to achieve
the best performance, but owing to the difficulty of obtaining accurate
annotations for real world data, practitioners train on large-scale synthetic
datasets. Unpaired image translation is one method used to address the ensuing
domain gap by generating more realistic training data in low-data regimes.
Current methods for unpaired image translation train generative adversarial
networks (GANs) to perform the translation and enforce pixel-level semantic
matching through cycle consistency. These methods do not guarantee that the
semantic matching holds, posing a problem for semantic segmentation where
performance is sensitive to noisy pixel labels. We propose a novel image
translation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that
guarantees semantic matching between the synthetic label and translation.
DA-KPN estimates pixel-wise input transformation parameters of a lightweight
and simple translation function. To ensure the pixel-wise transformation is
realistic, DA-KPN uses multi-scale discriminators to distinguish between
translated and target samples. We show DA-KPN outperforms previous GAN-based
methods on syn2real benchmarks for semantic segmentation with limited access to
real image labels and achieves comparable performance on face parsing.

</details>


### [49] [Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion](https://arxiv.org/abs/2507.08555)
*Enyu Liu,En Yu,Sijia Chen,Wenbing Tao*

Main category: cs.CV

TL;DR: DISC是一种新的双流范式，通过类别查询而非体素查询来增强3D语义场景补全，在SemanticKITTI和SSCBench-KITTI-360上实现了SOTA性能，甚至超越了多帧方法，并在实例分割上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义场景补全（SSC）方法主要关注细化体素级特征来构建3D场景，但将体素作为基本交互单元限制了类别级信息的利用，而类别级信息对于提高补全结果的精细度至关重要。

Method: 提出了一种名为DISC（Disentangling Instance and Scene Contexts）的新型双流范式，通过分离优化来增强实例和场景类别的学习。具体来说，使用区分性类别查询（包含类别特定的几何和语义先验）替换体素查询，并利用类的内在属性设计专门的解码模块，以促进目标交互和高效的类别级信息流。

Result: 实验结果表明，DISC在SemanticKITTI和SSCBench-KITTI-360基准上均取得了最先进（SOTA）的性能，mIoU得分分别为17.35和20.55。仅使用单帧输入，DISC就超越了多帧输入的SOTA方法，并且在实例类别性能上有了显著提升，在SemanticKITTI隐藏测试集上分别超越了单帧和多帧SOTA实例mIoU的17.9%和11.9%。

Conclusion: DISC在SemanticKITTI和SSCBench-KITTI-360基准上均取得了最先进（SOTA）的性能，mIoU得分分别为17.35和20.55。值得注意的是，DISC仅使用单帧输入就超越了多帧输入的SOTA方法，并且在实例类别性能上有了显著提升，在SemanticKITTI隐藏测试集上分别超越了单帧和多帧SOTA实例mIoU的17.9%和11.9%。

Abstract: 3D Semantic Scene Completion (SSC) has gained increasing attention due to its
pivotal role in 3D perception. Recent advancements have primarily focused on
refining voxel-level features to construct 3D scenes. However, treating voxels
as the basic interaction units inherently limits the utilization of class-level
information, which is proven critical for enhancing the granularity of
completion results. To address this, we propose \textbf{D}isentangling Instance
and Scene Contexts (DISC), a novel dual-stream paradigm that enhances learning
for both instance and scene categories through separated optimization.
Specifically, we replace voxel queries with discriminative class queries, which
incorporate class-specific geometric and semantic priors. Additionally, we
exploit the intrinsic properties of classes to design specialized decoding
modules, facilitating targeted interactions and efficient class-level
information flow. Experimental results demonstrate that DISC achieves
state-of-the-art (SOTA) performance on both SemanticKITTI and
SSCBench-KITTI-360 benchmarks, with mIoU scores of 17.35 and 20.55,
respectively. Remarkably, DISC even outperforms multi-frame SOTA methods using
only single-frame input and significantly improves instance category
performance, surpassing both single-frame and multi-frame SOTA instance mIoU by
17.9\% and 11.9\%, respectively, on the SemanticKITTI hidden test. The code is
available at https://github.com/Enyu-Liu/DISC.

</details>


### [50] [A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism](https://arxiv.org/abs/2507.08574)
*Mingda Zhang,Kaiwen Pan*

Main category: cs.CV

TL;DR: 提出了一种结合了3D MRI数据、临床文本描述和双向交互注意力机制的多模态融合框架，用于脑肿瘤分割。该方法在BraTS 2020数据集上取得了优于现有方法的性能，提高了分割准确性和边界清晰度。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种新颖的多模态融合框架用于脑肿瘤分割，通过整合空间-语言-视觉信息和双向交互注意力机制来提高分割准确性和边界清晰度。

Method: 提出了一种新颖的多模态融合框架，通过双向交互注意力机制整合空间-语言-视觉信息。该框架包含两个核心组件：多模态语义融合适配器（MSFA），通过分层语义解耦整合3D MRI数据和临床文本描述；以及双向交互视觉-语义注意力（BIVA），实现模态间的迭代信息交换。

Result: 在BraTS 2020数据集（包含369个来自多个机构的MRI扫描）上进行评估，所提出的方法在增强肿瘤、肿瘤核心和整个肿瘤区域的平均Dice系数为0.8505，95% Hausdorff距离为2.8256mm，优于SCAU-Net、CA-Net和3D U-Net等最先进的方法。消融研究证实了语义和空间模块对边界精度的关键贡献。

Conclusion: 多模态语义融合结合双向交互注意力显著提高了脑肿瘤分割性能，为将临床知识整合到医学图像分析中建立了新的范例。

Abstract: This study aims to develop a novel multi-modal fusion framework for brain
tumor segmentation that integrates spatial-language-vision information through
bidirectional interactive attention mechanisms to improve segmentation accuracy
and boundary delineation. Methods: We propose two core components: Multi-modal
Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text
descriptions through hierarchical semantic decoupling, and Bidirectional
Interactive Visual-semantic Attention (BIVA) enabling iterative information
exchange between modalities. The framework was evaluated on BraTS 2020 dataset
comprising 369 multi-institutional MRI scans. Results: The proposed method
achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of
2.8256mm across enhancing tumor, tumor core, and whole tumor regions,
outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D
U-Net. Ablation studies confirmed critical contributions of semantic and
spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion
combined with bidirectional interactive attention significantly enhances brain
tumor segmentation performance, establishing new paradigms for integrating
clinical knowledge into medical image analysis.

</details>


### [51] [BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis](https://arxiv.org/abs/2507.08607)
*Shuang Cui,Jinglin Xu,Yi Li,Xiongxin Tang,Jiangmeng Li,Jiahuan Zhou,Fanjiang Xu,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 我们提出了BayesTTA，一种用于持续时间测试时适应（CT-TTA）的贝叶斯适应框架，以解决由时间演变分布引起的现有方法的局限性。BayesTTA通过时间上一致的预测和动态对齐的表示来提高性能，并在广泛的基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有持续测试时适应（CTTA）方法通常围绕突然和严重分布转移而构建，忽略了时间连续性，导致了有限的内存缓存限制了远程分布建模（引起灾难性遗忘），基于熵的置信度在时间漂移下变得不可靠（加剧了误差累积），以及静态视觉表示与不断变化的输入不匹配。

Method: BayesTTA是一个贝叶斯适应框架，它强制执行时间上一致的预测并动态地对齐视觉表示。具体来说，BayesTTA在不存储原始数据的情况下，增量地估计了类别条件高斯混合分布，通过统计假设检验自适应地选择协方差结构，并使用高斯判别分析（GDA）进行校准推理。这些校准后的预测监督了归一化层的自步适应，确保了高效稳定的表示对齐。

Result: 在四个时间演变数据集上建立了全面的CT-TTA基准，并在十个标准TTA数据集上进一步评估了泛化能力。实验表明，BayesTTA在保持效率的同时，持续优于最先进的方法，实现了显著的收益。

Conclusion: BayesTTA在与最先进的方法相比，持续优于最先进的方法，在提高效率的同时实现了显著的收益。

Abstract: Vision-language models (VLMs) such as CLIP achieve strong zero-shot
recognition but degrade significantly under \textit{temporally evolving
distribution shifts} common in real-world scenarios (e.g., gradual illumination
or seasonal changes). Existing continual test-time adaptation (CTTA) methods
are typically built around sudden and severe distribution shifts and neglect
temporal continuity, leading to three core defects: limited memory cache
restricts long-range distribution modeling, causing catastrophic forgetting;
entropy-based confidence becomes unreliable under temporal drift, worsening
error accumulation; and static visual representations misalign with evolving
inputs. We formalize this practical problem as \textit{Continual-Temporal
Test-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over
time. To address it, we propose \textit{BayesTTA}, a Bayesian adaptation
framework that enforces temporally consistent predictions and dynamically
aligns visual representations. Specifically, BayesTTA incrementally estimates
class-conditional Gaussian mixture distributions without storing raw data,
adaptively selects covariance structures through statistical hypothesis
testing, and performs calibrated inference using Gaussian discriminant analysis
(GDA). These calibrated predictions supervise self-paced adaptation of
normalization layers, ensuring efficient and stable representation alignment.
We establish a comprehensive CT-TTA benchmark across four temporally evolving
datasets and further evaluate generalization on ten standard TTA datasets.
Extensive experiments show that BayesTTA consistently outperforms
state-of-the-art methods, achieving significant gains while maintaining
efficiency. Code is available at
\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.

</details>


### [52] [Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates](https://arxiv.org/abs/2507.08636)
*Natalia Bottaioli,Solène Tarride,Jérémy Anger,Seginus Mowlavi,Marina Gardella,Antoine Tadros,Gabriele Facciolo,Rafael Grompone von Gioi,Christopher Kermorvant,Jean-Michel Morel,Javier Preciozzi*

Main category: cs.CV

TL;DR: 本研究评估了文档注意力网络（DAN）在处理乌拉圭出生证明手写信息方面的能力，并比较了两种注释策略（归一化和外交）对不同类型字段（可标准化与不可标准化）提取信息的效果。


<details>
  <summary>Details</summary>
Motivation: 为了评估最近提出的文档注意力网络（DAN）在从乌拉圭出生证明（西班牙语手写）中提取关键值信息方面的性能，并研究两种自动转录手写文档的注释策略，以最少的训练数据和注释工作量对DAN进行微调。

Method: 本研究采用文档注意力网络（DAN）来提取乌拉圭出生证明上的关键值信息，并对两种注释策略进行了研究，以最少的训练数据和注释工作量对DAN进行微调。

Result: 实验结果表明，归一化标注在标准化字段（如出生日期和地点）方面更有效，而外交标注在处理姓名和姓氏等无法标准化的字段时表现更好。

Conclusion: 归一化标注对于出生日期和地点等可标准化字段更有效，而外交标注对于无法标准化的姓名和姓氏等字段效果更好。

Abstract: This study evaluates the recently proposed Document Attention Network (DAN)
for extracting key-value information from Uruguayan birth certificates,
handwritten in Spanish. We investigate two annotation strategies for
automatically transcribing handwritten documents, fine-tuning DAN with minimal
training data and annotation effort. Experiments were conducted on two datasets
containing the same images (201 scans of birth certificates written by more
than 15 different writers) but with different annotation methods. Our findings
indicate that normalized annotation is more effective for fields that can be
standardized, such as dates and places of birth, whereas diplomatic annotation
performs much better for fields containing names and surnames, which can not be
standardized.

</details>


### [53] [OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception](https://arxiv.org/abs/2507.08644)
*Junho Koh,Youngwoo Lee,Jungho Kim,Dongyoung Lee,Jun Won Choi*

Main category: cs.CV

TL;DR: OnlineBEV是一种利用递归结构和运动引导的BEV融合网络来处理BEV特征时态对齐的新型3D感知方法，在nuScenes基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角相机3D感知方法虽然可以通过结合多个相机帧的时态BEV特征来提升性能，但在融合大量图像帧时，性能增益受限于物体运动引起的BEV特征时态变化。

Method: 提出了一种名为OnlineBEV的新型时态3D感知方法，该方法使用递归结构将BEV特征随时间进行组合，从而在最小内存占用的情况下增加组合特征的有效数量。此外，还提出了一种运动引导的BEV融合网络（MBFNet），用于提取运动特征并动态地将历史BEV特征与当前特征对齐。最后，使用时态一致性学习损失来显式地强制进行时态特征对齐。

Result: OnlineBEV在nuScenes基准测试中取得了显著的性能提升，相比于当前最佳方法SOLOFusion，其在nuScenes测试集上达到了63.9%的NDS，实现了仅使用相机的3D目标检测任务的最先进性能。

Conclusion: 所提出的OnlineBEV方法在nuScenes基准测试中取得了显著的性能提升，其在nuScenes测试集上达到了63.9%的NDS，在仅使用相机的3D目标检测任务中创造了最先进的性能。

Abstract: Multi-view camera-based 3D perception can be conducted using bird's eye view
(BEV) features obtained through perspective view-to-BEV transformations.
Several studies have shown that the performance of these 3D perception methods
can be further enhanced by combining sequential BEV features obtained from
multiple camera frames. However, even after compensating for the ego-motion of
an autonomous agent, the performance gain from temporal aggregation is limited
when combining a large number of image frames. This limitation arises due to
dynamic changes in BEV features over time caused by object motion. In this
paper, we introduce a novel temporal 3D perception method called OnlineBEV,
which combines BEV features over time using a recurrent structure. This
structure increases the effective number of combined features with minimal
memory usage. However, it is critical to spatially align the features over time
to maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion
Network (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion
features from consecutive BEV frames and dynamically aligns historical BEV
features with current ones using these motion features. To enforce temporal
feature alignment explicitly, we use Temporal Consistency Learning Loss, which
captures discrepancies between historical and target BEV features. Experiments
conducted on the nuScenes benchmark demonstrate that OnlineBEV achieves
significant performance gains over the current best method, SOLOFusion.
OnlineBEV achieves 63.9% NDS on the nuScenes test set, recording
state-of-the-art performance in the camera-only 3D object detection task.

</details>


### [54] [DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images](https://arxiv.org/abs/2507.08648)
*Haoran Sun,Haoyu Bian,Shaoning Zeng,Yunbo Rao,Xu Xu,Lin Mei,Jianping Gou*

Main category: cs.CV

TL;DR: DatasetAgent是一个利用多模态大语言模型和图像优化工具的多智能体系统，可以自动从真实图像构建高质量数据集，用于训练视觉模型。


<details>
  <summary>Details</summary>
Motivation: 解决了传统图像数据集构建依赖耗时且低效的手动收集和标注方法的问题，并认识到真实世界数据相较于人工智能生成数据的价值。

Method: 提出了一种名为DatasetAgent的多智能体协作系统，该系统包含四个配备多模态大语言模型的智能体和一套图像优化工具包，用于从真实世界图像自动构建数据集。

Result: 通过在多个开源数据集上进行扩展现有数据集和从头创建新数据集的两种实验，证明了DatasetAgent在训练各种视觉模型方面的有效性。

Conclusion: DatasetAgent能够根据用户指定的要求，通过协调四个配备多模态大语言模型的智能体以及一套图像优化工具包，来自动构建高质量的图像数据集，并成功应用于图像分类、目标检测和图像分割等多种视觉模型的训练。

Abstract: Common knowledge indicates that the process of constructing image datasets
usually depends on the time-intensive and inefficient method of manual
collection and annotation. Large models offer a solution via data generation.
Nonetheless, real-world data are obviously more valuable comparing to
artificially intelligence generated data, particularly in constructing image
datasets. For this reason, we propose a novel method for auto-constructing
datasets from real-world images by a multiagent collaborative system, named as
DatasetAgent. By coordinating four different agents equipped with Multi-modal
Large Language Models (MLLMs), as well as a tool package for image
optimization, DatasetAgent is able to construct high-quality image datasets
according to user-specified requirements. In particular, two types of
experiments are conducted, including expanding existing datasets and creating
new ones from scratch, on a variety of open-source datasets. In both cases,
multiple image datasets constructed by DatasetAgent are used to train various
vision models for image classification, object detection, and image
segmentation.

</details>


### [55] [Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model](https://arxiv.org/abs/2507.08655)
*Zach Eidex,Mojtaba Safari,Tonghe Wang,Vanessa Wildman,David S. Yu,Hui Mao,Erik Middlebrooks,Aparna Kesewala,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出了一种名为7T-Restormer的Transformer模型，能够从低场强（1.5T/3T）MRI合成高场强（7T）T1图，在图像质量和参数效率上优于现有方法，有助于普及7T MRI的临床应用。


<details>
  <summary>Details</summary>
Motivation: 7T MRI提供了比标准临床场强（1.5T，3T）更好的分辨率和对比度。然而，7T扫描仪成本高、稀少，并带来磁敏感伪影等额外挑战。本研究旨在通过合成技术，克服这些限制。

Method: 提出了一种高效的基于Transformer的模型（7T-Restormer），用于从常规的1.5T或3T T1加权（T1W）图像合成7T质量的T1图。该模型在35个1.5T和108个3T T1w MRI与相应的7T T1图配对的MS患者数据上进行了验证。

Result: 7T-Restormer模型在1.5T输入上实现了26.0 +/- 4.6 dB的PSNR、0.861 +/- 0.072的SSIM和0.019 +/- 0.011的NMSE，在3T输入上实现了25.9 +/- 4.9 dB的PSNR和0.866 +/- 0.077的SSIM。与ResShift和ResViT模型相比，该模型在减少NMSE方面表现出显著优势，尤其是在混合1.5T+3T数据集上训练时效果更佳。

Conclusion: 提出了一种从1.5T和3T T1W扫描预测定量7T MP2RAGE图的新方法，其质量优于现有最先进的方法。该方法使7T MRI的优势更容易在标准临床工作流程中使用。

Abstract: Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over
standard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,
scarce, and introduce additional challenges such as susceptibility artifacts.
We propose an efficient transformer-based model (7T-Restormer) to synthesize
7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:
Our model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding
7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128
slices) were randomly divided into 105 (25; 80) training cases (19,204 slices),
19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145
slices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,
respectively. The synthetic 7T T1 maps were compared against the ResViT and
ResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-
4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,
and 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using
10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter
ResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter
ResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T
(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus
was superior to single-field strategies. Restricting the model to 1.5T
increased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely
on 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We
propose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T
and 3T T1W scans with higher quality than existing state-of-the-art methods.
Our approach makes the benefits of 7T MRI more accessible to standard clinical
workflows.

</details>


### [56] [ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way](https://arxiv.org/abs/2507.08679)
*Rajarshi Roy,Devleena Das,Ankesh Banerjee,Arjya Bhattacharjee,Kousik Dasgupta,Subarna Tripathi*

Main category: cs.CV

TL;DR: ByDeWay框架通过深度感知提示（LDP）增强了MLLMs的空间推理和接地能力，无需训练即可减少幻觉并提高性能。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决多模态大语言模型（MLLMs）在空间推理和图像接地方面存在的不足，特别是幻觉现象，并提出一种无需训练即可提升其性能的框架。

Method: 本研究提出了一种名为ByDeWay的训练无关框架，并采用了一种名为Layered-Depth-Based Prompting (LDP)的新颖提示策略。该策略通过单目深度估计将场景分割为最近、中间和最远三个层级，并利用带有接地的视觉-语言模型生成特定区域的字幕。这些结构化、深度感知的字幕被添加到图像-问题提示中，以增强空间上下文，从而指导MLLMs产生更具接地性、更少幻觉的响应。

Result: 在POPE和GQA基准测试中，ByDeWay框架在多种MLLMs上均展现出了一致的性能提升，证明了其在零训练设置下通过深度感知提示有效改善模型表现的能力。

Conclusion: ByDeWay框架通过其新颖的Layered-Depth-Based Prompting (LDP)策略，在不修改模型参数的情况下，显著提高了多模态大语言模型（MLLMs）的空间推理和接地能力。实验证明，该方法在POPE和GQA等基准测试中对多种MLLMs均有效，验证了在零训练设置下，深度感知提示的有效性。

Abstract: We introduce ByDeWay, a training-free framework designed to enhance the
performance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel
prompting strategy called Layered-Depth-Based Prompting (LDP), which improves
spatial reasoning and grounding without modifying any model parameters. It
segments the scene into closest, mid-range, and farthest layers using monocular
depth estimation, then generates region-specific captions with a grounded
vision-language model. These structured, depth-aware captions are appended to
the image-question prompt, enriching it with spatial context. This guides MLLMs
to produce more grounded and less hallucinated responses. Our method is
lightweight, modular, and compatible with black-box MLLMs. Experiments on
hallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show
consistent improvements across multiple MLLMs, validating the effectiveness of
depth-aware prompting in a zero-training setting.

</details>


### [57] [MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing](https://arxiv.org/abs/2507.08683)
*Debashis Gupta,Aditi Golder,Rongkhun Zhu,Kangning Cui,Wei Tang,Fan Yang,Ovidiu Csillik,Sarra Alaqahtani,V. Paul Pauca*

Main category: cs.CV

TL;DR: MoSAiC是一个用于多模态卫星图像的框架，通过结合模态内和模态间的对比学习来改进表示学习，尤其是在具有挑战性的ESO场景中。


<details>
  <summary>Details</summary>
Motivation: 对比学习（CL）在学习可迁移表示方面非常强大，尤其适用于地球系统观测（ESO），因为卫星数据具有多种模态且能提供同一地理区域的对齐视图。然而，ESO面临着类别相似性高、场景杂乱和边界模糊等挑战，尤其是在标签稀疏、多标签的情况下，给表示学习带来困难。现有的CL框架要么侧重于模态内的自监督，要么缺乏跨模态的多标签对齐和语义精度机制。

Method: 提出了一种名为MoSAiC的统一框架，该框架通过多标签监督对比损失联合优化了模态内和模态间的对比学习，专门用于多模态卫星图像。

Result: MoSAiC能够实现更精细的语义分离和更鲁棒的表示学习，即使在光谱相似和空间复杂的类别中也是如此。

Conclusion: MoSAiC框架在BigEarthNet V2.0和Sent12MS两个基准数据集上进行了实验，结果表明该框架在准确性、聚类相干性和低标签/高类别重叠场景下的泛化能力方面，始终优于全监督和自监督基线。

Abstract: Contrastive learning (CL) has emerged as a powerful paradigm for learning
transferable representations without the reliance on large labeled datasets.
Its ability to capture intrinsic similarities and differences among data
samples has led to state-of-the-art results in computer vision tasks. These
strengths make CL particularly well-suited for Earth System Observation (ESO),
where diverse satellite modalities such as optical and SAR imagery offer
naturally aligned views of the same geospatial regions. However, ESO presents
unique challenges, including high inter-class similarity, scene clutter, and
ambiguous boundaries, which complicate representation learning -- especially in
low-label, multi-label settings. Existing CL frameworks often focus on
intra-modality self-supervision or lack mechanisms for multi-label alignment
and semantic precision across modalities. In this work, we introduce MoSAiC, a
unified framework that jointly optimizes intra- and inter-modality contrastive
learning with a multi-label supervised contrastive loss. Designed specifically
for multi-modal satellite imagery, MoSAiC enables finer semantic
disentanglement and more robust representation learning across spectrally
similar and spatially complex classes. Experiments on two benchmark datasets,
BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both
fully supervised and self-supervised baselines in terms of accuracy, cluster
coherence, and generalization in low-label and high-class-overlap scenarios.

</details>


### [58] [An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan](https://arxiv.org/abs/2507.08690)
*Mengyuan Liu,Jeongkyu Lee*

Main category: cs.CV

TL;DR: 这项研究提出了一种无需训练的关键点跟踪方法来进行肌肉分割，其性能与最先进的CNN方法相当，但计算成本更低，可解释性更强。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于卷积神经网络（CNN）的自动分割方法计算成本高、需要大量训练数据、对小肌肉分割精度低以及泛化性差、可解释性不足等问题。

Method: 本研究提出了一种基于关键点跟踪的无训练分割方法，该方法集成了关键点选择和Lucas-Kanade光流。

Result: 该方法实现了0.6至0.7的平均Dice相似系数（DSC），与最先进的基于CNN的模型相当，同时显著降低了计算成本并提高了可解释性。

Conclusion: 所提出的基于关键点跟踪的无训练分割方法，通过集成关键点选择和Lucas-Kanade光流，在计算需求和可解释性方面提供了可扩展的框架，可作为临床和研究应用中肌肉分割的稳健且可解释的替代方案。

Abstract: Magnetic resonance imaging (MRI) enables non-invasive, high-resolution
analysis of muscle structures. However, automated segmentation remains limited
by high computational costs, reliance on large training datasets, and reduced
accuracy in segmenting smaller muscles. Convolutional neural network
(CNN)-based methods, while powerful, often suffer from substantial
computational overhead, limited generalizability, and poor interpretability
across diverse populations. This study proposes a training-free segmentation
approach based on keypoint tracking, which integrates keypoint selection with
Lucas-Kanade optical flow. The proposed method achieves a mean Dice similarity
coefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection
strategy, performing comparably to state-of-the-art CNN-based models while
substantially reducing computational demands and enhancing interpretability.
This scalable framework presents a robust and explainable alternative for
muscle segmentation in clinical and research applications.

</details>


### [59] [L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training](https://arxiv.org/abs/2507.08710)
*Li Li,Yingzhe Peng,Xu Yang,Ruoxi Cheng,Haiyang Xu,Ming Yan,Fei Huang*

Main category: cs.CV

TL;DR: 提出L-CLIPScore，一种高效的字幕评估指标，它使用轻量级CLIP模型（L-CLIP），通过压缩和蒸馏技术实现。L-CLIPScore在评估和训练方面都表现良好，但单独使用可能导致训练失败，需要与n-gram指标结合。


<details>
  <summary>Details</summary>
Motivation: 提出一种高效的字幕评估指标，用于评估字幕质量和训练字幕模型，同时减少计算资源和运行时间。

Method: L-CLIPScore的计算基于轻量级CLIP（L-CLIP），该模型采用了权重复用和矩阵分解技术来减少编码器和词嵌入矩阵的参数，并通过多模态相似性调节器（SR）损失进行蒸馏，以增强视觉-语言对齐能力。

Result: L-CLIP在保持与原始CLIP相当的多模态对齐能力的同时，所需的计算资源和运行时间更少。实验验证了L-CLIPScore作为评估指标的效率和有效性。研究还发现，在训练模型时，L-CLIPScore应与基于n-gram的指标混合使用。

Conclusion: L-CLIPScore是一种新颖的基于嵌入的字幕评估指标，它通过轻量级CLIP（L-CLIP）计算得出。L-CLIP通过权重复用和矩阵分解技术压缩了编码器和词嵌入矩阵的参数，并设计了一种新颖的多模态相似性调节器（SR）损失来传递更多视觉-语言对齐知识。实验证明，L-CLIPScore在评估字幕质量和训练字幕模型方面都具有效率和有效性。

Abstract: We propose a novel embedding-based captioning metric termed as L-CLIPScore
that can be used for efficiently evaluating caption quality and training
captioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),
which is a dual-encoder architecture compressed and distilled from CLIP. To
compress, we apply two powerful techniques which are weight multiplexing and
matrix decomposition for reducing the parameters of encoders and word embedding
matrix, respectively. To distill, we design a novel multi-modal Similarity
Regulator (SR) loss to transfer more vision-language alignment knowledge.
Specifically, SR loss amplifies the multi-modal embedding similarity if the
given image-text pair is matched and diminishes the similarity if the pair is
non-matched. By compressing and distilling by this novel SR loss, our L-CLIP
achieves comparable multi-modal alignment ability to the original CLIP while it
requires fewer computation resources and running time. We carry out exhaustive
experiments to validate the efficiency and effectiveness of L-CLIPScore when
using it as the judge to evaluate caption quality. We also discover that when
using L-CLIPScore as the supervisor to train the captioning model, it should be
mixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore
only will cause fail training.

</details>


### [60] [SGPMIL: Sparse Gaussian Process Multiple Instance Learning](https://arxiv.org/abs/2507.08711)
*Andreas Lolos,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz,Aris Moustakas*

Main category: cs.CV

TL;DR: SGPMIL 是一种新的概率注意力 MIL 框架，利用稀疏高斯过程来量化数字病理学图像中实例相关性的不确定性，提高了预测的可靠性和可解释性，并且通过特征缩放提高了效率。


<details>
  <summary>Details</summary>
Motivation: 在数字病理学等领域，通常只有粗略的、袋级别的标签可用，而没有实例级别的注释。现有的确定性注意力 MIL 方法虽然在袋级别性能上表现出色，但常常忽略实例相关性中固有的不确定性。

Method: SGPMIL 是一种新的概率注意力 MIL 框架，它以稀疏高斯过程 (SGP) 为基础，通过学习注意力分数的后验分布来实现原则性的不确定性估计。

Result: SGPMIL 实现了更可靠和校准的实例相关性图，在保留有竞争力的袋级别性能的同时，显著提高了不确定性下实例级预测的质量和可解释性。

Conclusion: SGPMIL 通过在 SGP 预测均值函数中引入特征缩放，实现了更快的训练、更高的效率和更强的实例级性能。在多个成熟的数字病理学数据集上进行的广泛实验强调了我们的方法在袋级别和实例级别评估中的有效性。

Abstract: Multiple Instance Learning (MIL) offers a natural solution for settings where
only coarse, bag-level labels are available, without having access to
instance-level annotations. This is usually the case in digital pathology,
which consists of gigapixel sized images. While deterministic attention-based
MIL approaches achieve strong bag-level performance, they often overlook the
uncertainty inherent in instance relevance. In this paper, we address the lack
of uncertainty quantification in instance-level attention scores by introducing
\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in
Sparse Gaussian Processes (SGP). By learning a posterior distribution over
attention scores, SGPMIL enables principled uncertainty estimation, resulting
in more reliable and calibrated instance relevance maps. Our approach not only
preserves competitive bag-level performance but also significantly improves the
quality and interpretability of instance-level predictions under uncertainty.
SGPMIL extends prior work by introducing feature scaling in the SGP predictive
mean function, leading to faster training, improved efficiency, and enhanced
instance-level performance. Extensive experiments on multiple well-established
digital pathology datasets highlight the effectiveness of our approach across
both bag- and instance-level evaluations. Our code will be made publicly
available.

</details>


### [61] [Unreal is all you need: Multimodal ISAC Data Simulation with Only One Engine](https://arxiv.org/abs/2507.08716)
*Kongwu Huang,Shiyi Mu,Jun Jiang,Yuan Gao,Shugong Xu*

Main category: cs.CV

TL;DR: Great-X是一个集成自动驾驶工具的单引擎多模态数据孪生平台，用于ISAC研究。它支持CSI、RGB、雷达和LiDAR数据的同步模拟，并已用于创建Great-MSD数据集和开发基于CSI的UAV 3D定位算法。


<details>
  <summary>Details</summary>
Motivation: 为了探索Scaling laws在集成感知和通信（ISAC）研究中的潜力，提出Great-X平台。

Method: 提出了一种名为Great-X的单引擎多模态数据孪生平台，该平台在Unreal Engine中重建了Sionna的光线追踪计算，并与自动驾驶工具深度集成，实现了CSI、RGB、雷达和LiDAR等多种模态数据的同步模拟。基于该平台构建了名为Great-MSD的大规模低空无人机多模态数据开放数据集，并提出了一种基于CSI的无人机3D定位算法。

Result: 成功构建了一个开源、大规模、低空UAV多模态数据合成数据集Great-MSD，并提出了一个基线CSI的UAV 3D定位算法，证明了其在不同CSI模拟引擎上的可行性和泛化能力。

Conclusion: Great-X平台能够有效地模拟和同步多种模态数据（包括CSI、RGB、雷达和LiDAR），并且能够为集成感知和通信（ISAC）研究提供一个可扩展的解决方案。该平台支持开源的Great-MSD数据集和基于CSI的无人机3D定位算法，证明了其在不同CSI模拟引擎上的可行性和泛化能力。

Abstract: Scaling laws have achieved success in LLM and foundation models. To explore
their potential in ISAC research, we propose Great-X. This single-engine
multimodal data twin platform reconstructs the ray-tracing computation of
Sionna within Unreal Engine and is deeply integrated with autonomous driving
tools. This enables efficient and synchronized simulation of multimodal data,
including CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an
open-source, large-scale, low-altitude UAV multimodal synaesthesia dataset
named Great-MSD, and propose a baseline CSI-based UAV 3D localization
algorithm, demonstrating its feasibility and generalizability across different
CSI simulation engines. The related code and dataset are publicly available at:
https://github.com/hkw-xg/Great-MCD.

</details>


### [62] [RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking](https://arxiv.org/abs/2507.08729)
*Yuqiang Lin,Sam Lockyer,Mingxuan Sui,Li Gan,Florian Stanek,Markus Zarbock,Wenbin Li,Adrian Evans,Nic Zhang*

Main category: cs.CV

TL;DR: RoundaboutHD：一个用于真实世界多摄像头车辆跟踪的高分辨率数据集，解决了现有数据集的局限性，并提供了基线结果和评估代码。


<details>
  <summary>Details</summary>
Motivation: 填补现有公开数据集在真实世界多摄像头车辆跟踪场景中的不足，如场景过于简单、分辨率低、条件不够多样化等问题，以缩小学术研究与实际应用之间的差距。

Method: 创建了一个名为RoundaboutHD的高分辨率多摄像头车辆跟踪数据集，包含40分钟的4K视频，记录了512个独特的车辆身份，并标注了跨摄像头关联数据。数据集包含多种子集，支持目标检测、单摄像头跟踪和基于图像的车辆重识别等任务，并提供了车辆模型和摄像头几何信息。

Result: 提供了包含目标检测、单摄像头跟踪、基于图像的车辆重识别和多摄像头跟踪的基线结果。数据集和评估代码已公开。

Conclusion: 该研究提出了RoundaboutHD数据集，解决了现有公开数据集在真实世界多摄像头车辆跟踪（MCVT）场景中的局限性，为智能城市应用提供了更可靠的基准。

Abstract: The multi-camera vehicle tracking (MCVT) framework holds significant
potential for smart city applications, including anomaly detection, traffic
density estimation, and suspect vehicle tracking. However, current publicly
available datasets exhibit limitations, such as overly simplistic scenarios,
low-resolution footage, and insufficiently diverse conditions, creating a
considerable gap between academic research and real-world scenario. To fill
this gap, we introduce RoundaboutHD, a comprehensive, high-resolution
multi-camera vehicle tracking benchmark dataset specifically designed to
represent real-world roundabout scenarios. RoundaboutHD provides a total of 40
minutes of labelled video footage captured by four non-overlapping,
high-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle
identities are annotated across different camera views, offering rich
cross-camera association data. RoundaboutHD offers temporal consistency video
footage and enhanced challenges, including increased occlusions and nonlinear
movement inside the roundabout. In addition to the full MCVT dataset, several
subsets are also available for object detection, single camera tracking, and
image-based vehicle re-identification (ReID) tasks. Vehicle model information
and camera modelling/ geometry information are also included to support further
analysis. We provide baseline results for vehicle detection, single-camera
tracking, image-based vehicle re-identification, and multi-camera tracking. The
dataset and the evaluation code are publicly available at:
https://github.com/siri-rouser/RoundaboutHD.git

</details>


### [63] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS是一个神经框架，通过预测屏幕帧来模拟GUI，能够渲染逼真的GUI序列并捕捉鼠标交互，但键盘交互仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 介绍一个直接预测屏幕帧以响应用户输入（如鼠标移动、点击和键盘事件）的神经框架，以模拟操作系统的图形用户界面（GUI）。

Method: NeuralOS框架结合了循环神经网络（RNN）来跟踪计算机状态，以及一个基于扩散的神经网络渲染器来生成屏幕图像。该模型在大规模的Ubuntu XFCE录制数据集上进行训练，该数据集包括随机生成的交互和由AI代理生成的真实交互。

Result: 实验表明，NeuralOS成功渲染了逼真的GUI序列，准确捕捉了鼠标交互，并可靠地预测了应用程序启动等状态转换。然而，精确模拟细粒度的键盘交互仍然是一个挑战。

Conclusion: NeuralOS是一个将RNN和基于扩散的神经渲染器相结合的神经框架，可以直接预测屏幕帧以响应用户输入，成功渲染了逼真的GUI序列，准确捕捉了鼠标交互，并可靠地预测了应用程序启动等状态转换。虽然精确建模细粒度的键盘交互仍然具有挑战性，但NeuralOS为创建完全自适应、生成式神经接口的未来人机交互系统迈出了重要一步。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


### [64] [Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study](https://arxiv.org/abs/2507.08735)
*Anna Rosenberg,John Kennedy,Zohar Keidar,Yehoshua Y. Zeevi,Guy Gilboa*

Main category: cs.CV

TL;DR: Ensembles of STV features outperform deep learning and Radiomics for predicting bone metastases from CT scans by analyzing texture at different scales.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the common problem of insufficient training data in computer vision and proposes using ensembles of weak learners based on spectral total-variation (STV) features as a solution. The motivation stems from the theoretical advantage of using lowly correlated weak learners in ensemble learning, and the empirical observation that STV features in two dimensions are lowly correlated.

Method: Ensembles of weak learners based on spectral total-variation (STV) features are proposed to address the lack of sufficient training data in computer vision. The features are related to nonlinear eigenfunctions of the total-variation subgradient and characterize textures at various scales. The study uses these features to design ensembles and evaluates their effectiveness on a medical imaging problem.

Result: STV learners achieved the best performance with an AUC of 0.87, outperforming deep-learning methods (AUC=0.75) and Radiomics features (AUC=0.79) in predicting high uptake in PET for skeletal metastases using CT data. Fine STV scales in CT images were found to be particularly indicative of high uptake in PET.

Conclusion: STV learners outperform deep learning and Radiomics features in predicting high uptake in PET for skeletal metastases based on CT data, with STV learners achieving an AUC of 0.87.

Abstract: Solving computer vision problems through machine learning, one often
encounters lack of sufficient training data. To mitigate this we propose the
use of ensembles of weak learners based on spectral total-variation (STV)
features (Gilboa 2014). The features are related to nonlinear eigenfunctions of
the total-variation subgradient and can characterize well textures at various
scales. It was shown (Burger et-al 2016) that, in the one-dimensional case,
orthogonal features are generated, whereas in two-dimensions the features are
empirically lowly correlated. Ensemble learning theory advocates the use of
lowly correlated weak learners. We thus propose here to design ensembles using
learners based on STV features. To show the effectiveness of this paradigm we
examine a hard real-world medical imaging problem: the predictive value of
computed tomography (CT) data for high uptake in positron emission tomography
(PET) for patients suspected of skeletal metastases. The database consists of
457 scans with 1524 unique pairs of registered CT and PET slices. Our approach
is compared to deep-learning methods and to Radiomics features, showing STV
learners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and
Radiomics (AUC=0.79). We observe that fine STV scales in CT images are
especially indicative for the presence of high uptake in PET.

</details>


### [65] [HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer](https://arxiv.org/abs/2507.08741)
*Tianlong Ai,Tianzhu Liu,Haochen Jiang,Yanfeng Gu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hierarchical land cover and land use (LCLU) classification aims to assign
pixel-wise labels with multiple levels of semantic granularity to remote
sensing (RS) imagery. However, existing deep learning-based methods face two
major challenges: 1) They predominantly adopt a flat classification paradigm,
which limits their ability to generate end-to-end multi-granularity
hierarchical predictions aligned with tree-structured hierarchies used in
practice. 2) Most cross-domain studies focus on performance degradation caused
by sensor or scene variations, with limited attention to transferring LCLU
models to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop
classification). These limitations hinder the flexibility and generalization of
LCLU models in practical applications. To address these challenges, we propose
HieraRS, a novel hierarchical interpretation paradigm that enables
multi-granularity predictions and supports the efficient transfer of LCLU
models to cross-domain tasks with heterogeneous tree-structured hierarchies. We
introduce the Bidirectional Hierarchical Consistency Constraint Mechanism
(BHCCM), which can be seamlessly integrated into mainstream flat classification
models to generate hierarchical predictions, while improving both semantic
consistency and classification accuracy. Furthermore, we present TransLU, a
dual-branch cross-domain transfer framework comprising two key components:
Cross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment
(CDSA). TransLU supports dynamic category expansion and facilitates the
effective adaptation of LCLU models to heterogeneous hierarchies. In addition,
we construct MM-5B, a large-scale multi-modal hierarchical land use dataset
featuring pixel-wise annotations. The code and MM-5B dataset will be released
at: https://github.com/AI-Tianlong/HieraRS.

</details>


### [66] [Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection](https://arxiv.org/abs/2507.08743)
*Rei Tamaru,Pei Li,Bin Ran*

Main category: cs.CV

TL;DR: 该研究提出了一种名为Geo-ORBIT的统一框架，用于构建交通系统的数字孪生。该框架通过GeoLane、Meta-GeoLane和FedMeta-GeoLane技术，实现了高效、可扩展且注重隐私的车道线检测和数字孪生同步，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的交通系统数字孪生方法依赖于静态地图或昂贵的传感器，限制了可扩展性和适应性。此外，大规模数字孪生在隐私、通信和计算效率方面面临挑战。因此，需要一种能够克服这些限制的统一框架。

Method: Geo-ORBIT框架整合了实时车道线检测、数字孪生同步以及联邦元学习。其核心是GeoLane，一个利用路侧摄像头和车辆轨迹数据学习车道几何的轻量级模型。Meta-GeoLane在此基础上进行个性化检测参数学习，而FedMeta-GeoLane则通过联邦学习策略实现跨路侧部署的可扩展和隐私保护的自适应。该系统与CARLA和SUMO集成，用于创建高保真数字孪生，实时渲染高速公路场景并捕捉交通流。

Result: 在各种城市场景的大规模实验表明，FedMeta-GeoLane的性能优于基线和元学习方法，实现了更低的几何误差和更强的泛化能力，同时大大降低了通信开销。

Conclusion: Geo-ORBIT框架为数字孪生提供了灵活、上下文感知的交通基础设施建模基础，FedMeta-GeoLane在各种城市场景下均优于基线和元学习方法，在几何误差和泛化到未知地点方面表现更佳，同时显著降低了通信开销。

Abstract: Digital Twins (DT) have the potential to transform traffic management and
operations by creating dynamic, virtual representations of transportation
systems that sense conditions, analyze operations, and support decision-making.
A key component for DT of the transportation system is dynamic roadway geometry
sensing. However, existing approaches often rely on static maps or costly
sensors, limiting scalability and adaptability. Additionally, large-scale DTs
that collect and analyze data from multiple sources face challenges in privacy,
communication, and computational efficiency. To address these challenges, we
introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated
Twin), a unified framework that combines real-time lane detection, DT
synchronization, and federated meta-learning. At the core of Geo-ORBIT is
GeoLane, a lightweight lane detection model that learns lane geometries from
vehicle trajectory data using roadside cameras. We extend this model through
Meta-GeoLane, which learns to personalize detection parameters for local
entities, and FedMeta-GeoLane, a federated learning strategy that ensures
scalable and privacy-preserving adaptation across roadside deployments. Our
system is integrated with CARLA and SUMO to create a high-fidelity DT that
renders highway scenarios and captures traffic flows in real-time. Extensive
experiments across diverse urban scenes show that FedMeta-GeoLane consistently
outperforms baseline and meta-learning approaches, achieving lower geometric
error and stronger generalization to unseen locations while drastically
reducing communication overhead. This work lays the foundation for flexible,
context-aware infrastructure modeling in DTs. The framework is publicly
available at https://github.com/raynbowy23/FedMeta-GeoLane.git.

</details>


### [67] [Compress Any Segment Anything Model (SAM)](https://arxiv.org/abs/2507.08765)
*Juntong Fan,Zhiwei Hao,Jianqiang Shen,Shang-Ling Jui,Yi Zhang,Jing-Xiao Liao,Feng-Lei Fan*

Main category: cs.CV

TL;DR: Birkhoff是一种无需数据的SAM压缩算法，可在60秒内实现高压缩率和快速推理，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于SAM及其变体在零样本分割方面表现出色，在医疗保健和智能制造等领域得到广泛应用，因此有效压缩SAM已成为日益迫切的实际需求。

Method: Birkhoff提出了一种名为“超压缩”的新型压缩算法，通过寻找密集轨迹将高维参数向量转换为低维标量。此外，还设计了专门的线性层算子“HyperLinear”来融合解压缩和矩阵乘法，从而显著加速压缩后SAM的推理。

Result: 在COCO、LVIS和SA-1B数据集上的18个SAM上进行的广泛实验表明，Birkhoff在压缩时间、压缩率、压缩后性能和推理速度方面表现一致且具有竞争力。例如，Birkhoff在SAM2-B上实现了5.17倍的压缩率，性能下降不到1%，且无需任何微调数据。此外，所有模型的压缩完成时间均在60秒以内。

Conclusion: Birkhoff是一种新颖的、无需数据的SAM压缩算法，在压缩时间、压缩率、压缩后性能和推理速度方面表现出色，并且部署灵活、模型尺寸紧凑。

Abstract: Due to the excellent performance in yielding high-quality, zero-shot
segmentation, Segment Anything Model (SAM) and its variants have been widely
applied in diverse scenarios such as healthcare and intelligent manufacturing.
Therefore, effectively compressing SAMs has become an increasingly pressing
practical need. In this study, we propose Birkhoff, a novel data-free
compression algorithm for SAM and its variants. Unlike quantization, pruning,
distillation, and other compression methods, Birkhoff embodies versatility
across model types, agility in deployment, faithfulness to the original model,
and compactness in model size. Specifically, Birkhoff introduces a novel
compression algorithm: Hyper-Compression, whose core principle is to find a
dense trajectory to turn a high-dimensional parameter vector into a
low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer
operator, HyperLinear, to fuse decompression and matrix multiplication to
significantly accelerate inference of the compressed SAMs. Extensive
experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff
performs consistently and competitively in compression time, compression ratio,
post-compression performance, and inference speed. For example, Birkhoff can
achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance
drop without using any fine-tuning data. Moreover, the compression is finished
within 60 seconds for all models.

</details>


### [68] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

TL;DR: 混合模型结合CNN和Hopfield网络在MNIST数据集上手写数字分类准确率达到99.2%.


<details>
  <summary>Details</summary>
Motivation: 为了提高手写数字分类的鲁棒性，能够处理类内变化（如不同的书写风格），并提供一个可解释的决策过程。

Method: 提出了一种混合模型，结合卷积神经网络（CNN）提取高维特征，并使用K-means聚类生成类原型。随后，利用多稳态Hopfield网络在能量函数中最小化能量来实现分类，该能量函数平衡了特征相似性和类别分配。

Result: 在MNIST数据集上，通过优化CNN架构和稳态数量，模型实现了99.2%的测试准确率，证明了其在图像分类任务中的有效性。

Conclusion: 该模型通过深度特征提取和充分的原型覆盖实现了高精度手写数字分类，在MNIST数据集上达到了99.2%的准确率，并为基于能量的决策过程提供了可解释性框架。

Abstract: This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>


### [69] [From One to More: Contextual Part Latents for 3D Generation](https://arxiv.org/abs/2507.08772)
*Shaocong Dong,Lihe Ding,Xiao Chen,Yaokun Li,Yuxin Wang,Yucheng Wang,Qi Wang,Jaehyeok Kim,Chenjian Gao,Zhanpeng Huang,Zibin Wang,Tianfan Xue,Dan Xu*

Main category: cs.CV

TL;DR: CoPart是一种新颖的3D生成框架，通过将3D对象分解为部件，实现了对复杂多部件几何形状的精确控制和生成。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有3D生成方法在处理多部件几何、部件独立性与相互关系以及全局条件控制方面的局限性。

Method: 提出了一种名为CoPart的部件感知扩散框架，该框架将3D对象分解为上下文部件潜在表示，以实现连贯的多部件生成，并通过互导策略对预训练的扩散模型进行微调以进行联合部件潜在去噪。

Result: CoPart能够有效处理复杂的多部件几何形状，并支持部件级别的精细控制，在部件编辑、铰接对象生成和场景组合方面取得了优于现有方法的性能。

Conclusion: CoPart在部件级编辑、铰接对象生成和场景组合方面展现出前所未有的可控性。

Abstract: Recent advances in 3D generation have transitioned from multi-view 2D
rendering approaches to 3D-native latent diffusion frameworks that exploit
geometric priors in ground truth data. Despite progress, three key limitations
persist: (1) Single-latent representations fail to capture complex multi-part
geometries, causing detail degradation; (2) Holistic latent coding neglects
part independence and interrelationships critical for compositional design; (3)
Global conditioning mechanisms lack fine-grained controllability. Inspired by
human 3D design workflows, we propose CoPart - a part-aware diffusion framework
that decomposes 3D objects into contextual part latents for coherent multi-part
generation. This paradigm offers three advantages: i) Reduces encoding
complexity through part decomposition; ii) Enables explicit part relationship
modeling; iii) Supports part-level conditioning. We further develop a mutual
guidance strategy to fine-tune pre-trained diffusion models for joint part
latent denoising, ensuring both geometric coherence and foundation model
priors. To enable large-scale training, we construct Partverse - a novel 3D
part dataset derived from Objaverse through automated mesh segmentation and
human-verified annotations. Extensive experiments demonstrate CoPart's superior
capabilities in part-level editing, articulated object generation, and scene
composition with unprecedented controllability.

</details>


### [70] [CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering](https://arxiv.org/abs/2507.08776)
*Zhengqing Wang,Yuefan Wu,Jiacheng Chen,Fuyang Zhang,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: CLiFT是一种神经渲染方法，它将场景表示为压缩的光场token，实现了高效渲染和新视图合成。


<details>
  <summary>Details</summary>
Motivation: 提出一种将场景表示为“压缩光场token（CLiFT）”的神经渲染方法，该方法保留了场景丰富的外观和几何信息。

Method: 该方法使用多视图编码器将图像进行分块，并使用相机姿态。潜在空间K-means使用这些token选择一组缩减的射线作为聚类中心。多视图“冷凝器”将所有token的信息压缩到中心token中以构建CLiFT。在测试时，系统收集指定数量的附近token，并使用计算自适应渲染器合成新视图。

Result: CLiFT实现了显著的数据缩减，渲染质量相当，并提供了数据大小、渲染质量和渲染速度之间的权衡，同时取得了最高的整体渲染分数。

Conclusion: CLiFT通过压缩的token实现计算高效渲染，并且能够更改token的数量来表示一个场景或使用一个训练好的网络渲染一个新视图。在测试时，给定目标视图和计算预算（即CLiFT的数量），系统会收集指定数量的附近token，并使用计算自适应渲染器合成新视图。该方法在RealEstate10K和DL3DV数据集上进行了广泛的实验，在数据缩减、渲染质量和渲染速度之间提供了权衡，并在渲染质量相当的情况下实现了显著的数据缩减和最高的整体渲染分数。

Abstract: This paper proposes a neural rendering approach that represents a scene as
"compressed light-field tokens (CLiFTs)", retaining rich appearance and
geometric information of a scene. CLiFT enables compute-efficient rendering by
compressed tokens, while being capable of changing the number of tokens to
represent a scene or render a novel view with one trained network. Concretely,
given a set of images, multi-view encoder tokenizes the images with the camera
poses. Latent-space K-means selects a reduced set of rays as cluster centroids
using the tokens. The multi-view ``condenser'' compresses the information of
all the tokens into the centroid tokens to construct CLiFTs. At test time,
given a target view and a compute budget (i.e., the number of CLiFTs), the
system collects the specified number of nearby tokens and synthesizes a novel
view using a compute-adaptive renderer. Extensive experiments on RealEstate10K
and DL3DV datasets quantitatively and qualitatively validate our approach,
achieving significant data reduction with comparable rendering quality and the
highest overall rendering score, while providing trade-offs of data size,
rendering quality, and rendering speed.

</details>


### [71] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: Lumos-1 是一款高效的 LLM 自回归视频生成模型，通过 MM-RoPE 和 AR-DF 等创新技术，在保持 LLM 架构的同时解决了现有方法的局限性，并在有限的计算资源下实现了顶尖的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频生成器要么偏离标准的 LLM 架构，要么依赖于笨重的外部文本编码器，要么由于 next-token 解码而产生过高的延迟。本文旨在解决这些问题，开发一种既能保持 LLM 架构又具有高效生成能力的自回归视频生成模型。

Method: 本文提出了一种名为 Lumos-1 的自回归视频生成模型，该模型保留了 LLM 架构。为了在 LLM 中注入时空相关性，我们采用了 3D RoPE，并提出了 MM-RoPE 来解决其频率频谱不平衡问题，从而更好地对多模态时空数据进行建模。此外，Lumos-1 采用了一种符合帧内双向性和帧间时间因果性的 token 依赖策略。基于此，我们提出了 AR-DF 方法来解决因空间信息冗余导致的帧损失不平衡问题，该方法在训练时引入时间轴掩码，并在推理时采用兼容的掩码策略以防止质量下降。

Result: Lumos-1 在仅使用 48 个 GPU 进行预训练的情况下，在 GenEval、VBench-I2V 和 VBench-T2V 等基准测试中取得了与 EMU3、COSMOS-Video2World 和 OpenSoraPlan 等模型相媲美的性能。

Conclusion: Lumos-1 是一款自回归视频生成模型，它在保持 LLM 架构的同时，通过引入 3D RoPE 和 MM-RoPE 来增强时空建模能力。通过结合其特有的 token 依赖策略和 AR-DF 方法，有效解决了训练中的帧损失不平衡问题，并在仅使用 48 个 GPU 进行预训练的情况下，在多个基准测试中达到了与现有先进模型相媲美的性能。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of
language tasks, inspiring preliminary efforts in autoregressive video
generation. Existing autoregressive video generators either diverge from
standard LLM architectures, depend on bulky external text encoders, or incur
prohibitive latency due to next-token decoding. In this paper, we introduce
Lumos-1, an autoregressive video generator that retains the LLM architecture
with minimal architectural modifications. To inject spatiotemporal correlations
in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its
imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE
scheme that preserves the original textual RoPE while providing comprehensive
frequency spectra and scaled 3D positions for modeling multimodal
spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy
that obeys intra-frame bidirectionality and inter-frame temporal causality.
Based on this dependency strategy, we identify the issue of frame-wise loss
imbalance caused by spatial information redundancy and solve it by proposing
Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal
tube masking during training with a compatible inference-time masking policy to
avoid quality degradation. By using memory-efficient training techniques, we
pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on
GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code
and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [72] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 通过PCA提取潜在特征并进行二次微调，可提升TTS模型的可控性。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于提示的文本到语音模型在控制语音的声学特征时存在的局限性，即控制受限于模型训练时暴露的声学特征，同时又存在因输入不当导致的不受控变异。

Method: 通过主成分分析（PCA）从合成样本中提取控制语音的潜在特征，并将这些特征作为新的标签进行二次微调。

Result: 在没有情绪表达的TTS模型上，该方法在改进模型可控性方面取得了显著成效，能够生成离散和连续的特征。

Conclusion: 该方法可用于改进模型的整体可控性，并能生成离散和连续的特征。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [73] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: MedicalBERT是一个针对生物医学领域优化的BERT模型，通过在特定数据集上预训练和添加领域词汇，提高了在医学文本理解任务中的表现，优于其他同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型（如Word2Vec和Bi-LSTM）在处理生物医学文献时存在挑战，因为该领域的术语具有专业性。像GPT和T5这样的模型虽然能捕捉上下文信息，但在需要双向理解的任务上不如BERT。因此，有必要开发一个能够更好地理解生物医学领域特定术语并有效处理相关NLP任务的模型。

Method: 提出了一种名为MedicalBERT的预训练BERT模型，该模型在大型生物医学数据集上进行了训练，并整合了领域特定的词汇。随后，对该模型进行了优化和微调，以适应包括命名实体识别、关系抽取、问答、句子相似度和文档分类在内的多种下游任务。通过F1分数、准确率和皮尔逊相关系数等性能指标，将MedicalBERT与BioBERT、SciBERT和ClinicalBERT等其他基于BERT的模型进行了比较。

Result: MedicalBERT在大多数生物医学NLP任务的基准测试中，其性能优于BioBERT、SciBERT和ClinicalBERT等模型，平均性能比通用的BERT模型高出5.67%。

Conclusion: 本研究提出了MedicalBERT，一个在大型生物医学数据集上预训练的BERT模型，通过加入领域特定的词汇来增强对生物医学术语的理解。该模型在命名实体识别、关系抽取、问答、句子相似度和文档分类等多种任务上进行了优化和微调。与BioBERT、SciBERT和ClinicalBERT等模型相比，MedicalBERT在大多数基准测试中表现更优，并且在所有评估任务中的平均表现比通用的BERT模型高出5.67%。这项工作强调了利用预训练BERT模型处理医学自然语言处理任务的潜力，并证明了迁移学习技术在捕获领域特定信息方面的有效性。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [74] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 越狱攻击的复杂性并未显著增加，AI安全演进受限于人类创造力，而非无休止的军备竞赛。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）的越狱策略的复杂性和演变对于AI安全至关重要。

Method: 使用覆盖200万真实对话数据的海量实证分析，结合概率、词汇多样性、压缩比和认知负荷等多种复杂性指标，分析越狱复杂性。

Result: 越狱尝试的复杂性与正常对话没有显著差异，且在不同平台和社区中均保持一致。用户攻击的毒性和复杂性随时间保持稳定，但助手的回复毒性有所下降，表明安全机制有所改进。复杂性分布中不存在幂律缩放，表明越狱发展存在自然限制。

Conclusion: LLM安全演进受限于人类的创造力，而防御措施持续进步，这挑战了攻防军备竞赛的普遍观点。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [75] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: CRMAgent是一个基于LLM的多代理系统，通过学习、检索和规则后备，帮助商家改进在私域流量渠道（如IM和邮件）中的营销文案，从而提高客户留存率和转化率。实验证明，CRMAgent的文案效果优于商家原有文案。


<details>
  <summary>Details</summary>
Motivation: 电子商务中的私域流量渠道（如即时通讯和电子邮件）的商家为了提高客户留存率和转化率，会直接与客户互动。然而，大多数商家由于缺乏专业知识和可扩展的工具，在撰写说服性文案方面遇到困难。

Method: CRMAgent 是一个基于大语言模型的自定义代理系统，通过三种互补模式生成高质量的消息模板和可操作的写作指南：1. 基于组的学习：代理可以学习同一受众细分内商户自己的热门消息，并重写表现不佳的消息。2. 检索和适应：提取具有相同受众细分、代金券类型和产品类别的高相似性模板，学习其成功模式，并为当前活动调整它们。3. 基于规则的后备：在没有合适参考的情况下，提供轻量级的零样本重写。

Result: CRMAgent 持续优于商户的原始模板，在受众匹配和营销效果指标方面均实现了显著的提升。

Conclusion: CRMAgent 在受众匹配和营销效果指标方面持续优于商户的原始模板，带来了显著的提升。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [76] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: FinGPT在金融领域特定任务上表现不俗，但在复杂推理和生成任务上落后于GPT-4，并非万能解决方案。


<details>
  <summary>Details</summary>
Motivation: 评估金融领域特定语言模型FinGPT的能力和局限性，为未来研究提供基准，并强调改进金融语言模型的必要性。

Method: 评估了FinGPT在六个关键的金融领域自然语言处理任务（情感分析、文本分类、命名实体识别、金融问答、文本摘要和股票走势预测）上的表现，使用了金融领域特定数据集，并与GPT-4和人类基准进行了比较。

Result: FinGPT在情感分析和头条新闻分类等分类任务上表现强劲，结果可与GPT-4相媲美；但在金融问答和摘要等涉及推理和生成的任务上表现显著较低，在数值准确性和复杂推理方面存在明显差距。

Conclusion: FinGPT在特定金融任务上表现良好，但在需要推理和生成的任务上表现不佳，与GPT-4和人类基准相比存在差距，尤其是在数值准确性和复杂推理方面。该模型并非全面的解决方案，需要架构改进和特定领域优化。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [77] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 多智能体LLM对话通过增加智能体数量、交互深度和角色多样性可以提升研究思路的新颖性，而增加评论方的多样性则能提高方案的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了优化LLM在创意任务（如研究思路生成）中的应用，需要明确最佳的交互设计，尽管已有研究表明结构化对话可以提高生成思路的新颖性和可行性。

Method: 通过比较不同数量的智能体、不同的角色分配以及对话深度，全面分析了多智能体LLM对话在科学构想生成中的应用。实验设置包括一个构想生成智能体和一个评论智能体，以支持迭代改进。

Result: 研究结果表明，增加智能体数量、增加交互深度以及提高智能体角色的多样性可以丰富生成思路的多样性。此外，在“构想-评论-修订”循环中增加评论方的多样性可以进一步提高最终方案的可行性。

Conclusion: 该研究为构建有效的科学构想多智能体LLM系统提供了实用的指导方针。

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [78] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 大型语言模型在理解方面取得进展，但其运作方式与人类有异。


<details>
  <summary>Details</summary>
Motivation: 介绍机械可解释性（MI）领域，反驳大型语言模型仅依赖表面统计的观点，并提出一个关于机器理解的新理论框架。

Method: 提出一个三层级的机器理解概念模型：概念理解（在潜在空间中形成特征作为方向）、对世界状态的理解（学习特征间的偶然事实联系并动态追踪世界变化）以及原则性理解（发现连接事实的“电路”）。

Result: 大型语言模型发展出内部结构，这些结构在功能上类似于人类的“关联性”理解能力。研究表明大型语言模型内部存在“平行机制”现象。

Conclusion: 大型语言模型表现出某种程度的理解能力，但其认知结构与人类不同，研究重点应从“大型语言模型是否理解”转向“大型语言模型的运作机制”。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [79] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: LLM驱动的多代理系统可以模拟决策会议，并能有效检测参与者之间的协议，为支持专家咨询提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 探索使用LLM模拟决策会议和检测代理之间协议的可行性。

Method: 评估六种不同的LLM在两个任务上的表现：立场检测（识别代理的立场）和立场极性检测（识别积极、消极或中立的情感）。然后，在多代理系统中评估这些模型，以确定它们在复杂模拟中的有效性。

Result: LLM可以可靠地检测协议，即使在动态和细微的辩论中也是如此。在系统中加入协议检测代理可以提高小组辩论的效率和质量，使其与现实世界的决策会议相当。

Conclusion: LLM驱动的多代理系统可以可靠地检测动态和细微辩论中的协议，并且在模拟决策会议方面具有巨大潜力，可用于支持专家咨询研讨会。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [80] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: R3框架通过打分、重构、优化来改进文本生成，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决迭代文本生成中模型识别和纠正自身错误效率低下的挑战。

Method: 提出了一种名为Review, Remask, Refine (R3)的框架，利用Process Reward Model (PRM)对中间生成块进行Review打分，根据分数制定Remask策略（分数越低，掩码比例越高），最后迫使模型Refine这些被掩码的片段，从而集中优化过去生成的不理想部分。

Result: R3框架可以应用于任何预训练的掩码文本扩散模型，并且能够通过有针对性地优化生成过程中的不理想部分，来提升最终的输出质量。

Conclusion: R3框架通过Review、Remask、Refine的迭代过程，能够无需额外训练即可应用于任何预训练的掩码文本扩散模型，并有效提高生成文本的质量。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [81] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: LLMs在招聘筛选中表现出与人类不同的模式，并且会根据公司背景调整行为。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在招聘筛选中的行为是一致性信号还是随机噪声，并将其表现与人类专家进行比较，为在自动化招聘系统中使用LLMs提供指导。

Method: 使用控制数据集测试了三种LLM（Claude、GPT和Gemini）在不同公司背景（无公司、Firm1、Firm2、减少上下文）下的表现，并与三位人类招聘专家进行了基准比较。通过方差分析和配对t检验来评估LLM行为的一致性以及与人类专家的差异。

Result: 在八个LLM单独的条件中，有四个表现出显著的平均差异。LLM与人类评估之间存在持续显著的差异（p < 0.01）。GPT对公司背景的适应性最强（p < 0.001），Gemini部分适应（p = 0.038），Claude适应性最小（p > 0.1）。所有LLM在所有背景下与人类专家的表现均存在显著差异。元认知分析显示LLMs的适应性加权模式与人类评估方法存在显著不同。

Conclusion: LLMs在招聘筛选中的表现与人类专家存在显著差异，并且会根据公司背景进行适应性调整。GPT对公司背景的适应性最强，Gemini次之，Claude适应性最弱。LLMs提供了可解释的模式，但与人类判断有很大不同。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [82] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: ETTA框架通过操纵嵌入空间的维度来攻击大型语言模型的安全对齐机制，成功绕过防御并保持文本连贯性，揭示了现有防御的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM在嵌入层级的安全对齐机制的理解不足，导致对更具威胁性的、有针对性的、精确的对抗性扰动技术研究不够充分。

Method: ETTA（Embedding Transformation Toxicity Attenuation）框架，通过在嵌入空间中进行线性变换来识别和削弱毒性敏感维度。

Result: ETTA在五个代表性的开源LLM上使用AdvBench基准测试进行评估，实现了88.61%的高平均攻击成功率，比最佳基线高出11.34%，并且能够泛化到经过安全增强的模型（例如，在指令调优防御上达到77.39%的攻击成功率）。

Conclusion: 本研究提出了ETTA框架，通过在嵌入空间中进行线性变换来识别和削弱毒性敏感维度，成功绕过了模型的拒绝行为，同时保持了语言的连贯性，并且无需对模型进行微调或访问训练数据。该方法在五个开源LLM和AdvBench基准测试中表现出色，平均攻击成功率为88.61%，优于现有最佳基线11.34%，并且能够泛化到经过安全增强的模型。这揭示了当前对齐策略的关键漏洞，并强调了开发嵌入感知防御机制的必要性。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [83] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本研究通过外部实验（样本数量、图像检索、字幕分配）和内部检查（注意力特征分析）来研究多模态大模型的上下文学习能力，为理解和优化多模态大模型提供了新方法和视角。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于，尽管大型语言模型在自然语言处理中的上下文学习能力已被证明是有效的，但研究人员在多模态大模型中对演示配置的探索仍然初步。此外，上下文示例的可控性为观察和分析多模态大模型在不同输入下的推理特征提供了一种有效且经济的方式。

Method: 本研究对图像字幕任务中的多模态上下文学习进行了全面的外部和内部调查。在外部，我们通过三个维度探索了演示配置策略：样本数量、图像检索和字幕分配。我们使用多种指标来系统、彻底地评估和总结关键发现。在内部，我们分析了典型的多模态大模型注意力特征，并开发了基于注意力的指标来量化模型行为。我们还进行了辅助实验，探索了基于注意力的模型加速和压缩的可行性。此外，我们比较了具有相同模型设计和预训练策略的多模态大模型之间的性能差异，并从预训练数据特征的角度解释了这些差异。

Result: 本研究揭示了上下文示例配置策略如何通过外部实验影响模型性能，并通过内部检查阐明了典型的特征模式，从而提供了理解多模态大模型中多模态上下文学习的双重视角。

Conclusion: 本研究通过结合外部和内部分析来研究大型模型，并提出新的指标，这些方法可以应用于更广泛的研究领域。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [84] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: 大多数大型语言模型在政治回应中表现出自由主义倾向，这源于其训练数据和反馈机制，而非编程错误。这种“自由倾斜”可能反映了“无知之幕”的哲学理念，并为审视集体推理提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前研究揭示了大多数商业大型语言模型（LLMs）在伦理和政治回应中存在一致的自由主义倾向，但其根本原因和具体影响尚不明确。

Method: 本研究采用多方面方法，包括道德基础理论、十几个成熟的政治意识形态量表以及一项新的当前政治争议指数，系统地调查了七种主流大型语言模型（LLMs）的政治倾向。

Result: 研究发现，大多数模型普遍优先考虑自由主义价值观，特别是关怀和公平。进一步分析将此趋势归因于四个相互关联的因素：自由主义的训练语料库、人类反馈强化学习（RLHF）、学术伦理讨论中自由主义框架的主导地位以及安全驱动的微调实践。研究还区分了政治“偏见”与合法的认识论差异，并警告不要将两者混淆。基础模型和微调模型对的比较表明，微调通常会增强自由主义倾向。

Conclusion: 研究发现，大型语言模型（LLMs）普遍存在自由主义倾向，这并非编程错误或程序员个人偏好，而是对以民主权利为中心的论述进行训练所产生的“自由倾斜”的涌现特性。该模型可能间接呼应了约翰·罗尔斯“无知之幕”的哲学愿望，反映了一种不与个人身份或利益挂钩的道德立场。这种模式非但不会破坏民主讨论，反而可能为审视集体推理提供新的视角。

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [85] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: 人工智能辅助招聘可提高招聘效率和候选人成功率，但可能存在偏见。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在量化人工智能对招聘效率和候选人选择的影响，因为目前这方面的实证证据有限。研究人员希望了解人工智能在招聘中的应用如何影响决策和人才获取。

Method: 通过随机分配约 37,000 名初级开发人员职位的申请人，将他们分配到传统的招聘流程（简历筛选和人工选择）或人工智能辅助招聘流程（包括人工智能驱动的结构化视频面试和人工评估）。人工智能系统通过分析人工智能生成的面试成绩单来选择候选人。研究人员通过分析候选人 5 个月后的领英 (LinkedIn) 个人资料来评估招聘效率和候选人质量。

Result: 与传统招聘流程相比，人工智能辅助招聘流程将通过面试的候选人比例从 34% 提高到 54%。此外，接受人工智能辅助招聘的候选人找到新工作的比例（23%）比接受传统招聘的候选人（18%）高 5.9 个百分点。然而，人工智能系统倾向于选择更年轻、经验更少、资历更少的申请人。

Conclusion: 该研究发现，人工智能辅助招聘流程将高绩效候选人的比例提高了近一倍（从 34% 提高到 54%），并使找到新工作的候选人的比例增加了 5.9 个百分点。然而，人工智能系统倾向于选择更年轻、经验更少、资历更少的申请人，这表明在招聘中实施人工智能时需要仔细考虑公平性和潜在的偏见。

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [86] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: AI医学模型免责声明使用率急剧下降，到2025年多数模型不再提示风险，需强制实施以保安全。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型（包括LLMs和VLMs）在医学领域的应用日益广泛，但其输出内容常包含不准确信息。为了保障用户安全，提醒用户AI输出未经专业审核且不能替代医疗建议，医学免责声明至关重要。然而，随着模型能力的提升和应用普及，这些安全措施的实施情况需要被评估和关注。

Method: 本研究评估了从2022年到2025年不同代次的大语言模型（LLMs）和视觉语言模型（VLMs）在医学影像解读和临床问题回答任务中输出的免责声明的使用情况。研究者使用了500张乳腺X光片、500张胸部X光片、500张皮肤科图像以及500个医学问题作为测试数据。通过筛选模型生成的输出内容，统计并分析了免责声明的出现频率。

Result: 研究结果显示，医学免责声明在LLM和VLM输出中的使用率显著下降。LLM输出中免责声明的比例从2022年的26.3%降至2025年的0.97%；VLM输出中免责声明的比例从2023年的19.6%降至2025年的1.05%。到2025年，绝大多数模型在生成医学相关内容时已不再包含任何形式的免责声明。

Conclusion: 随着生成式AI模型能力的增强和在医学领域的广泛应用，其输出的准确性和安全性日益受到关注。研究发现，尽管早期模型（2022-2023年）的输出中包含医学免责声明的比例尚可，但到2025年，大多数模型已不再包含此类声明。这种趋势令人担忧，因为AI生成的医学信息若无适当的免责声明，可能被用户误解为专业的医疗建议，从而带来潜在的健康风险。因此，研究强调，在AI模型（特别是LLM和VLM）日益强大并深入临床应用的同时，必须强制实施和更新免责声明，以确保用户了解AI输出的局限性，并将其作为专业医疗意见的补充而非替代。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [87] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: SLMs在精神健康理解方面表现出色，接近LLMs的性能，并且通过少量样本学习能够实现显著改进，这表明它们在精神健康筛查方面具有潜力。


<details>
  <summary>Details</summary>
Motivation: SLMs作为保护隐私的替代品在敏感应用中日益普及，引发了对其与LLMs相比的内在理解能力的基本问题。

Method: 通过在各种分类任务中进行系统评估，研究当前SLMs的精神健康理解能力。采用零样本和少量样本学习范例，并与公认的LLM基线进行性能比较，以阐明它们在此关键领域相对的优势和局限性。评估了五个最先进的SLMs（Phi-3、Phi-3.5、Qwen2.5、Llama-3.2、Gemma2）与三个LLMs（GPT-4、FLAN-T5-XXL、Alpaca-7B）在六项精神健康理解任务上的表现。

Result: SLMs在二元分类任务（零样本设置下的F1得分分别为0.64和0.66）上实现了平均性能接近LLMs（相差2%以内），尽管参数量少几个数量级，但仍显示出显著的能力。两种模型类别在多类严重性任务上都经历了类似的性能下降（下降超过30%），这表明细微的临床理解挑战超越了模型规模。少量样本提示为SLMs提供了实质性改进（高达14.6%），而LLMs的改进则更为多变。

Conclusion: SLMs在精神健康理解方面具有巨大潜力，可以成为分析敏感在线文本数据的有效隐私保护工具。特别是，它们能够通过少量示例学习以最少的数据进行快速适应和专业化，这使它们成为可扩展的精神健康筛查工具的有希望的候选者。

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [88] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Athena的框架，该框架通过集成外部工具来增强大型语言模型（LLM）在教育场景下的问答能力，尤其是在数学和科学推理方面取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）在缺乏相关上下文信息时回答质量差或产生幻觉的问题，以及提升其在教育场景下的应用能力。

Method: 提出并实现了一个允许LLM访问外部API以获取相关信息和执行计算（如使用计算器或日历）的框架。

Result: 在MMLU数据集上的评估显示，Athena框架在数学推理方面达到83%的准确率，在科学推理方面达到88%的准确率，显著优于包括GPT-4o在内的所有测试模型（基线模型LLaMA-Large的准确率分别为67%和79%）。

Conclusion: 该研究提出的Athena框架通过集成外部工具（如API、计算器和日历）显著提高了LLM在教育领域的问答能力，特别是在数学和科学推理方面，准确率分别达到83%和88%，超越了包括GPT-4o在内的现有先进模型。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [89] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: MedVQA在医学影像解读方面潜力巨大，但目前的研究存在诸多问题，例如问答对缺乏临床相关性、模型不支持关键的临床功能（如多视图、EHR整合）、评估指标与临床需求脱节等。临床医生普遍认为现有系统实用性不高，并强调了整合患者病史、领域知识以及支持多视图和对话式交互的重要性。若要实现MedVQA的临床应用，必须解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统性地审查医学视觉问答（MedVQA）的文献，并调查临床医生对其在临床工作流程中实际效用的看法，以解决MedVQA尽管取得进展但临床整合仍然有限的问题，并找出阻碍其广泛应用的挑战和差距。

Method: 本研究采用系统性文献回顾（涵盖2018-2024年间的68篇出版物）和对来自印度和泰国的50名临床医生的问卷调查相结合的方法。文献回顾遵循Arksey和O'Malley的范围界定审查框架，旨在识别放射学工作流程中的关键概念、进展和研究空白。临床医生调查则用于收集他们对MedVQA临床相关性的看法。

Result: 研究发现，近60%的问答对缺乏诊断性和临床相关性。现有的大多数数据集和模型不支持多视图、多分辨率成像、电子健康记录（EHR）整合或领域知识，这些都是临床诊断的关键功能。此外，当前的评估指标与临床需求存在明显不匹配。临床医生调查也证实了这种脱节，仅有29.8%的医生认为MedVQA系统非常有用。医生们的主要担忧包括缺乏患者病史或领域知识（87.2%），偏好手动策划的数据集（51.1%），以及需要支持多视图图像（78.7%）。此外，66%的医生倾向于关注特定解剖区域的模型，89.4%的医生偏好对话式交互系统。

Conclusion: 尽管MedVQA具有巨大潜力，但目前的研究和模型在多模态分析、患者信息整合和评估指标方面存在局限性。为了实现有效的临床应用，需要解决这些挑战，重点关注特定解剖区域的模型以及对话式交互系统。

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [90] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: CRISP数据集通过在数学和代码生成任务上进行计划生成，提升了LLM的推理能力，并且通用性强。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在复杂问题解决方面需要更强的推理能力，CoT推理尚有不足。尽管显式高级计划生成是一个有前景的方向，但现有方法大多假设LLM仅通过few-shot prompting就能生成有效的计划，而无需额外训练。

Method: 提出CRISP数据集，包含用于数学推理和代码生成的显式高级计划。通过LLM进行内在验证，并通过下游任务性能进行外在验证。在CRISP上微调模型以生成计划。

Result: 与使用few-shot prompting的更大模型相比，在CRISP上微调的小模型能够生成更高质量的计划，并且显著优于Chain-of-Thought推理。在跨域评估中，在一个域上微调可以改善另一个域上的计划生成，证明了学习到的规划能力的通用性。

Conclusion: 通过在CRISP数据集上微调模型可以生成更高质量的计划，并且优于使用few-shot prompting的更大模型，同时显著优于CoT推理。在跨域评估中，在一个域上微调可以改善另一个域上的计划生成能力，这表明了学习到的规划能力的通用性。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [91] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 本研究提出了AblationBench，一个用于评估AI研究中消融实验规划的基准。实验显示，当前语言模型在此任务上表现不佳，但链式思考提示优于代理方法。


<details>
  <summary>Details</summary>
Motivation: 随着自主语言模型在科研领域的应用日益广泛，需要一个评估这些模型在设计消融实验（科研中的关键环节）方面的能力的基准。本研究旨在为AI研究人员提供一个评估语言模型在消融实验规划任务上表现的工具和数据集。

Method: 开发了一个名为AblationBench的基准套件，包含两个任务：AuthorAblation（基于方法部分提出消融实验）和ReviewerAblation（找出论文中缺失的消融实验）。并为此开发了基于语言模型的评判器作为自动评估框架。通过对前沿语言模型进行实验，评估了它们在这些任务上的表现。

Result: 在AblationBench基准套件上的实验表明，即使是目前最先进的语言模型，在消融实验规划任务上也面临巨大挑战，平均仅能识别出29%的原始消融项。研究还发现，链式思考提示（chain-of-thought prompting）在这些任务上的表现优于现有的基于代理的方法。

Conclusion: 该研究介绍了AblationBench，一个用于评估语言模型在科学研究中的消融实验规划能力的基准套件。结果表明，尽管最先进的语言模型表现出一定的能力，但消融实验规划仍然是一个挑战。研究还发现，链式思考提示优于目前基于代理的方法。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [92] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul通过动态压缩KV缓存来提高多轮对话LLM的效率和准确性，显著减少了响应时间和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多轮对话中通过压缩KV缓存来提高效率，但通常采用固定的压缩方案，忽略了对话特定的注意力动态，可能导致准确性下降。因此，需要一种能够根据注意力相似性动态选择压缩策略的系统。

Method: Krul是一个多轮LLM推理系统，通过动态选择压缩策略和重计算-加载流水线来恢复KV缓存。其关键创新包括：1) 先发制人的压缩策略选择器；2) 令牌级别的异构注意力相似性估计器；3) 无气泡恢复调度器。

Result: Krul在真实世界的任务中实现了1.5倍至2.68倍的TTFT缩减和1.33倍至2.35倍的KV缓存存储缩减，且没有损害生成质量。

Conclusion: Krul通过动态选择压缩策略、异构注意力相似性估计和无气泡恢复调度，在保持生成质量的同时，实现了1.5倍至2.68倍的首个令牌时间（TTFT）缩减和1.33倍至2.35倍的KV缓存存储缩减。

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [93] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出了一种无需微调即可从自然语言生成SPARQL查询的新方法，该方法利用LLM探索知识图谱，并在多个基准上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 提出一种从自然语言问题或关键字查询生成SPARQL查询的新方法，用于处理RDF知识图谱。

Method: 使用大型语言模型（LLM）探索知识图谱，通过战略性地执行SPARQL查询来搜索相关的IRIs和字面量，无需进行微调。

Result: 在Wikidata上达到了零样本设置下的最先进结果，在Freebase上接近了少样本方法，并在其他不常用的知识图谱和基准上也表现良好。

Conclusion: 在各种基准和语言模型上，我们的方法都取得了良好的性能，并且在Wikidata上达到了零样本设置下的最先进结果，在Freebase上接近了少样本方法。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [94] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 提出一个框架，用于安全、可解释和可审计地集成语言模型到异步代码中，并通过人类反馈在线改进性能。在公共评论处理任务中进行了评估，结果令人鼓舞。


<details>
  <summary>Details</summary>
Motivation: 为了安全、可解释和可审计地利用语言模型，同时最小化风险并使人类专家能够专注于知情决策，而不是数据处理或提示工程。

Method: 提出一个框架，用于声明类型化的、由语言模型驱动的子程序，并将其集成到传统的异步代码中。该框架记录所有语言模型产生的工件（提示、输入、输出和数据依赖项），以便按需审计。通过使用人类专家的稀疏反馈来在线改进每个子程序的性能。

Result: 在国家环境政策法案（NEPA）要求的公共评论处理背景下，开发了一个名为“CommentNEPA”的应用程序。该应用程序能够汇编、组织和总结为环境审查项目提交的公众评论语料库。在没有人类反馈的情况下，其输出与人类注释者标记的历史“真实”数据进行了定量评估。

Conclusion: 该框架提供了一种将语言模型安全、可解释和可审计地集成到传统异步代码中的方法，并允许人类专家通过稀疏反馈在线改进模型性能。

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [95] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: Compactor通过利用近似杠杆分数来压缩KV缓存，减少了内存占用，同时保持了LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLM）虽然能够处理更长的上下文窗口，但KV缓存的内存需求随上下文长度线性增长，这成为实际部署中的主要瓶颈，限制了吞吐量并增加了服务成本。

Method: Compactor是一种参数自由、查询无关的KV压缩策略，利用近似杠杆分数来确定令牌的重要性。此外，还引入了一种上下文校准压缩程序，可以推断给定上下文支持的最大压缩率。

Result: Compactor在保持性能不变的情况下，将令牌数量减少了一半，并且计算开销极小。在Longbench上，使用上下文校准压缩的Compactor实现了完整的KV性能，同时平均将KV内存开销减少了63%。该方法已在Qwen 2.5和Llama 3.1模型家族的27个合成和真实世界任务中得到验证。

Conclusion: Compactor是一种参数自由、查询无关的KV压缩策略，可将KV缓存中的令牌数量减少一半，同时保持与现有方法相同的性能，并且计算开销最小。通过上下文校准压缩，Compactor在Longbench上实现了完整的KV性能，同时平均将KV内存开销减少了63%。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [96] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 通过创新的两步微调和专门设计的提示，成功地将LLM的共情能力转移到了更小的模型中，使小型模型在对话中表现出更强的共情能力。


<details>
  <summary>Details</summary>
Motivation: 在将大型语言模型（LLM）的知识蒸馏到小型语言模型（SLM）以减小模型尺寸的过程中，保留LLM的核心能力至关重要，特别是共情能力，因为SLM常用于资源受限且需要人机交互的场景（如智能手机）。

Method: 该方法采用一种两步微调过程，利用从LLM蒸馏出的共情对话响应数据集。探索了多种蒸馏方法，并提出了四组独特的提示，用于针对性地提升共情能力。

Result: 所提出的两步微调方法，结合蒸馏数据集和针对性共情提升提示，能够显著提升SLM生成共情回应的能力，在评估中显示出90%的胜率。相比之下，基础的直接提示方法仅有10%的提升。

Conclusion: 通过针对性的共情提升提示和两步微调过程，可以有效地将共情能力从大型语言模型（LLM）蒸馏到小型语言模型（SLM）中，从而显著提高SLM生成共情回应的能力。

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [97] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: TruthTorchLM是一个开源Python库，包含超过30种真实性预测方法，支持本地和API模型，并能在多种数据集上进行评估，旨在解决LLM输出的真实性问题。


<details>
  <summary>Details</summary>
Motivation: 为了加速真实性预测研究并使相关方法更容易获得，作者开发了TruthTorchLM库，以解决生成式语言模型（LLMs）输出中不可避免的虚假信息问题，尤其是在高风险应用场景下。

Method: TruthTorchLM库集成了多种真实性预测方法，涵盖了计算成本、访问级别（黑盒与白盒）、文档依赖性和监督类型（自监督或监督）等不同权衡。它提供了一个统一的接口，用于生成、评估、校准和长篇真实性预测，并允许用户轻松扩展库以支持新方法。

Result: 在TriviaQA、GSM8K和FactScore-Bio三个数据集上对代表性的真实性预测方法进行了评估，验证了TruthTorchLM库的有效性。

Conclusion: TruthTorchLM是一个全面的开源Python库，提供了超过30种用于预测生成式语言模型（LLMs）输出真实性的方法，并支持HuggingFace和LiteLLM，能够对本地托管和API模型进行支持。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [98] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: LLMs fine-tuned with LoRA develop 'out-of-context reasoning' (OOCR) because LoRA adds a steering vector that guides the model towards general concepts, improving performance across related tasks. This effect can be replicated by training steering vectors directly, even for tasks like backdoors, suggesting unconditional steering is sufficient for OOCR.


<details>
  <summary>Details</summary>
Motivation: To understand the phenomenon of out-of-context reasoning (OOCR) in fine-tuned LLMs, where models show generalization beyond their fine-tuning data, and to explain why LLMs possess this advanced reasoning capability, which is crucial for their safe and reliable deployment.

Method: Investigated the phenomenon of out-of-context reasoning (OOCR) in fine-tuned LLMs mechanistically. Found that LoRA fine-tuning adds a constant steering vector that steers the model towards a general concept. Demonstrated that directly training steering vectors from scratch also induces OOCR, even for tasks seemingly requiring conditional behavior.

Result: Identified that LoRA fine-tuning adds a constant steering vector, which steers the model towards a general concept, leading to OOCR. Showed that directly training steering vectors from scratch also induces OOCR, even for tasks like model backdoors, where unconditional steering is sufficient. This provides a mechanistic explanation for OOCR.

Conclusion: Fine-tuned LLMs exhibit OOCR by implicitly internalizing and acting on consequences of observations scattered throughout the fine-tuning data. LoRA fine-tuning adds a constant steering vector that steers the model towards a general concept, improving performance on the fine-tuning task and related domains, thus causing the surprising generalization. This method can also be used to train steering vectors from scratch, inducing OOCR even for tasks like model backdoors where conditional behavior seems necessary.

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [99] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: LLM在作为代理学生方面存在局限性，需要改进训练和评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为代理学生在智能辅导系统和考试问题试点中模仿真实学生行为和特征的程度。

Method: 收集了489个来自NAEP（国家教育进展评估）的数学和阅读理解题目（涵盖4、8、12年级），并应用项目反应理论（IRT）模型。

Result: 即使是通用的LLM在所有年级都表现优于平均水平的学生。然而，使用年级提示会改变LLM的表现，但其与平均年级学生的一致性因模型和提示而异，没有模型-提示对能在所有科目和年级都符合要求。

Conclusion: LLM作为代理学生在智能辅导系统和考试问题试点中的应用仍有待改进，需要新的训练和评估策略。通用能力强的LLM在所有年级都优于平均学生，而能力弱或领域不匹配的LLM可能偶然吻合。使用年级提示可以改变LLM的表现，但与平均年级学生的一致性因模型和提示而异。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [100] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 一项NLP研究发现，女性在描述疼痛时更侧重情感表达，且某些病症（如偏头痛、鼻窦炎）在女性中更普遍。研究还指出止痛药对不同性别的效果存在差异。


<details>
  <summary>Details</summary>
Motivation: 早期研究常常忽略性别在疼痛体验中的作用，本研究旨在通过NLP技术深入理解个体的疼痛体验，并着重探讨性别差异。

Method: 本研究利用自然语言处理（NLP）技术，结合隐属性模型-卷积神经网络（HAM-CNN），对用户帖子进行分析，并根据用户名将帖子聚类，成功将帖子划分为男性和女性语料库，F1得分为0.86。

Result: 使用HAM-CNN模型成功将帖子按性别分类，F1得分为0.86。分析揭示了性别间的语言差异，女性帖子更具情感性。此外，研究发现偏头痛和鼻窦炎在女性中更常见，并探讨了止痛药对不同性别的个体产生差异性影响。

Conclusion: 本研究通过NLP技术分析了性别在疼痛体验中的作用，发现性别之间存在语言差异，女性帖子更侧重情感表达，且偏头痛和鼻窦炎等病症在女性中更常见。研究还探讨了止痛药对不同性别的个体影响不同。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [101] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: Kwaipilot-AutoThink (KAT) 是一个开源的 40B 大型语言模型，通过自动思维训练范式解决了推理密集型任务中的过度思考问题，该范式根据任务复杂性动态地在推理和非推理模式之间切换。它通过双重模型数据集、基于 MTP 的知识蒸馏、冷启动初始化和 Step-SRPO 强化学习算法实现。KAT 在推理任务中的表现与最先进的模型相当甚至更好，同时减少了令牌使用量，并已成功部署在 Kuaishou 的编码助手 Kwaipilot 中，从而提高了开发工作流程。此外，正在训练的 200B MoE 模型显示了该 AutoThink 范式的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决推理密集型任务中的过度思考问题。

Method: 1. 自动思维训练范式：根据任务复杂性动态地在推理和非推理模式之间切换。 2. 双重模型数据集：采用新颖的标签处理流程和多代理合成策略来构建。 3. 基于多令牌预测 (MTP) 的知识蒸馏：实现高效、细粒度的推理迁移，并最大程度地减少预训练成本。 4. 冷启动初始化策略：使用多数投票信号和意图感知提示来引入模式选择先验。 5. Step-SRPO 强化学习算法：将中间监督纳入 GRPO 框架，为推理模式选择和响应准确性提供结构化指导。

Result: KAT 在多个基准测试中持续匹配甚至超越了包括 DeepSeek-R1-0528 和 Qwen3-235B-A22B 在内的现有最先进模型，在广泛的推理密集型任务中，令牌使用量减少了约 30%。

Conclusion: Kwaipilot-AutoThink (KAT) 在各种需要密集推理的任务中与最先进的模型相媲美甚至超越，同时将令牌使用量减少了约 30%。KAT 已成功部署在 Kuaishou 的内部编码助手 Kwaipilot 中，并提高了现实世界开发工作流的准确性、效率和可控推理行为。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [102] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 为了解决大型多模态模型在文档图像机器翻译（DIMT）中面临的跨模态和跨语言挑战，以及微调过程中可能出现的单语能力（如OCR）遗忘问题，我们提出了一种名为同步自审查（SSR）的新型微调范式。该范式受到“双语认知优势”的启发，通过让模型在生成翻译文本前先生成OCR文本，来利用其已有的OCR能力来学习翻译。实验结果表明，SSR能够有效缓解灾难性遗忘，并提升模型在OCR和DIMT任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，在DIMT数据集上进行监督微调（SFT）会使模型遗忘其现有的单语能力（如OCR）。为了解决这个问题，我们引入了一种新的微调范式SSR，它借鉴了“双语认知优势”的概念。

Method: SSR通过首先提示模型生成OCR文本，然后生成翻译文本，从而利用其强大的单语OCR能力来学习跨语言翻译。

Result: 实验证明，SSR学习能够有效减轻灾难性遗忘，并提升MLLM在OCR和DIMT任务上的性能。

Conclusion: 所提出的SSR学习有助于减轻灾难性遗忘，提高MLLM在OCR和DIMT任务上的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [103] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: MK2通过Gemini 2.5优化提示，再由GPT-4.1生成创意，最后用Qwen3-8B评估，实现从专利到产品创意的自动化生成，并在多项测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索如何将专利转化为三年内可行的产品创意，并提出了一种名为MK2的创新方法。

Method: MK2是一个以提示为中心的流程：Gemini 2.5迭代地起草和编辑提示，并融合较弱输出中的有用片段；GPT-4.1利用该提示为每个专利生成一个想法；最后，由Qwen3-8B评估的Elo循环在没有额外训练数据的情况下选择最佳提示。

Result: MK2在自动排行榜上名列前茅，并在大部分测试中胜出，但其在材料化学领域的表现有待提高，这表明需要更深入的领域知识。

Conclusion: MK2在三个领域、两种评估器和六个标准下都取得了领先地位，在36项测试中赢得了25项，表明轻量级的提示工程已能从专利中产生具有商业竞争力的想法。

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [104] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 对比学习和知识蒸馏是训练重排模型的两种主要策略。研究表明，知识蒸馏在从更大的教师模型中学习时效果更好，但从相同容量的教师模型中学习时效果不佳。因此，建议在有更强教师模型时使用知识蒸馏，否则使用对比学习。


<details>
  <summary>Details</summary>
Motivation: 虽然对比学习和知识蒸馏是训练文本重排模型的两种主要策略，但对于在实际条件下训练交叉编码器重排模型，需要对其有效性进行清晰的比较。

Method: 通过在相同数据上使用两种方法（对比学习和知识蒸馏）训练不同大小和架构的重排模型，并使用一个强大的对比学习模型作为蒸馏教师，对这两种策略进行了实证比较。

Result: 实验结果表明，当从更大的教师模型进行蒸馏时，知识蒸馏在领域内和跨领域重排性能上通常优于对比学习。这一发现对于不同学生模型的尺寸和架构都是一致的。

Conclusion: 知识蒸馏通常在利用更大的教师模型时，比对比学习在领域内和跨领域重排方面产生更好的性能。然而，当蒸馏来自相同容量的教师模型时，这种优势并不明显，尤其是在跨领域任务上。因此，建议在有更大、更强的教师模型可用时，使用知识蒸馏来训练更小的重排模型；否则，对比学习是更可靠的选择。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [105] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 本研究评估了提示方法、代理框架和多语言对齐方法对金融问答中LLM和RLLM性能的影响。结果显示，虽然这些方法能提升LLM性能，但对RLLM的提升效果有限，且多语言对齐方法对RLLM几乎无效。


<details>
  <summary>Details</summary>
Motivation: 为了系统地探索能够充分发挥LLM和RLLM在金融领域性能的方法，并研究各种方法对LLM和RLLM的影响。

Method: 使用五种LLM和三种RLLM，评估了提示方法、代理框架和多语言对齐方法在金融问答任务中的影响。

Result: 1. 当前的提示方法和代理框架通过模拟长推理过程来提高LLM在金融问答中的性能；2. RLLM具有固有的长推理能力，这限制了传统方法进一步提升其性能的有效性；3. 当前先进的多语言对齐方法主要通过延长推理长度来提高LLM的多语言性能，但对RLLM的益处甚微。

Conclusion: 该研究表明，当前的提示方法和代理框架可以改善LLM在金融问答任务中的表现，而RLLM具有固有的长推理能力，对这些方法的敏感度较低。此外，多语言对齐方法对RLLM的效益甚微。

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [106] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 评估指标在不同语言中的表现不同，特别是n-gram指标在融合语中表现不佳，而神经指标则更具潜力。


<details>
  <summary>Details</summary>
Motivation: 评估生成任务的指标（如ROUGE）在英语之外的其他语言中的适用性尚不明确。

Method: 设计了一个包含八种语言（涵盖四种语系：黏着语、孤立语、低融合语和高融合语）的大规模评估套件，涵盖低资源和高资源场景，以分析其与人类判断的相关性。

Result: 研究结果表明，评估指标对语言类型敏感。例如，在融合语中，n-gram指标与人类评估的相关性低于孤立语和黏着语。此外，在词法丰富的融合语中，正确的标记化可以显著缓解这个问题。专门为评估任务训练的神经指标（如COMET）在低资源语言中表现优于其他神经指标，并且与人类判断的相关性更高。

Conclusion: n-gram指标在融合语中相关性较低，而专门为评估任务训练的神经指标（如COMET）在低资源语言中表现更好，因此需要更多地投入到评估任务的神经指标上。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [107] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 微调语言模型以减少幻觉：使用模型自己认为正确的、经过模型自身过滤的数据比使用事实金数据效果更好。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言模型容易产生事实错误（幻觉）的问题，探究在微调过程中应该使用何种数据来有效减少这种幻觉。

Method: 研究了微调数据的性质（事实金数据 vs. 模型生成数据）以及过滤策略（模型内部判断过滤 vs. 金数据支持过滤）对语言模型长文本生成事实性的影响。

Result: 与事实金数据相比，使用模型生成的、模型认为事实的数据进行微调，在减少幻觉方面效果更好。通过模型自身内部判断过滤后的模型生成数据，能够显著提高生成内容的整体事实性，优于其他过滤或训练策略。这些改进在三个研究领域均有体现。

Conclusion: 通过对事实金数据和模型生成的、模型认为是事实的数据进行微调，研究发现后者在减少语言模型幻觉方面效果更佳。对模型生成数据进行过滤，特别是利用模型自身的内部判断进行过滤，在提高事实性方面优于其他方法，包括使用金数据过滤或单独使用金数据进行训练。这些发现表明，模型的自我判断可以作为提高事实性的有力信号，并且这种改进在不同领域具有普适性。

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [108] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 本篇综述全面介绍了大语言模型（LLMs）在数学、物理、化学、生物、人文社科等跨学科领域的应用，重点分析了监督微调、检索增强生成等关键技术，并指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能大语言模型（LLMs）在众多学科研究中展现出巨大的潜力，但其在不同学科的融合应用仍有待系统性探索。本研究旨在填补这一空白，提供一个全面的视角来理解LLMs在跨学科研究中的整合情况。

Method: 本文采用文献综述的方法，系统性地梳理和归纳了人工智能大语言模型在不同学科研究中的应用。从技术角度，重点考察了监督微调、检索增强生成、基于代理的方法和工具使用集成等关键技术；从应用角度，探讨了模型在数学、物理、化学、生物以及人文学科和社会科学等领域的应用情况。

Result: 本研究对LLMs在数学、物理、化学、生物、人文学科和社会科学等多个领域的应用进行了分类和分析，考察了监督微调、检索增强生成、基于代理的方法和工具使用集成等关键技术，并探讨了该领域的挑战与未来研究方向。

Conclusion: 本篇综述性论文为研究人员提供了关于人工智能大语言模型在跨学科研究中应用的全面概述，重点介绍了技术发展和实际应用，旨在成为该领域研究的宝贵资源。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [109] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: ChainEdit通过结合知识图谱和LLM推理，解决了语言模型知识编辑中的逻辑一致性问题，并在涟漪效应方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 目前的语言模型知识编辑方法在将涟漪效应传播到相关事实时，在保持逻辑一致性方面存在不足。

Method: ChainEdit通过自动从结构化知识库中提取逻辑模式并将其与LLM的内部逻辑对齐，动态地生成和编辑逻辑上连接的知识集群。

Result: 实验证明，在保持编辑可靠性和特异性的同时，逻辑泛化能力比基线提高了30%以上。

Conclusion: ChainEdit在确保知识编辑后的内部逻辑一致性的同时，在涟漪效应方面确立了新的最先进性能。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [110] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 为解决LLM评估中的错误归因问题，本研究提出了一个包含6类和15个子类的错误归因框架、一个名为AttriData的数据集以及一个名为MisAttributionLLM的微调模型。该模型是首个能同时生成分数、错误归因和反馈的通用评判模型，并经过实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 为了高效分析大型语言模型（LLMs）的性能和诊断其回答中的错误，需要一个能够系统地分类和归因错误的自动化框架。然而，现有的评估模型缺乏错误归因能力。

Method: 提出了一个包含6个主要类别和15个次要类别的综合性错误归因框架。基于此框架，创建了一个名为AttriData的数据集，用于错误归因，并包含相应的分数和反馈。最后，提出了MisAttributionLLM，一个在AttriData上进行微调的模型。

Result: 成功构建了一个包含6个主要和15个次要类别的错误归因框架，并创建了一个名为AttriData的数据集。提出的MisAttributionLLM模型能够同时生成分数、错误归因和反馈，并且实验证明了其有效性和鲁棒性。

Conclusion: 该研究提出了一个名为MisAttributionLLM的微调模型，该模型是首个能够同时生成分数、错误归因和反馈的通用评判模型，并通过大量实验证明了其有效性和鲁棒性。

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [111] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLM）在处理奥地利和欧盟增值税（VAT）法律事务中的应用潜力。通过微调和检索增强生成（RAG）等方法，并结合教科书和实际案例进行测试，研究发现LLM能在一定程度上协助税务专业人士处理VAT相关任务，提供法律依据。但目前LLM尚不能完全自动化处理，尤其在处理隐含客户知识和上下文信息方面存在局限，仍需进一步的研究与整合结构化背景信息。


<details>
  <summary>Details</summary>
Motivation: 在税务咨询实践中，客户通常使用自然语言描述案例，这使得大型语言模型（LLM）成为支持自动化决策和减轻税务专业人士工作负担的理想选择。鉴于法律领域需要有法律依据且理由充分的分析，LLM产生幻觉的倾向带来了相当大的挑战。

Method: 本研究实验性地评估了大型语言模型（LLM）在奥地利和欧盟增值税（VAT）法律框架内协助法律决策的能力。研究采用了两种增强LLM性能的常用方法：微调（fine-tuning）和检索增强生成（Retrieval-Augmented Generation, RAG）。这些方法应用于教科书案例和税务咨询公司的实际案例，以系统地确定基于LLM的系统的最佳配置，并评估LLM的法律推理能力。

Result: 研究结果强调了使用LLM通过自动化常规任务和提供初步分析来支持税务顾问的潜力。尽管在处理隐含的客户知识和特定上下文的文档方面仍然存在局限性，但研究表明，经过适当配置的LLM可以有效地支持税务专业人士处理增值税事务，并为决策提供法律依据。

Conclusion: 虽然目前的原型还不能完全自动化，但研究结果表明，经过适当配置的语言模型可以有效地协助税务专业人士处理增值税（VAT）事务，并为决策提供法律依据。

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [112] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 提出迭代式LoRA训练（ILT）结合迭代式伪标签策略，解决语音识别中LoRA过拟合问题。实验在Whisper-large-v3和Qwen2-Audio上进行，包含三个训练阶段，并在MLC-SLM挑战赛中取得佳绩。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型和自动语音识别系统深度融合中，LoRA在监督微调（SFT）阶段常见的过拟合问题。

Method: 通过包含焦点训练、反馈训练和固定训练三个阶段的系统性实验，在Whisper-large-v3和Qwen2-Audio基础上进行。

Result: 实验结果证明了所提出方法的有效性，并在2025年语音语言建模挑战赛（MLC-SLM）的两个赛道中取得了优异成绩（赛道1第四名，赛道2第一名），展示了该方法的实际可行性和强大的应用潜力。

Conclusion: 所提出的迭代式LoRA训练（ILT）结合迭代式伪标签策略，有效提升了模型性能的理论上限。

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [113] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 本文提出了一种基于项目反应理论的粘性评分预测方法，以调整机器学习模型生成的评分，并取得了优于传统方法的成果。


<details>
  <summary>Details</summary>
Motivation: 为了应对在教育人工智能领域中自动评估文章内聚性所面临的挑战，特别是机器学习算法在评估文本时通常不考虑实例的个体特征的问题，本研究旨在探索一种改进自动评估方法。

Method: 本文提出并分析了一种基于项目反应理论的粘性评分预测方法，该方法通过从325个语言特征中提取信息，并将问题视为一个机器学习回归任务，以调整机器学习模型生成的评分。

Result: 实验结果表明，所提出的方法在多个评估指标上优于传统的机器学习模型和集成方法，表明其在提高自动评分准确性方面的潜力。

Conclusion: 本文提出了一种基于项目反应理论的粘性评分预测方法，以调整机器学习模型生成的评分，并且该方法在多个评估指标上优于传统的机器学习模型和集成方法，为改进教育论文的自动粘性评估提供了一种潜在的方法。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [114] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 本文提出了一种新的LLM评估方法——对话游戏评估，并发布了一个名为clembench的工具，该工具易于使用且可扩展，可以方便地评估LLM并创建新的评估测试。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有LLM评估范式（基于参考和基于偏好）的局限性，本文提出了一种新的评估范式——对话游戏评估，它结合了两种范式的优点，并旨在解决现有实现工具不成熟、不易复用的问题。

Method: 本文提出了一种对话游戏评估方法，并介绍了一个名为clembench的开源工具，该工具自2023年以来一直在持续开发中，并针对易用性进行了优化。用户可以使用clembench提供的基准游戏实例来评估自己的模型，还可以轻松地扩展基准测试集以包含新的、定制化的定向测试。

Result: 本文发布了一个名为clembench的开源工具，该工具旨在简化对话游戏评估的实施过程，并为评估LLM的性能提供了一个可扩展的平台。

Conclusion: 现有的LLM评估范式包括基于参考的评估和基于偏好的评估。基于参考的评估在控制测试内容方面表现出色，而基于偏好的评估具有更高的生态效度，可测试实际用例。本文提出了一种结合这两种范式优点的对话游戏评估方法，并发布了相应的实现工具clembench，该工具易于使用且可扩展，可用于评估现有模型和创建新的评估测试。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [115] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa是一个用于多模态程序化规划的视觉-语言模型框架，通过任务-环境重新排序器（TER）和反事实活动检索器（CAR）模块，提高了计划的质量和推理能力，在基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）在具身人工智能系统的程序化规划方面存在的将多模态输入和反事实推理结合起来的不足，我们引入了LLaPa。

Method: LLaPa是一个视觉-语言模型框架，利用视觉-语言模型（VLMs）从文本任务描述和视觉环境图像生成可执行动作序列。它还包括任务-环境重新排序器（TER）和反事实活动检索器（CAR）两个辅助模块。TER利用面向任务的分割来创建任务敏感特征空间，将文本描述与视觉环境对齐，并强调程序执行的关键区域。CAR识别并强调潜在的反事实条件，增强模型在反事实场景中的推理能力。

Result: 通过TER和CAR模块的增强，LLaPa在ActPlan-1K和ALFRED基准测试中展示了其生成更高质量计划的能力，在LCS和正确性方面均优于现有先进模型。

Conclusion: LLaPa在ActPlan-1K和ALFRED基准测试中生成了更高质量的计划，具有更优越的LCS和正确性，优于先进模型。

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [116] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: LLM用于LDA的后处理阶段有提升，但用于初始化阶段则效果不佳，并非总是优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探究将LLM融入主题模型（LDA）的有效性，以期提升主题模型的性能，并验证LLM是否总是优于传统文本挖掘方法的观点。

Method: 将LLM融入LDA模型的初始化和后处理两个阶段。在初始化阶段，通过LLM指导的聚类来初始化Gibbs采样算法。在后处理阶段，利用LLM进行主题的后处理纠正。

Result: 在初始化阶段，LLM指导的聚类虽然在早期迭代中提升了LDA的表现，但对收敛性没有影响，并且在整体表现上不如基线方法。在后处理阶段，LLM的引入使相干性评估指标提升了5.86%。

Conclusion: LLM在LDA的后处理阶段能够带来5.86%的提升，但并未提升其收敛性，并且在初始化阶段表现不如基线。这表明LLM并非在所有方面都优于传统模型。

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [117] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 本论文提出了一种用于多语言短文本情感检测的特征中心框架，评估了文档表示、降维和模型训练，并发现了特定语言的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 本论文介绍了我们为SemEval 2025任务11：弥合基于文本的情感检测的差距（Track A）提出的系统，该任务侧重于短文本中的多标签情感检测。

Method: 我们提出了一个以特征为中心的框架，动态地调整文档表示和学习算法，以优化特定语言的性能。我们的研究评估了三个关键组成部分：28种语言的文档表示、降维和模型训练，并重点分析了其中五种语言。

Result: 结果表明，TF-IDF在低资源语言中仍然非常有效，而像FastText这样的上下文嵌入和像Sentence-BERT产生的基于Transformer的文档表示则表现出特定语言的优势。主成分分析（PCA）在不影响性能的情况下减少了训练时间，特别是对FastText和多层感知器（MLP）等神经网络模型有益。计算效率分析强调了模型复杂性和处理成本之间的权衡。

Conclusion: 该框架提供了一个可扩展的解决方案，用于多语言情绪检测，解决了语言多样性和资源限制的挑战。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [118] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: A new benchmark and leaderboard evaluate LLM performance in up to 200 languages, focusing on low-resource languages, to promote equitable access and transparency in AI.


<details>
  <summary>Details</summary>
Motivation: To ensure equitable access to the benefits of LLMs, it is essential to evaluate their capabilities across the world's languages, especially low-resource languages.

Method: The study introduces the AI Language Proficiency Monitor, a multilingual benchmark that evaluates LLM performance across up to 200 languages. It aggregates diverse tasks such as translation, question answering, math, and reasoning, using datasets like FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. The system includes an open-source, auto-updating leaderboard and dashboard.

Result: The benchmark systematically assesses LLM performance across up to 200 languages, with a focus on low-resource languages. It provides an open-source, auto-updating leaderboard and dashboard with descriptive insights, including a global proficiency map and trends over time, to support researchers, developers, and policymakers.

Conclusion: The AI Language Proficiency Monitor aims to foster transparency, inclusivity, and progress in multilingual AI by providing a comprehensive multilingual benchmark, an open-source leaderboard, and descriptive insights to help identify strengths and gaps in LLM performance across a wide range of languages, with a focus on low-resource languages.

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [119] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT是一种新的布局感知BERT模型，它使用相对极坐标系中的自注意力机制来理解文档，并且在较小的数据集上取得了最先进的成果，证明了其效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 介绍一种新的文档理解模型DocPolarBERT，旨在通过改进的注意力机制来克服对大量预训练数据或绝对二维位置嵌入的依赖。

Method: 提出了一种名为DocPolarBERT的、用于文档理解的、符合布局的BERT模型，它使用相对极坐标系而不是笛卡尔坐标系来扩展自注意力机制，以考虑文本块的位置，从而无需绝对二维位置嵌入。

Result: 尽管DocPolarBERT的预训练数据集比常用的IIT-CDIP语料库小六倍多，但它取得了最先进的结果。

Conclusion: DocPolarBERT通过精心设计的注意力机制，即使在较小的预训练数据集上也能取得最先进的结果，证明了其在文档理解方面的效率和有效性。

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [120] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 该研究首次广泛分析了使用LLM和提示算法的可用争论数据集，并对模型进行了基准测试，展示了现有提示算法的弱点，并指出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 为了应对虽然大型语言模型（LLM）在争论挖掘领域取得了显著进展，但缺乏对其在公开争论分类数据库上运行情况的广泛研究和结果。

Method: 对选定的LLM（包括GPT、Llama和DeepSeek的版本，以及结合了思维链算法的推理增强变体）进行了测试，使用了Args.me和UKP等多种数据集。

Result: ChatGPT-4o在争论分类基准测试中表现优于其他模型，而结合了推理能力并采用思维链算法的模型中，Deepseek-R1表现出优越性。该研究还讨论了所有模型中最常见的错误，并展示了现有争论数据集的局限性。

Conclusion: 尽管像ChatGPT-4o和Deepseek-R1这样的模型在争论分类基准测试中表现出色，但它们仍然会犯错，并且在已知的提示算法在论证分析方面存在一些弱点，但也指明了改进方向。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [121] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: ASR错误转录在说话人归因任务中表现良好，甚至优于人类转录。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，常遇到由ASR系统产生的带有错误的转录文本，而以往的研究主要关注由人类标注的转录文本的说话人归因问题。因此，研究ASR转录对说话人归因的影响具有实际意义。

Method: 本研究通过实证研究，分析了ASR转录错误对说话人归因性能的影响程度，并探讨了ASR系统的属性如何影响归因结果。

Result: 研究发现，说话人归因对于单词级别的转录错误具有出乎意料的鲁棒性，并且恢复真实转录的目标与归因性能的相关性很小。在有ASR错误的转录上进行说话人归因，效果可能与使用人类转录数据相当，甚至更好，这可能是因为ASR的转录错误捕捉到了能揭示说话人身份的说话人特定特征。

Conclusion: 本研究首次全面研究了自动语音识别（ASR）转录对说话人归因性能的影响。研究发现，说话人归因对于包含ASR错误的转录具有一定的鲁棒性，并且ASR的转录准确性与说话人归因性能之间的相关性很低。结果表明，在有ASR错误的转录上进行说话人归因，效果可能与使用人类转录数据相当，甚至更好。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [122] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: KELPS是一个神经符号框架，通过将非正式数学转化为多种形式化语言，解决了多语言平行语料库的限制，并在数学定理证明任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在将非正式数学转化为机器可验证定理方面取得了进展，但面临多语言平行语料库数量和质量有限的瓶颈。

Method: KELPS框架是一个迭代过程，首先将自然语言翻译为新设计的知识方程（KEs），然后通过严格定义的规则将其转换为目标形式化语言（Lean、Coq、Isabelle），保证了句法结构和语义的保持。

Result: KELPS框架生成了超过60,000个问题的平行语料库，并在MiniF2F数据集上实现了88.9%的句法准确率（pass@1），超过了Deepseek-V3（81%）和Herald（81.3%）等现有最先进模型。

Conclusion: 该研究提出了一种新颖的神经符号框架KELPS，通过将非正式数学转化为多种形式化语言（Lean、Coq和Isabelle），有效解决了多语言平行语料库数量和质量有限的问题，并在MiniF2F数据集上达到了88.9%的准确率，优于现有最先进模型。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [123] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 本研究提出了一种名为KGA的即时知识图谱增强框架，用于LLM。它通过一种无需参数更新的动态知识融合机制，解决了现有方法的局限性，并在实验中表现出与传统方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱增强方法多依赖于参数微调，容易导致灾难性遗忘并降低预训练模型的泛化能力。同时，静态的集成框架也限制了它们对实时知识更新的适应性。因此，需要一种无需参数更新且能适应实时知识更新的知识图谱增强方法。

Method: 本研究提出了一种即时知识图谱增强框架，核心是知识图谱引导注意（KGA）模块。该模块通过外向聚合和内向聚合两个通路，在测试时动态融合知识图谱信息，无需更新模型参数。外向聚合通过输入驱动的知识图谱融合将外部知识整合到输入表示中；内向聚合则通过知识图谱引导的过滤来精炼输入表示，抑制无关信号并增强相关知识模式。

Result: 通过外向和内向聚合的协同作用，该框架能够在测试时进行实时的知识融合，并且无需任何参数修改。在五个基准测试上的大量实验证明了KGA在知识融合方面具有可比的性能。

Conclusion: 该研究首次提出了一个用于大型语言模型（LLM）的即时知识图谱增强框架，通过知识图谱引导注意（KGA）模块实现动态知识融合，无需参数更新。该框架通过外向和内向聚合两种机制，分别实现了外部知识的动态整合和任务无关信号的抑制与相关知识的增强，形成了一个闭环增强机制。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [124] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: MM-Coder是一个多语言多模态软件开发工具，它结合了文本和UML图/流程图等视觉信息来生成代码，旨在解决现有代码生成模型忽视视觉辅助工具的问题。我们还发布了一个新的数据集（MMc-Instruct）和一个评估基准（MMEval），以推动该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在代码生成方面取得了显著进展，但大多数模型仅限于处理文本信息，忽略了现实世界软件开发中常用的图表和流程图等关键视觉辅助工具。为了解决这一差距，我们的工作旨在让LLMs能够理解和实现通过文本和视觉设计传达的复杂规范，从而革新工业编程。

Method: 我们开发了一个名为MM-Coder的多语言多模态软件开发工具。该工具通过整合UML图和流程图（统称为Visual Workflow）等视觉输入与文本指令，来提升代码生成的准确性和架构对齐。我们还创建了一个名为MMc-Instruct的多样化多模态指令调优数据集，其中包含基于Visual Workflow的代码生成任务。此外，我们提出了MMEval，一个用于评估多模态代码生成的新基准。

Result: 通过在MMEval基准上的评估，我们发现当前模型在精确捕获视觉信息、遵循指令和掌握高级编程知识方面仍面临重大挑战。这表明多模态代码生成领域仍有很大的发展空间。

Conclusion: MM-Coder通过整合UML图和流程图等视觉信息，并辅以文本指令，显著提升了代码生成的准确性和架构对齐能力。为实现这一点，我们开发了MMc-Instruct多模态指令调优数据集，并引入了MMEval基准来评估多模态代码生成能力。评估结果表明，在精确捕捉视觉信息、遵循指令和高级编程知识方面仍存在挑战，但MM-Coder的出现旨在通过让大型语言模型理解和实现文本与视觉设计相结合的复杂规范，彻底改变工业编程。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [125] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 缓存转向是一种通过键值缓存单次干预来引导语言模型的方法，无需微调或提示修改，可用于诱导链式思考推理，并提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 为了在无需进行微调或修改提示的情况下，隐式地引导语言模型（例如，诱导小语言模型进行链式思考推理）。

Method: 通过GPT-4o生成的推理轨迹来构建转向向量，将模型行为引导至更明确、多步骤的推理，而无需进行微调或修改提示。

Result: 缓存转向成功地诱导了小语言模型的链式思考推理，提高了模型推理的定性结构和定量任务性能。与需要连续干预的先前激活转向技术相比，单次缓存转向在超参数稳定性、推理效率和易于集成方面具有优势。

Conclusion: 缓存转向是一种轻量级的方法，通过直接对键值缓存应用单次干预来隐式地引导语言模型。实验评估表明，缓存转向可以提高模型推理的定性结构和定量任务性能，并且在超参数稳定性、推理效率和易于集成方面具有优势。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [126] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: QRPINNs 使用低差异序列代替随机抽样，在解高维 PDE 时比 PINNs 和自适应抽样方法有更好的收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 虽然物理信息神经网络在通过将物理约束整合到神经网络训练中来求解偏微分方程 (PDE) 方面显示出前景，但其性能对采样点敏感。

Method: 提出了一种名为拟牛顿物理信息神经网络 (QRPINNs) 的方法，该方法使用低差异序列进行采样，而不是直接从域中随机采样点。

Result: 理论上，已证明 QRPINNs 比 PINNs 具有更好的收敛率。经验上，实验表明 QRPINNs 显著优于 PINNs 和一些代表性的自适应采样方法，特别是在高维 PDE 中。

Conclusion: QRPINNs 显著优于 PINNs 和一些代表性的自适应采样方法，特别是在高维 PDE 中。此外，将 QRPINNs 与自适应采样相结合可以进一步提高性能。

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [127] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 因果学习为网络物理系统异常检测提供了一种可解释、自适应的方法，解决了黑盒模型的局限性，并为未来研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的异常检测方法（主要依赖黑盒深度学习）在可解释性、分布变化适应性和动态系统鲁棒性方面存在挑战。因果学习被提倡用于解决这些问题。

Method: 提出并形式化了因果图谱构建、多视角融合和持续因果图学习三个关键方向，以发现跨越时间和空间的动态因果关系。

Result: 通过真实世界的水处理基础设施等系统案例说明，因果模型能提供早期预警信号和根本原因归因，克服了黑盒检测器的局限性。

Conclusion: 因果学习为网络物理系统中的异常检测提供了可扩展、自适应、可解释且具有空间定位能力的解决方案，有望推动网络安全研究的范式转变，以应对互联基础设施中不断演变的威胁。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [128] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: SPLASH是一种新的逆强化学习方法，可以从次优演示中学习长期和对抗性任务，并在机器人应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数逆强化学习方法假设专家演示是可用的，或者无法处理次优演示，而SPLASH旨在解决这些限制，以便在长期和对抗性任务中进行机器人学习。

Method: SPLASH（Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations）通过从次优演示中学习，解决了在长期和对抗性任务中逆强化学习的局限性。

Result: SPLASH在模拟的海上夺旗任务和真实的自主无人水面飞行器上进行了验证，并在从次优演示中学习奖励方面取得了显著成果。

Conclusion: SPLASH在奖励学习方面显著优于从次优演示中学习的最先进方法，并在模拟和真实世界的自主无人水面飞行器上进行了经验验证。

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [129] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种创新的联邦小样本学习框架，通过元随机梯度下降和差分隐私技术，有效解决了医疗数据标注稀缺和隐私保护的挑战，能够准确诊断呼吸系统疾病。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注的劳动密集性导致高质量标记数据集稀缺，尤其是在资源受限的环境下。患者隐私问题阻碍了跨机构的数据共享，而现有的集中式数据驱动方法可能损害隐私。

Method: 提出了一种联邦小样本学习框架，其中包含元随机梯度下降算法和差分隐私机制。通过加权平均算法聚合本地模型。

Result: 实验结果表明，所提出的方法在实现差分隐私的同时，能够有效诊断呼吸系统疾病，并且具有良好的跨数据结构、类别和分布的适应性。

Conclusion: 该研究提出了一种联邦小样本学习框架，结合了隐私保护机制，以解决呼吸系统疾病诊断中标记数据有限和隐私保护的问题。通过引入元随机梯度下降算法来缓解数据不足导致的过拟合问题，并集成差分隐私噪声来防止梯度泄露和重构医疗图像。同时，采用加权平均算法聚合来自不同客户端的本地诊断模型，以提高模型在不同场景下的适应性。实验结果表明，该方法在实现差分隐私的同时，能够有效诊断不同结构、类别和分布的呼吸系统疾病。

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [130] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 提出了一种改进的 TPE 算法，用于组合优化，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: TPE 在深度学习领域得到了广泛应用，但其在化学和生物学等领域的组合优化应用仍是未被充分挖掘的重要课题。

Method: 提出了一种泛化分类核并引入距离结构以处理大的组合搜索空间，并降低了核计算的时间复杂度。

Result: 实验结果表明，所提出的方法比原始 TPE 能用更少的评估次数识别出更好的解决方案。

Conclusion: 该研究提出了一种用于 TPE 的高效组合优化算法，该算法通过泛化并修改分类核来处理大的组合搜索空间，并在合成问题实验中验证了其优于原始 TPE 的性能。

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [131] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 本研究利用量子机器学习和可穿戴传感器数据，开发了一种更具隐私保护性的情绪识别方法，其性能优于传统方法，特别适用于沟通能力受损的人群。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种比传统面部识别技术更具隐私保护性的情绪识别替代方案，研究了仅从生理信号推断情绪状态的可能性。

Method: 通过对经典机器学习算法和混合量子机器学习（QML）方法（包括量子核方法）进行性能比较，研究了仅从生理信号推断情绪状态的可行性。

Result: 量子增强的SVM在所有情绪类别的分类性能上均优于经典方法，即使在有限的数据集上，F1分数也超过80%，召回率最高可提高36%。

Conclusion: 该研究展示了仅从生理信号推断情绪状态的可行性，并提出了一种保护隐私的替代面部识别技术。研究结果表明，量子增强的SVM在分类性能上优于经典方法，即使在有限的数据集上也是如此，F1分数超过80%，召回率最高可提高36%。将可穿戴传感器数据与量子机器学习相结合，不仅提高了准确性和鲁棒性，还实现了不易察觉的情绪识别。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [132] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: 量子机器学习（QML）利用量子计算来增强人工智能，并在处理复杂数据方面具有潜力。本综述探讨了其理论基础、应用（如量子化学和传感）和挑战（如NISQ设备的噪声和可扩展性）。虽然QML有潜力，但其广泛应用仍取决于克服技术和方法上的障碍。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（QML）代表了量子计算和人工智能交叉领域的一个有前途的前沿，旨在利用量子计算优势来加强数据驱动的任务。本综述探讨了量子机器学习解决经典机器学习的计算瓶颈的潜力，特别是在处理复杂数据集方面。

Method: 本综述介绍了量子机器学习的理论基础，包括量子数据编码、量子学习理论和优化技术，并根据数据类型和计算架构对量子机器学习方法进行了分类。对量子主成分分析、量子增强传感和材料科学中的应用进行了关键评估，重点关注其理论加速和实际局限性。

Result: 评估了量子主成分分析、量子增强传感和材料科学中的应用的关键进展，并讨论了噪声中等规模量子（NISQ）设备带来的挑战，包括硬件噪声、可扩展性限制和数据编码开销。

Conclusion: 虽然量子机器学习在量子化学和传感等特定应用方面具有巨大潜力，但其在现实世界场景中的更广泛效用取决于克服技术和方法上的挑战。

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [133] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: PHQFNO是一种新的混合量子-经典模型，可用于科学机器学习，在求解流体力学问题时精度更高、稳定性更好。


<details>
  <summary>Details</summary>
Motivation: 介绍PHQFNO，作为QFNO的泛化，用于科学机器学习，能够实现跨经典和量子资源的混合计算，并扩展到更高维度和分布式执行。

Method: PHQFNO将傅里叶算子计算划分为经典和量子资源，实现了可调的量子-经典混合和跨量子经典设备的分布式执行。该方法将QFNO扩展到更高维度，并结合了消息传递框架来跨不同分区分发数据。输入数据使用一元编码编码为量子态，量子电路参数使用变分方案进行优化。PHQFNO使用PennyLane和PyTorch集成实现。

Result: PHQFNO在伯格斯方程、不可压和可压纳维-斯托克斯方程上进行了评估，恢复了经典的FNO精度，在不可压纳维-斯托克斯方程上取得了比经典方法更高的精度，并在输入噪声下表现出比经典基线更好的稳定性。

Conclusion: PHQFNO在求解纳维-斯托克斯方程等问题上，在保持经典FNO精度的同时，实现了更高的精度，并且在输入噪声下表现出比经典基线更好的稳定性。

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [134] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: ALCo-FM通过动态选择上下文窗口、多模态数据融合和先进的Transformer架构，在交通事故风险预测方面取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 交通事故是罕见但影响重大的事件，需要长上下文多模态推理来进行准确的风险预测。

Method: ALCo-FM是一个统一的自适应长上下文基础模型，通过浅层交叉注意力对多模态数据进行编码和融合，并利用局部GAT层和基于BigBird的稀疏全局Transformer在H3六边形网格上进行处理。它还采用蒙特卡洛Dropout进行置信度估计，并通过计算波动性预评分来动态选择上下文窗口。

Result: ALCo-FM在模拟训练数据和在五个未公开的城市进行微调后，在准确率、F1分数和ECE等指标上均表现出色，证明了其在城市风险预测方面的有效性。

Conclusion: ALCo-FM在城市风险预测方面取得了优于20多种最先进基线模型的性能，实现了0.94的准确率、0.92的F1分数和0.04的ECE。

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [135] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 介绍了一种名为InsightBuild的框架，该框架利用因果推断和大型语言模型来解释智能建筑中的异常能耗模式，帮助设施经理提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 智能建筑产生大量传感器和控制数据，但设施经理常常缺乏对异常能源使用情况的明确解释。

Method: 提出一个名为InsightBuild的两阶段框架，该框架整合了因果分析和一个经过微调的大型语言模型（LLM），用于提供可读的、因果关系的能耗模式解释。第一阶段：一个轻量级的因果推断模块，在来自Google Smart Buildings和Berkeley Office数据集的建筑遥测数据（例如，温度、暖通空调设置、入住率）上应用格兰杰因果关系检验和结构因果发现。第二阶段：一个在带有文本解释的传感器级原因对上进行了微调的LLM，接收检测到的因果关系作为输入，并生成简洁、可操作的解释。

Result: 在两个真实世界数据集（Google：2017-2022；Berkeley：2018-2020）上评估了InsightBuild，并使用专家注释的真实原因来验证其在已隐藏异常集上的表现。结果表明，该方法能够生成有助于诊断和缓解能源效率低下的清晰、精确的解释。

Conclusion: 结合显式因果发现和基于大语言模型（LLM）的自然语言生成，可以为设施经理提供清晰、精确的解释，以帮助他们诊断和缓解能源效率低下问题。

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [136] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: 量子-UnIMP利用量子计算增强了LLM在混合数据插补任务中的表现，通过量子特征图捕捉更复杂的模式，从而在减少插补误差和提高分类准确率方面取得了显著进步。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）在处理表格数据插补方面虽然能力出色，但依赖于经典的嵌入方法，这限制了它们捕捉混合类型数据（包括数值、分类和文本特征）中复杂的非线性相关性的能力。为了解决这一挑战，本研究旨在通过引入量子计算来增强LLM的插补性能。

Method: 本研究提出了量子-UnIMP框架，该框架将浅层量子电路集成到基于大语言模型（LLM）的插补架构中。其核心创新在于使用由瞬时量子多项式（IQP）电路生成的量子特征图来替换传统经典输入嵌入。

Result: 实验结果表明，量子-UnIMP在混合类型数据集上的表现优于最先进的经典和基于LLM的方法。具体而言，它将数值特征的插补误差（RMSE）降低了高达15.2%，并将分类任务的准确率（F1分数）提高了8.7%。

Conclusion: 量子-UnIMP框架通过整合浅层量子电路到基于LLM的插补架构中，并使用IQP电路生成的量子特征图替代经典的输入嵌入，成功利用了量子叠加和纠缠等现象，学习到更丰富的数据表示，从而在混合类型数据插补方面取得了显著成效，其在数值特征上的插补误差最多减少了15.2%（RMSE），在分类任务上的准确率最多提高了8.7%（F1分数），证明了量子增强表示在处理复杂数据插补任务中的巨大潜力，即使在近期量子硬件上也能实现。

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [137] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: 通过结合长轮次的监督微调（SFT）和强化学习（RL），可以提高大型语言模型（LLMs）在数学推理方面的准确性和效率。研究表明，长轮次的 SFT 是提高性能的关键，而强化学习则用于优化解决方案的长度。该方法在 AI 数学奥林匹克竞赛（AIMO）等基准测试中表现出色，并将在 GitHub 上开源以供复现。


<details>
  <summary>Details</summary>
Motivation: 解决如何有效结合监督微调（SFT）和强化学习（RL）以最大化大型语言模型（LLMs）在数学推理方面的准确性和效率这一关键挑战。

Method: 提出了一种结合了扩展监督微调（SFT）和来自在线推理的强化学习（GRPO）的实际且有效的训练方法。该方法首先通过长时间的 SFT 阶段将模型的准确性推向极限，然后通过 GRPO 阶段在保持峰值性能的同时显著提高代币效率。

Result: 实验表明，将 SFT 扩展到 10 个轮次对于性能突破至关重要，而 GRPO 在此框架中的主要作用是优化解决方案的长度。该方法在包括 AI 数学奥林匹克竞赛（AIMO）在内的具有挑战性的基准测试中取得了顶级性能，并在 2,200 多个团队中名列前茅。

Conclusion: 该研究提供了一个经过实战检验的蓝图，用于开发既非常准确又非常高效的先进数学推理模型。通过将扩展的监督微调（SFT）与来自在线推理的强化学习（GRPO）相结合，并在具有挑战性的基准测试中取得了顶级性能，包括在 AI 数学奥林匹克竞赛（AIMO）中名列前茅。

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [138] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 本研究提出了一种创新的安全护栏框架，利用合成数据生成和对抗性训练，使小型语言模型在内容审核任务中的表现优于大型模型，同时降低了成本和提高了安全性。


<details>
  <summary>Details</summary>
Motivation: 为了应对内容审核任务中对安全护栏的需求，并探索小型语言模型（SLM）在其中实现高绩效的可能性，本研究旨在提供一种轻量级、高效且具有成本效益的解决方案，以应对大型模型带来的高昂计算成本和潜在的脆弱性。

Method: 本研究提出了一种轻量级的安全护栏框架，利用高保真合成数据生成和对抗性训练。该方法首先使用人类策划的种子数据，通过查询增强和释义来生成多样化和丰富的示例。然后，借鉴生成对抗网络（GAN）架构，采用强化学习来指导生成器创建具有挑战性的合成示例，并用于微调安全分类器，从而提高其检测和减轻有害内容的能力。此外，研究还结合了高效语言模型训练的策略，利用小型模型的强大功能来提升大型生成模型的性能。

Result: 研究表明，通过高保真合成数据生成和对抗性训练，小型语言模型在内容审核任务中的表现能够达到甚至超越大型模型。该框架有效提升了小型模型在检测和减轻有害内容方面的能力，并增强了其对恶意攻击的抵御能力。

Conclusion: 该框架使小型语言模型（SLM）能够作为强大的安全护栏，从而降低计算开销并提高对恶意攻击的抵御能力，为人工智能系统中的内容审核提供了一种可扩展且高效的解决方案。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [139] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: WERSA 是一种具有线性 O(n) 时间复杂度的注意力机制，可在不影响性能的情况下处理长序列。它在各种任务和模型上均优于现有方法，并且在处理超长序列时尤其有效，可用于更可持续和可扩展的 AI 开发。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在长序列上计算成本高昂，因为标准的注意力机制具有二次方的 O(n^2) 时间复杂度。WERSA 是一种新颖的线性 O(n) 时间复杂度机制，能够成功处理长序列而没有性能损失。

Method: WERSA 融合了内容自适应随机谱特征、多分辨率哈尔小波和可学习参数，以选择性地关注数据信息量大的尺度，同时保持线性效率。

Result: 大型规模的比较（在单 GPU 上）以及跨各种基准（视觉、NLP、分层推理）和各种注意力机制（如多头注意力、FlashAttention-2、FNet、Linformer、Performer、Waveformer）的比较显示了 WERSA 的普遍优势。它在所有测试中都取得了最佳准确率。在 ArXiv 分类任务上，WERSA 将准确率提高了 1.2%（86.2% vs 85.0%），同时将训练时间减少了 81%（296 秒 vs 1554 秒），并将 FLOPS 减少了 73.4%（26.2G vs 98.4G）。值得注意的是，在标准的和 FlashAttention-2 失败的情况下，WERSA 表现出色：在 ArXiv-128k 的超长序列上，它在可行的 WERSA 方法中取得了最佳准确率（79.1%）和 AUC（0.979），处理的 OOM 错误数据量是二次方法无法处理的，并且速度是其次佳竞争对手 Waveformer 的两倍。

Conclusion: WERSA 通过显著降低计算负载且不影响准确性，使得更实用、更经济的长上下文模型成为可能，尤其是在资源受限的硬件上，从而实现更可持续、更可扩展的AI开发。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [140] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: BlockFFN是一种新型的MoE架构，通过改进路由和引入新的训练目标，实现了更高的token级和块级稀疏性，并结合了推测解码技术，从而在端侧设备上实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 为了减轻大型语言模型（LLMs）的计算负担，具有激活稀疏性的架构（如MoE）受到关注。然而，原始MoE的不可微分和不灵活路由会损害模型性能。此外，稀疏激活架构虽然在token级别实现了稀疏，但在块级别稀疏性较低，这不利于低资源条件下的加速，并且与主流加速技术（如推测解码）不兼容。

Method: 提出了一种名为BlockFFN的新型MoE架构，并结合了其高效训练和部署技术。具体来说，采用整合ReLU激活和RMSNorm的路由器进行路由，设计了CLS感知训练目标来同时提升TLS和CLS，并实现了高效加速内核，结合了激活稀疏性和推测解码。

Result: 实验结果表明，BlockFFN的性能优于其他MoE基线，实现了超过80%的TLS和70%的8-token CLS。所提出的内核在真实端侧设备上实现了比密集模型高达3.67倍的加速。

Conclusion: BlockFFN通过整合ReLU激活和RMSNorm的路由器实现了可微分和灵活的路由，并通过设计CLS感知训练目标来提升token级稀疏性（TLS）和块级稀疏性（CLS），使其更易于加速。此外，首次结合了激活稀疏性和推测解码的高效加速内核，在真实端侧设备上实现了比密集模型高达3.67倍的加速。

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [141] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: Attention heads following space-filling curves generalize transformers to geometric domains, exemplified by the Spiroformer on the 2-sphere.


<details>
  <summary>Details</summary>
Motivation: Transformers excel with sequential data, but generalizing them to geometric domains like manifolds is challenging due to the lack of a well-defined global order.

Method: The proposed solution involves attention heads following a space-filling curve, with an experimental example being the Spiroformer which follows a polar spiral on the 2-sphere.

Result: The Spiroformer is presented as a transformer that follows a polar spiral on the 2-sphere, demonstrating the potential of space-filling curves for generalizing transformers to geometric domains.

Conclusion: Transformers can be generalized to geometric domains like manifolds by using attention heads that follow a space-filling curve.

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [142] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 生成式奖励模型（LLM作为评判者）在RLVR中存在漏洞，容易被表面变化（如符号或推理开头词）操纵，导致错误奖励。研究提出数据增强策略以提高鲁棒性，并强调需要更可靠的LLM评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的奖励模型在强化学习可验证奖励（RLVR）中被广泛应用，但它们容易受到表面操纵的影响，可能导致评估不准确，这对于依赖这些模型的算法（如拒绝采样、偏好优化和RLVR）构成威胁。

Method: 本研究通过实验发现生成式奖励模型（LLM作为评判者）在用于强化学习可验证奖励（RLVR）时，存在对表面变化（如非单词符号或推理开头词）敏感的漏洞。研究人员提出了一种数据增强策略来训练一个更鲁棒的奖励模型，以应对这些挑战。

Result: 研究发现，非单词符号（如“:”或“.”）或推理开头词（如“Thought process:”和“Let's solve this problem step by step.”）等表面变化会导致生成式奖励模型给出错误的奖励。这种弱点在不同的LLM、数据集和提示格式中普遍存在。通过采用一种数据增强策略，研究人员训练了一个具有显著提高的鲁棒性的奖励模型。

Conclusion: 尽管LLM作为评判者在评估答案质量方面有其优势，但它们容易受到表面变化的操纵，例如添加非单词符号或推理开头词，这可能导致错误的奖励。为了解决这个问题，我们提出了一种数据增强策略来训练一个更鲁棒的奖励模型，并强调需要更可靠的基于LLM的评估方法。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [143] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: AdamW、Lion 和 Sophia 是三种主要的 LLM 优化器。在本次研究中，虽然它们的结果相似，但 AdamW 在下游任务中表现最好，Lion 速度最快，Sophia 损失最低。


<details>
  <summary>Details</summary>
Motivation: 为了减少大型语言模型（LLM）的预训练时间并获得性能更好的模型，优化器的选择至关重要。

Method: 研究人员使用最大更新参数化和更小的代理模型，为每种基础架构和优化器的组合分别调整了相关超参数。他们使用了单 эпох和多 эпох的方法，同时保持 token 数量不变，并在两种不同的基础架构上进行训练。

Result: 所有三种优化器的结果都在相似的范围内，但 Sophia 的训练和验证损失最低，Lion 的训练速度最快（以 GPU 小时计算），而 AdamW 在下游评估中表现最佳。

Conclusion: AdamW 在下游评估中表现最佳，Lion 的训练速度最快，而 Sophia 的训练和验证损失最低。

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [144] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: 本研究提出了一种名为FedAKD的联邦学习方法，通过异步知识蒸馏来解决不平衡协变量偏移问题，在提高公平性和预测准确性方面取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法在处理不平衡协变量偏移（一种实际且复杂的异质性形式）方面存在不足。本文旨在解决这一挑战，设计一种能够平衡预测准确性和协作公平性的方法。

Method: FedAKD是一种联邦异步知识蒸馏方法。它包括客户端更新和服务器更新。在客户端更新中，采用基于异步知识蒸馏的策略，首先用传统知识蒸馏更新客户端模型，然后选择预测正确且置信度高的样本来更新全局模型。服务器更新聚合所有客户端模型。

Result: 实验结果表明，FedAKD在FashionMNIST、CIFAR10和电子健康记录数据集上表现优于现有方法。

Conclusion: FedAKD在高度异构的数据分布下，显著提高了协作公平性、预测准确性，并促进了客户端参与。

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [145] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 提出 LoGIC，一种基于多智能体强化学习的游戏化方法，用于无监督图像描述。该方法通过训练‘说话者’和‘倾听者’智能体，在不依赖额外标签的情况下，显著提升了图像描述的性能，尤其是在使用预训练模型或轻量级组件时。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有标注数据集已全部用于训练大型视觉语言模型（VLMs），提升其性能面临挑战。因此，研究相对探索不足的无监督图像描述任务变得至关重要。

Method: 提出了一种名为 LoGIC (Lewis Communication Game for Image Captioning) 的多智能体强化学习方法，该方法包含一个‘说话者’（speaker）和一个‘倾听者’（listener）。训练在合作性共同奖励环境下使用 GRPO 算法进行。

Result: 使用预训练 VLMs 作为‘说话者’和 LLM 作为‘倾听者’，通过 LoGIC 微调后达到 46 BLEU 分数，优于未微调的 VLM（44 BLEU 分）。当‘说话者’使用 ViT 和 GPT2 且从头开始训练时，在无监督设置下获得 31 BLEU 分数，优于现有无监督方法 10 分。

Conclusion: LoGIC 在无监督图像描述任务中展现出显著优势，相比现有方法，使用预训练 VLM 时 BLEU 分数提升了 2 个单位（达到 46 分），而使用轻量级组件训练时，BLEU 分数达到 31 分，超越现有无监督方法 10 分。

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [146] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: 测试时自适应（TTA）结合风险监控，可以检测模型失效点。


<details>
  <summary>Details</summary>
Motivation: 为了解决TTA方法只能暂时延长模型寿命，最终仍需重新训练的问题，提出检测模型失效点的需求。

Method: 提出将风险监控框架与TTA相结合，并扩展了基于顺序检验的监控工具，以适应TTA场景和无标签数据的情况。

Result: 所提出的TTA监控框架在多种数据集、分布偏移类型和TTA方法上都展现了有效性。

Conclusion: 测试时自适应（TTA）结合风险监控可以有效地检测模型失效点，确保模型性能。

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [147] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: 在深度学习中，在训练的最后阶段跟踪参数比在整个训练过程中跟踪参数更能有效地防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度神经网络中灾难性遗忘的问题，即学习新任务会因知识覆盖而降低先前学习任务的性能。

Method: 通过在训练的最后阶段跟踪参数来识别对保留旧知识至关重要的参数，认为在最后训练平台期内活动性更高的参数揭示了损失景观中相对平坦的方向，适合适应新任务并保留先前任务的知识。

Result: 提出的方法在平衡灾难性遗忘缓解和新任务的强性能方面取得了优于现有方法的性能。

Conclusion: 该方法在缓解灾难性遗忘和新任务性能之间取得了卓越的平衡

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [148] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: 一种新的自适应NVAR模型使用可学习的MLP来处理固定非线性的局限性，在高维和噪声数据中表现更好且更具扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准的NVAR和RC模型在预测混沌动力学系统方面有潜力，但它们依赖于固定的非线性（NVAR中的多项式展开或RC中的随机特征映射），这限制了它们对高噪声或真实世界数据的适应性。此外，由于读出计算过程中昂贵的矩阵求逆，这些方法在高维设置下的扩展性也很差。

Method: 提出了一种自适应NVAR模型，结合了延迟嵌入的线性输入和由浅层、可学习的多层感知机（MLP）生成的特征。MLP和线性读出通过基于梯度的优化进行联合训练，使模型能够学习数据驱动的非线性，同时保持简单的读出结构。该方法避免了对脊值和延迟参数进行详尽和敏感的网格搜索，而是将调整限制在神经网络的超参数上，提高了可扩展性。

Result: 在混沌系统上进行的初始实验，包括在无噪声和合成噪声条件下的测试，表明自适应模型在预测精度上优于标准NVAR，并在噪声条件下以较低的观测频率显示出鲁棒的预测能力。

Conclusion: 与标准的NVAR相比，我们提出的自适应模型在预测精度上更优，并且在较低的观测频率和噪声条件下表现出鲁棒的预测能力。

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [149] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 离线强化学习的Q值外推误差问题，通过奖励缩放（RS-LN）和不可行动作惩罚（PA）机制的PARS算法得到解决，并在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在处理Q值外推误差时存在困难，尤其是在Q函数超出数据范围进行线性外推时问题尤为严重。

Method: 提出了一种通过奖励缩放（RS-LN）和不可行动作惩罚（PA）机制来引导Q值在数据范围外逐渐减小的策略，并结合这两种机制开发了名为PARS的新算法。

Result: PARS算法在D4RL基准测试的多个任务中表现出色，无论是在离线训练还是在线微调阶段，其性能均优于当前最先进的算法，并在AntMaze Ultra等挑战性任务中取得了显著成功。

Conclusion: 所提出的PARS算法通过结合奖励缩放（RS-LN）和不可行动作惩罚（PA）机制，成功缓解了离线强化学习中的Q值外推误差问题，并在D4RL基准测试中展现出优于现有算法的性能，尤其在AntMaze Ultra等具有挑战性的任务上取得了显著效果。

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [150] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 提出了一种名为ORAC的探索性方法，用于风险规避约束强化学习，通过最大化奖励的置信上限和最小化成本的置信下限来提高回报-成本权衡，并防止收敛到次优策略。


<details>
  <summary>Details</summary>
Motivation: 风险规避约束强化学习（RaCRL）旨在学习能够最小化由环境固有随机性引起的罕见灾难性约束违反可能性的策略。然而，风险规避通常会导致对环境的保守探索，从而收敛到未能充分最大化回报或未能实现目标的次优策略。

Method: ORAC是一种基于探索的方法，通过最大化状态-动作奖励值函数的局部置信上限并最小化风险规避状态-动作成本值函数的局部置信下限来构建探索性策略。具体来说，在每一步中，如果成本值超过或低于安全约束值，则会增加或减少分配给成本值的权重。

Result: 实验结果表明，ORAC方法能够防止收敛到次优策略，并在各种连续控制任务中显著改善回报-成本权衡。

Conclusion: 所提出的ORAC方法能够防止收敛到次优策略，并在Safety-Gymnasium和CityLearn等各种连续控制任务中显著改善回报-成本权衡。

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [151] [Noise-Enabled Goal Attainment in Crowded Collectives](https://arxiv.org/abs/2507.08100)
*Lucy Liu,Justin Werfel,Federico Toschi,L. Mahadevan*

Main category: cs.RO

TL;DR: 本文研究了有噪声的运动如何帮助代理在拥挤的环境中导航，结合了仿真、理论和机器人实验。研究发现，高于某个临界噪声水平，交通堵塞不会持续。通过分析，研究人员找到了最优的代理密度和噪声水平以最大化目标达成率。简单的反应式导航方法在适度密度下表现良好且计算效率高，为实际应用提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 在拥挤的环境中，个人必须绕过其他居住者以到达目的地。理解和控制这些空间中的交通流量对于协调机器人群和设计密集人群的基础设施至关重要。

Method: 本文结合了仿真、理论和机器人实验来研究有噪声的运动如何扰乱交通堵塞并实现流量，因为代理会到达个体目标。

Result: 在临界噪声水平之上，大的交通堵塞不会持续。基于此观察，我们分析性地将目标达成率近似为噪声水平的函数，然后求解最优的代理密度和噪声水平以最大化群体目标达成率。机器人实验证实了我们的模拟和理论结果。

Conclusion: 简单的反应式方法在适度密度下表现良好，并且比中央规划器更具计算效率，这为实际问题提供了经验教训。

Abstract: In crowded environments, individuals must navigate around other occupants to
reach their destinations. Understanding and controlling traffic flows in these
spaces is relevant to coordinating robot swarms and designing infrastructure
for dense populations. Here, we combine simulations, theory, and robotic
experiments to study how noisy motion can disrupt traffic jams and enable flow
as agents travel to individual goals. Above a critical noise level, large jams
do not persist. From this observation, we analytically approximate the goal
attainment rate as a function of the noise level, then solve for the optimal
agent density and noise level that maximize the swarm's goal attainment rate.
We perform robotic experiments to corroborate our simulated and theoretical
results. Finally, we compare simple, local navigation approaches with a
sophisticated but computationally costly central planner. A simple reactive
scheme performs well up to moderate densities and is far more computationally
efficient than a planner, suggesting lessons for real-world problems.

</details>


### [152] [Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion](https://arxiv.org/abs/2507.08112)
*Lamiaa H. Zain,Hossam H. Ammar,Raafat E. Shalaby*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Obstacle avoidance is crucial for mobile robots' navigation in both known and
unknown environments. This research designs, trains, and tests two custom
Convolutional Neural Networks (CNNs), using color and depth images from a depth
camera as inputs. Both networks adopt sensor fusion to produce an output: the
mobile robot's angular velocity, which serves as the robot's steering command.
A newly obtained visual dataset for navigation was collected in diverse
environments with varying lighting conditions and dynamic obstacles. During
data collection, a communication link was established over Wi-Fi between a
remote server and the robot, using Robot Operating System (ROS) topics.
Velocity commands were transmitted from the server to the robot, enabling
synchronized recording of visual data and the corresponding steering commands.
Various evaluation metrics, such as Mean Squared Error, Variance Score, and
Feed-Forward time, provided a clear comparison between the two networks and
clarified which one to use for the application.

</details>


### [153] [Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning](https://arxiv.org/abs/2507.08224)
*Chan Young Park,Jillian Fisher,Marius Memmel,Dipika Khullar,Andy Yun,Abhishek Gupta,Yejin Choi*

Main category: cs.RO

TL;DR: SelfReVision是一个创新的框架，通过自我改进提升了视觉语言模型在机器人程序规划中的能力，即使是小型模型也能生成高质量、可执行的计划，并优于更大模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在机器人程序规划方面显示出潜力，但其以人类为中心的推理常常忽略机器人执行所需的低级、具体细节。视觉语言模型（VLMs）为实现更具感知基础的计划提供了一条途径，但当前方法要么依赖昂贵的、大规模的模型，要么仅限于狭窄的模拟环境。

Method: SelfReVision是一个轻量级且可扩展的自我改进框架，用于视觉语言程序规划。它从思维链提示和自我指导范式中汲取灵感，使小型VLM能够迭代地进行自我批评、自我修正和自我验证。

Result: 使用从3B到72B不等的模型，结果表明SelfReVision不仅在弱基线VLM的基础上提升了性能，而且在下游具身任务中，其表现优于规模大100倍的模型，并能实现改进的控制。

Conclusion: SelfReVision通过自我提炼循环，能够让小型视觉语言模型（VLM）在没有外部监督或教师模型的情况下，迭代地审视、修改和验证自身的计划，从而生成更高质量、可执行的计划，可用于推理和持续微调。

Abstract: Large language models (LLMs) have shown promise in robotic procedural
planning, yet their human-centric reasoning often omits the low-level, grounded
details needed for robotic execution. Vision-language models (VLMs) offer a
path toward more perceptually grounded plans, but current methods either rely
on expensive, large-scale models or are constrained to narrow simulation
settings. We introduce SelfReVision, a lightweight and scalable
self-improvement framework for vision-language procedural planning.
SelfReVision enables small VLMs to iteratively critique, revise, and verify
their own plans-without external supervision or teacher models-drawing
inspiration from chain-of-thought prompting and self-instruct paradigms.
Through this self-distillation loop, models generate higher-quality,
execution-ready plans that can be used both at inference and for continued
fine-tuning. Using models varying from 3B to 72B, our results show that
SelfReVision not only boosts performance over weak base VLMs but also
outperforms models 100X the size, yielding improved control in downstream
embodied tasks.

</details>


### [154] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: CL3R是一个3D预训练框架，通过结合点云学习和2D模型融合，提升了机器人操控的感知能力和跨视点泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将预训练的2D基础模型用于机器人感知时，难以捕捉3D空间信息和跨越不同相机视点进行泛化，这在精细的机器人操控任务中限制了策略的有效性。

Method: CL3R框架集成了点云掩码自动编码器以学习3D表示，并利用对比学习融合预训练的2D基础模型以传递语义知识。通过统一坐标系和随机融合多视图点云，解决了相机视点模糊性问题，提高了泛化能力。

Result: 实验结果表明，CL3R在模拟和真实世界环境中均优于现有方法，有效提升了机器人操控策略的学习能力，实现了从新视角进行的鲁棒感知。

Conclusion: CL3R通过结合点云掩码自动编码器和对比学习，有效解决了现有方法在3D空间信息捕捉和跨视点泛化能力方面的不足，显著提升了机器人操控策略的学习效果。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [155] [Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots](https://arxiv.org/abs/2507.08303)
*Yang Zhang,Zhanxiang Cao,Buqing Nie,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: 提出一种鲁棒对抗训练方法，通过可学习的对抗攻击网络识别并利用运动策略的弱点，增强机器人运动的鲁棒性，以解决仿真到现实的差距问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习驱动的运动策略在真实机器人中常常因为仿真到现实的动力学差异而出现鲁棒性下降的问题，影响了机器人的敏捷性。

Method: 提出了一种新颖的鲁棒对抗训练范式，该范式包含一个可学习的对抗攻击网络，用于识别运动策略中的漏洞并施加有针对性的扰动，从而迫使运动策略通过动态对抗训练来提高其对扰动的鲁棒性。

Result: 实验结果表明，该方法显著增强了机器人真实世界运动的鲁棒性，能够成功穿越崎岖地形和实现高敏捷的全身体轨迹跟踪。

Conclusion: 该方法显著增强了机器人真实世界运动的鲁棒性，使其能够成功穿越崎岖地形和实现高敏捷的全身体轨迹跟踪。

Abstract: Humanoid robots show significant potential in daily tasks. However,
reinforcement learning-based motion policies often suffer from robustness
degradation due to the sim-to-real dynamics gap, thereby affecting the agility
of real robots. In this work, we propose a novel robust adversarial training
paradigm designed to enhance the robustness of humanoid motion policies in real
worlds. The paradigm introduces a learnable adversarial attack network that
precisely identifies vulnerabilities in motion policies and applies targeted
perturbations, forcing the motion policy to enhance its robustness against
perturbations through dynamic adversarial training. We conduct experiments on
the Unitree G1 humanoid robot for both perceptive locomotion and whole-body
control tasks. The results demonstrate that our proposed method significantly
enhances the robot's motion robustness in real world environments, enabling
successful traversal of challenging terrains and highly agile whole-body
trajectory tracking.

</details>


### [156] [Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles](https://arxiv.org/abs/2507.08349)
*Junhui Wang,Yan Qiao,Chao Gao,Naiqi Wu*

Main category: cs.RO

TL;DR: 提出了一种无需人工靶标的激光雷达-GINS外部校准方法，该方法利用GINS的安装高度来解决平面运动中的可观测性问题，并通过联合优化框架提高了校准精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能采矿环境中，为了实现可靠的传感器融合，需要对多个激光雷达传感器和GNSS辅助惯性导航系统（GINS）进行准确的外部校准。这种校准通过将来自车载传感器的数据对齐到一个统一的全局参考系，从而实现车辆-道路协作。然而，现有方法通常依赖于人工靶标、重叠的视场或精确的轨迹估计，这些假设在实际中可能不成立。此外，矿用车辆的平面运动会导致可观测性问题，从而降低校准性能。

Method: 提出了一种无需人工靶标的外部校准方法，该方法将多个车载激光雷达传感器与GINS坐标系对齐，无需重叠的传感器视场或外部靶标。该方法引入了基于GINS单元已知安装高度的观测模型，以约束平面运动下的不可观测校准参数。通过整合来自几何对应和运动一致性的多个约束，开发了一个联合优化框架来同时优化外部参数和GINS轨迹。

Result: 开发了一种无需人工靶标的外部校准方法，该方法将多个车载激光雷达传感器与GINS坐标系对齐，无需重叠的传感器视场或外部靶标。该方法引入了基于GINS单元已知安装高度的观测模型，以约束平面运动下的不可观测校准参数。通过整合来自几何对应和运动一致性的多个约束，开发了一个联合优化框架来同时优化外部参数和GINS轨迹。

Conclusion: 该方法适用于异构激光雷达配置，包括机械和固态传感器。仿真和实际数据集上的大量实验证明了该方法在各种传感器设置下的准确性、鲁棒性和实用性。

Abstract: Accurate extrinsic calibration between multiple LiDAR sensors and a
GNSS-aided inertial navigation system (GINS) is essential for achieving
reliable sensor fusion in intelligent mining environments. Such calibration
enables vehicle-road collaboration by aligning perception data from
vehicle-mounted sensors to a unified global reference frame. However, existing
methods often depend on artificial targets, overlapping fields of view, or
precise trajectory estimation, which are assumptions that may not hold in
practice. Moreover, the planar motion of mining vehicles leads to observability
issues that degrade calibration performance. This paper presents a targetless
extrinsic calibration method that aligns multiple onboard LiDAR sensors to the
GINS coordinate system without requiring overlapping sensor views or external
targets. The proposed approach introduces an observation model based on the
known installation height of the GINS unit to constrain unobservable
calibration parameters under planar motion. A joint optimization framework is
developed to refine both the extrinsic parameters and GINS trajectory by
integrating multiple constraints derived from geometric correspondences and
motion consistency. The proposed method is applicable to heterogeneous LiDAR
configurations, including both mechanical and solid-state sensors. Extensive
experiments on simulated and real-world datasets demonstrate the accuracy,
robustness, and practical applicability of the approach under diverse sensor
setups.

</details>


### [157] [Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework](https://arxiv.org/abs/2507.08364)
*Deteng Zhang,Junjie Zhang,Yan Sun,Tao Li,Hao Yin,Hongzhao Xie,Jie Yin*

Main category: cs.RO

TL;DR: 该研究通过新的数据集和融合框架提升SLAM在复杂环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有针对结构化环境的SLAM方法在处理极端情况时鲁棒性不足，并且缺乏标准化的基准测试来系统评估SLAM算法在不同退化场景下的表现。此外，现有的SLAM框架主要融合有限的传感器类型，未能有效解决不同环境条件下的自适应传感器选择策略问题。

Method: 提出了一种名为Ground-Fusion++的模块化多传感器融合框架，并结合了GNSS、RGB-D、LiDAR、IMU和轮式里程计。同时，构建了一个包含视觉挑战、LiDAR退化、车轮打滑和GNSS拒止等多种退化场景的M3DGR数据集，并在该数据集上对40种SLAM系统进行了综合评估。

Result: 研究构建了M3DGR数据集，并在其中对40种SLAM系统进行了评估，揭示了它们在挑战性真实条件下的鲁棒性和局限性。所提出的Ground-Fusion++框架通过融合多种传感器（GNSS、RGB-D、LiDAR、IMU和轮式里程计）展现了稳健的性能。

Conclusion: 该研究通过M3DGR数据集和Ground-Fusion++框架，为SLAM算法在结构化环境下的鲁棒性评估和提升提供了新的解决方案，解决了现有基准测试和传感器融合方法的局限性。

Abstract: Considerable advancements have been achieved in SLAM methods tailored for
structured environments, yet their robustness under challenging corner cases
remains a critical limitation. Although multi-sensor fusion approaches
integrating diverse sensors have shown promising performance improvements, the
research community faces two key barriers: On one hand, the lack of
standardized and configurable benchmarks that systematically evaluate SLAM
algorithms under diverse degradation scenarios hinders comprehensive
performance assessment. While on the other hand, existing SLAM frameworks
primarily focus on fusing a limited set of sensor types, without effectively
addressing adaptive sensor selection strategies for varying environmental
conditions.
  To bridge these gaps, we make three key contributions: First, we introduce
M3DGR dataset: a sensor-rich benchmark with systematically induced degradation
patterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS
denial. Second, we conduct a comprehensive evaluation of forty SLAM systems on
M3DGR, providing critical insights into their robustness and limitations under
challenging real-world conditions. Third, we develop a resilient modular
multi-sensor fusion framework named Ground-Fusion++, which demonstrates robust
performance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and
wheel odometry. Codes and datasets are publicly available.

</details>


### [158] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 本研究提出了一种名为TD3-HD的AI方法，用于提高卫星在反应轮故障时的姿态控制能力，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了应对自主卫星在动态和不确定环境中运行日益增长的需求，以及传统控制器在故障适应性和实时适应性方面的不足，本研究旨在提高卫星在反应轮故障情况下的姿态控制韧性。

Method: 提出了一种结合了Twin Delayed Deep Deterministic Policy Gradient (TD3)、Hindsight Experience Replay (HER) 和 Dimension Wise Clipping (DWC) 的深度强化学习控制策略，称为TD3-HD，以提高在稀疏奖励环境下的学习效率，并在反应轮故障期间维持卫星的稳定性。

Result: 与PD控制和主流DRL算法相比，TD3-HD在故障条件下实现了显著更低的姿态误差、更优的角速度调节和更强的稳定性。

Conclusion: 该研究提出了TD3-HD方法，一种结合了TD3、HER和DWC的深度强化学习控制策略，用于提高卫星在故障条件下的鲁棒性和适应性。实验结果表明，与传统的PD控制和现有的DRL算法相比，TD3-HD在姿态误差、角速度调节和故障条件下的稳定性方面表现更优。该方法有潜力成为一种强大的、容错的、可部署在卫星上的AI解决方案，用于自主卫星姿态控制。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


### [159] [LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps](https://arxiv.org/abs/2507.08420)
*Haitian Wang,Hezam Albaqami,Xinyu Wang,Muhammad Ibrahim,Zainy M. Malakan,Abdullah M. Algamdi,Mohammed H. Alghamdi,Ajmal Mian*

Main category: cs.RO

TL;DR: 该研究提出了一种融合激光雷达、GNSS和IMU数据的3D地图构建框架，解决了GNSS受限环境下的累积漂移问题，通过DTW、卡尔曼滤波、NDT和姿态图优化等技术，将全局对齐误差降低了61.4%，并发布了一个大规模数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 为了解决激光雷达3D地图构建中由于累积漂移导致的全局错位问题，特别是在GNSS受限的环境中。

Method: 该框架融合了激光雷达、GNSS和IMU数据。通过动态时间规整（DTW）进行基于速度的时间对齐，并利用扩展卡尔曼滤波优化GNSS和IMU信号。使用正态分布变换（NDT）进行局部地图构建和姿态图优化（包含回环检测），同时利用GNSS约束锚点和重叠段的精细配准来保证全局一致性。

Result: 通过对齐公路中心线和交叉口的指标进行评估，该方法将平均全局对齐误差从3.32米降低到1.24米，提高了61.4%。

Conclusion: 该方法通过融合激光雷达、GNSS和IMU数据，并结合多种优化技术，显著提高了城市规模3D地图构建的精度和全局一致性，将平均全局对齐误差从3.32米降低到1.24米，改进率为61.4%，为GNSS受限环境下的3D城市测绘树立了新的标杆。

Abstract: LiDAR-based 3D mapping suffers from cumulative drift causing global
misalignment, particularly in GNSS-constrained environments. To address this,
we propose a unified framework that fuses LiDAR, GNSS, and IMU data for
high-resolution city-scale mapping. The method performs velocity-based temporal
alignment using Dynamic Time Warping and refines GNSS and IMU signals via
extended Kalman filtering. Local maps are built using Normal Distributions
Transform-based registration and pose graph optimization with loop closure
detection, while global consistency is enforced using GNSS-constrained anchors
followed by fine registration of overlapping segments. We also introduce a
large-scale multimodal dataset captured in Perth, Western Australia to
facilitate future research in this direction. Our dataset comprises 144{,}000
frames acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS
trajectories, and MEMS-IMU measurements across 21 urban loops. To assess
geometric consistency, we evaluated our method using alignment metrics based on
road centerlines and intersections to capture both global and local accuracy.
Our method reduces the average global alignment error from 3.32\,m to 1.24\,m,
achieving a 61.4\% improvement. The constructed high-fidelity map supports a
wide range of applications, including smart city planning, geospatial data
integration, infrastructure monitoring, and GPS-free navigation. Our method,
and dataset together establish a new benchmark for evaluating 3D city mapping
in GNSS-constrained environments. The dataset and code will be released
publicly.

</details>


### [160] [Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer](https://arxiv.org/abs/2507.08572)
*Juraj Gavura,Michal Vavrecka,Igor Farkas,Connor Gade*

Main category: cs.RO

TL;DR: 机器人仿真与现实末端执行器位置不符？用触觉反馈校准+神经网络解决！实验证明全非线性神经网络效果最好，误差更小。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，当采用逆运动学（IK）控制机械臂时，仿真器中的机器人末端执行器（EE）位置与真实物理末端执行器位置之间常常存在差异。在大多数机器人场景的仿真到现实（sim-to-real）迁移中，虽然可以同时获得仿真和现实中的关节位置信息，但通常只有仿真环境下的末端执行器位置信息是可用的。因此，需要一种方法来解决这种信息不对称的问题。

Method: 提出了一种基于触觉反馈校准的新方法，利用放置在机器人前方的触摸屏获取真实环境中末端执行器的位置信息。通过机器人接触屏幕上的特定点进行数据采集，然后基于线性变换和神经网络构建了一个变换函数，该函数能够从部分输入（仿真/真实关节/末端执行器位置）输出所有缺失的变量。

Result: 实验结果表明，所提出的方法能够显著减少定位误差，其中采用全非线性神经网络的模型取得了最佳性能。

Conclusion: 通过基于触觉反馈校准的方法，利用线性变换和神经网络构建变换函数，可以有效地解决机器人仿真与现实末端执行器位置的差异问题，其中全非线性神经网络模型表现最佳，能显著降低定位误差。

Abstract: When inverse kinematics (IK) is adopted to control robotic arms in
manipulation tasks, there is often a discrepancy between the end effector (EE)
position of the robot model in the simulator and the physical EE in reality. In
most robotic scenarios with sim-to-real transfer, we have information about
joint positions in both simulation and reality, but the EE position is only
available in simulation. We developed a novel method to overcome this
difficulty based on haptic feedback calibration, using a touchscreen in front
of the robot that provides information on the EE position in the real
environment. During the calibration procedure, the robot touches specific
points on the screen, and the information is stored. In the next stage, we
build a transformation function from the data based on linear transformation
and neural networks that is capable of outputting all missing variables from
any partial input (simulated/real joint/EE position). Our results demonstrate
that a fully nonlinear neural network model performs best, significantly
reducing positioning errors.

</details>


### [161] [Multi-critic Learning for Whole-body End-effector Twist Tracking](https://arxiv.org/abs/2507.08656)
*Aravind Elanjimattathil Vijayan,Andrei Cramariuc,Mattia Risiglione,Christian Gehring,Marco Hutter*

Main category: cs.RO

TL;DR: 本研究提出了一种新的强化学习框架，用于同时控制机器人的行走和机械臂运动，解决了任务冲突和速度控制问题，并在仿真和硬件实验中取得了成功。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在同时控制机器人行走和机械臂运动时存在挑战，因为两种任务的目标可能冲突（例如，行走倾向于水平的基座方向，而机械臂跟踪可能受益于基座倾斜）。此外，基于姿态的任务规定缺乏对末端执行器速度的直接控制，难以平滑地执行轨迹。

Method: 提出了一种基于强化学习的框架，采用多判别器Actor架构来分离奖励信号，并设计了基于末端执行器运动（twist-based）的任务制定方法，以实现对离散姿态和运动轨迹的跟踪。

Result: 所提出的控制器能够同时执行行走和机械臂运动，并在仿真和硬件实验中得到了验证。机器人底盘能够辅助机械臂扩展工作空间，展现出涌现的全身行为。

Conclusion: 该研究提出了一种基于强化学习的框架，能够实现机器人全身的运动控制，包括四足行走的运动和机械臂的运动。通过采用多判别器Actor架构和基于扭矩的末端执行器任务制定，该控制器能够有效解决两种任务之间的冲突，并能同时执行行走和末端执行器运动，甚至展现出机器人底盘辅助机械臂扩展工作空间的涌现行为。

Abstract: Learning whole-body control for locomotion and arm motions in a single policy
has challenges, as the two tasks have conflicting goals. For instance,
efficient locomotion typically favors a horizontal base orientation, while
end-effector tracking may benefit from base tilting to extend reachability.
Additionally, current Reinforcement Learning (RL) approaches using a pose-based
task specification lack the ability to directly control the end-effector
velocity, making smoothly executing trajectories very challenging. To address
these limitations, we propose an RL-based framework that allows for dynamic,
velocity-aware whole-body end-effector control. Our method introduces a
multi-critic actor architecture that decouples the reward signals for
locomotion and manipulation, simplifying reward tuning and allowing the policy
to resolve task conflicts more effectively. Furthermore, we design a
twist-based end-effector task formulation that can track both discrete poses
and motion trajectories. We validate our approach through a set of simulation
and hardware experiments using a quadruped robot equipped with a robotic arm.
The resulting controller can simultaneously walk and move its end-effector and
shows emergent whole-body behaviors, where the base assists the arm in
extending the workspace, despite a lack of explicit formulations.

</details>


### [162] [Learning human-to-robot handovers through 3D scene reconstruction](https://arxiv.org/abs/2507.08726)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 该研究提出了一种名为H2RH-SGS的新方法，利用高斯泼溅技术从稀疏视图的RGB图像中学习机器人交接策略，无需真实机器人数据，并成功将其应用于真实机器人交接任务。


<details>
  <summary>Details</summary>
Motivation: 在真实环境中从原始图像数据中学习机器人操作策略需要大量的物理环境机器人动作试验，而模拟训练虽然成本较低，但模拟与机器人工作空间之间的视觉域间隙仍然是一个主要限制。高斯泼溅视觉重建方法通过生成逼真的环境为机器人操作提供了新的方向。

Method: 利用稀疏视图高斯泼溅重建人机交接场景，生成包含图像-动作对的机器人演示，并将模拟相机位姿变化直接转换为夹持器位姿变化。

Result: 在16种家用物体上收集的演示数据上训练的机器人策略可以直接部署到真实环境中。在高斯泼溅重建场景和真实世界人机交接实验中进行的实验表明，H2RH-SGS是人机交接任务一种新颖且有效的方法。

Conclusion: 该研究提出了首个仅使用RGB图像学习基于监督的机器人交接策略的方法，无需真实机器人训练或数据收集。

Abstract: Learning robot manipulation policies from raw, real-world image data requires
a large number of robot-action trials in the physical environment. Although
training using simulations offers a cost-effective alternative, the visual
domain gap between simulation and robot workspace remains a major limitation.
Gaussian Splatting visual reconstruction methods have recently provided new
directions for robot manipulation by generating realistic environments. In this
paper, we propose the first method for learning supervised-based robot
handovers solely from RGB images without the need of real-robot training or
real-robot data collection. The proposed policy learner, Human-to-Robot
Handover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view
Gaussian Splatting reconstruction of human-to-robot handover scenes to generate
robot demonstrations containing image-action pairs captured with a camera
mounted on the robot gripper. As a result, the simulated camera pose changes in
the reconstructed scene can be directly translated into gripper pose changes.
We train a robot policy on demonstrations collected with 16 household objects
and {\em directly} deploy this policy in the real environment. Experiments in
both Gaussian Splatting reconstructed scene and real-world human-to-robot
handover experiments demonstrate that H2RH-SGS serves as a new and effective
representation for the human-to-robot handover task.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [163] [Experimental and numerical study on current distribution in parallel co-wound no-insulation coils](https://arxiv.org/abs/2507.08552)
*Yulong Liu,Peng Song,Mianjun Xiao,Liangjun Shao,Ziyang Xu,Cedric Korte,Timing Qu*

Main category: physics.app-ph

TL;DR: 平行并绕NI线圈电流分布不均，影响稳定性。通过实验和仿真研究了不均和影响因素，为应用提供参考。


<details>
  <summary>Details</summary>
Motivation: 平行并绕NI线圈在提高充电速度和保持热稳定性方面具有潜力，但其中超导带之间可能存在电流分布不均的问题，影响其稳定性和应用。

Method: 通过实验测量和建立场-路耦合模型（基于T-A公式和等效电路模型）相结合的方式，研究平行并绕NI线圈中各超导带的电流分布及其影响因素，并讨论了终端电阻的影响。

Result: 实验和仿真结果均表明，在充电过程中，平行并绕NI线圈的电流分布非常不均匀，部分带甚至出现反向电流。研究还分析了不同绝缘方式和终端电阻对电流分布的影响。

Conclusion: 通过实验和仿真研究了平行并绕绝缘（NI）线圈中电流分布不均匀的问题，并分析了影响因素，为大型或强磁场磁体系统的应用提供参考。

Abstract: No-insulation (NI) coils are known for their high thermal stability and
self-protection features due to turn-to-turn contacts. Parallel co-winding is a
promising method to reduce the charging delay of NI coils while maintaining
thermal stability, demonstrating significant potential for applications in
fusion and other large-scale or high-field magnets. The non-uniform current
distribution among parallel superconducting tapes in parallel co-wound NI coils
may lead to thermal and mechanical stability issues. In this work, we conducted
current measurement experiments on small parallel co-wound NI REBCO coils to
investigate the non-uniform current distribution and its influencing factors.
The parallel tapes in the input and output sections of the test coils were
separated and a series of Rogowski coils was used to measure the current in
each tape during ramping charging process. We combined a field-circuit coupled
model based on the T-A formulation with an equivalent circuit model to
calculate the current distribution in co-wound coils. Both the measured and
calculated results indicated that the current distribution during ramping was
highly non-uniform, with some tapes carrying reverse currents. We calculated
the current distribution in co-wound coils with different insulation methods
and analyzed the influencing factors of the reverse current. The influence of
the terminal resistance on current distribution was also discussed. This work
could contribute to a deeper understanding of current distribution behavior in
co-wound coils and provide insights for their application in large-scale or
high-field magnet systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [164] [Unveiling the electronic structure of the charge density wave and topological semimetal TaTe4 through high-field magnetotransport measurements](https://arxiv.org/abs/2507.08192)
*D. Silvera-Vega,J. Rojas-Castillo,E. Herrera-Vasco,E. Ramos-Rodríguez,A. F. Santander-Syro,J. A. Galvis,B. Uribe,R. González-Hernández,A. C. García-Castro,P. Giraldo-Gallo*

Main category: cond-mat.mtrl-sci

TL;DR: 高场磁输运和DFT研究了TaTe$_4$的费米面，发现了新的空穴口袋和磁致击穿现象，证实了CDW驱动的费米面重构对电子结构的影响。


<details>
  <summary>Details</summary>
Motivation: 理解拓扑与关联电子态之间的相互作用是研究量子材料的核心。TaTe$_4$是一种被预测能จัย拓扑相的电荷密度波（CDW）化合物，为探索这种相互作用提供了一个平台。

Method: 通过高场磁输运测量和密度泛函理论（DFT）计算，结合三种不同的场流配置，研究了TaTe$_4$的费米面（FS）。

Result: 解析了CDW相中预测的五个FS口袋中的四个，未发现残留的非CDW特征。识别出一个先前未观察到的准圆柱形空穴口袋，并观察到由重构FS片之间的磁致击穿引起的对反抗的量子振荡贡献，从中估计CDW隙约为0.29 eV。当电流沿a轴流动时，在所有场取向下观察到鲁棒的线性磁阻（MR），当场沿c轴附近时，出现了一个独特的强场线性MR机制，这与FS热点和磁致击穿是一致的。

Conclusion: TaTe$_4$是一种范式的系统，其中由电荷密度波驱动的费米面重构控制着体电子结构，从而为研究无关联电子态和拓扑能带特征的交叉点所产生的奇特的物理学开辟了新的途径。

Abstract: Understanding the interplay between topology and correlated electron states
is central to the study of quantum materials. TaTe$_4$, a charge density wave
(CDW) compound predicted to host topological phases, offers a platform to
explore this interplay. Here, we report a comprehensive study of the Fermi
surface (FS) of TaTe$_4$ via high-field magnetotransport measurements and
density functional theory (DFT) calculations. Using three distinct
field-current configurations, we resolve four out of five FS pockets predicted
in the CDW phase, with no evidence of residual non-CDW features - in contrast
to recent ARPES studies. Remarkably, we identify a previously unobserved
quasi-cylindrical hole pocket and a high-frequency quantum oscillation
contribution to the magnetoresistance attributable to magnetic breakdown
between reconstructed FS sheets, from which we estimate a CDW gap of $\sim$0.29
eV. Additionally, we observe a robust linear magnetoresistance (MR) for all
field orientations when current flows along the $a-$axis, with a distinct
high-field linear MR regime emerging near the $c-$axis, consistent with FS hot
spots and magnetic breakdown. Our results establish TaTe$_4$ as a paradigmatic
system where CDW-driven FS reconstruction governs the bulk electronic
structure, enabling new routes to probe the novel physics that can arise in the
intersection between correlated electronic states and topological band
features.

</details>


### [165] [Corner-Sharing PS$_4$-BS$_4$ Modes Facilitate Fast Ion Conduction in Lithium Thioborophosphate Iodide Glassy Solid Electrolytes](https://arxiv.org/abs/2507.08215)
*Yun An*

Main category: cond-mat.mtrl-sci

TL;DR: P2S5 打断 BxSy 链，提高锂离子传导。


<details>
  <summary>Details</summary>
Motivation: 尽管 LBPSI 在室温下表现出优异的离子电导率和低活化能，但其超快离子传输的潜在机制仍不清楚。

Method: 通过对 MACE-MP-0 模型进行微调，并进行大规模机器学习分子动力学模拟，研究了 LBPSI GSE 的结构和离子动力学。

Result: B2S3 主要形成阻碍 Li+ 传导的多桥接 BxSy 长链网络，而 P2S5 形成的单四面体 PS4³⁻ 和双四面体 P2S7⁴⁻ 与 BS4⁵⁻ 四面体之间的角共享模式可以打断 BxSy 链，提高 Li+ 迁移率。PS4³⁻ 和 BS4⁵⁻ 的多面体旋转也可能促进快速 Li+ 传导。

Conclusion: 研究表明，P2S5 形成的四面体结构以及其与 BS4 五负离子四面体的角共享模式可以打断 BxSy 链，从而提高锂离子的迁移率。

Abstract: Glassy solid electrolytes (GSEs), with their amorphous nature and the absence
of grain boundaries, make them highly attractive for applications in
all-solid-state lithium batteries (ASSLBs), a leading candidate for
next-generation energy storage technologies. A recently developed lithium
thioborophosphate iodide GSE, composed of
30Li$_2$S-25B$_2$S$_3$-45LiI-5P$_2$S$_5$ (LBPSI), has demonstrated excellent
room-temperature ionic conductivity and low activation energy. Despite this
exciting finding, the underlying mechanism behind this ultrafast ion transport
remains ambiguous. Here, we accurately fine-tune the foundational MACE-MP-0
model and perform large-scale machine learning molecular dynamics simulations
to investigate the structural and ion dynamics in LBPSI GSE. Our results reveal
that B$_2$S$_3$ glass formers primarily form multi-bridged B$_x$S$_y$
long-chain networks that impede Li$^+$ conduction. In contrast, P$_2$S$_5$
gives rise to mono-tetrahedral PS$_4$$^{3-}$ and di-tetrahedral
P$_2$S$_7$$^{4-}$ tetrahedra, which engage in distinctive corner-sharing modes
with BS$_4$$^{5-}$ tetrahedra, effectively disrupting the B$_x$S$_y$ chains and
enhancing Li$^+$ mobility. Furthermore, the polyhedral anion rotations of
PS$_4$$^{3-}$ and BS$_4$$^{5-}$ in the corner-sharing PS$_4$-BS$_4$ motifs may
further promote fast Li$^+$ conduction.

</details>


### [166] [Sensitive infrared surface photovoltage in quasi-equilibrium in a layered semiconductor at low-intensity low-temperature condition](https://arxiv.org/abs/2507.08279)
*Qiang Wan,Keming Zhao,Guohao Dong,Enting Li,Tianyu Yang,Hao Wang,Yaobo Huang,Yao Wen,Yiwei Li,Jun He,Youguo Shi,Hong Ding,Nan Xu*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究在NbSi0.5Te2中实现了高灵敏度的红外表面光伏效应（SPV），光响应率高达2.4*10^6 V/(W*cm^(-2))（LILT），在量子信息和深空探测等领域具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 二维范德瓦尔斯材料具有可调带隙和表面光伏效应（SPV），在低功耗光电探测方面展现出巨大潜力。然而，在红外区域实现高灵敏度的SPV，尤其是在准稳态光照条件下，对于层状半导体来说仍然是一个挑战。

Method: 本研究采用角度分辨光电子能谱法（ARPES）来探测和证实NbSi0.5Te2中的表面光伏效应（SPV），并通过观察登伯效应（Dember effect）进一步验证了SPV的灵敏度。同时，进行了温度依赖性测量以探究超高光响应机制。

Result: 在NbSi0.5Te2中观察到了高灵敏度的表面光伏效应（SPV），在低强度低温（LILT）条件下，光响应率高达2.4*10^6 V/(W*cm^(-2))。通过登伯效应的观察，确认了光生载流子密度足够高，扩散电流抑制了SPV。温度依赖性测量表明，低温下本征载流子的冻结导致了超高光响应，而准平衡状态下少量的光生载流子则主导了体系。

Conclusion: 本研究通过角度分辨光电子能谱法，在准平衡状态下，在NbSi0.5Te2中实现了高灵敏度的表面光伏效应（SPV），在低强度低温（LILT）条件下，光响应率高达2.4*10^6 V/(W*cm^(-2))。该材料在红外探测领域具有应用潜力，如量子信息和深空探测。此外，通过冻结体载流子，本研究为增强光-物质相互作用提供了一种新颖的方法。

Abstract: Benefit to layer-dependent bandgap, van der Waals materials with surface
photovoltaic effect (SPV) enable photodetection over a tunable wavelength range
with low power consumption. However, sensitive SPV in the infrared region,
especially in a quasi-steady illumination condition, is still elusive in
layered semiconductors. Here, using angle-resolved photoemission spectroscopy,
we report a sensitive SPV in quasi-equilibrium in NbSi0.5Te2, with
photoresponsivity up to 2.4*10^6 V/(W*cm^(-2)) at low intensity low temperature
condition (LILT). The sensitive SPV is further confirmed by observing the
Dember effect, where the photogenerated carrier density is high enough and
diffusion currents suppress SPV. Temperature-dependent measurements indicate
that intrinsic carriers freezing at low temperature leads to the ultrahigh
photoresponse, while a small amount of photon-generated carriers in
quasi-equilibrium dominate the system. Our work not only provides a promising
layered semiconductor for Infrared optoelectronic devices with strong infrared
SPV at LILT, which has application potential in fields such as quantum
information and deep-space exploration, but also paves a novel way to enhance
light-matter interaction effect by freezing bulk carriers.

</details>


### [167] [Rocksalt rare-earth monoxides as electronic and magnetic materials](https://arxiv.org/abs/2507.08292)
*Tomoteru Fukumura,Satoshi Sasaki,Masamichi Negishi,Daichi Oka*

Main category: cond-mat.mtrl-sci

TL;DR: 本综述介绍了二价稀土氧化物（REOs）的合成和基本性质，这些氧化物具有高导电性、超导性和室温铁磁性，使其成为有前途的电子、磁性和自旋电子材料。


<details>
  <summary>Details</summary>
Motivation: 由于其亚稳态性质，二价稀土离子一氧化物（REOs）的合成很少，基本性质也未被揭示。

Method: 通过薄膜外延技术合成，并介绍了它们的根本性质。

Result: 与RE2O3不同，REOs具有高导电性，并表现出超导性、室温铁磁性等性质。

Conclusion: REOs是有前途的电子、磁性和自旋电子材料，为f电子系统开辟了新的范式。

Abstract: Stable binary rare earth (RE) oxides are usually trivalent RE ion
sesquioxides (RE2O3), that are highly insulating and either nonmagnetic or
antiferromagnetic. On the other hand, rocksalt-type divalent RE ion monoxides
(REOs) have been scarcely synthesized owing to their metastable nature.
Accordingly, their fundamental properties have not been unveiled. Recently,
thin film epitaxy was successfully applied to synthesize various REOs. In stark
contrast with RE2O3, REOs are highly electrically conducting, and exhibit
superconductivity, room temperature ferromagnetism, and so on. Therefore, REOs
are new and simple f-electron system, promising as electronic, magnetic, and
spintronic materials. In this review, their fundamental properties are
introduced, and their significance and future prospects are discussed toward a
new paradigm in f-electron systems.

</details>


### [168] [Electronic and optical properties of the thio-apatites phases Ba$_5$(VS$_α$O$_β$)$_3$X [X=Cl, F, Br, I]: impact of multiple anionic substitution](https://arxiv.org/abs/2507.08308)
*Smritijit Sen,Houria Kabbour*

Main category: cond-mat.mtrl-sci

TL;DR: 通过第一性原理计算研究了含硫磷灰石 Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X 的电子结构和光学性质，发现其带隙和光学性质可通过硫含量和卤素种类进行调节，特别是在 Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$I 中表现出优异的光催化水分解潜力。


<details>
  <summary>Details</summary>
Motivation: 研究 Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X (X= Cl, F, Br, I) 的电子结构和光学性质，探索其作为光催化剂在水分解反应中的应用潜力。

Method: 利用第一性原理密度泛函理论（DFT）模拟系统研究了 Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X (X= Cl, F, Br, I) 的电子结构和光学性质。

Result: 硫掺杂和卤素种类影响带隙宽度，可以通过调节价带顶能级来优化带隙。缺陷态也对带隙调制起重要作用。Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$I 被认为是最有潜力用于光催化水分解的材料。硫掺杂引起光学各向异性，增强光吸收。

Conclusion: Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X (X= Cl, F, Br, I) 化合物的电子结构和光学性质可以通过硫掺杂和卤素通道进行调节，有望用于光催化水分解，特别是 Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$I。

Abstract: A systematic study of the electronic structure and optical properties of the
thio-apatites Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X (X= Cl, F, Br, I) is
carried out through first principles density functional theory simulations. The
band gap and properties evolution from fluorine to iodine on fixed O/S ratios,
as well as by substituting sulfur (S) for oxygen (O) are discussed. The
reduction of the band gap by raising valence band energy levels, with an
increasing S/O ratio can also be further modulated by the type of halide in the
channels of the structure, thus promoting fine tuning of the band gap region.
Defect states also play a crucial role in band gap modulation. Furthermore, the
examination of the band edges properties in
Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$X compounds suggests they can be potential
photocatalysts candidates for the water splitting reaction, with reduced band
gaps enabling efficient light-driven reactions, particularly in
Ba$_5$(VS$_{\alpha}$O$_{\beta}$)$_3$I. Optical investigations reveal that
sulfur doping induces optical anisotropy, enhancing light absorption and
offering tailored optical behaviour. These results provide new insights for the
design of functional materials in the broad family of apatites.

</details>


### [169] [Nanoscale symmetry protection of the reciprocal acoustoelectric effect](https://arxiv.org/abs/2507.08314)
*Sandeep Vijayan,Stephan Suffit,Scott E. Cooper,Yejun Feng*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究探索了表面声波驱动的声电效应的对称性起源，发现倒易和非倒易效应与材料的对称性密切相关，并揭示了其在不同SAW配置下的具体表现。


<details>
  <summary>Details</summary>
Motivation: 探索表面声波（SAW）驱动的声电（AE）效应的对称性起源，特别是倒易型和非倒易型AE效应。

Method: 本研究通过实验探索了由表面声波（SAW）驱动的声电（AE）效应的两种类型：倒易型和非倒易型。研究了对称性对材料物理性质的影响，特别是在表面现象中。

Result: 研究发现，倒易型AE效应的对称性起源于镜像平面或垂直于衬底表面的偶数阶旋转轴。另一半的配置没有半空间的不对称性操作，但SAW传播和表面法线方向被交换，其倒易性受到纳米应变张量对称结构的保护。倒易性与两种配置之间的对应关系源于SAW在两个正交方向上分别由压缩波和剪切波组成。

Conclusion: 本研究实验性地探索了由表面声波（SAW）驱动的声电（AE）效应的两种类型：倒易型和非倒易型。非倒易型AE电压与器件工程中的单相单向换能器相关，而倒易型AE效应存在于某些具有不同对称性起源的SAW配置中。

Abstract: Neumann's principle states that all physical properties of a material are
bound by its symmetry. While bulk crystals follow well-defined point and space
groups, phenomena at a substrate's surface could have less apparent symmetry
origins. Here we experimentally explore both reciprocal and non-reciprocal
types of acoustoelectric (AE) effects driven by surface acoustic waves (SAW).
The non-reciprocal AE voltage is connected to the natural single-phase
unidirectional transducer from device engineering. On the other hand,
reciprocal AE effect exists in certain SAW configurations that are of different
symmetry origins. Half of the configurations have a valid
reciprocity-preserving symmetry element of either a mirror plane or an
even-order rotational axis that is perpendicular to the substrate surface. The
other half of the configurations do not possess reciprocity-preserving symmetry
operations of the half-space but have the SAW propagation and the surface
normal directions interchanged from the first scenario. Here, the reciprocity
of SAW states is protected by the symmetric structure of the nanoscale strain
tensor. The correspondence between two types of configurations has its origin
imbedded in the SAW composition of both compression and shear waves along two
orthogonal directions respectively.

</details>


### [170] [Interfacially ordered phase states enable high-strength ductile eutectic Al alloys](https://arxiv.org/abs/2507.08327)
*Hemant Kumar,Praveen Kumar,Dierk Raabe,Baptiste Gault,Surendra Kumar Makineni*

Main category: cond-mat.mtrl-sci

TL;DR: 通过原子界面设计，解决了铝基低共熔合金的延展性问题，使其能够应用于交通和航空航天领域。


<details>
  <summary>Details</summary>
Motivation: 为了解决铝基低共熔合金因脆性低共熔相与韧性基体之间的界面载荷传递不足而容易发生灾难性失效的问题，以满足其在交通和航空航天领域作为轻质高强度结构材料的应用需求。

Method: 通过在铝（Al）-钆（Gd）基低共熔合金中添加锆（Zr），促进了在脆性Al3Gd低共熔相周围形成相干界面有序相（IOP），并在初生Al基体中形成纳米核壳有序沉淀物。

Result: 添加Zr后，在室温和250°C下，材料的抗拉强度分别达到295 MPa和130 MPa，同时拉伸塑性提高了400%。

Conclusion: 通过在原子尺度上设计界面，该方法成功解决了铝基低共熔合金的灾难性失效问题，提高了其在航空航天和交通运输领域的应用潜力，并展示了原子界面设计在克服轻质高强度韧性低共熔合金延展性限制方面的作用。

Abstract: Lightweight, high-strength structural materials are component enablers in
transportation and aerospace, improving carbon footprint and fuel efficiency.
Aluminium (Al)-based eutectics have property combinations that qualify them for
such applications. However, they are prone to catastrophic failure because of
insufficient load transfer across the interfaces between the brittle eutectic
phase and the ductile matrix. Here we present a general solution to this
problem by engineering these interfaces at the atomic scale, equipping them
with excellent load transfer capabilities, thus qualifying such composites for
lightweight structural applications. We demonstrate the approach by adding Zr
to an Al-Gd-based hypoeutectic alloy, promoting the formation of a coherent
Interfacial-Ordered-Phase (IOP) around the brittle Al3Gd eutectic phase and
nanosized core-shell ordered precipitates in the primary Al matrix. This
enables a 400% increase in tensile plasticity while retaining a high tensile
strength of 295 MPa at room temperature and 130 MPa at 250C. This exceptional
increase in formability is attributed to the ability of the IOP layer to
prevent dislocations from accumulating at the weak fibre/matrix interfaces,
avoiding stress concentrations that would otherwise initiate fibre breakage and
debonding. The core-shell precipitates in Al cause a large number of
dislocation cross/multiple-slips on different {111} planes, forming ultra-fine
(10 nm) dislocation networks that leverage substantial plastic strain
accumulation. The approach shows how atomic interface design overcomes the
ductility limitations of lightweight high-strength ductile eutectic alloys for
structural applications.

</details>


### [171] [Observation of quasi-steady dark excitons and gap phase in a doped semiconductor](https://arxiv.org/abs/2507.08419)
*Shangkun Mo,Yunfei Bai,Chunlong Wu,Xingxia Cui,Guangqiang Mei,Qiang Wan,Renzhe Li,Cao Peng,Keming Zhao,Dingkun Qin,Shuming Yu,Hao Zhong,Xingzhe Wang,Enting Li,Yiwei Li,Limin Cao,Min Feng,Sheng Meng,Nan Xu*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究利用ARPES技术在SnSe2中成功观测到准平衡分布下的暗激子及其激子绝缘体相变，为研究暗激子及其在半导体器件中的应用提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 暗激子虽然在光学和相关行为中起着重要作用，但其在准平衡分布下的电子调制效应（对电子器件工作状态至关重要）仍然难以捉摸。因此，本研究旨在揭示暗激子的电子调制效应，并探索其在准平衡分布下的行为。

Method: 本研究采用角分辨光电子能谱法（ARPES）进行实验观测。

Result: 研究首次观察到激子绝缘体相变，且导带出现各向异性的能隙。

Conclusion: 本研究通过角分辨光电子能谱法，在掺杂半导体SnSe2中成功制备、探测并控制了准平衡分布下的暗激子，揭示了其电子调制效应，并首次观察到激子绝缘体相变，且导带出现各向异性的能隙。

Abstract: Exciton plays an important role in optics and optics-related behaviors and
leads to novel correlated phases like charge order, exciton insulator, and
exciton-polariton condensation. Dark exciton shows distinct properties from
bright one. However, it cannot be directly detected by conventional optic
measurements. The electronic modulation effect of dark excitons in
quasi-equilibrium distribution, critical for electronic devices in working
status, is still elusive. Here, using angle-resolved photoemission
spectroscopy, we report creating, detecting, and controlling dark excitons in
the quasi-equilibrium distribution in a doped semiconductor SnSe2.
Surprisingly, we observe an excitonic gap phase, with a conduction band opening
an anisotropic gap. Our results broaden the scope of dark excitons, extending
their studies from the picosecond timescale in the ultrafast photoemission
process to conditions occurring under quasi-equilibrium. We reveal the
light-matter interaction in the engineering of electronic structures and
provide a new way to realize the excitonic gap phase in semiconductors with
large band gaps.

</details>


### [172] [Extending Nonlocal Kinetic Energy Density Functionals to Isolated Systems via a Density-Functional-Dependent Kernel](https://arxiv.org/abs/2507.08442)
*Liang Sun,Mohan Chen*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过改进非局域动能密度泛函解决了其在孤立系统中的不稳定性问题，新泛函在单原子和体相金属体系中均表现出更高的精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 由于Wang-Teter-like nonlocal kinetic energy density functional (KEDF)在孤立系统中的Blanc-Cances不稳定性问题，导致总能量无下界，该研究旨在解决此问题。

Method: 通过分析和重构非局域动能密度泛函（KEDF），特别是Wang-Teter（WT）KEDF，解决了其在孤立系统中的Blanc-Cances不稳定性问题。研究人员通过构建依赖于密度泛函的核来解决这个问题，并通过对56种元素的单原子系统进行基准测试来评估其性能。

Result: 新KEDF在保持计算效率的同时，精度比WT KEDF提高了两个数量级，并且在体相金属中也表现优于半局域泛函。

Conclusion: 该研究通过构建依赖于密度泛函的核，解决了非局域动能密度泛函在孤立系统中的不稳定性问题，同时保持了原始框架的精确性。通过对56种元素的单原子系统进行基准测试，发现新动能密度泛函在保持计算效率的同时，精度提高了两个数量级。此外，新动能密度泛函在体相金属中也保持了WT的优越精度，在两个体系中均优于半局域泛函。

Abstract: The Wang-Teter-like nonlocal kinetic energy density functional (KEDF) in the
framework of orbital-free density functional theory, while successful in some
bulk systems, exhibits a critical Blanc-Cances instability [J. Chem. Phys. 122,
214106 (2005)] when applied to isolated systems, where the total energy becomes
unbounded from below. We trace this instability to the use of an ill-defined
average charge density, which causes the functional to simultaneously violate
the scaling law and the positivity of the Pauli energy. By rigorously
constructing a density-functional-dependent kernel, we resolve these
pathologies while preserving the formal exactness of the original framework. By
systematically benchmarking single-atom systems of 56 elements, we find the
resulting KEDF retains computational efficiency while achieving an
order-of-magnitude accuracy enhancement over the WT KEDF. In addition, the new
KEDF preserves WT's superior accuracy in bulk metals, outperforming the
semilocal functionals in both regimes.

</details>


### [173] [The viscoelastic rheology of transient diffusion creep](https://arxiv.org/abs/2507.08484)
*John F. Rudge*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究提出了一个描述多晶材料瞬态扩散蠕变的参考模型，该模型基于有限元计算和参数化的扩展玻格子模型，并与实验结果进行了比较。


<details>
  <summary>Details</summary>
Motivation: 研究多晶材料的粘弹性流变学，特别是能量耗散机制，如扩散辅助的晶界滑动（瞬态扩散蠕变）。

Method: 基于二维规则六边形和三维十四面体等简单晶粒形状的有限元计算，构建了一个参数化的扩展玻格子模型来描述线性粘弹性行为。

Result: 参数化的扩展玻格子模型可以很好地描述有限元模型的线性粘弹性行为，该模型在低频下表现为马克斯韦尔模型，在高频下表现为安德雷模型。模型的安德雷指数仅取决于晶界在 ثلاثي junctions 的交角，并与纯弹性模型中 ثلاثي junctions 的应力奇异性指数相关。

Conclusion: 该模型为瞬态扩散蠕变提供了一个下限，但实验中存在未被此模型描述的額外耗散过程。

Abstract: Polycrystalline materials have a viscoelastic rheology where the strains
produced by stresses depend on the timescale of deformation. Energy can be
stored elastically within grain interiors and dissipated by a variety of
different mechanisms. One such dissipation mechanism is
diffusionally-accommodated/-assisted grain boundary sliding, also known as
transient diffusion creep. Here we detail a simple reference model of transient
diffusion creep, based on finite element calculations with simple grain shapes:
a regular hexagon in 2D and a tetrakaidecadedron in 3D. The linear viscoelastic
behaviour of the finite element models can be well described by a parameterised
extended Burgers model, which behaves as a Maxwell model at low frequencies and
as an Andrade model at high frequencies. The parametrisation has a specific
relaxation strength, Andrade exponent and Andrade time. The Andrade exponent
depends only on the angles at which grains meet at triple junctions, and can be
related to the exponents of stress singularities that occur at triple junctions
in purely elastic models without diffusion. A comparison with laboratory
experiments shows that the simple models here provide a lower bound on the
observed attenuation. However, there are also clearly additional dissipative
processes occurring in laboratory experiments that are not described by these
simple models.

</details>


### [174] [Abinit 2025: New Capabilities for the Predictive Modeling of Solids and Nanomaterials](https://arxiv.org/abs/2507.08578)
*Matthieu J. Verstraete,Joao Abreu,Guillaume E. Allemand,Bernard Amadon,Gabriel Antonius,Maryam Azizi,Lucas Baguet,Clementine Barat,Louis Bastogne,Romuald Bejaud,Jean-Michel Beuken,Jordan Bieder,Augustin Blanchet,Francois Bottin,Johann Bouchet,Julien Bouquiaux,Eric Bousquet,James Boust,Fabien Brieuc,Veronique Brousseau-Couture,Nils Brouwer Fabien Bruneval,Alois Castellano,Emmanuel Castiel,Jean-Baptiste Charraud,Jean Clerouin,Michel Cote,Clement Duval,Alejandro Gallo,Frederic Gendron,Gregory Geneste,Philippe Ghosez,Matteo Giantomassi,Olivier Gingras,Fernando Gomez-Ortiz,Xavier Gonze,Felix Antoine Goudreault,Andreas Gruneis,Raveena Gupta,Bogdan Guster,Donald R. Hamann,Xu He,Olle Hellman,Natalie Holzwarth,Francois Jollet,Pierre Kestener,Ioanna-Maria Lygatsika,Olivier Nadeau,Lorien MacEnulty,Enrico Marazzi,Maxime Mignolet,David D. O'Regan,Robinson Outerovitch,Charles Paillard,Guido Petretto,Samuel Ponce,Francesco Ricci,Gian-Marco Rignanese,Mauricio Rodriguez-Mayorga,Aldo H. Romero,Samare Rostami,Miquel Royo,Marc Sarraute,Alireza Sasani,Francois Soubiran,Massimiliano Stengel,Christian Tantardini,Marc Torrent,Victor Trinquet,Vasilii Vasilchencko,David Waroquiers,Asier Zabalo,Austin Zadoks,Huazhang Zhang,Josef Zwanziger*

Main category: cond-mat.mtrl-sci

TL;DR: Abinit has been significantly updated with new scientific methods and technical improvements, including GPU support and automated workflows, to meet the demands of modern scientific research.


<details>
  <summary>Details</summary>
Motivation: The motivation for these advancements is the evolution of hardware platforms, the need for high-throughput calculations, and the increasing use of machine learning.

Method: The paper describes new methodologies for ground states, density functional perturbation theory, and excited states, as well as technical advances for GPU execution and parallelism. It also mentions second principles methods and workflow automation.

Result: The implemented features extend Abinit's capabilities for various scientific applications and improve its performance and usability through technical advances and workflow automation.

Conclusion: The paper details the new features and capabilities of the Abinit software package over the last 5 years, highlighting technical and scientific advancements.

Abstract: Abinit is a widely used scientific software package implementing density
functional theory and many related functionalities for excited states and
response properties. This paper presents the novel features and capabilities,
both technical and scientific, which have been implemented over the past 5
years. This evolution occurred in the context of evolving hardware platforms,
high-throughput calculation campaigns, and the growing use of machine learning
to predict properties based on databases of first principles results. We
present new methodologies for ground states with constrained charge, spin or
temperature; for density functional perturbation theory extensions to
flexoelectricity and polarons; and for excited states in many-body frameworks
including GW, dynamical mean field theory, and coupled cluster. Technical
advances have extended abinit high-performance execution to graphical
processing units and intensive parallelism. Second principles methods build
effective models on top of first principles results to scale up in length and
time scales. Finally, workflows have been developed in different community
frameworks to automate \abinit calculations and enable users to simulate
hundreds or thousands of materials in controlled and reproducible conditions.

</details>


### [175] [Band Meandering due to Charged Impurity Effects and Carrier Transport in Ternary Topological Insulators](https://arxiv.org/abs/2507.08579)
*Kanav Sharma,Niranjay K R,Radha Krishna Gopal,Chiranjib Mitra*

Main category: cond-mat.mtrl-sci

TL;DR: 带电杂质影响拓扑绝缘体 BST 和 IBST 的电学性质，特别是在局域化、电势起伏和迁移率方面。铟掺杂会加剧这些效应。在较高温度下，观察到从 n 型到 p 型的转变。杂质管理对于优化拓扑绝缘体的应用至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究带电杂质对拓扑绝缘体（BST 和 IBST）电学性质的影响，以期优化其在自旋电子学和量子计算中的应用。

Method: 通过场效应栅控研究了带电杂质对拓扑绝缘体 (Bi0.3Sb0.7)2Te3 (BST) 及其掺铟对应物 In0.14(Bi0.3Sb0.7)1.86Te3 (IBST) 的电学性质的影响。

Result: 研究发现，薄 BST 中的带电杂质导致带尾型局域化，而厚 BST 中的固有杂质引起了电势起伏，形成了 n 型和 p 型区域。铟掺杂增加了 IBST 的杂质密度，降低了迁移率。门控依赖性分析表明，在较高温度下（约 170 K），存在从 n 型到 p 型的转变，这归因于 p 型区域中空穴的热激活。

Conclusion: 这项研究强调了杂质管理在优化拓扑绝缘体在自旋电子学和量子计算应用中的作用。

Abstract: In this study, we investigated the impact of charged impurities on the
electrical properties of the topological insulator $\left(\mathrm{Bi}_{0.3}
\mathrm{Sb}_{0.7}\right)_2 \mathrm{Te}_3$ (BST) and its indium-doped
counterpart, $\mathrm{In}_{0.14}\left(\mathrm{Bi}_{0.3}
\mathrm{Sb}_{0.7}\right)_{1.86} \mathrm{Te}_3$ (IBST), using field-effect
gating. For thin BST (30 nm), charged impurities, potentially from silicon
dioxide substrate traps, contribute to band tail-type localization, with the
impurity density ($ n_{\text{imp}} = 10^{11} \text{ cm}^{-2} $) suggesting an
external influence, though quantitative confirmation is needed. In contrast,
thick BST (60 nm) exhibits potential fluctuations driven by inherent impurities
($ n_{\text{imp}} = 2.23 \times 10^{14} \text{ cm}^{-2} $), forming
electron-rich (n-type) and hole-rich (p-type) regions that cause band
meandering near the chemical potential. Indium doping in thin IBST
significantly increases the impurity density to $ 1.53 \times 10^{15} \text{
cm}^{-2} $, exacerbating these fluctuations and reducing mobility due to
enhanced scattering. Gate-dependence analysis further revealed a
temperature-driven transition from n-type to p-type characteristics at elevated
temperatures (~170 K), attributed to thermal activation of holes in p-type
regions. These findings highlight the critical role of impurity management in
optimizing topological insulators for applications in spintronics and quantum
computing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [176] [Uncertainty Relations and Entanglement for PQ-deformed Supersymmetric Coherent States](https://arxiv.org/abs/2507.08023)
*Oktay K Pashaev,Aygul Kocak*

Main category: quant-ph

TL;DR: 研究提出了超对称变形量子振荡器，并结合双参数量子群结构，定义了超对称相干态，并分析了其纠缠和不确定性关系，结果显示了参数依赖性和非经典特性。


<details>
  <summary>Details</summary>
Motivation: 为了扩展变形量子振荡器模型，并结合超对称性和双参数量子群结构，以包含更多经典的振荡器模型，并研究由此产生的超对称相干态的性质，特别是它们的纠缠和不确定性关系。

Method: 通过引入pq-变形的超对称湮灭算符，定义了相应的超对称相干态，这些态可以通过Fock空间中的pq-量子态对或无限多个量子比特态来表征。研究还量化了具有pq-变形玻色子的费米子的纠缠，使用共度量来衡量，其形式为Gram行列式。

Result: 共度量取决于p和q参数值，并在最大纠缠态时达到1。超相干态的纠缠和不确定性关系被计算，并发现非最小不确定性关系体现了纠缠态的非经典性质。

Conclusion: 该研究通过引入超对称性来扩展变形量子振荡器，并结合双参数量子群结构，从而将对称和非对称q振荡器、斐波那契和黄金振荡器层级、Tamm-Dankov振荡器等多种振荡器模型纳入其中。通过引入pq-变形的超对称湮灭算符，定义了相应的超对称相干态，这些态可以通过Fock空间中的pq-量子态对或无限多个量子比特态来表征。研究还量化了具有pq-变形玻色子的费米子的纠缠，使用共度量来衡量，其形式为Gram行列式。结果表明，共度量取决于p和q参数值，并在最大纠缠态时达到1。此外，研究还计算了超相干态的纠缠以及这些状态下的坐标和动量不确定性关系，并指出纠缠态的非经典性质体现在其不确定性关系的非最小特征上。

Abstract: We propose supersymmetric extension of deformed quantum oscillator with two
parameters quantum group structure. As particular cases, specified by values of
$p$ and $q$ parameters it includes symmetric and non-symmetric $q$-oscillators,
Fibonacci and Fibonacci divisors hierarchy of Golden oscillators, Tamm-Dankov
oscillator etc. By $pq$-deformed supersymmetric annihilation operator, the set
of corresponding supersymmetric coherent states is introduced. The states are
characterized by the pair of $pq$-quantum states from the Fock space or
equivalently, by the set of infinite number of qubit states. Entanglement of
fermions with $pq$-deformed bosons is characterized by the concurrence as the
linear entropy, taking form of the Gram determinant of inner products. As
shown, for two types of the reference states, the concurrence depends on values
of $p$ and $q$ parameters, and it reaches the value one for the maximally
entangled states. Entanglement of the super-coherent states and the uncertainty
relations for the coordinate and momentum in these states are calculated.
Non-classical nature of the entangled states is reflected in non-minimal
character of the uncertainty relations.

</details>


### [177] [Overcoming a challenge for Bohmian mechanics](https://arxiv.org/abs/2507.08049)
*Hrvoje Nikolic*

Main category: quant-ph

TL;DR: 波姆力学因粒子运动不只依赖于相位梯度而受到挑战，但通过显式构建速度已成功克服此挑战。


<details>
  <summary>Details</summary>
Motivation: 波姆力学已被证明是错误的，因为它研究了一个粒子运动不能仅与波函数相位梯度相关联的系统。

Method: 通过构建显式速度来解决波姆力学所面临的挑战。

Result: 通过显式构建速度，成功克服了对波姆力学的挑战。

Conclusion: 波姆力学已经被提出，其速度定义可以通过连续性方程导出，并且不一定只依赖于相位梯度。我们构建了适当的速度，解决了这个挑战。

Abstract: Recently, Bohmian mechanics has been challenged [Nature 643, 67 (2025)] by
studying a system in which the motion of particles cannot be associated only
with the gradient of phase of the wave function. We point out that, in general,
Bohmian velocity is defined by the continuity equation, which does not always
lead to velocity depending only on the phase gradient. By constructing the
appropriate velocity explicitly, we overcome the challenge.

</details>


### [178] [Enhanced entanglement from quantum ergodicity](https://arxiv.org/abs/2507.08067)
*Amit Vikram*

Main category: quant-ph

TL;DR: 量子混沌中的遍历动力学可用于制备高纠缠量子态，并可能成为量子信息任务的资源。


<details>
  <summary>Details</summary>
Motivation: 反驳了量子混沌中的谱统计与量子遍历性之间的关联缺乏实际意义的观点，并探索了直接利用遍历动力学制备高纠缠量子态的可能性。

Method: 通过研究量子系统耦合以及“非破坏性”相互作用的守恒荷，推导了纠缠演化与谱统计度量之间的精确关系，并通过Krylov向量遍历性来解释这种联系。

Result: 证明了与最大随机酉相比，通过遍历动力学制备的量子态可以具有更高的纠缠度，并且EPR态的制备能力更强，为量子信息任务提供了新的潜在资源。

Conclusion: 该研究表明，量子系统中的遍历动力学可以直接用于制备比最大随机酉更具纠缠性的量子态，并提出将“遍历”谱统计作为量子信息任务的潜在资源。

Abstract: The quantum chaos conjecture associates the spectral statistics of a quantum
system with abstract notions of quantum ergodicity. Such associations are taken
to be of fundamental and sometimes defining importance for quantum chaos, but
their practical relevance has been challenged by theoretical and experimental
developments. Here, in counterpoint, we show that ergodic dynamics can be
directly utilized for the preparation of quantum states with parametrically
higher entanglement than generated by maximally scrambling circuits such as
random unitaries. Our setting involves quantum systems coupled via a
"non-demolition" interaction of conserved charges. We derive an exact relation
between the evolving entanglement of an initial product state and a measure of
spectral statistics of the interacting charges in this state. This connection
is explained via a notion of Krylov vector ergodicity, tied to the ability of
quantum dynamics to generate orthonormal states over time. We consider
exploiting this phenomenon for the preparation of approximate
Einstein-Podolsky-Rosen (EPR) states between complex systems, a crucial
resource for tasks such as quantum teleportation. We quantitatively show that
the transfer of operators between entangled systems, which underlies the
utility of the EPR state, can be performed with parametrically larger capacity
for entanglement generated via ergodic dynamics than with maximal scrambling.
Our analysis suggests a direct application of "ergodic" spectral statistics as
a potential resource for quantum information tasks.

</details>


### [179] [A dynamic circuit for the honeycomb Floquet code](https://arxiv.org/abs/2507.08069)
*Jahan Claes*

Main category: quant-ph

TL;DR: 动态电路在Floquet码中表现优于传统方法，能有效提高性能并减少量子比特需求。


<details>
  <summary>Details</summary>
Motivation: 介绍量子纠错码的典型实现方式，并提出动态电路作为一种无需辅助量子比特的替代方案。

Method: 通过动态电路测量稳定子，无需辅助量子比特，并演示了其在Floquet码中的应用。

Result: 动态电路在Floquet码中的应用，显著提高了阈值（threshold）和降低了逻辑错误率（logical error rate），并估计在特定条件下可大幅减少所需的量子比特数量。

Conclusion: 动态电路在Floquet码中能够提高时间like距离，自动消除泄漏，并显著提高阈值和降低逻辑错误率，与基于辅助比特的电路相比，在10^-3的物理错误率下，达到10^-12逻辑错误率所需的量子比特数量减少了近3倍。

Abstract: In the typical implementation of a quantum error-correcting code, each
stabilizer is measured by entangling one or more ancilla qubits with the data
qubits and measuring the ancilla qubits to deduce the value of the stabilizer.
Recently, the dynamic circuit approach has been introduced, in which
stabilizers are measured without ancilla qubits. Here, we demonstrate that
dynamic circuits are particularly useful for the Floquet code. Our dynamic
circuit increases the timelike distance of the code, automatically removes
leakage, and both significantly increases the threshold and lowers the logical
error rate compared to the standard ancilla-based circuit. At a physical error
rate of $10^{-3}$, we estimate a nearly $3\times$ reduction in the number of
qubits required to reach a $10^{-12}$ logical error rate.

</details>


### [180] [Real-Time Dynamics in a (2+1)-D Gauge Theory: The Stringy Nature on a Superconducting Quantum Simulator](https://arxiv.org/abs/2507.08088)
*Jesús Cobos,Joana Fraxanet,César Benito,Francesco di Marcantonio,Pedro Rivero,Kornél Kapás,Miklós Antal Werner,Örs Legeza,Alejandro Bermudez,Enrique Rico*

Main category: quant-ph

TL;DR: 利用超导量子计算机模拟了（2+1）维规范场论中的电그런，首次实现了对电그런的实时观测，揭示了其动力学性质，为理解物质约束和弦理论提供了新的实验证据。


<details>
  <summary>Details</summary>
Motivation: 理解规范场论中的约束机制以及有效弦描述的普适性是现代物理学中的一个基本挑战。通过数字量子模拟研究带电物质的弦模式运动，为解决这一挑战提供了新的实验途径。

Method: 利用包含多达144个量子比特的超导量子处理器，通过优化的嵌入方法，在Z2-希格斯模型中实现了物质场和规范场到顶点和链路量子比特的直接映射。采用先进的误差抑制、缓解和纠正策略，并结合张量网络模拟进行验证，实现了对电그런的动态演化和多그런过程的观测。

Result: 成功观测到了电그런的纵向振荡和端点的横向弯曲，这被认为是强子化和介子旋转谱的前兆。此外，还探索了多그런过程，观察到了그런的碎裂和重组。实验数据的高保真度得到了大量测量样本和张量网络模拟的验证。

Conclusion: 本工作通过超导量子模拟首次实现了对（2+1）维规范场论中电그런的实时观测和操纵，解决了规范场论中的一个基本挑战，并为理解规范约束机制和有效弦描述的普适性提供了新的视角。

Abstract: Understanding the confinement mechanism in gauge theories and the
universality of effective string-like descriptions of gauge flux tubes remains
a fundamental challenge in modern physics. We probe string modes of motion with
dynamical matter in a digital quantum simulation of a (2+1) dimensional gauge
theory using a superconducting quantum processor with up to 144 qubits,
stretching the hardware capabilities with quantum-circuit depths comprising up
to 192 two-qubit layers. We realize the $Z_2$-Higgs model ($Z_2$HM) through an
optimized embedding into a heavy-hex superconducting qubit architecture,
directly mapping matter and gauge fields to vertex and link superconducting
qubits, respectively. Using the structure of local gauge symmetries, we
implement a comprehensive suite of error suppression, mitigation, and
correction strategies to enable real-time observation and manipulation of
electric strings connecting dynamical charges. Our results resolve a dynamical
hierarchy of longitudinal oscillations and transverse bending at the end points
of the string, which are precursors to hadronization and rotational spectra of
mesons. We further explore multi-string processes, observing the fragmentation
and recombination of strings. The experimental design supports 300,000
measurement shots per circuit, totaling 600,000 shots per time step, enabling
high-fidelity statistics. We employ extensive tensor network simulations using
the basis update and Galerkin method to predict large-scale real-time dynamics
and validate our error-aware protocols. This work establishes a milestone for
probing non-perturbative gauge dynamics via superconducting quantum simulation
and elucidates the real-time behavior of confining strings.

</details>


### [181] [Improving Transmon Qubit Performance with Fluorine-based Surface Treatments](https://arxiv.org/abs/2507.08089)
*Michael A. Gingras,Bethany M. Niedzielski,Kevin A. Grossklaus,Duncan Miller,Felipe Contipelli,Kate Azar,Luke D. Burkhart,Gregory Calusine,Daniel Davis,Renée DePencier Piñero,Jeffrey M. Gertler,Thomas M. Hazard,Cyrus F. Hirjibehedin,David K. Kim,Jeffrey M. Knecht,Alexander J. Melville,Christopher O'Connell,Robert A. Rood,Ali Sabbah,Hannah Stickler,Jonilyn L. Yoder,William D. Oliver,Mollie E. Schwartz,Kyle Serniak*

Main category: quant-ph

TL;DR: 通过对 Josephson 结下方的硅表面进行氟基湿法蚀刻处理，成功消除了锗残留物，并将超导量子比特的能量弛豫时间提高了 334 μs，表明金属-衬底界面是微波损耗的关键。


<details>
  <summary>Details</summary>
Motivation: 为了开发基于超导量子比特的实用规模量子处理器，需要减少材料和加工引起的退相干。本研究旨在解决 Josephson 结下方硅表面残留物导致的微波损耗问题。

Method: 采用两种基于氟的湿法蚀刻技术处理超导量子比特中 Josephson 结下方的硅表面，并利用多种材料分析技术（如未具体说明的技术）来评估处理效果和量子比特性能。

Result: 湿法蚀刻处理能够去除 Josephson 结制造过程中引入的锗残留物，而不会影响整体工艺流程。经过处理的量子比特能量弛豫时间显著提高，中值为 334 μs，品质因数为 6.6x10^6，表明金属-衬底界面是导致微波损耗的主要原因。

Conclusion: 通过使用基于氟的湿法蚀刻处理 Josephson 结下方的硅表面，可以有效去除德国残留物并提高超导量子比特的能量弛豫时间，最高可达 334 μs（品质因数 6.6x10^6）。这表明金属-衬底界面是导致微波损耗的关键因素，并且材料分析与量子器件性能指标相结合是提高超导量子比特性能的有效途径。

Abstract: Reducing materials and processing-induced decoherence is critical to the
development of utility-scale quantum processors based on superconducting
qubits. Here we report on the impact of two fluorine-based wet etches, which we
use to treat the silicon surface underneath the Josephson junctions (JJs) of
fixed-frequency transmon qubits made with aluminum base metallization. Using
several materials analysis techniques, we demonstrate that these surface
treatments can remove germanium residue introduced by our JJ fabrication with
no other changes to the overall process flow. These surface treatments result
in significantly improved energy relaxation times for the highest performing
process, with median $T_1=334~\mu$s, corresponding to quality factor
$Q=6.6\times10^6$. This result suggests that the metal-substrate interface
directly underneath the JJs was a major contributor to microwave loss in these
transmon qubit circuits prior to integration of these surface treatments.
Furthermore, this work illustrates how materials analysis can be used in
conjunction with quantum device performance metrics to improve performance in
superconducting qubits.

</details>


### [182] [Photonic quantum information with time-bins: Principles and applications](https://arxiv.org/abs/2507.08102)
*Ashutosh Singh,Anuj Sethia,Leili Esmaeilifar,Raju Valivarthi,Neil Sinclair,Maria Spiropulu,Daniel Oblak*

Main category: quant-ph

TL;DR: 本文全面回顾了时间 নির্ভরশীল 的制备、表征、传输和应用，强调了其在量子通信和计算中的潜力。


<details>
  <summary>Details</summary>
Motivation: 时间 নির্ভরশীল 的编码由于其对机械和热扰动、折射率变化引起的去极化以及光纤介质中的双折射具有弹性，已成为量子通信、分布式量子计算和传感应用的有希望的候选者。

Method: 本文的分析方法是提供关于制备和表征时间 নির্ভরশীল 的实验方法的全面概述，评估它们的优缺点，讨论传输挑战和克服它们的方法，分析关键参数和组件要求，探讨时间 নির্ভরশীল 的制备和表征，检查时间 নির্ভরশীল 的干扰要求，涵盖高维时间 নির্ভরশীল 状态和时间 নির্ভরশীল 纠缠时间 নির্ভরশীল 对的制备和表征，回顾时间 নির্ভরশীল 纠缠和关键实验实现，并展示从量子通信协议到光量子计算的时间 নির্ভরশীল 编码量子态的显著应用。

Result: 本文对时间 নির্ভরশীল 的制备、表征、传输和应用进行了全面的回顾，为研究人员提供了有价值的见解。

Conclusion: 本文全面概述了用于量子通信协议的时间 নির্ভরশীল 的制备和表征实验方法，并评估了它们的优缺点。此外，我们还讨论了在光纤和自由空间信道上传输时间 নির্ভরশীল 时遇到的挑战以及克服这些挑战的方法。我们还分析了实验中关键时间 নির্ভরশীল 参数和组件要求的选择。

Abstract: Long-range quantum communication, distributed quantum computing, and sensing
applications require robust and reliable ways to encode transmitted quantum
information. In this context, time-bin encoding has emerged as a promising
candidate due to its resilience to mechanical and thermal perturbations,
depolarization from refractive index changes, and birefringence in fiber optic
media. Time-bin quantum bits (qubits) can be produced in various ways, and each
implementation calls for different considerations regarding design parameters,
component compatibility (optical, electrical, electro-optical), and measurement
procedures. Here, we provide a comprehensive overview of experimental methods
for preparing and characterizing time-bin qubits (TBQs) for quantum
communication protocols, with an assessment of their advantages and
limitations. We discuss challenges in transmitting TBQs over optical fibers and
free-space channels, and methods to overcome them. We also analyze the
selection of key time-bin parameters and component requirements across
experiments. This leads us to explore the preparation and characterization of
time-bin entanglement and examine requirements for interference of time-bins
from separate sources. Further, we cover preparation and characterization
techniques for high-dimensional time-bin states, namely qudits, and the
generation of time-bin entangled qudit pairs. We review time-energy
entanglement and key experimental realizations. Finally, we present notable
applications of time-bin encoded quantum states, from quantum communication
protocols to photonic quantum computation. This work serves as an accessible
introduction and a comprehensive review of recent developments.

</details>


### [183] [Quantum Simulation of Two-Level $PT$-Symmetric Systems Using Hermitian Hamiltonians](https://arxiv.org/abs/2507.08129)
*Maryam Abbasi,Koray Aydogan,Anthony W. Schlimgen,Kade Head-Marsden*

Main category: quant-ph

TL;DR: A new method simulates PT-symmetric quantum systems on quantum computers using Hermitian equivalents, with demonstrated potential for advanced quantum applications.


<details>
  <summary>Details</summary>
Motivation: PT-symmetric Hamiltonians offer unique approaches to quantum sensing and entanglement generation due to their non-unitary dynamics and real spectra. Simulating these systems on unitary-gate-based quantum computers is challenging but important.

Method: We present a method for simulating PT-symmetric systems on quantum computers using Hermitian equivalents via similarity transformations. Two algorithms are introduced: a hybrid classical-quantum algorithm and an ancilla qubit-assisted algorithm for simulating systems near exceptional points. Perturbation theory is used to extend the method to two interacting PT-symmetric systems.

Result: The algorithms successfully simulate the quantum dynamics of PT-symmetric systems, including those near exceptional points, and demonstrate the potential for simulating interacting PT-symmetric systems.

Conclusion: The proposed algorithms are validated on quantum devices and noisy simulators, showing potential for scalable simulations of PT-symmetric systems and pseudo-Hermitian operators in quantum computation.

Abstract: Parity-time ($PT$)-symmetric Hamiltonians exhibit non-unitary dynamical
evolution while maintaining real spectra, and offer unique approaches to
quantum sensing and entanglement generation. Here we present a method for
simulating the quantum dynamics of $PT$-symmetric systems on unitary-gate-based
quantum computers by leveraging Hermitian equivalents through similarity
transformations. We introduce two algorithms for simulating $PT$-symmetric
systems near exceptional points where eigenvalues and eigenstates coalesce. The
first is a hybrid classical-quantum algorithm and the second reduces the
classical component by using an ancilla qubit. We then use perturbation theory
to extend this method to consider the dynamics of two weakly interacting
$PT$-symmetric systems. Demonstrations on a quantum device and noisy simulators
validate the algorithm on current quantum devices, offering potential for
scalable simulations of $PT$-symmetric systems and pseudo-Hermitian operators
in quantum computation.

</details>


### [184] [Quantum and Hybrid Machine-Learning Models for Materials-Science Tasks](https://arxiv.org/abs/2507.08155)
*Leyang Wang,Yilun Gong,Zongrui Pei*

Main category: quant-ph

TL;DR: 本研究利用量子支持向量机（QSVM）和量子神经网络（QNN）来解决材料科学中的实际问题，例如预测堆叠层错能量和能够使镁延展的溶质。研究人员调整了这些模型并测试了不同的超参数组合，最终找到了在预测任务中达到约90%验证分数的优化参数。


<details>
  <summary>Details</summary>
Motivation: 为了解决实际材料科学问题，如预测堆叠层错能量和能够使镁延展的溶质，我们设计并评估了量子机器学习和混合量子-经典模型。

Method: 我们采用了量子支持向量机（QSVM）和量子神经网络（QNN）这两种代表性的量子算法，并根据应用场景进行了调整，系统地测试了所选的超参数组合对模型性能的影响。

Result: 我们确定了某些超参数组合，使得QSVM和混合QNN在两个任务中的验证分数均达到约90%。

Conclusion: 我们为材料科学任务构建了具有优化参数的量子模型，用于回归和分类，以基于元素体积、电负性、电负性和体积模量来预测目标溶质。

Abstract: Quantum computing has become increasingly practical in solving real-world
problems due to advances in hardware and algorithms. In this paper, we aim to
design and estimate quantum machine learning and hybrid quantum-classical
models in a few practical materials science tasks, i.e., predicting stacking
fault energies and solutes that can ductilize magnesium. To this end, we adopt
two different representative quantum algorithms, i.e., quantum support vector
machines (QSVM) and quantum neural networks (QNN), and adjust them to our
application scenarios. We systematically test the performance with respect to
the hyperparameters of selected ansatzes. We identify a few combinations of
hyperparameters that yield validation scores of approximately 90\% for QSVM and
hybrid QNN in both tasks. Eventually, we construct quantum models with
optimized parameters for regression and classification that predict targeted
solutes based on the elemental volumes, electronegativities, and bulk moduli of
chemical elements.

</details>


### [185] [Topological network analysis using a programmable photonic quantum processor](https://arxiv.org/abs/2507.08157)
*Shang Yu,Jinzhao Sun,Zhenghao Li,Ewan Mer,Yazeed K Alwehaibi,Oscar Scholin,Gerard J. Machado,Kuan-Cheng Chen,Aonan Zhang,Raj B Patel,Ying Dong,Ian A. Walmsley,Vlatko Vedral,Ginestra Biancon*

Main category: quant-ph

TL;DR: A quantum processor is used to find topological structures in networks.


<details>
  <summary>Details</summary>
Motivation: Understanding topological features in networks is crucial for unravelling complex phenomena across fields such as neuroscience, condensed matter, and high-energy physics. However, identifying higher-order topological structures -- such as k-cliques, fundamental building blocks of complex networks -- remains a significant challenge.

Method: develop a universal programmable photonic quantum processor that enables the encoding of arbitrary complex-weight networks, providing a direct pathway to uncovering their topological structures. We demonstrate how this quantum approach can identify weighted k-cliques and estimate Betti numbers by leveraging the Gaussian boson sampling algorithm's ability to preferentially select high-weight, dense subgraphs.

Result: The unique capabilities of our programmable quantum processor allow us to observe topological phase transitions and identify clique percolation phenomena directly from the entropy of the sampling results.

Conclusion:  photonic quantum computing can be applied to analyse the topological characteristics of real-world complex networks, opening new possibilities for quantum-enhanced data analysis.

Abstract: Understanding topological features in networks is crucial for unravelling
complex phenomena across fields such as neuroscience, condensed matter, and
high-energy physics. However, identifying higher-order topological structures
-- such as $k$-cliques, fundamental building blocks of complex networks --
remains a significant challenge. Here we develop a universal programmable
photonic quantum processor that enables the encoding of arbitrary
complex-weight networks, providing a direct pathway to uncovering their
topological structures. We demonstrate how this quantum approach can identify
weighted $k$-cliques and estimate Betti numbers by leveraging the Gaussian
boson sampling algorithm's ability to preferentially select high-weight, dense
subgraphs. The unique capabilities of our programmable quantum processor allow
us to observe topological phase transitions and identify clique percolation
phenomena directly from the entropy of the sampling results. These findings
showcase how photonic quantum computing can be applied to analyse the
topological characteristics of real-world complex networks, opening new
possibilities for quantum-enhanced data analysis.

</details>


### [186] [Emergent Harmonics in Josephson Tunnel Junctions Due to Series Inductance](https://arxiv.org/abs/2507.08171)
*Junghyun Kim,Max Hays,Ilan T. Rosen,Junyoung An,Helin Zhang,Aranya Goswami,Kate Azar,Jeffrey M. Gertler,Bethany M. Niedzielski,Mollie E. Schwartz,Terry P. Orlando,Jeffrey A. Grover,Kyle Serniak,William D. Oliver*

Main category: quant-ph

TL;DR: 该研究通过测量近乎对称的SQUID器件，发现约瑟夫森结中的谐波效应主要由连接痕量电感引起，而非结本身的性质，这有助于优化量子电路设计。


<details>
  <summary>Details</summary>
Motivation: 约瑟夫森隧穿结是超导量子电路的基本组成部分，其 $2
u$周期正弦势的偏差（谐波）会影响电路行为。该研究旨在区分谐波的来源，因为这可能源于隧穿结本身的电流-相位关系或连接电路的金属痕量电感。

Method: 利用近乎对称的超导量子干涉器件（SQUID）进行光谱学测量，分析了器件的能级跃迁，并与包含二阶谐波贡献的模型进行了对比。

Result: 测量结果显示，标准余弦势模型无法解释器件的行为，但加入二阶谐波贡献后能精确匹配。通过分析谐波与约瑟夫森结尺寸的关系，发现约98%的二阶谐波效应归因于痕量电感。

Conclusion: 该研究提出的方法能够区分谐波的来源，其中约98%的二阶谐波效应可能源于痕量电感，这为设计下一代超导量子信息处理电路和研究超电流二极管效应提供了指导。

Abstract: Josephson tunnel junctions are essential elements of superconducting quantum
circuits. The operability of these circuits presumes a $2\pi$-periodic
sinusoidal potential of a tunnel junction, but higher-order corrections to this
Josephson potential, often referred to as "harmonics," cause deviations from
the expected circuit behavior. Two potential sources for these harmonics are
the intrinsic current-phase relationship of the Josephson junction and the
inductance of the metallic traces connecting the junction to other circuit
elements. Here, we introduce a method to distinguish the origin of the observed
harmonics using nearly-symmetric superconducting quantum interference devices
(SQUIDs). Spectroscopic measurements of level transitions in multiple devices
reveal features that cannot be explained by a standard cosine potential, but
are accurately reproduced when accounting for a second-harmonic contribution to
the model. The observed scaling of the second harmonic with Josephson-junction
size indicates that it is due almost entirely to the trace inductance. These
results inform the design of next-generation superconducting circuits for
quantum information processing and the investigation of the supercurrent diode
effect.

</details>


### [187] [Parametrized Quantum Circuit Learning for Quantum Chemical Applications](https://arxiv.org/abs/2507.08183)
*Grier M. Jones,Viki Kumar Prasad,Ulrich Fekl,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 量子化学机器学习：PQC在化学数据集上表现尚可，但与经典方法仍有差距，硬件限制需关注。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索参数化量子电路（PQC）在量子化学领域的数据集上的应用潜力、优势和局限性。尽管PQC在机器学习领域前景广阔，但其在量子化学数据集上的应用研究尚不充分。因此，本研究选取了化学键分离能数据集（BSE49）和水分子构象数据集作为研究对象，以期为PQC在量子化学领域的应用提供参考。

Method: 本研究构建了包含14种数据编码策略和12种变分ansätze的168种参数化量子电路（PQC），并在5和16个量子比特的电路上进行了评估。研究采用了状态向量模拟来分析电路结构对模型性能的影响，并探讨了电路深度和训练集大小对模型性能的影响。最后，在噪声模拟和实际量子设备上评估了性能最佳的PQC。

Result: 研究发现，尽管PQC在某些量子化学任务上表现出一定的潜力，但与经典的机器学习方法相比，其性能仍有差距。电路结构、深度和训练集大小等因素对PQC的性能有显著影响。在实际量子硬件上的表现也受到噪声等因素的限制。

Conclusion: 该研究表明，尽管参数化量子电路（PQC）在量子化学相关问题上具有潜力，但与经典的机器学习方法相比，它们仍然面临挑战。研究结果强调了在将PQC应用于化学问题时，需要考虑电路结构、深度和训练集大小等因素，并且在实际量子硬件上的表现仍需进一步优化。

Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits
(PQCs) -- constructed using a combination of fixed and tunable quantum gates --
provide a promising hybrid framework for tackling complex machine learning
problems. Despite numerous proposed applications, there remains limited
exploration of datasets relevant to quantum chemistry. In this study, we
investigate the potential benefits and limitations of PQCs on two chemically
meaningful datasets: (1) the BSE49 dataset, containing bond separation energies
for 49 different classes of chemical bonds, and (2) a dataset of water
conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions
are predicted from lower-level electronic structure methods using the
data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set
of 168 PQCs by combining 14 data encoding strategies with 12 variational
ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits.
Our initial analysis examines the impact of circuit structure on model
performance using state-vector simulations. We then explore how circuit depth
and training set size influence model performance. Finally, we assess the
performance of the best-performing PQCs on current quantum hardware, using both
noisy simulations ("fake" backends) and real quantum devices. Our findings
underscore the challenges of applying PQCs to chemically relevant problems that
are straightforward for classical machine learning methods but remain
non-trivial for quantum approaches.

</details>


### [188] [Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks](https://arxiv.org/abs/2507.08202)
*Sounak Bhowmik,Travis S. Humble,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 本研究首次提出了一种针对量子神经网络的隐蔽木马攻击（QuPTs），可显著降低其性能。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络（QNN）的安全性和鲁棒性仍有待探索，本研究旨在提出一种新的攻击方法来揭示QNN的潜在漏洞。

Method: 提出了一种基于量子门酉性质和哈达玛门叠加性质的新型量子属性木马（QuPTs），用于在量子神经网络（QNN）中插入噪声和叠加以实现攻击。

Result: 提出的QuPTs比现有方法更隐蔽，并严重影响了QNN的性能，其中最具影响力的QuPT导致受损QNN的准确率下降了23%。

Conclusion: 本工作首次提出了针对完全量子神经网络的木马攻击，该攻击独立于任何混合经典-量子架构。

Abstract: Quantum neural networks (QNN) hold immense potential for the future of
quantum machine learning (QML). However, QNN security and robustness remain
largely unexplored. In this work, we proposed novel Trojan attacks based on the
quantum computing properties in a QNN-based binary classifier. Our proposed
Quantum Properties Trojans (QuPTs) are based on the unitary property of quantum
gates to insert noise and Hadamard gates to enable superposition to develop
Trojans and attack QNNs. We showed that the proposed QuPTs are significantly
stealthier and heavily impact the quantum circuits' performance, specifically
QNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the
compromised QNN under the experimental setup. To the best of our knowledge,
this is the first work on the Trojan attack on a fully quantum neural network
independent of any hybrid classical-quantum architecture.

</details>


### [189] [Exponential onset of scalable entanglement via twist-and-turn dynamics in XY models](https://arxiv.org/abs/2507.08206)
*Tommaso Roscilde,Meenu Kumari,Alexandre Cooper,Fabio Mezzacapo*

Main category: quant-ph

TL;DR: 本研究提出“扭转”（TaT）动力学是一种有效制备量子纠缠的方法，可实现可扩展的多方纠缠和海森堡缩放的量子信息处理，且所需时间随系统规模的增长缓慢。


<details>
  <summary>Details</summary>
Motivation: 为了开发下一代量子设备，需要制备可扩展的多方纠缠。

Method: 通过分析具有U(1)对称相互作用和横向场的自旋系综的“扭转”（TaT）动力学来研究可扩展多方纠缠的制备。研究了XY模型和具有空间衰减相互作用（如偶极相互作用）的系统，并利用数值和精确解法进行了分析。

Result: 在具有高连接性的模型中，TaT动力学在短时间内表现出可扩展的压缩，在较长时间内表现出具有海森堡缩放的量子Fisher信息。同时，可扩展的多方纠缠（高达海森堡缩放）的实现，其时间仅随系统尺寸的对数增长，并伴随着量子关联的指数增长。对于偶极相互作用，纠缠动力学与热化过程截然不同，并似乎达到了Lieb-Robinson界（推广到幂律相互作用系统）所允许的最大纠缠增长速度。

Conclusion: 该研究表明，具有U(1)对称相互作用和横向场的“扭转”（TaT）动力学可以作为制备可扩展多方纠缠的重要资源，这是量子设备发展的关键目标。

Abstract: The efficient preparation of scalable multipartite entanglement is a central
goal in the development of next-generation quantum devices. In this work, we
show that the so-called ``twist-and-turn" (TaT) dynamics for interacting spin
ensembles, generated by Hamiltonians with U(1)-symmetric interactions and with
a transverse field, can offer an important resource to reach this goal. For
models with sufficiently high connectivity, TaT dynamics exhibits two key
features: 1) it features both scalable squeezing at short times, as well as
quantum Fisher information with Heisenberg scaling at later times; and 2)
scalable multipartite entanglement (up to Heisenberg scaling) is reached in a
time growing only logarithmically with system size, associated with an
exponential buildup of quantum correlations. These results can be shown exactly
in the XY model with a Rabi field and infinite range interactions, and
numerically in the case of spatially decaying XY interactions, such as dipolar
interactions in two dimensions, provided that unstable spin-wave modes do not
develop for large system sizes and/or strong fields. For dipolar interactions,
the entanglement dynamics at intermediate times is completely at odds with
thermalization; and it appears to saturate the maximum speed of entanglement
buildup allowed by Lieb-Robinson bounds generalized to power-law interacting
systems.

</details>


### [190] [Witnessing nonlocality in quantum network of continuous-variable systems by generalized quasiprobability functions](https://arxiv.org/abs/2507.08252)
*Taotao Yan,Jinchuan Hou,Xiaofei Qi,Kan He*

Main category: quant-ph

TL;DR: 该研究提出了一种新的非线性贝尔型不等式和上确界策略，用于检测连续变量量子网络的非局域性，并为实验验证提供了指导。


<details>
  <summary>Details</summary>
Motivation: 高斯测量无法在连续变量系统中证明非局域性，因此需要特殊类型的非高斯测量。

Method: 提出了一种适用于有限维或无限维系统量子网络的非线性贝尔型不等式，并通过利用基于广义拟概率函数的非高斯测量，提出了一种用于检测具有任何多方多模高斯态源态的连续变量系统的网络非局域性的上确界策略。

Result: 提出的非线性贝尔型不等式可以用于检测连续变量网络中的非局域性，并且易于构建和实现。通过对链状、星状、树状和环状网络以及纠缠交换网络的实例进行说明，证明了该方法的有效性。

Conclusion: 该研究为连续变量系统的网络非局域性提供了一个强有力的信号，并为其实验验证提供了精确的方案。

Abstract: Gaussian measurements can not be used to witness nonlocality in Gaussian
states as well as the network nonlocality in networks of continuous-variable
(CV) systems. Thus special non-Gaussian measurements have to be utilized.
  In the present paper, we first propose a kind of nonlinear Bell-type
inequality that is applicable to quantum networks of both finite or infinite
dimensional systems. Violation of the inequality will witness the network
nonlocality. This inequality allows us to propose a method of the supremum
strategy for detecting network nonlocality in CV systems with source states
being any multipartite multi-mode Gaussian states according to the
configurations of the networks by utilizing non-Gaussian measurements based on
generalized quasiprobability functions. The nonlinear Bell-type inequalities
for CV networks, which depend solely on the generalized quasiprobability
functions of Gaussian states, are straightforward to construct and implement.
As illustrations, we propose the corresponding nonlinear Bell-type inequalities
for any chain, star, tree-shaped and cyclic networks in CV systems with source
states being $(1+1)$-mode Gaussian states. The examples show that this approach
works well for witnessing the nonlocality in networks of CV systems.
Particularly, a thorough discussion is given for the entanglement swapping
network. Our study provide a strong signature for the network nonlocality
nature of CV systems and lead to precise recipes for its experimental
verification.

</details>


### [191] [Comparison of Quantum and Semiclassical Rabi models near multiphoton resonances in the presence of parametric modulation](https://arxiv.org/abs/2507.08271)
*M V S de Paula,M A Damasceno Faustino,A V Dodonov*

Main category: quant-ph

TL;DR: 比较了半经典和量子模型在有外调制和多光子共振条件下的两能级原子与单模场动力学。发现两者在初始时间且场强的条件下表现一致，但长时间后半经典模型失效。文末还分析了场态的演化。


<details>
  <summary>Details</summary>
Motivation: 比较了两能级原子与单模电磁场在参数调制下，在多光子原子-场共振条件下，半经典和量子预言的幺元动力学。

Method: 推导了半经典拉比模型近似解析解，其中原子跃迁频率和原子-场耦合强度经过谐波外调制。对量子拉比模型进行了数值求解，用于平均光子数约为10^4的大平均光子数相干态，处于三光子共振区。

Result: 半经典动力学在初始时间和足够强的相干态下与量子动力学表现一致，但长时间后由于缺乏塌陷-复生行为而失效。此外，还描述了场态如何通过演化得到修改，并展示了平均光子数、熵（与原子-场纠缠相关）以及其他表征电磁场光子数统计量的量化的数值结果。

Conclusion: 相干态初始时间的动力学演化，在足够强的相干态下，经典动力学与量子动力学表现一致。然而，在长时间演化中，经典动力学由于缺乏塌陷-复生行为而失效。

Abstract: We compare the semiclassical and quantum predictions for the unitary dynamics
of a two-level atom interacting with a single-mode electromagnetic field in the
presence of parametric modulation of the atomic parameters, in the regime of
multiphoton atom-field resonances. We derive approximate analytic solutions for
the semiclassical Rabi model when the atomic transition frequency and the
atom-field coupling strength undergo harmonic external modulations. These
solutions are compared to the predictions of the quantum Rabi model, which is
solved numerically for the initial coherent state with a large average photon
number, of the order of $10^4$, in the regime of three-photon resonance. We
show that for initial times and sufficiently intense coherent state the
semiclassical dynamics agrees quite well with the quantum one, although for
large times it inevitably fails due to the lack of the collapse-revival
behavior. Furthermore, we describe how the field state is modified throughout
the evolution, presenting numeric results for the average photon number,
entropies (related to the atom-field entanglement) and other quantities that
characterize the photon number statistics of the electromagnetic field.

</details>


### [192] [Computational Study of the Spectral Properties of Isospectrally Patterned Lattices](https://arxiv.org/abs/2507.08351)
*Peter Schmelcher*

Main category: quant-ph

TL;DR: 通过调节相位来控制正则格点（IPL）的局域化和非局域化性质。


<details>
  <summary>Details</summary>
Motivation: 本文旨在系统性地设计具有独特性质的格点哈密顿量，通过选择不同相位的正则格点来实现。

Method: 本文进行了计算光谱分析，研究了正则格点（IPL）的性质。通过对细胞进行参数化，使其具有相同的特征值集，并探索了不同相位下的格点行为。

Result: 研究表明，正则格点（IPL）的局域化和非局域化性质可以通过相位的变化进行调控，特别是在非对称情况下，可以任意移动局域化中心。通过引入完整的相位革命，低能量和高能量的局域化态会分离，并在能量增加时合并为非局域化态。局域化态以近简并对的形式出现，并在进入非局域化区域时解除简并。

Conclusion: 这项工作对二维非对称和对称的正则格点进行了解析，并展示了其在能量谱中的局域化和非局域化性质，以及相位的应用。

Abstract: We perform a computational spectral analysis of different isospectrally
patterned lattices (IPL). Having been introduced very recently, the lattice
Hamiltonian of IPL consist of coupled cells which possess all the same set of
eigenvalues. The latter is achieved in a controllable manner by parametrizing
the cells via the phases of the involved orthogonal (or unitary)
transformations. This opens the doorway of systematically designing lattice
Hamiltonians with unique properties by choosing correspondingly varying phases
across the lattice. Here we focus on two-dimensional cells and explore
symmetric as well as asymmetric IPL w.r.t. a given center. A tunable fraction
of localized vs. delocalized eigenstates belonging to the three subdomains of
the corresponding energy bands is demonstrated and analyzed with different
measures of localization. In the asymmetric case the center of localization can
be shifted arbitrarily by shifting the underlying phase grid. Introducing a
complete phase revolution leads for low and high energies to two well-separated
branches of localized states which finally merge with increasing energy into
the branch of delocalized states. Remarkably, the localized states appear in
near-degenerate pairs and this near-degeneracy is lifted upon entering the
delocalization regime. A corresponding generalization to several phase
revolutions is provided showing a characteristic nodal pattern among the
near-degenerate eigenstates.

</details>


### [193] [Complexity of mixed Schatten norms of quantum maps](https://arxiv.org/abs/2507.08358)
*Jan Kochanowski,Omar Fawzi,Cambyse Rouzé*

Main category: quant-ph

TL;DR: 该研究探讨了计算不同类型线性映射的混合 Schatten 范数的计算复杂性，发现在某些情况下（如完全正映射且 $q 	o p$ 范数），计算是有效的，但在其他情况下（如特定类型的完全正映射或一类特殊映射），计算会变得非常困难（NP 完全）。然而，对于完全有界映射，计算该范数则相对容易，存在多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 研究计算混合 Schatten 范数的复杂性，特别是关注 $q 	o p$ 范数在不同映射类型下的计算难度。

Method: 分析了计算线性映射的混合 Schatten 范数的复杂性。

Result: 证明了对于完全正映射，在 $q \geq p$ 的情况下计算混合 Schatten $q 	o p$ 范数是有效的；但即使是特定类型的映射，在 $p>1$ 时计算混合 Schatten $1 	o p$ 范数也是 NP 完全的。此外，还证明了对于一类特殊的映射，计算其混合 Schatten $1 	o 1$ 范数也是 NP 完全的。最后，对于完全有界映射，提出了一种计算 $1 	o p$ 范数的多项式时间算法。

Conclusion: 对于完全正的映射，当 $q \geq p$ 时，混合 Schatten $q 	o p$ 范数可以有效计算。然而，即使对于 प्रकारचे特定映射，当 $p>1$ 时，计算混合 Schatten $1 	o p$ 范数仍然是 NP 完全问题。对于完全有界映射，存在一个多项式时间算法来计算 $1 	o p$ 范数和相关范数。

Abstract: We study the complexity of computing the mixed Schatten $\|\Phi\|_{q\to p}$
norms of linear maps $\Phi$ between matrix spaces. When $\Phi$ is completely
positive, we show that $\| \Phi \|_{q \to p}$ can be computed efficiently when
$q \geq p$. The regime $q \geq p$ is known as the non-hypercontractive regime
and is also known to be easy for the mixed vector norms $\ell_{q} \to \ell_{p}$
[Boyd, 1974]. However, even for entanglement-breaking completely-positive
trace-preserving maps $\Phi$, we show that computing $\| \Phi \|_{1 \to p}$ is
$\mathsf{NP}$-complete when $p>1$. Moving beyond the completely-positive case
and considering $\Phi$ to be difference of entanglement breaking
completely-positive trace-preserving maps, we prove that computing $\| \Phi
\|^+_{1 \to 1}$ is $\mathsf{NP}$-complete. In contrast, for the
completely-bounded (cb) case, we describe a polynomial-time algorithm to
compute $\|\Phi\|_{cb,1\to p}$ and $\|\Phi\|^+_{cb,1\to p}$ for any linear map
$\Phi$ and $p\geq1$.

</details>


### [194] [On the Impact of Classical and Quantum Communication Networks Upon Modular Quantum Computing Architecture System Performance](https://arxiv.org/abs/2507.08378)
*Pau Escofet,Abhijit Das,Sahar Ben Rached,Santiago Rodrigo,Jordi Domingo,Fabio Sebastiano,Masoud Babaie,Batuhan Keskin,Edoardo Charbon,Peter Haring Bolívar,Maurizio Palesi,Elena Blokhina,Bogdan Staszewski,Avishek Nag,Artur Garcia-Sáez,Sergi Abadal,Eduard Alarcón,Carmen G. Almudéver*

Main category: quant-ph

TL;DR: This paper analyzes how network infrastructure impacts modular quantum computers. It finds that classical communication isn't a bottleneck for large systems and increasing quantum communication helps, but the best setup depends on the specific task. This research helps design better, scalable quantum computers.


<details>
  <summary>Details</summary>
Motivation: Modular architectures are a promising approach for scaling quantum computers, but non-local communications between quantum processors can significantly affect performance. This work aims to understand and address the impact of network infrastructure and communication constraints on modular quantum computing.

Method: The paper investigates the impact of network infrastructure on modular quantum computing architectures, focusing on coherence loss due to communication constraints. It analyzes the effect of classical network latency on quantum teleportation, studies different network topologies and their effect on communication resources, and performs a full-stack evaluation under varying communication parameters.

Result: Classical communication does not appear to be a bottleneck for systems exceeding one million qubits under current technological assumptions. Increasing quantum communication resources generally shortens execution time but can introduce overhead. The optimal number of quantum links is contingent upon the algorithm and topology.

Conclusion: The study provides valuable guidance for designing modular quantum computing architectures to enable scalable quantum computing. It indicates that classical communication is unlikely to be a bottleneck for systems with over a million qubits, even with modest specifications, and that increasing quantum communication resources generally reduces execution time, though it may add overhead. The optimal number of quantum links is dependent on the specific algorithm and topology.

Abstract: Modular architectures are a promising approach to scaling quantum computers
beyond the limits of monolithic designs. However, non-local communications
between different quantum processors might significantly impact overall system
performance. In this work, we investigate the role of the network
infrastructure in modular quantum computing architectures, focusing on
coherence loss due to communication constraints. We analyze the impact of
classical network latency on quantum teleportation and identify conditions
under which it becomes a bottleneck. Additionally, we study different network
topologies and assess how communication resources affect the number and
parallelization of inter-core communications. Finally, we conduct a full-stack
evaluation of the architecture under varying communication parameters,
demonstrating how these factors influence the overall system performance. The
results show that classical communication does not become a bottleneck for
systems exceeding one million qubits, given current technology assumptions,
even with modest clock frequencies and parallel wired interconnects.
Additionally, increasing quantum communication resources generally shortens
execution time, although it may introduce additional communication overhead.
The optimal number of quantum links between QCores depends on both the
algorithm being executed and the chosen inter-core topology. Our findings offer
valuable guidance for designing modular architectures, enabling scalable
quantum computing.

</details>


### [195] [New scattering zones in quantum speckle propagation](https://arxiv.org/abs/2507.08408)
*Shaurya Aarav,S. A. Wadood,Jason W Fleischer*

Main category: quant-ph

TL;DR: 本研究首次探索了量子散斑在短传播距离下的行为，发现了新的菲涅尔区域和具有特定传播特性的方形和椭圆形散斑，为量子传感和成像开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 量子散斑由于其高维度而表现出比经典对应物更丰富的行为。此前未考虑量子散斑在较短传播距离下的行为。

Method: 通过理论、数值和实验相结合的方式，研究了各向同性散射体产生的双光子纠缠在任意传播距离下的近轴演化。

Result: 研究发现在近场和远场之间存在一个新的菲涅尔区域，该区域受双光子长度尺度控制。此外，还发现量子近场具有在传播过程中保持不变的方形散斑，而中间区域的散斑形状可以从方形过渡到椭圆形，并且其大小可以沿着某些坐标保持恒定或线性扩展。

Conclusion: 本研究将量子相干性与散射统计相结合，并提出了基于关联的量子传感和成像的新机制。

Abstract: Quantum speckles exhibit significantly richer behavior than their classical
counterparts due to their higher dimensionality. A simple example is the
far-field speckle pattern in 1D light scattering: classical light forms 1D
speckles defined by the numerical aperture, whereas biphoton scattering depends
in addition on the photon correlation length, forming 2D elliptical speckles.
To date, the behavior of quantum speckles for shorter propagation distances has
not been considered. We remedy this here by considering the paraxial evolution
of two-photon entanglement at arbitrary propagation distances from an isotropic
scatterer. We show, theoretically, numerically, and experimentally, that the
two length scales of the biphoton introduce a new Fresnel regime between the
conventional near and far fields. Further, we show that the quantum near field
is characterized by speckles with a square shape that remain constant during
propagation. In contrast, the intermediate regime can be engineered to have a
constant speckle size along the sum coordinate but a linearly expanding speckle
size along the difference coordinate, with a speckle shape that transitions
from square to elliptical. The results merge quantum coherence with scattering
statistics and suggest new regimes of operation for correlation-based quantum
sensing and imaging.

</details>


### [196] [Time correlations from steady-state expectation values](https://arxiv.org/abs/2507.08661)
*Wojciech Górecki,Simone Felicetti,Lorenzo Maccone,Roberto Di Candia*

Main category: quant-ph

TL;DR: 提出一种基于量子计量学和连续测量的通用方法，用于计算松弛时间和二阶相关时间的一般下界，仅需稳态期望值及其对控制参数的导数，可用于表征超快系统和分析难解模型。


<details>
  <summary>Details</summary>
Motivation: 克服在恢复相关函数的性质时，实验上需要精密的时间分辨率和分析/数值上求解系统演化这两个挑战。

Method: 利用量子计量学中的连续测量推导出松弛时间和二阶相关时间的一般下界，该下界仅基于稳态期望值及其对控制参数的导数。

Result: 在临界驱动耗散谐振器和无限大范围伊辛模型两个临界量子系统上验证了该方法的有效性，其中该下界匹配了解析结果，或提供了超越现有分析方法的解析信息。

Conclusion: 该方法可用于实验表征超快系统，并为分析分析或数值上难以处理的动力学多体模型提供信息。

Abstract: Recovering properties of correlation functions is typically challenging. On
one hand, experimentally, it requires measurements with a temporal resolution
finer than the system's dynamics. On the other hand, analytical or numerical
analysis requires solving the system evolution. Here, we use recent results of
quantum metrology with continuous measurements to derive general lower bounds
on the relaxation and second-order correlation times that are both easy to
calculate and measure. These bounds are based solely on steady-state
expectation values and their derivatives with respect to a control parameter,
and can be readily extended to the autocorrelation of arbitrary observables. We
validate our method on two examples of critical quantum systems: a critical
driven-dissipative resonator, where the bound matches analytical results for
the dynamics, and the infinite-range Ising model, where only the steady state
is solvable and thus the bound provides information beyond the reach of
existing analytical approaches. Our results can be applied to experimentally
characterize ultrafast systems, and to theoretically analyze many-body models
with dynamics that are analytically or numerically hard.

</details>


### [197] [Continuous-time parametrization of neural quantum states for quantum dynamics](https://arxiv.org/abs/2507.08418)
*Dingzu Wang,Wenxuan Zhang,Xiansong Xu,Dario Poletti*

Main category: quant-ph

TL;DR: 一种新的平滑神经量子态（s-NQS）方法，通过时间连续参数化，实现了量子动力学模拟的准确性和效率，并能在训练区间外进行评估。


<details>
  <summary>Details</summary>
Motivation: 为了在模拟量子动力学时实现时间连续和可微分的神经网络参数化，以提高表示能力和效率。

Method: 将神经网络参数参数化为时间基函数的线性组合，并使用可训练、与时间无关的系数。损失函数定义在扩展的时间区间上。

Result: 通过在sudden quench的非积分多体量子自旋链上进行测试，证明了s-NQS的准确性和效率，即使使用受限玻尔兹曼机作为基本网络架构也是如此。此外，s-NQS允许在未包含在训练集中的时间点进行初始化和评估。

Conclusion: 该研究提出的平滑神经量子态（s-NQS）能够高效且准确地模拟量子动力学，其参数化方法允许在训练区间之外进行评估和初始化。

Abstract: Neural quantum states are a promising framework for simulating many-body
quantum dynamics, as they can represent states with volume-law entanglement. As
time evolves, the neural network parameters are typically optimized at discrete
time steps to approximate the wave function at each point in time. Given the
differentiability of the wave function stemming from the Schr\"odinger
equation, here we impose a time-continuous and differentiable parameterization
of the neural network by expressing its parameters as linear combinations of
temporal basis functions with trainable, time-independent coefficients. We test
this ansatz, referred to as the smooth neural quantum state ($s$-NQS) with a
loss function defined over an extended time interval, under a sudden quench of
a non-integrable many-body quantum spin chain. We demonstrate accurate time
evolution using simply a restricted Boltzmann machine as the instantaneous
neural network architecture. Furthermore, we demonstrate that the
parameterization is efficient in the number of parameters and the smooth neural
quantum state allows us to initialize and evaluate the wave function at times
not included in the training set, both within and beyond the training interval.

</details>


### [198] [Reversing adiabatic state preparation](https://arxiv.org/abs/2507.08439)
*L. Romanato,N. Eshaqi-Sani,L. Lepori,T. Kirova,E. Arimondo,S. Wimberger*

Main category: quant-ph

TL;DR: 研究了一种用于三能级量子系统的绝热状态制备方法，结合了反绝热控制以提高速度和保真度，并实现了可重复的循环演化。结果表明Berry相位可用于检测状态传输中的不完美性。


<details>
  <summary>Details</summary>
Motivation: 对有效三能级量子系统中的绝热状态制备进行详细研究，旨在实现高速度和高保真度的状态制备，并探索其在状态空间中的循环演化及其在Berry相位控制和检测不完美状态传输方面的应用。

Method: 通过添加一个反绝热量子控制协议来实现高速度和高保真度的状态制备，然后反转制备协议以返回初始状态，形成状态空间中的整体循环演化。分析了绝热循环路径上Berry的相位的控制，并表明它可用于检测不完美的状态传输。

Result: 提出了一种结合反绝热量子控制协议的绝热状态制备方法，实现了高速度和高保真度的状态制备。该方法能够实现状态空间中的整体循环演化，并且可以通过重复该过程来提高鲁棒性。研究表明，Berry的相位可以作为探测不完美状态传输的敏感指标。

Conclusion: Berry的相位可以作为不完美状态传输的敏感探测器。

Abstract: We present a detailed study of an adiabatic state preparation in an effective
three-level quantum system. States can be prepared with high speed and fidelity
by adding a counterdiabatic quantum control protocol. As a second step, we
invert the preparation protocol to get back to the initial state. This
describes an overall cyclic evolution in state space. Using counterdiabatic
terms, the resulting composed fast evolution can be repeated many times. We
then analyze the control of Berry's phase along the adiabatic cyclic path, and
show that Berry's phase can act as a sensitive detector of non-perfect state
transfer.

</details>


### [199] [Bi-Contextuality: A Novel Non-Classical Phenomenon in Bipartite Quantum Systems](https://arxiv.org/abs/2507.08461)
*Gabriel Ruffolo,Nigel Benjamin Lee Junsheng,Kim Mu Young,Dzmitry Matsukevich,Rafael Rabelo,Dagomir Kaszlikowski,Paweł Kurzyński*

Main category: quant-ph

TL;DR: Researchers discovered 'bi-contextuality' in quantum systems from two independent sources. This phenomenon, which defies classical explanations, serves as a new way to verify quantum measurements in networks and challenges existing assumptions in quantum mechanics, particularly those related to the PBR theorem.


<details>
  <summary>Details</summary>
Motivation: To confirm the quantum nature of measurements within a single network node and to explore a new phenomenon in network nonlocality.

Method: Presenting and experimentally demonstrating bi-contextuality, a novel non-classical phenomenon observed in quantum systems prepared by two independent sources. This involves combining systems from separate sources for joint measurements, contrasting with Bell scenarios that split a single system.

Result: Demonstrated bi-contextuality, where outcomes defy classical models assuming independence and non-contextuality. This phenomenon is a subset of the Peres-Mermin (PM) square framework.

Conclusion: Bi-contextuality challenges classical models by requiring contextuality or dependence of preparations, impacting the PBR theorem.

Abstract: We present and experimentally demonstrate a novel non-classical phenomenon,
bi-contextuality, observed in quantum systems prepared by two independent
sources. This discovery plays a key role in the developing framework of network
nonlocality, offering a new method for confirming the quantum nature of
measurements within a single network node. Bi-contextuality acts as a reversed
Bell scenario: while Bell scenarios involve splitting a system for independent
measurements, our approach combines systems from separate independent sources
for joint measurements. The outcomes defy classical models that assume
independence and non-contextuality. The simplest Bell scenario can be seen as a
subset of the Peres-Mermin (PM) square, and our phenomenon represents another
important subset of this framework. Moreover, bi-contextuality has notable
consequences related to the Pusey-Barrett-Rudolph (PBR) theorem, suggesting
that classical psi-ontic models must account for contextuality or the
dependence of preparations, challenging established assumptions.

</details>


### [200] [Towards solving large QUBO problems using quantum algorithms: improving the LogQ scheme](https://arxiv.org/abs/2507.08489)
*Yagnik Chatterjee,Jérémie Messud*

Main category: quant-ph

TL;DR: 新LogQ参数化方法使用梯度启发式优化，减少资源消耗，增强了LogQ在大型问题上相对于QAOA的优势。


<details>
  <summary>Details</summary>
Motivation: 现有LogQ算法在优化其自由参数方面存在资源密集的问题，需要复杂的优化算法，这削弱了其相对于QAOA的优势。本研究旨在解决这一挑战。

Method: 提出了一种新的LogQ参数化方法，该方法可以使用梯度启发式方法进行优化，相较于传统的演进或全局优化算法，所需资源更少。

Result: 通过在分析模型和MaxCut问题上进行大规模数值结果的展示，说明了新方法在优化效率和LogQ算法优势方面的特点。

Conclusion: LogQ算法通过使用梯度启发式方法优化其参数，克服了现有LogQ参数优化资源密集的问题，从而在处理大规模/工业问题时增强了其相对于QAOA的优势。

Abstract: The LogQ algorithm encodes Quadratic Unconstrained Binary Optimization (QUBO)
problems with exponentially fewer qubits than the Quantum Approximate
Optimization Algorithm (QAOA). The advantages of conventional LogQ are
accompanied by a challenge related to the optimization of its free parameters,
which requires the usage of resource intensive evolutionary or even global
optimization algorithms. We propose a new LogQ parameterization that can be
optimized with a gradient-inspired method, which is less resource-intensive and
thus strengthens the advantage of LogQ over QAOA for large/industrial problems.
We illustrate the features of our method on an analytical model and present
larger scale numerical results on MaxCut problems.

</details>


### [201] [Spin-Orbit Structure and Helicity Anomaly in Relativistic Electron Vortex Beams](https://arxiv.org/abs/2507.08493)
*Zhongze Guo,Bei Xu,Qiang Gu*

Main category: quant-ph

TL;DR: 通过求解狄拉克方程，为研究相对论电子涡旋束的性质提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 相对论电子涡旋束（REVB）因其非平凡的自旋轨道结构而备受关注，而狄拉克方程的精确解是理解REVB角动量特性的可靠起点。

Method: 采用广义级数展开方法，在复圆柱坐标系中推导出一组狄拉克方程的精确本征解。

Result: 证明了本征态携带净角动量，其中涡旋电荷是传播方向上的总角动量量子数，并推导了内禀自旋-轨道耦合强度的显式表达式。此外，研究表明在涡旋态中表现出异常的螺旋度可以作为REVB的实用表征量。

Conclusion: 这项工作为进一步探索相对论电子涡旋束（REVB）的理论和实验奠定了理论基础。

Abstract: The relativistic electron vortex beam (REVB) has attracted increasing
attention due to its nontrivial spin-orbit structure recently. As relativistic
electrons are governed by the Dirac equation, exact solutions to this equation
provide the most reliable starting point for understanding angular momentum
characteristics of REVBs. In this work, a set of exact eigensolutions of the
Dirac equation are derived in a complex cylindrical coordinate system using a
generalized series expansion method. We demonstrate that the eigenstate carries
net angular momentum with the vortex charge being the quantum number of the
total angular momentum along the propagation direction and deduce the explicit
expression for the intrinsic spin-orbit coupling strength. Furthermore, we show
that helicity, which exhibits anomaly in the vortex state, can serve as a
practical characterizing quantity for the REVB. This work lays a theoretical
foundation for further exploration of REVBs in both theory and experiment.

</details>


### [202] [Enhancing Decoding Performance using Efficient Error Learning](https://arxiv.org/abs/2507.08536)
*Pavithran Iyer,Aditya Jain,Stephen D. Bartlett,Joseph Emerson*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Lowering the resource overhead needed to achieve fault-tolerant quantum
computation is crucial to building scalable quantum computers. We show that
adapting conventional maximum likelihood (ML) decoders to a small subset of
efficiently learnable physical error characteristics can significantly improve
the logical performance of a quantum error-correcting code. Specifically, we
leverage error information obtained from efficient characterization methods
based on Cycle Error Reconstruction (CER), which yields Pauli error rates on
the $n$ qubits of an error-correcting code. Although the total number of Pauli
error rates needed to describe a general noise process is exponentially large
in $n$, we show that only a few of the largest few Pauli error rates are needed
and that a heuristic technique can complete the Pauli error distribution for ML
decoding from this restricted dataset. Using these techniques, we demonstrate
significant performance improvements for decoding quantum codes under a variety
of physically relevant error models. For instance, with CER data that
constitute merely $1\%$ of the Pauli error rates in the system, we achieve a
$10X$ gain in performance compared to the case where decoding is based solely
on the fidelity of the underlying noise process. Our conclusions underscore the
promise of recent error characterization methods for improving quantum error
correction and lowering overheads.

</details>


### [203] [Quantum Algorithms for Projection-Free Sparse Convex Optimization](https://arxiv.org/abs/2507.08543)
*Jianhao He,John C. S. Lui*

Main category: quant-ph

TL;DR: 本文针对向量和矩阵域的稀疏凸优化问题，提出了两种量子算法，显著优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习和数据科学中的投影无关稀疏凸优化问题。

Method: 针对向量域和矩阵域的稀疏凸优化问题，提出了两种量子算法。向量域算法利用函数值预言，查询复杂度分别为 O(sqrt(d)/ε) 和 O(1/ε)。矩阵域算法针对核范数约束，更新步骤的时间复杂度为 O(rd/ε^2) 和 O(sqrt(r)d/ε^3)。

Result: 量子算法在查询复杂度和时间复杂度上相比经典算法分别降低了 O(sqrt(d)) 和 O(d)（向量域），以及至少 O(sqrt(d))（矩阵域），证明了量子优势。

Conclusion: 本文提出的量子算法在投影无关稀疏凸优化问题中展现出量子优势，其在维度 d 的依赖性上优于最优经典方法。

Abstract: This paper considers the projection-free sparse convex optimization problem
for the vector domain and the matrix domain, which covers a large number of
important applications in machine learning and data science. For the vector
domain $\mathcal{D} \subset \mathbb{R}^d$, we propose two quantum algorithms
for sparse constraints that finds a $\varepsilon$-optimal solution with the
query complexity of $O(\sqrt{d}/\varepsilon)$ and $O(1/\varepsilon)$ by using
the function value oracle, reducing a factor of $O(\sqrt{d})$ and $O(d)$ over
the best classical algorithm, respectively, where $d$ is the dimension. For the
matrix domain $\mathcal{D} \subset \mathbb{R}^{d\times d}$, we propose two
quantum algorithms for nuclear norm constraints that improve the time
complexity to $\tilde{O}(rd/\varepsilon^2)$ and
$\tilde{O}(\sqrt{r}d/\varepsilon^3)$ for computing the update step, reducing at
least a factor of $O(\sqrt{d})$ over the best classical algorithm, where $r$ is
the rank of the gradient matrix. Our algorithms show quantum advantages in
projection-free sparse convex optimization problems as they outperform the
optimal classical methods in dependence on the dimension $d$.

</details>


### [204] [Photonic processor benchmarking for variational quantum process tomography](https://arxiv.org/abs/2507.08570)
*Vladlen Galetsky,Paul Kohl,Janis Nötzel*

Main category: quant-ph

TL;DR: 本研究使用光学处理器对变分量子过程层析进行了量子类比实验演示，并将其与IBM、QuTech和Quandela等平台的性能进行了比较。结果表明，光子处理器在保真度和收敛性方面优于超导处理器，证明了其在近期量子算法部署中的潜力。


<details>
  <summary>Details</summary>
Motivation: 展示利用光学处理器进行变分量子过程层析的可行性，并将其与现有的量子计算平台进行基准比较，以评估其在近期量子算法部署中的潜力。

Method: 使用光学处理器进行量子类比实验演示变分量子过程层析，利用经典独热编码和酉分解在光子平台上执行变分量子算法。

Result: 光子处理器在保真度和收敛行为方面优于超导处理器，在9次迭代后达到0.8的保真度，尤其是在深度较大的情况下。热噪声是影响保真度的主要因素。

Conclusion: 光子处理器在近期的量子算法部署中，特别是在混合变分算法中，是非常有竞争力的。该分析对于状态和过程层析以及涉及变分量子电路算法的各种应用都很有价值。

Abstract: We present a quantum-analogous experimental demonstration of variational
quantum process tomography using an optical processor. This approach leverages
classical one-hot encoding and unitary decomposition to perform the variational
quantum algorithm on a photonic platform. We create the first benchmark for
variational quantum process tomography evaluating the performance of the
quantum-analogous experiment on the optical processor against several publicly
accessible quantum computing platforms, including IBM's 127-qubit Sherbrooke
processor, QuTech's 5-qubit Tuna-5 processor, and Quandela's 12-mode Ascella
quantum optical processor. We evaluate each method using process fidelity, cost
function convergence, and processing time per iteration for variational quantum
circuit depths of $d=3$ and $d=6$. Our results indicate that the optical
processors outperform their superconducting counterparts in terms of fidelity
and convergence behavior reaching fidelities of $0.8$ after $9$ iterations,
particularly at higher depths, where the noise of decoherence and dephasing
affect the superconducting processors significantly.
  We further investigate the influence of any additional quantum optical
effects in our platform relative to the classical one-hot encoding. From the
process fidelity results it shows that the (classical) thermal noise in the
phase-shifters dominates over other optical imperfections, such as mode
mismatch and dark counts from single-photon sources.
  The benchmarking framework and experimental results demonstrate that photonic
processors are strong contenders for near-term quantum algorithm deployment,
particularly in hybrid variational contexts. This analysis is valuable not only
for state and process tomography but also for a wide range of applications
involving variational quantum circuit based algorithms.

</details>


### [205] [Arbitrary high-fidelity binomial codes from multiphoton spin-boson interactions](https://arxiv.org/abs/2507.08585)
*Pradip Laha,Peter van Loock*

Main category: quant-ph

TL;DR: 本工作提出了一种利用连续变量模式和双能级系统之间非线性多光子相互作用来生成二项式量子比特编码的新方法，并探讨了提高实验可行性的途径。


<details>
  <summary>Details</summary>
Motivation: 为了弥补目前生成任意二项式量子比特编码方法的不足，并提高实验可行性。

Method: 通过利用连续变量模式和双能级系统之间的非线性多光子相互作用来生成二项式量子比特编码。

Result: 提出了一种新的生成二项式量子比特编码的方法，并展示了如何通过减少所需高阶相互作用来提高实验可行性。

Conclusion: 本工作提出了一种利用连续变量（CV）模式和双能级系统之间非线性多光子相互作用来生成二项式量子比特编码的方法。

Abstract: Encoding a qubit in the continuous degrees of freedom of a quantum system,
such as bosonic modes, is a powerful alternative to modern quantum error
correction (QEC). Among the most prominent bosonic QEC codes, binomial codes
provide protection against loss and dephasing errors by encoding logical states
in superpositions of Fock states with binomially weighted coefficients. While
much attention has been given to their error-correcting capabilities and
integration into fault-tolerant architectures, efficient methods for generating
arbitrary binomial codewords remain scarce. In this work, we propose a novel
scheme for generating these codewords by exploiting nonlinear multiphoton
interactions between a continuous-variable bosonic mode (oscillator) and a
two-level system (spin/qubit). Our proposed scheme assumes the ability to
prepare the oscillator in an arbitrary Fock state and the qubit in an arbitrary
superposition of its basis states and access to arbitrarily high multiphoton
interactions. To enhance the experimental feasibility of our scheme, we further
demonstrate how to reduce the required order parameter of multiphoton
interactions by a factor of two for a special class of code states.

</details>


### [206] [Bayesian Interpretation of Husimi Function and Wehrl Entropy](https://arxiv.org/abs/2507.08600)
*Chen Xu,Yiqi Yu,Peng Zhang*

Main category: quant-ph

TL;DR: 提供了Husimi函数和Wehrl熵的概率解释，揭示了其与经典力学的联系。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解Husimi函数和Wehrl熵的统计意义，特别是它们与量子纠缠的联系，以及探索其与经典力学的对应关系。

Method: 利用贝叶斯定理为Husimi函数和Wehrl熵提供了一种不依赖于辅助系统或非正交基的概率解释，并将其与N自旋1/2粒子系统和经典陀螺联系起来。

Result: 提出了Husimi函数和Wehrl熵的替代性概率解释，揭示了它们与N个经典陀螺的相空间概率分布及其熵的经典对应关系，并讨论了其在连续变量系统中的推广。

Conclusion: 该研究为Husimi函数和Wehrl熵提供了新的概率解释，并阐明了它们与经典对应关系的联系，同时探讨了其在连续变量系统中的推广应用。

Abstract: Husimi function (Q-function) of a quantum state is the distribution function
of the density operator in the coherent state representation. It is widely used
in theoretical research, such as in quantum optics. The Wehrl entropy is the
Shannon entropy of the Husimi function, and is non-zero even for pure states.
This entropy has been extensively studied in mathematical physics. Recent
research also suggests a significant connection between the Wehrl entropy and
many-body quantum entanglement in spin systems. We investigate the statistical
interpretation of the Husimi function and the Wehrl entropy, taking the system
of $N$ spin-1/2 particles as an example. Due to the completeness of coherent
states, the Husimi function and Wehrl entropy can be explained via the positive
operator-valued measurement (POVM) theory, although the coherent states are not
a set of orthonormal basis. Here, with the help of the Bayes' theorem, we
provide an alternative probabilistic interpretation for the Husimi function and
the Wehrl entropy. This interpretation is based on direct measurements of the
system, and thus does not require the introduction of an ancillary system as in
POVM theory. Moreover, under this interpretation the classical correspondences
of the Husimi function and Wehrl entropy are just phase-space probability
distribution function of $N$ classical tops, and its associated entropy,
respectively. Therefore, this explanation contributes to a better understanding
of the relationship between the Husimi function, Wehrl entropy, and
classical-quantum correspondence. The generalization of this statistical
interpretation to continuous-variable systems is also discussed.

</details>


### [207] [Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security](https://arxiv.org/abs/2507.08623)
*Pascal Debus,Maximilian Wendlinger,Kilian Tscharke,Daniel Herr,Cedric Brügmann,Daniel Ohl de Mello,Juris Ulmanis,Alexander Erhard,Arthur Schmidt,Fabian Petsch*

Main category: quant-ph

TL;DR: QML面临着来自物理和算法层的独特安全威胁，但现有研究缺乏整体性。本研究提出将“杀伤链”模型应用于QML，通过详细分类和分析攻击向量及其相互关系，为QML安全提供更结构化的威胁建模和防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有的QML安全研究往往孤立地分析各种攻击向量，并基于不切实际的假设，这阻碍了有效防御策略的制定。因此，需要一种更结构化的方法来模拟QML的攻击面，理解攻击的关联性、先决条件和潜在影响。

Method: 通过对现有QML攻击向量进行广泛的文献分析，将其映射到一个量子感知的杀伤链模型中，该模型借鉴了MITRE ATLAS框架，并突出了不同层面威胁之间的相互依赖性。

Result: 提出了一种将杀伤链模型应用于QML安全威胁的框架，并提供了一个详细的QML攻击向量分类法，将其与量子感知的杀伤链模型阶段相对应。该框架强调了物理层面、数据和算法操纵以及隐私攻击之间的相互作用，为QML系统的威胁建模和安全设计奠定了基础。

Conclusion: 该研究提出了一个将经典网络安全中的“杀伤链”模型应用于量子机器学习（QML）安全威胁的框架，旨在为QML系统的安全设计提供更结构化、更全面的方法。

Abstract: Quantum Machine Learning (QML) systems inherit vulnerabilities from classical
machine learning while introducing new attack surfaces rooted in the physical
and algorithmic layers of quantum computing. Despite a growing body of research
on individual attack vectors - ranging from adversarial poisoning and evasion
to circuit-level backdoors, side-channel leakage, and model extraction - these
threats are often analyzed in isolation, with unrealistic assumptions about
attacker capabilities and system environments. This fragmentation hampers the
development of effective, holistic defense strategies. In this work, we argue
that QML security requires more structured modeling of the attack surface,
capturing not only individual techniques but also their relationships,
prerequisites, and potential impact across the QML pipeline. We propose
adapting kill chain models, widely used in classical IT and cybersecurity, to
the quantum machine learning context. Such models allow for structured
reasoning about attacker objectives, capabilities, and possible multi-stage
attack paths - spanning reconnaissance, initial access, manipulation,
persistence, and exfiltration. Based on extensive literature analysis, we
present a detailed taxonomy of QML attack vectors mapped to corresponding
stages in a quantum-aware kill chain framework that is inspired by the MITRE
ATLAS for classical machine learning. We highlight interdependencies between
physical-level threats (like side-channel leakage and crosstalk faults), data
and algorithm manipulation (such as poisoning or circuit backdoors), and
privacy attacks (including model extraction and training data inference). This
work provides a foundation for more realistic threat modeling and proactive
security-in-depth design in the emerging field of quantum machine learning.

</details>


### [208] [Magic Steady State Production: Non-Hermitian and Stochastic pathways](https://arxiv.org/abs/2507.08676)
*Pablo Martinez-Azcona,Matthieu Sarkis,Alexandre Tkatchenko,Aurélia Chenu*

Main category: quant-ph

TL;DR: 利用非厄米动力学和耗散量子比特制备量子魔态，可制备|H>和|T>态，不依赖初始状态，且对经典噪声具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 通用量子计算机需要同时具备纠缠和非稳定性的量子态，而非稳定性（量子魔性）是实现这一目标的关键。本研究旨在探索一种新的方法来制备量子魔态。

Method: 利用非厄米动力学和耗散量子比特制备量子魔态，寻找最优参数以制备|H>和|T>态，并分析了经典噪声对量子魔性的影响。

Result: 提出了一种利用非厄米动力学制备量子魔态的协议，可以制备|H>和|T>态。该方法不依赖于初始状态，并且在加入经典噪声时仍能保持量子魔性。

Conclusion: 本研究提出了一种利用非厄米动力学制备量子比特的方法，可以有效地制备具有纠缠和非稳定性的量子态，并找到了制备|H>和|T>态的最优参数。该方法不依赖于初始状态的制备，因为Bloch球面上的所有状态都会收敛到这些稳态。此外，研究还发现，在非厄米部分加入经典噪声不一定会损害量子魔性，单条轨迹会围绕魔性稳态值集中。

Abstract: Universal quantum computers require states with both entanglement and
non-stabilizerness, the latter property being commonly known as \textit{quantum
magic}. Here, we introduce a protocol that prepares magic steady states by
leveraging non-Hermitian dynamics, which, contrary to unitary dynamics, can
host pure-state attractors. We investigate the dissipative qubit, in which we
find the optimal parameters to prepare $|H\rangle$ and $|T\rangle$ states.
Interestingly, this approach does not require knowledge or preparation of an
initial state, since all the states of the Bloch sphere converge to these
steady states. We also consider the addition of classical noise in the
anti-hermitian part, and show that it does not necessarily hinder magic, rather
the single trajectories show concentration around the magic steady state value.

</details>


### [209] [A novel quantum circuit for the quantum Fourier transform](https://arxiv.org/abs/2507.08699)
*Juan M. Romero,Emiliano Montoya-González,Guillermo Cruz,Roberto C. Romero*

Main category: quant-ph

TL;DR: 提出了一种更有效的量子傅里叶变换电路和改进的HHL算法。


<details>
  <summary>Details</summary>
Motivation: 为了寻找更优的量子傅里叶变换（QFT）实现方式，并提升HHL等量子算法的性能。

Method: 提出了一种新的量子傅里叶变换（QFT）的实现方法，并设计了一种新的量子电路来实现QFT。在此基础上，开发了一个HHL算法的替代版本。

Result: 新设计的量子电路比传统设计更有效。基于新电路的HHL算法版本比标准实现有更优的性能。

Conclusion: 本论文提出了一种新的量子傅里叶变换（QFT）的实现方法，并基于此方法设计了一种更高效的量子电路。此外，利用该电路改进了HHL算法，并验证了其性能的提升。

Abstract: The Quantum Fourier Transform (QFT) is a fundamental component of many
quantum computing algorithms. In this paper, we present an alternative method
for factoring this transformation. Inspired by this approach, we introduce a
new quantum circuit for implementing the QFT. We show that this circuit is more
efficient than the conventional design. Furthermore, using this circuit, we
develop an alternative version of the HHL algorithm, which also demonstrates
improved performance compared to the standard implementation.

</details>


### [210] [Simulated non-Markovian Noise Resilience of Silicon-Based Spin Qubits with Surface Code Error Correction](https://arxiv.org/abs/2507.08713)
*Oscar Gravier,Thomas Ayral,Benoît Vermersch,Tristan Meunier,Valentin Savin*

Main category: quant-ph

TL;DR: 量子纠错能提升硅基自旋量子比特在非马尔可夫噪声下的性能，相干时间与噪声呈四次方关系。


<details>
  <summary>Details</summary>
Motivation: 在量子纠错框架内研究硅基自旋量子比特对非马尔可夫噪声的恢复能力。

Method: 采用数值模拟和量子纠错理论，研究了硅基自旋量子比特在非马尔可夫噪声下的恢复能力，评估了距离为3的旋转表面码及其XZZX变体的性能。

Result: 量子纠错将非马尔可夫物理噪声转化为马尔可夫逻辑噪声，相干时间呈现物理量子比特和逻辑量子比特之间的四次方依赖关系。分析了空间噪声相关和稀疏结构的影响，证实了硅基自旋量子比特系统中量子纠错的鲁棒性。

Conclusion: 量子纠错将非马尔可夫物理噪声转化为马尔可夫逻辑噪声，导致物理和逻辑量子比特的相干时间之间存在四次方依赖关系。该研究证实了量子纠错在硅基自旋量子比特系统中的鲁棒性。

Abstract: We investigate the resilience of silicon-based spin qubits against
non-Markovian noise within the framework of quantum error correction. We
consider a realistic non-Markovian noise model that affects both the Larmor
frequency and exchange energy of qubits, allowing accurate simulations of noisy
quantum circuits. We employ numerical emulation to assess the performance of
the distance-3 rotated surface code and its XZZX variant, using a logical qubit
coherence time metric based on Ramsey-like experiments. Our numerical results
suggest that quantum error correction converts non-Markovian physical noise
into Markovian logical noise, resulting in a quartic dependence of coherence
time between physical and logical qubits. Additionally, we analyze the effects
of spatial noise correlations and sparse architectures, substantiating the
robustness of quantum error correction in silicon-based spin qubit systems.

</details>


### [211] [Routing Quantum Control of Causal Order](https://arxiv.org/abs/2507.08781)
*Maarten Grothus,Alastair A. Abbott,Augustin Vanrietvelde,Cyril Branciard*

Main category: quant-ph

TL;DR: 该研究连接了量子控制量子电路和路由量子电路框架，证明了可以使用路由图系统地分解QC-QC，并为不确定因果顺序领域的研究开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 填补了路由量子电路分解在QC-QCs方面的空白，并为解决不确定因果顺序领域中的开放性问题提供了新的途径。

Method: 通过将量子电路与量子控制因果顺序（QC-QCs）框架与路由量子电路形式主义联系起来，并提供了一个明确的构造来系统地获得任何具有N方的QC-QC的路由电路分解。

Result: 证明了对于任何给定的N，可以使用单个路由图来系统地获得具有N方的任何QC-QC的路由电路分解，并展示了与其他路由电路分解的对比，以及该方法在处理不确定因果顺序领域中的开放性问题中的潜在应用。

Conclusion: 该研究通过将两种框架联系起来，证明了可以使用单个路由图系统地获得具有N方的任何量子控制量子电路（QC-QC）的路由电路分解，并指出了这种联系在解决不确定因果顺序领域中的各种开放性问题中的应用潜力。

Abstract: In recent years, various frameworks have been proposed for the study of
quantum processes with indefinite causal order. In particular, quantum circuits
with quantum control of causal order (QC-QCs) form a broad class of physical
supermaps obtained from a bottom-up construction and are believed to represent
all quantum processes physically realisable in a fixed spacetime.
Complementarily, the formalism of routed quantum circuits introduces quantum
operations constrained by "routes" to represent processes in terms of a more
fine-grained routed circuit decomposition. This decomposition, formalised using
a so-called routed graph, represents the information flow within the respective
process. However, the existence of routed circuit decompositions has only been
established for a small set of processes so far, including both certain
specific QC-QCs and more exotic processes as examples.
  In this work, we remedy this fact by connecting these two frameworks. We
prove that for any given $N$, one can use a single routed graph to
systematically obtain a routed circuit decomposition for any QC-QC with $N$
parties. We detail this construction explicitly and contrast it with other
routed circuit decompositions of QC-QCs, which we obtain from alternative
routed graphs. We conclude by pointing out how this connection can be useful to
tackle various open problems in the field of indefinite causal order,
particularly establishing circuit representations of subclasses of QC-QCs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [212] [AI-Augmented Visible Light Communication: A Framework for Noise Mitigation and Secure Data Transmission](https://arxiv.org/abs/2507.08145)
*A. A. Nutfaji,Moustafa Hassan Elmallah*

Main category: eess.SP

TL;DR: 本研究提出了一种基于深度神经网络的可见光通信信号均衡方法，通过Python仿真验证了其在降低比特误率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决可见光通信（VLC）系统中常见的挑战，提高信号完整性。

Method: 通过Python仿真模拟了一个受高斯白噪声（AWGN）影响的基础可见光通信系统，并训练深度神经网络（DNN）对接收到的噪声信号进行均衡。

Result: 所提出的深度学习模型成功实现了对噪声信号的均衡，并显著降低了比特误率（BER）。

Conclusion: 本研究提出的深度神经网络模型能有效提高可见光通信系统的信号完整性，通过仿真和比特误率（BER）对比验证了其有效性。

Abstract: This paper presents a proposed AI Deep Learning model that addresses common
challenges encountered in Visible Light Communication (VLC) systems. In this
work, we run a Python simulation that models a basic VLC system primarily
affected by Additive White Gaussian Noise (AWGN). A Deep Neural Network (DNN)
is then trained to equalize the noisy signal received and improve signal
integrity. The system evaluates and compares the Bit Error Rate (BER) before
and after equalization to demonstrate the effectiveness of the proposed model.
This paper starts by introducing the concept of visible light communication,
then it dives deep into some details about the process of VLC and the
challenges it faces, shortly after we propose our project which helps overcome
these challenges. We finally conclude with a lead for future work, highlighting
the areas that are most suitable for future improvements.

</details>


### [213] [Ambiguity Function Analysis of AFDM Signals for Integrated Sensing and Communications](https://arxiv.org/abs/2507.08293)
*Haoran Yin,Yanqun Tang,Yuanhan Ni,Zulin Wang,Gaojie Chen,Jun Xiong,Kai Yang,Marios Kountouris,Yong Liang Guan,Yong Zeng*

Main category: eess.SP

TL;DR: AFDM信号通过其独特的模糊函数特性，在单站和双站场景下都能提供精确的范围和速度估计，并且通过插入保护符号可以避免干扰，使其成为ISAC应用的理想选择。


<details>
  <summary>Details</summary>
Motivation: 研究AFDM信号的模糊函数（AF），以表征其在单站和双站设置下的范围和速度估计能力，并探索其在ISAC（集成感知与通信）应用中的潜力。

Method: 推导了AFDM啁啾子载波的自模糊函数（AAF）和互模糊函数（CAF），并分析了不同典型AFDM帧的AF，考虑了确定性导频和随机数据符号。

Result: AFDM信号的AAF具有“类似尖峰”的局部特性和“类似周期”的全局特性，形成一个近似的平行四边形，能够实现无模糊的目标感知。CAF在多普勒维度上具有额外的移位。AFDM的模糊函数性质使其能够支持ISAC应用。

Conclusion: AFDM信号的模糊函数（AF）在单站和双站设置下都具有良好的范围和速度估计能力。通过在AFDM帧中插入保护符号，可以实现无干扰感知。模拟结果验证了AFDM在ISAC应用中的强大潜力。

Abstract: Affine frequency division multiplexing (AFDM) is a promising chirp-based
waveform with high flexibility and resilience, making it well-suited for
next-generation wireless networks, particularly in high-mobility scenarios. In
this paper, we investigate the ambiguity functions (AFs) of AFDM signals, which
fundamentally characterize their range and velocity estimation capabilities in
both monostatic and bistatic settings. Specifically, we first derive the
auto-ambiguity function (AAF) of an AFDM chirp subcarrier, revealing its
"spike-like" local property and "periodic-like" global property along the
rotated delay and Doppler dimensions. This structure naturally forms a
parallelogram for each localized pulse of the AAF of the AFDM chirp subcarrier,
enabling unambiguous target sensing. Then, we study the cross-ambiguity
function (CAF) between two different AFDM chirp subcarriers, which exhibits the
same local and global properties as the AAF but with an additional shift along
the Doppler dimension. We then extend our analysis to the AF of various typical
AFDM frames, considering both deterministic pilot and random data symbols. In
particular, we demonstrate that inserting guard symbols in AFDM facilitates
interference-free sensing. Simulation results validate our theoretical
findings, highlighting AFDM's strong potential for ISAC applications.

</details>


### [214] [Unobtrusive Reflectance Photoplethysmography for Detecting and Severity Grading of Sleep Apnea via Oxygen Desaturation Index](https://arxiv.org/abs/2507.08399)
*Karen Adam,Clémentine Aguet,Patrick Theurillat,Florent Baty,Maximilian Boesch,Damien Ferrario,Mathieu Lemay,Martin Brutsche,Fabian Braun*

Main category: eess.SP

TL;DR: 使用可穿戴设备在上臂测量ODI可有效筛查睡眠呼吸暂停。


<details>
  <summary>Details</summary>
Motivation: 睡眠呼吸暂停是一种常见的慢性睡眠相关疾病，常与其他心脑血管疾病并发。目前的诊断方法需要夜间多导睡眠图检查，该检查通常在睡眠实验室进行。本研究旨在探索使用可穿戴设备进行睡眠呼吸暂停筛查的可行性。

Method: 使用可穿戴设备测量腕部和上臂的PPG信号，计算ODI，并将其与AHI进行比较，评估其作为睡眠呼吸暂停诊断和严重程度评估的替代指标的可行性，并比较不同测量位置的ODI诊断性能。

Result: 在170名接受睡眠呼吸暂停筛查的患者中，上臂ODI是预测中度或重度睡眠呼吸暂停的良好指标，准确率为86%，敏感率为96%，特异率为70%；而腕部ODI的诊断效果较差。

Conclusion: 使用可穿戴设备在上臂测量得出的人氧去饱和指数（ODI）可以作为睡眠呼吸暂停诊断的替代指标，准确率为86%，敏感率为96%，特异率为70%。

Abstract: Sleep apnea is a common chronic sleep-related disorder which is known to be a
comorbidity for cerebro- and cardio-vascular disease. Diagnosis of sleep apnea
usually requires an overnight polysomnography at the sleep laboratory. In this
paper, we used a wearable device which measures reflectance
photoplethysmography (PPG) at the wrist and upper arm to estimate continuous
SpO2 levels during sleep and subsequently derive an oxygen desaturation index
(ODI) for each patient. On a cohort of 170 patients undergoing sleep apnea
screening, we evaluated whether this ODI value could represent a surrogate
marker for the apnea-hypopnea index (AHI) for the diagnosis and severity
assessment of sleep apnea. As the ODI was simultaneously obtained at the
fingertip, upper arm and wrist, we compared ODI diagnostic performance
depending on the measurement location. We then further evaluated the accuracy
of ODI as a direct predictor for moderate and severe sleep apnea as defined by
established AHI thresholds. We found that ODI values obtained at the upper arm
were good predictors for moderate or severe sleep apnea, with 86% accuracy, 96%
sensitivity and 70% specificity, whereas ODI values obtained at the wrist were
less reliable as a diagnostic tool.

</details>


### [215] [Exploiting Cognition in ISAR Processing for Spectral Compatibility Applications](https://arxiv.org/abs/2507.08423)
*Massimo Rosamilia,Augusto Aubry,Alessio Balleri,Antonio De Maio,Marco Martorella*

Main category: eess.SP

TL;DR: 该研究提出了一种认知ISAR，通过感知和行动交替进行，合成定制雷达波形，实现拥挤电磁环境下的频谱兼容和高质量成像。


<details>
  <summary>Details</summary>
Motivation: 为了在拥挤的电磁环境中实现ISAR成像的频谱兼容性，并允许与其他射频信号共存。

Method: 提出了一种认知ISAR方法，包括环境感知（频谱感知）和行动（定制波形合成），并采用压缩感知或秩最小化策略进行数据恢复。

Result: 实验结果表明，该认知ISAR系统能够实现频谱兼容，同时生成高质量的ISAR图像，并支持其他射频活动。

Conclusion: 该研究提出了一种认知逆合成孔径雷达（ISAR），能够在拥挤的电磁环境中实现频谱兼容，并通过感知和行动交替进行，合成定制的雷达波形，以在不干扰其他射频（RF）源的情况下完成成像任务。所提出的方法通过频谱感知模块识别环境中的发射源，并利用压缩感知或秩最小化恢复策略来恢复数据，以实现高分辨率ISAR成像和频谱共存。

Abstract: This paper introduces and analyzes the concept of a cognitive inverse
synthetic aperture radar (ISAR) ensuring spectral compatibility in crowded
electromagnetic environments. In such a context, the proposed approach
alternates between environmental perception, recognizing possible emitters in
its frequency range, and an action stage, synthesizing and transmitting a
tailored radar waveform to achieve the desired imaging task while guaranteeing
spectral coexistence with overlaid emitters. The perception is carried out by a
spectrum sensing module providing the true relevant spectral parameters of the
sources in the environment. The action stage employs a tailored signal design
process, synthesizing a radar waveform with bespoke spectral notches, enabling
ISAR imaging over a wide spectral bandwidth without interfering with the other
radio frequency (RF) sources. A key enabling requirement for the proposed
application is the capability to successfully recover possible missing data in
the frequency domain (induced by spectral notches) and in the slow-time
dimension (enabling concurrent RF activities still in a cognitive fashion).
This process is carried out by resorting to advanced methods based on either
the compressed-sensing framework or a rank-minimization recovery strategy. The
capabilities of the proposed system are assessed exploiting a dataset of drone
measurements in the frequency band between 13 GHz and 15 GHz. Results highlight
the effectiveness of the devised architecture to enable spectral compatibility
while delivering high-quality ISAR images as well as additional RF activities.

</details>


### [216] [A Temporal Gaussian Noise Model for Equalization-enhanced Phase Noise](https://arxiv.org/abs/2507.08470)
*Benedikt Geiger,Fred Buchali,Vahid Aref,Laurent Schmalen*

Main category: eess.SP

TL;DR: 提出时间高斯噪声模型以解决高速率传输系统中的突发式失真问题，通过仿真和实验验证了其预测性能的准确性和简化性。


<details>
  <summary>Details</summary>
Motivation: 均衡增强相位噪声在高速率传输系统中会导致突发式失真。

Method: 提出了一种时间高斯噪声模型，该模型通过引入时变失真功率来捕获均衡增强相位噪声引起的突发式失真。

Result: 通过仿真和实验验证，该模型能够准确且简化地预测高速率传输系统的性能。它捕捉了均衡增强相位噪声引起的突发式失真，其特点是具有时变失真功率。

Conclusion: 该模型能够准确且简化地预测高速率传输系统的性能。

Abstract: Equalization-enhanced Phase Noise causes burst-like distortions in high
symbol-rate transmission systems. We propose a temporal Gaussian noise model
that captures these distortions by introducing a time-varying distortion power.
Validated through simulations and experiments, it enables accurate and simple
performance prediction for high symbol-rate transmission systems.

</details>


### [217] [Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees](https://arxiv.org/abs/2507.08653)
*Berire Gunes Reyhan,Sinem Coleri*

Main category: eess.SP

TL;DR: 本研究提出了一种基于优化理论的安全深度强化学习框架，用于超可靠无线网络控制系统（WNCS），通过最小化功耗来满足峰值信息年龄（PAoI）违反概率、传输功率和有限块长度下的可调度性等约束。


<details>
  <summary>Details</summary>
Motivation: 为了确保超可靠无线网络控制系统（WNCS）中的约束满足和性能优化，需要对控制和通信系统进行协同设计。

Method: 该框架包括两个阶段：优化理论和安全深度强化学习（DRL）。优化理论阶段推导了最优性条件，以建立变量之间的数学关系，从而简化和分解问题。安全DRL阶段采用教师-学生框架来指导DRL代理（学生），其中控制机制（教师）评估系统约束的合规性，并在需要时建议最接近的可行操作。

Result: 模拟结果表明，与基于规则和其他基于优化理论的DRL基准相比，该框架具有更快的收敛速度、更高的奖励和更大的稳定性。

Conclusion: 所提出的框架在满足系统约束的同时优化了性能，并在模拟中优于基线方法。

Abstract: In Wireless Networked Control Systems (WNCSs), control and communication
systems must be co-designed due to their strong interdependence. This paper
presents a novel optimization theory-based safe deep reinforcement learning
(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction
while optimizing performance, for the first time in the literature. The
approach minimizes power consumption under key constraints, including Peak Age
of Information (PAoI) violation probability, transmit power, and schedulability
in the finite blocklength regime. PAoI violation probability is uniquely
derived by combining stochastic maximum allowable transfer interval (MATI) and
maximum allowable packet delay (MAD) constraints in a multi-sensor network. The
framework consists of two stages: optimization theory and safe DRL. The first
stage derives optimality conditions to establish mathematical relationships
among variables, simplifying and decomposing the problem. The second stage
employs a safe DRL model where a teacher-student framework guides the DRL agent
(student). The control mechanism (teacher) evaluates compliance with system
constraints and suggests the nearest feasible action when needed. Extensive
simulations show that the proposed framework outperforms rule-based and other
optimization theory based DRL benchmarks, achieving faster convergence, higher
rewards, and greater stability.

</details>


### [218] [Multi-Symbol Digital AirComp via Modulation Design and Power Adaptation](https://arxiv.org/abs/2507.08670)
*Xiaojing Yan,Saeed Razavikia,Carlo Fischione*

Main category: eess.SP

TL;DR: 提出了一种名为SeMAC的新型调制框架，通过多符号调制提高空口计算的可靠性，并提出了一种用于固定调制格式的功率自适应方案。


<details>
  <summary>Details</summary>
Motivation: 为了提高数字空口计算（AirComp）的可靠性，特别是在存在信道噪声的情况下，提出了一种新的多符号调制框架SeMAC。

Method: 将SeMAC的调制设计制定为一个非凸优化问题，利用矩阵提升将其松弛为半定规划（SDP），并通过求解低秩近似来恢复可行的调制方案。对于无法更改调制格式的场景，还开发了一种功率自适应方案。

Result: SeMAC能够通过减少高达18 dB的计算误差来实现可靠计算，特别是在乘积函数场景下，相比其他现有方法表现更优。

Conclusion: SeMAC通过在多时隙中采用不同的星座图将输入值映射到调制符号序列，从而实现了可靠的计算，与现有方法相比，计算误差最多可降低18 dB，尤其是在乘积函数场景下。

Abstract: In this paper, we consider digital over-the-air computation (AirComp) and
introduce a new multi-symbol modulation framework called sequential modulation
for AirComp (SeMAC). Building upon ChannelComp, a general framework for
designing modulation schemes to support arbitrary function computation over a
multiple access channel (MAC), SeMAC maps each input value to a sequence of
modulated symbols using distinct constellation diagrams across multiple time
slots. This extension generalizes ChannelComp by enabling flexible modulation
design across multiple transmissions, thereby enhancing reliability against
channel noise. We formulate the modulation design as a non-convex optimization
problem, apply matrix lifting to relax it into a semidefinite programming
(SDP), and recover a feasible modulation solution by solving a low rank
approximation. For scenarios where the modulation formats cannot be changed, we
further develop a power adaptation scheme that adjusts amplitude and phase of
the modulated symbols while preserving the modulation structure. Numerical
results show that SeMAC can achieve a reliable computation by reducing the
computation error up to 18 dB compared to other existing methods, particularly
for the product function.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [219] [On the Parallel Complexity of Finding a Matroid Basis](https://arxiv.org/abs/2507.08194)
*Sanjeev Khanna,Aaron Putterman,Junkai Song*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A fundamental question in parallel computation, posed by Karp, Upfal, and
Wigderson (FOCS 1985, JCSS 1988), asks: \emph{given only independence-oracle
access to a matroid on $n$ elements, how many rounds are required to find a
basis using only polynomially many queries?} This question generalizes, among
others, the complexity of finding bases of linear spaces, partition matroids,
and spanning forests in graphs. In their work, they established an upper bound
of $O(\sqrt{n})$ rounds and a lower bound of $\widetilde{\Omega}(n^{1/3})$
rounds for this problem, and these bounds have remained unimproved since then.
  In this work, we make the first progress in narrowing this gap by designing a
parallel algorithm that finds a basis of an arbitrary matroid in
$\tilde{O}(n^{7/15})$ rounds (using polynomially many independence queries per
round) with high probability, surpassing the long-standing $O(\sqrt{n})$
barrier. Our approach introduces a novel matroid decomposition technique and
other structural insights that not only yield this general result but also lead
to a much improved new algorithm for the class of \emph{partition matroids}
(which underlies the $\widetilde\Omega(n^{1/3})$ lower bound of Karp, Upfal,
and Wigderson). Specifically, we develop an $\tilde{O}(n^{1/3})$-round
algorithm, thereby settling the round complexity of finding a basis in
partition matroids.

</details>


### [220] [Approximation Algorithms for the Cumulative Vehicle Routing Problem with Stochastic Demands](https://arxiv.org/abs/2507.08316)
*Jingyang Zhao,Mingyu Xiao*

Main category: cs.DS

TL;DR: 该研究通过随机近似算法改进了累积车辆路径问题（Cu-VRP）及其随机需求变体（Cu-VRPSD）和车辆路径问题随机需求变体（VRPSD）的近似比。


<details>
  <summary>Details</summary>
Motivation: 解决累积车辆路径问题（Cu-VRP）及其随机需求变体（Cu-VRPSD）和车辆路径问题随机需求变体（VRPSD），并旨在提高现有算法的近似比。

Method: 通过提出随机近似算法来解决累积车辆路径问题（Cu-VRP）及其随机需求变体（Cu-VRPSD）和车辆路径问题随机需求变体（VRPSD）。具体来说，该研究提出了Cu-VRPSD的随机3.456-近似算法，VRPSD的随机3.25-近似算法，以及Cu-VRP的随机3.194-近似算法。

Result: 实现了Cu-VRPSD的随机3.456-近似算法、VRPSD的随机3.25-近似算法以及Cu-VRP的随机3.194-近似算法，这些算法均优于先前已知的最佳近似比。

Conclusion: 该研究提出了用于Cu-VRPSD的随机3.456-近似算法，优于已知的最佳近似比6（Discret. Appl. Math. 2020）。对于VRPSD，该研究也提出了随机3.25-近似算法，优于已知的最佳近似比3.5（Oper. Res. 2012）。此外，对于Cu-VRP，研究提出了随机3.194-近似算法，优于已知的最佳近似比4（Oper. Res. Lett. 2013）。最后，该研究还探讨了在允许客户使用多个行程的情况下，进一步改进Cu-VRPSD和Cu-VRP的方法。

Abstract: In the Cumulative Vehicle Routing Problem (Cu-VRP), we need to find a
feasible itinerary for a capacitated vehicle located at the depot to satisfy
customers' demand, as in the well-known Vehicle Routing Problem (VRP), but the
goal is to minimize the cumulative cost of the vehicle, which is based on the
vehicle's load throughout the itinerary. If the demand of each customer is
unknown until the vehicle visits it, the problem is called Cu-VRP with
Stochastic Demands (Cu-VRPSD). Assume that the approximation ratio of metric
TSP is $1.5$. In this paper, we propose a randomized $3.456$-approximation
algorithm for Cu-VRPSD, improving the best-known approximation ratio of $6$
(Discret. Appl. Math. 2020). Since VRP with Stochastic Demands (VRPSD) is a
special case of Cu-VRPSD, as a corollary, we also obtain a randomized
$3.25$-approximation algorithm for VRPSD, improving the best-known
approximation ratio of $3.5$ (Oper. Res. 2012). For Cu-VRP, we give a
randomized $3.194$-approximation algorithm, improving the best-known
approximation ratio of $4$ (Oper. Res. Lett. 2013). Moreover, if each customer
is allowed to be satisfied by using multiple tours, we obtain further
improvements for Cu-VRPSD and Cu-VRP.

</details>


### [221] [H-Planarity and Parametric Extensions: when Modulators Act Globally](https://arxiv.org/abs/2507.08541)
*Fedor V. Fomin,Petr A. Golovach,Laure Morelle,Dimitrios M. Thilikos*

Main category: cs.DS

TL;DR: 研究通过引入平面H-调制器和相关的参数化概念（H-planar treedepth和H-planar treewidth），扩展了平面图的算法能力，并为图着色、完美匹配计数和最大独立集等问题提供了新的高效算法。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是扩展平面图的算法潜力，并为图算法提供新的参数化方法。通过引入基于调制器/目标修改方案的图分解，研究旨在开发能够处理更广泛图类问题的新算法。特别是，通过定义H-planar treedepth和H-planar treewidth，研究将现有的算法技术（如消除距离和树分解）推广到更一般的图类H，从而为解决各种图问题提供更强大的工具。

Method: 该研究的核心是一种计算平面H-调制器的多项式时间算法。给定一个图类H，图G的平面H-调制器是指一个集合X，使得X的躯干是平面图，并且G-X的所有连通分量都属于H。这里，X的躯干是通过对G[X]进行操作得到的，具体方法是：对于G-X的每一个连通分量，将其在G[X]上的邻域形成一个团。研究将H-Planarity定义为判断图G是否存在平面H-调制器的问题。

Result: 研究提出了一种计算平面H-调制器的多项式时间算法，并证明了在特定条件下（H是遗传的、可由CMSO定义的并且可在多项式时间内解决），H-Planarity问题也是多项式时间可解的。此外，研究还定义了H-planar treedepth和H-planar treewidth，并提供了参数化的FPT算法。这些方法在图着色、完美匹配计数和最大独立集等问题上取得了显著的算法改进，包括近似算法和高效多项式时间近似方案（EPTAS）。

Conclusion: 该研究提出了基于调制器/目标修改问题的图分解方法，并将其应用于参数化扩展图算法能力的各种算法应用中。研究证明，如果H是遗传的、可由CMSO定义的并且可以在多项式时间内解决，那么H-Planarity问题的解决时间也是多项式时间的。此外，通过定义H-planar treedepth和H-planar treewidth的概念，研究将H-Planarity参数化扩展，这些概念是对消除距离和树分解的泛化。结合现有的用于各种H-调制器问题的FPT算法，研究为许多图类H提供了参数化为H-planar treedepth和H-planar treewidth的FPT算法。通过结合平面图和具有界限树宽图的已知算法属性，计算H-planar treedepth和H-planar treewidth的方法产生了各种算法应用，例如，对于图着色可以得到加法近似算法，对于计数（加权）完美匹配可以得到多项式时间算法。此外，研究还为包括最大独立集在内的多个问题设计了高效多项式时间近似方案（EPTAS）。

Abstract: We introduce a series of graph decompositions based on the modulator/target
scheme of modification problems that enable several algorithmic applications
that parametrically extend the algorithmic potential of planarity. In the core
of our approach is a polynomial time algorithm for computing planar
H-modulators. Given a graph class H, a planar H-modulator of a graph G is a set
X \subseteq V(G) such that the ``torso'' of X is planar and all connected
components of G - X belong to H. Here, the torso of X is obtained from G[X] if,
for every connected component of G-X, we form a clique out of its neighborhood
on G[X]. We introduce H-Planarity as the problem of deciding whether a graph G
has a planar H-modulator. We prove that, if H is hereditary, CMSO-definable,
and decidable in polynomial time, then H-Planarity is solvable in polynomial
time. Further, we introduce two parametric extensions of H-Planarity by
defining the notions of H-planar treedepth and H-planar treewidth, which
generalize the concepts of elimination distance and tree decompositions to the
class H. Combining this result with existing FPT algorithms for various
H-modulator problems, we thereby obtain FPT algorithms parameterized by
H-planar treedepth and H-planar treewidth for numerous graph classes H. By
combining the well-known algorithmic properties of planar graphs and graphs of
bounded treewidth, our methods for computing H-planar treedepth and H-planar
treewidth lead to a variety of algorithmic applications. For instance, once we
know that a given graph has bounded H-planar treedepth or bounded H-planar
treewidth, we can derive additive approximation algorithms for graph coloring
and polynomial-time algorithms for counting (weighted) perfect matchings.
Furthermore, we design Efficient Polynomial-Time Approximation Schemes
(EPTAS-es) for several problems, including Maximum Independent Set.

</details>


### [222] [Beer Path Problems in Temporal Graphs](https://arxiv.org/abs/2507.08685)
*Andrea D'Ascenzo,Giuseppe F. Italiano,Sotiris Kanellopoulos,Anna Mpanti,Aris Pagourtzis,Christos Pergaminelis*

Main category: cs.DS

TL;DR: This paper extends the 'beer path' problem to temporal graphs, where edges and vertices have time dependencies. It offers efficient algorithms for finding various temporal beer paths and introduces preprocessing methods to handle dynamic changes, like shop opening/closing times, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for the beer path problem are limited to static graphs and do not account for temporal information, which is crucial in real-world scenarios where services have fixed schedules and accessibility varies over time. This work aims to incorporate temporal aspects into the beer path problem.

Method: The paper formally defines the problems of computing earliest-arrival, latest-departure, fastest, and shortest temporal beer paths in temporal graphs. It proposes efficient algorithms for these problems considering both edge stream and adjacency list representations. Preprocessing techniques, including precomputation of selected paths and transformation into equivalent static graphs, are also presented to handle dynamic conditions.

Result: The paper introduces and defines temporal beer path problems in temporal graphs and proposes efficient algorithms for earliest-arrival, latest-departure, fastest, and shortest paths. The algorithms maintain efficiency comparable to standard temporal pathfinding algorithms. Preprocessing techniques are also presented to enable efficient querying in dynamic environments.

Conclusion: The paper introduces temporal beer paths in temporal graphs, addressing the limitations of existing approaches for static graphs. It proposes efficient algorithms for various temporal beer path problems (earliest-arrival, latest-departure, fastest, and shortest) in both edge stream and adjacency list representations, with time complexities comparable to standard temporal pathfinding algorithms. The work also presents preprocessing techniques for efficient querying under dynamic conditions, achieved through precomputation or graph transformation.

Abstract: Computing paths in graph structures is a fundamental operation in a wide
range of applications, from transportation networks to data analysis. The beer
path problem, which captures the option of visiting points of interest, such as
gas stations or convenience stops, prior to reaching the final destination, has
been recently introduced and extensively studied in static graphs. However,
existing approaches do not account for temporal information, which is often
crucial in real-world scenarios. For instance, transit services may follow
fixed schedules, and shops may only be accessible during certain hours.
  In this work, we introduce the notion of beer paths in temporal graphs, where
edges are time-dependent and certain vertices (beer vertices) are active only
at specific time instances. We formally define the problems of computing
earliest-arrival, latest-departure, fastest, and shortest temporal beer paths
and propose efficient algorithms for these problems under both edge stream and
adjacency list representations. The time complexity of each of our algorithms
is aligned with that of corresponding temporal pathfinding algorithms, thus
preserving efficiency.
  Additionally, we present preprocessing techniques that enable efficient query
answering under dynamic conditions, for example new openings or closings of
shops. We achieve this through appropriate precomputation of selected paths or
by transforming a temporal graph into an equivalent static graph.

</details>


### [223] [On the Constant-Factor Approximability of Minimum Cost Constraint Satisfaction Problems](https://arxiv.org/abs/2507.08693)
*Ian DeHaan,Neng Huang,Euiwoong Lee*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study minimum cost constraint satisfaction problems (MinCostCSP) through
the algebraic lens. We show that for any constraint language $\Gamma$ which has
the dual discriminator operation as a polymorphism, there exists a
$|D|$-approximation algorithm for MinCostCSP$(\Gamma)$ where $D$ is the domain.
Complementing our algorithmic result, we show that any constraint language
$\Gamma$ where MinCostCSP$(\Gamma)$ admits a constant-factor approximation must
have a \emph{near-unanimity} (NU) polymorphism unless P = NP, extending a
similar result by Dalmau et al. on MinCSPs. These results imply a dichotomy of
constant-factor approximability for constraint languages that contain all
permutation relations (a natural generalization for Boolean CSPs that allow
variable negation): either MinCostCSP$(\Gamma)$ has an NU polymorphism and is
$|D|$-approximable, or it does not have any NU polymorphism and is NP-hard to
approximate within any constant factor. Finally, we present a constraint
language which has a majority polymorphism, but is nonetheless NP-hard to
approximate within any constant factor assuming the Unique Games Conjecture,
showing that the condition of having an NU polymorphism is in general not
sufficient unless UGC fails.

</details>


### [224] [To buy or not to buy: deterministic rent-or-buy problems on node-weighted graphs](https://arxiv.org/abs/2507.08698)
*Sander Borst,Moritz Venzin*

Main category: cs.DS

TL;DR: 本文针对带节点和边权重的在线斯坦纳森林问题，提出了更优的确定性及随机化算法，特别是租赁-购买变体，并引入了新颖的收费方案和技术以获得改进的竞争比。


<details>
  <summary>Details</summary>
Motivation: 研究在线斯坦纳森林问题的租赁或购买变体，旨在为节点和边加权的图提供更优的竞争比算法。

Method: 本文提出了一种新颖的收费方案，将其应用于在线征收集覆盖问题，从而将Umboh (2015)的见证技术扩展到节点加权场景，并在仅购买场景下获得了关于 $\bar{n}$ 的改进保证。

Result: 1. 确定性算法，竞争比为 $O(\log n \log \bar{n})$，优于先前算法。
2. 确定性算法，竞争比为 $O(\bar{n}\log \tilde{k})$，推广了现有成果。
3. 随机化算法，竞争比为 $O(\log \tilde{k} \log \bar{n})$。

Conclusion: 本文提出了一种用于节点和边加权的在线斯坦纳森林问题的租赁或购买变体的确定性算法，实现了 $O(\log n \log \bar{n})$-竞争比，优于先前最先进的 $O(\log^4 n)$-竞争算法。此外，还提出了一种确定性算法，实现了 $O(\bar{n}\log \tilde{k})$-竞争比，并推广了纯边加权情况下的算法。同时，也提出了一种随机化算法，实现了 $O(\log \tilde{k} \log \bar{n})$-竞争比。

Abstract: We study the rent-or-buy variant of the online Steiner forest problem on
node- and edge-weighted graphs. For $n$-node graphs with at most $\bar{n}$
non-zero node-weights, and at most $\tilde{k}$ different arriving terminal
pairs, we obtain a deterministic, $O(\log n \log \bar{n})$-competitive
algorithm. This improves on the previous best, $O(\log^4 n)$-competitive
algorithm obtained by the black-box reduction from (Bartal et al. 2021)
combined with the previously best deterministic algorithms for the simpler
'buy-only' setting. We also obtain a deterministic, $O(\bar{n}\log
\tilde{k})$-competitive algorithm. This generalizes the $O(\log
\tilde{k})$-competitive algorithm for the purely edge-weighted setting from
(Umboh 2015). We also obtain a randomized, $O(\log \tilde{k} \log
\bar{n})$-competitive algorithm. All previous approaches were based on the
randomized, black-box reduction from~\cite{AwerbuchAzarBartal96} that achieves
a $O(\log \tilde{k} \log n)$-competitive ratio when combined with an algorithm
for the 'buy-only' setting. Our key technical ingredient is a novel charging
scheme to an instance of \emph{online prize-collecting set cover}. This allows
us to extend the witness-technique of (Umboh 2015) to the node-weighted setting
and obtain refined guarantees with respect to $\bar{n}$, already in the much
simpler 'buy-only' setting.

</details>


### [225] [On Fair Epsilon Net and Geometric Hitting Set](https://arxiv.org/abs/2507.08758)
*Mohsen Dehghankar,Stavros Sintos,Abolfazl Asudeh*

Main category: cs.DS

TL;DR: 本研究将公平性引入几何近似问题（如 ε-net），提出了两种算法（采样和差异理论），并在实验中验证了其有效性，实现了零不公平性且输出大小增加有限。


<details>
  <summary>Details</summary>
Motivation: 公平性已成为数据驱动决策中的一个严峻挑战。为了填补将公平性引入经典几何近似问题的研究空白，本研究将公平性概念应用于 ε-net、ε-sample 和几何 hitting set 等问题。

Method: 本研究提出了两种强制执行公平性的算法：一种基于采样，另一种基于差异理论。其中，基于采样算法计算出的公平 ε-net 的大小仅比标准的（不公平的）ε-net 大一个对数因子（log k），其中 k 是人口统计群体的数量。基于差异理论的算法稍慢，但计算出的公平 ε-net 更小。

Result: 研究结果表明，基于采样算法计算出的公平 ε-net 的大小仅比标准的（不公平的）ε-net 大一个对数因子（log k）。基于差异理论的算法稍慢，但计算出的公平 ε-net 更小。此外，研究还将公平的几何 hitting set 问题转化为寻找公平 ε-net 的问题，从而实现了公平几何 hitting set 的 O(log OPT × log k) 近似。研究还发现，在某些输入分布下，构建公平 ε-sample 可能不可行，这凸显了公平采样存在的局限性。实验结果验证了所提出算法的实际有效性，实现了零不公平性，且输出大小仅略有增加。

Conclusion: 本研究将公平性纳入了经典的几何近似问题，如 ε-net、ε-sample 和几何 hitting set，并提出了两种算法（基于采样和基于差异理论）来强制执行公平性。研究结果表明，这些算法在实践中是有效的，并且可以实现零不公平性，同时输出大小仅略有增加。

Abstract: Fairness has emerged as a formidable challenge in data-driven decisions. Many
of the data problems, such as creating compact data summaries for approximate
query processing, can be effectively tackled using concepts from computational
geometry, such as $\varepsilon$-nets. However, these powerful tools have yet to
be examined from the perspective of fairness. To fill this research gap, we add
fairness to classical geometric approximation problems of $\varepsilon$-net,
$\varepsilon$-sample, and geometric hitting set. We introduce and address two
notions of group fairness: demographic parity, which requires preserving group
proportions from the input distribution, and custom-ratios fairness, which
demands satisfying arbitrary target ratios. We develop two algorithms to
enforce fairness: one based on sampling and another on discrepancy theory. The
sampling-based algorithm is faster and computes a fair $\varepsilon$-net of
size which is only larger by a $\log(k)$ factor compared to the standard
(unfair) $\varepsilon$-net, where $k$ is the number of demographic groups. The
discrepancy-based algorithm is slightly slower (for bounded VC dimension), but
it computes a smaller fair $\varepsilon$-net. Notably, we reduce the fair
geometric hitting set problem to finding fair $\varepsilon$-nets. This results
in a $O(\log \mathsf{OPT} \times \log k)$ approximation of a fair geometric
hitting set. Additionally, we show that under certain input distributions,
constructing fair $\varepsilon$-samples can be infeasible, highlighting
limitations in fair sampling. Beyond the theoretical guarantees, our
experimental results validate the practical effectiveness of the proposed
algorithms. In particular, we achieve zero unfairness with only a modest
increase in output size compared to the unfair setting.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [226] [Heterogeneous Dynamic Logic: Provability Modulo Program Theories](https://arxiv.org/abs/2507.08581)
*Samuel Teuber,Mattias Ulbrich,André Platzer,Bernhard Beckert*

Main category: cs.LO

TL;DR: HDL 是一个框架，可以组合不同的动态逻辑来处理多语言系统。它允许通过 Lifting 添加新构造，通过 Combination 实现跨语言推理。在 Isabelle 中已形式化并证明可靠和相对完整。它已通过汽车控制系统的示例得到验证。


<details>
  <summary>Details</summary>
Motivation: 在涉及多种编程语言的系统的形式化规范和验证方面存在固有的挑战。

Method: 本文提出了一种名为异构动态逻辑 (HDL) 的框架，该框架通过模块化和组合的方式结合了不同（动态）程序逻辑的推理原则。HDL 模仿了可满足性模理论 (SMT) 的架构，将各个动态逻辑及其演算视为动态理论，可以灵活地组合起来对异构系统进行推理。HDL 提供了两种关键操作：Lifting 将单个动态理论与新的程序构造（例如，赋值或常规程序）相结合，并自动为其演算添加新的构造的可靠推理原则；Combination 通过异构动态理论在单一模式下实现跨语言推理，从而促进现有证明基础结构的可重用性。在 Isabelle 中对动态理论及其 Lifting 和 Combination 操作进行了形式化，并证明了所有证明规则的可靠性。此外，还证明了 Lifting 和 Combination 的相对完备性：在常见的假设下，关于 Lifting 或组合理论的推理不比关于组成动态理论及其常见一阶结构（即“数据理论”）的推理更难。

Result: HDL 在 Isabelle 中被形式化并证明是可靠的，并且其 Lifting 和 Combination 操作具有相对完备性。通过一个汽车案例研究证明了 HDL 的实用性，其中一个 Java 控制器被用来控制一个使用微分动态逻辑的工厂模型。

Conclusion: HDL 可用于组合使用不同程序逻辑验证的异构系统的推理，如使用 Java 和微分动态逻辑的汽车控制系统示例所示。

Abstract: Formally specifying, let alone verifying, properties of systems involving
multiple programming languages is inherently challenging. We introduce
Heterogeneous Dynamic Logic (HDL), a framework for combining reasoning
principles from distinct (dynamic) program logics in a modular and
compositional way. HDL mirrors the architecture of satisfiability modulo
theories (SMT): Individual dynamic logics, along with their calculi, are
treated as dynamic theories that can be flexibly combined to reason about
heterogeneous systems whose components are verified using different program
logics. HDL provides two key operations: Lifting extends an individual dynamic
theory with new program constructs (e.g., the havoc operation or regular
programs) and automatically augments its calculus with sound reasoning
principles for the new constructs; and Combination enables cross-language
reasoning in a single modality via Heterogeneous Dynamic Theories, facilitating
the reuse of existing proof infrastructure. We formalize dynamic theories,
their lifting and combination in Isabelle, and prove the soundness of all proof
rules. We also prove relative completeness theorems for lifting and
combination: Under common assumptions, reasoning about lifted or combined
theories is no harder than reasoning about the constituent dynamic theories and
their common first-order structure (i.e., the "data theory"). We demonstrate
HDL's utility by verifying an automotive case study in which a Java controller
(formalized in Java dynamic logic) steers a plant model (formalized in
differential dynamic logic).

</details>


### [227] [A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes](https://arxiv.org/abs/2507.08701)
*Ricardo Contreras,Filip Smola,Nuša Farič,Jiawei Zheng,Jane Hillston,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 一项用于独立居家老年人的框架，通过整合传感器数据和访谈信息，利用线性时序逻辑和模型检查来个性化地提升他们的安全性和福祉。


<details>
  <summary>Details</summary>
Motivation: 为了满足日益增长的独立居住老年人口对生活质量的需求，需要提供个性化的解决方案，关注个体及其偏好和环境。

Method: 该框架整合了来自传感器的数据和上下文信息（包括半结构化访谈、家庭布局和社会学观察），以创建个性化的正式模型。使用线性时序逻辑对每个人的特定需求进行编码，并使用模型检查器验证模型是否满足这些需求。当需求未被满足时，会生成反例来指出违规原因。

Result: 成功创建了用于表示和推理独立居家老年人日常生活活动的框架，并验证了其在不同参与者身上的通用性。

Conclusion: 该框架具有普遍适用性，可应用于不同参与者，有潜力提高老年人居家养老的安全性和福祉。

Abstract: There is an imperative need to provide quality of life to a growing
population of older adults living independently. Personalised solutions that
focus on the person and take into consideration their preferences and context
are key. In this work, we introduce a framework for representing and reasoning
about the Activities of Daily Living of older adults living independently at
home. The framework integrates data from sensors and contextual information
that aggregates semi-structured interviews, home layouts and sociological
observations from the participants. We use these data to create formal models,
personalised for each participant according to their preferences and context.
We formulate requirements that are specific to each individual as properties
encoded in Linear Temporal Logic and use a model checker to verify whether each
property is satisfied by the model. When a property is violated, a
counterexample is generated giving the cause of the violation. We demonstrate
the framework's generalisability by applying it to different participants,
highlighting its potential to enhance the safety and well-being of older adults
ageing in place.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [228] [FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields](https://arxiv.org/abs/2507.08285)
*Gwanhyeong Koo,Sunjae Yoon,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: FlowDrag通过结合3D网格变形和UNet去噪，实现了精确且保持结构完整的拖拽编辑，并提出了新的VFD数据集来评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有的拖拽编辑方法往往只关注匹配用户定义的点，忽略了更广泛的几何形状，从而导致了人为痕迹或不稳定的编辑，存在几何不一致的问题。为了实现更精确、更连贯的变换，需要一种能够利用几何信息的方法。此外，现有拖拽编辑基准缺乏地面真实值，难以评估编辑的准确性。

Method: FlowDrag首先从图像构建3D网格，然后使用一个能量函数，基于用户定义的拖拽点来指导网格变形。接着，将产生的网格位移投影到2D，并整合到一个UNet去噪过程中，以实现精确的手柄到目标点的对齐，并保持结构完整性。为解决现有拖拽编辑基准缺乏地面真实值的问题，提出了VFD（VidFrameDrag）数据集，该数据集使用视频数据集中的连续帧作为地面真实帧。

Result: FlowDrag在VFD Bench和DragBench上的表现均优于现有拖拽编辑方法，证明了其在精确性和结构保持性方面的优势。VFD（VidFrameDrag）数据集的提出为拖拽编辑的评估提供了地面真实值。

Conclusion: FlowDrag通过利用几何信息解决了现有拖拽编辑中存在的几何不一致问题，实现了更精确、更连贯的变换。它通过构建3D网格并利用能量函数引导变形，将网格位移投影到2D并结合到UNet去噪过程中，从而在精确对齐拖拽点的同时保持了结构完整性。此外，为解决现有基准缺乏地面真实值的问题，提出了VFD（VidFrameDrag）数据集，提供了基于视频连续帧的地面真实帧。FlowDrag在VFD Bench和DragBench上的表现优于现有拖拽编辑方法。

Abstract: Drag-based editing allows precise object manipulation through point-based
control, offering user convenience. However, current methods often suffer from
a geometric inconsistency problem by focusing exclusively on matching
user-defined points, neglecting the broader geometry and leading to artifacts
or unstable edits. We propose FlowDrag, which leverages geometric information
for more accurate and coherent transformations. Our approach constructs a 3D
mesh from the image, using an energy function to guide mesh deformation based
on user-defined drag points. The resulting mesh displacements are projected
into 2D and incorporated into a UNet denoising process, enabling precise
handle-to-target point alignment while preserving structural integrity.
Additionally, existing drag-editing benchmarks provide no ground truth, making
it difficult to assess how accurately the edits match the intended
transformations. To address this, we present VFD (VidFrameDrag) benchmark
dataset, which provides ground-truth frames using consecutive shots in a video
dataset. FlowDrag outperforms existing drag-based editing methods on both VFD
Bench and DragBench.

</details>


### [229] [Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation](https://arxiv.org/abs/2507.08513)
*Liu He,Xiao Zeng,Yizhi Song,Albert Y. C. Chen,Lu Xia,Shashwat Verma,Sankalp Dayal,Min Sun,Cheng-Hao Kuo,Daniel Aliaga*

Main category: cs.GR

TL;DR: 为解决MLLMs在相机-对象关系识别方面的不足，我们提出了一个合成数据生成流程，创建了一个大规模的3D视觉指令数据集Ultimate3D，并在该数据集上微调的模型在相关任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs在准确捕捉相机-对象关系（特别是对象方向、相机视角和相机镜头）方面存在困难，这是由于它们在具有有限多样性相机-对象关系及其文本描述的图像上进行训练。

Method: 提出一个合成生成流程，利用3D资源、渲染和基于扩散的模型来创建具有精确相机-对象关系的超现实图像。LLM用于生成文本提示，以指导视觉指令调整和控制图像生成。创建了一个包含240K VQA和相应相机-对象注释的数据集Ultimate3D。

Result: 在Ultimate3D数据集上微调的MLLMs在相机-对象关系识别任务上取得了显著的性能提升，平均准确率提高了33.4%。

Conclusion: 通过在包含精确相机-对象关系的合成数据集上进行微调，MLLMs在相机-对象关系识别任务上的表现优于商业模型，准确率平均提高了33.4%。

Abstract: Multimodal Large Language Models (MLLMs) struggle with accurately capturing
camera-object relations, especially for object orientation, camera viewpoint,
and camera shots. This stems from the fact that existing MLLMs are trained on
images with limited diverse camera-object relations and corresponding textual
descriptions. To address this, we propose a synthetic generation pipeline to
create large-scale 3D visual instruction datasets. Our framework takes 3D
assets as input and uses rendering and diffusion-based image generation models
to create photorealistic images preserving precise camera-object relations.
Additionally, large language models (LLMs) are used to generate text prompts
for guiding visual instruction tuning and controlling image generation. We
create Ultimate3D, a dataset of 240K VQAs with precise camera-object
annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed
dataset outperform commercial models by a large margin, achieving an average
accuracy improvement of 33.4% on camera-object relation recognition tasks. Our
code, dataset, and benchmark will contribute to broad MLLM applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [230] [Supporting Intel(r) SGX on Multi-Package Platforms](https://arxiv.org/abs/2507.08190)
*Simon Johnson,Raghunandan Makaram,Amy Santoni,Vinnie Scarlata*

Main category: cs.DC

TL;DR: SGX技术需要在多路服务器云环境中进行平台增强，以实现可扩展、高性能和安全的机密计算。


<details>
  <summary>Details</summary>
Motivation: SGX技术在云环境中的应用潜力巨大，但现有技术在多路服务器云环境中存在可扩展性、性能和安全性方面的挑战。

Method: 描述了实现云端SGX所需的平台增强，但未具体说明。

Result: 提出了对SGX平台增强的需求，以支持云环境下的机密计算，但未提供具体的实现结果或性能评估。

Conclusion: SGX技术需要进一步的平台增强才能在多路服务器云环境中实现可扩展、高性能和安全的机密计算。

Abstract: Intel(r) Software Guard Extensions (SGX) was originally released on client
platforms and later extended to single socket server platforms. As developers
have become familiar with the capabilities of the technology, the applicability
of this capability in the cloud has been tested. Various Cloud Service
Providers (CSPs) are demonstrating the value of using SGX based Trusted
Execution Environments (TEE) to create a new paradigm of Confidential Cloud
Computing. This paper describes the additional platform enhancements we believe
are necessary to deliver a user programmable Trusted Execution Environment that
scales to cloud usages, performs and is secure on multi-package platforms.

</details>


### [231] [Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling](https://arxiv.org/abs/2507.08281)
*Ahmad Zaki Akmal,Azkario Rizky Pratama,Guntur Dharma Putra*

Main category: cs.DC

TL;DR: 通过将交互式操作与共识最终确定性分离，可以实现具有BFT安全性的快速响应。


<details>
  <summary>Details</summary>
Motivation: 解决BFT Web服务在安全性和响应能力之间存在的根本性矛盾，因为现有的BFT Web服务存在显著的延迟挑战，影响了交互式用户体验。

Method: 提出了一种新颖的两层架构，包括一个会话感知的事务缓冲区层（第二层），通过共识模拟为用户提供即时反馈，同时将批量操作定期提交到完全拜占庭容错的共识层（第一层）。

Result: 与第一层相比，第二层操作的性能提高了四倍，同时基本保持了端到端的事务完整性。

Conclusion: 该方法通过分离交互操作和共识最终确定性，实现了低于200毫秒的响应速度，同时保持了强大的BFT安全保证。它能够将BFT应用程序扩展到以前因延迟限制而无法实现的领域，例如元宇宙环境。

Abstract: Byzantine fault-tolerant (BFT) web services provide critical integrity
guarantees for distributed applications but face significant latency challenges
that hinder interactive user experiences. We propose a novel two-layer
architecture that addresses this fundamental tension between security and
responsiveness in BFT systems. Our approach introduces a session-aware
transaction buffer layer (Layer 2) that delivers immediate feedback to users
through consensus simulation, while periodically committing batched operations
to a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating
interactive operations from consensus finalization, our system achieves
responsive user experiences of under 200ms, while maintaining strong BFT
security guarantees. We demonstrate the efficacy of our architecture through a
supply chain management implementation, where operators require both immediate
feedback during multi-step workflows and tamper-proof record keeping. Our
evaluation shows that our Layer 2 operations perform four times faster than the
Layer 1 counterpart, while substantially preserving the end-to-end transaction
integrity. Our approach enables BFT applications in domains previously
considered impractical due to latency constraints, such as metaverse
environments, where users require both responsive interaction and guaranteed
state consistency.

</details>


### [232] [Content-Oblivious Leader Election in 2-Edge-Connected Networks](https://arxiv.org/abs/2507.08348)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 本工作提出了一个异步无感知领导者选举算法，该算法在任何 2 边连通网络中都能静默终止，并能完全反驳需要预先指定领导者的猜想。


<details>
  <summary>Details</summary>
Motivation: 反驳了 Censor-Hillel 等人提出的关于在完全有缺陷的网络中进行非平凡计算需要预先指定领导者的猜想。

Method: 提出了一个异步无感知领导者选举算法，该算法在任何 2 边连通网络中都能进行静默终止，其消息复杂度为 O(m * N * ID_min)。

Result: 设计了一个异步无感知领导者选举算法，该算法在任何 2 边连通网络中都能静默终止，并结合之前的模拟结果，完全反驳了需要预先指定领导者的猜想。

Conclusion: 本工作提出了一个异步无感知领导者选举算法，该算法在任何 2 边连通网络中都能进行静默终止，其消息复杂度为 O(m * N * ID_min)，其中 m 是边的数量，N 是节点数量的已知上限，ID_min 是最小的 ID。结合之前提出的模拟结果，我们的发现意味着在完全有缺陷的环境中，可以模拟任何无噪声环境下的算法，而无需预先假定领导者，从而完全反驳了最初的猜想。

Abstract: Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \& Distributed Computing
2023) studied fully-defective asynchronous networks, where communication
channels may suffer an extreme form of alteration errors, rendering messages
completely corrupted. The model is equivalent to content-oblivious computation,
where nodes communicate solely via pulses. They showed that if the network is
2-edge-connected, then any algorithm for a noiseless setting can be simulated
in the fully-defective setting; otherwise, no non-trivial computation is
possible in the fully-defective setting. However, their simulation requires a
predesignated leader, which they conjectured to be necessary for any
non-trivial content-oblivious task.
  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture
for the special case of oriented ring topology. They designed two asynchronous
content-oblivious leader election algorithms with message complexity $O(n \cdot
\mathsf{ID}_{\max})$, where $n$ is the number of nodes and $\mathsf{ID}_{\max}$
is the maximum $\mathsf{ID}$. The first algorithm stabilizes in unoriented
rings without termination detection. The second algorithm quiescently
terminates in oriented rings, thus enabling the execution of the simulation
algorithm after leader election.
  In this work, we present an asynchronous content-oblivious leader election
algorithm that quiescently terminates in any 2-edge connected network with
message complexity $O(m \cdot N \cdot \mathsf{ID}_{\min})$, where $m$ is the
number of edges, $N$ is a known upper bound on the number of nodes, and
$\mathsf{ID}_{\min}$ is the smallest $\mathsf{ID}$. Combined with the previous
simulation result, our finding implies that any algorithm from the noiseless
setting can be simulated in the fully-defective setting without assuming a
preselected leader, entirely refuting the original conjecture.

</details>


### [233] [Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint](https://arxiv.org/abs/2507.08725)
*Dominik Schweisgut,Anne Benoit,Yves Robert,Henning Meyerhenke*

Main category: cs.DC

TL;DR: 通过名为CaWoSched的启发式框架，在混合能源数据中心优化DAG工作流调度，显著减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型数据和计算中心消耗大量能源，而工作流是其中的重要组成部分，并且通常在混合能源供应环境下运行，因此有必要在绿色能源充足的时段执行任务，以减少碳排放。

Method: 提出了一种名为CaWoSched的启发式框架，该框架结合了多种贪心方法和局部搜索技术来解决DAG工作流的调度问题。针对单处理器情况，证明了问题可以在多项式时间内解决；而对于至少两个处理器的情况，问题被证明是NP难的。为了评估16种不同的启发式组合，还设计了一个简单的基线算法和一个精确的整数线性规划（ILP）方法。

Result: 实验结果表明，与基线算法相比，所提出的启发式方法能够显著节省碳排放。

Conclusion: 该工作提出了一种名为CaWoSched的启发式框架，结合了多种贪心方法和局部搜索技术，用于优化具有依赖性任务的DAG工作流在具有混合能源供应的数据中心的调度，以减少碳排放。实验结果表明，与基线算法相比，该框架能显著减少碳排放。

Abstract: Large data and computing centers consume a significant share of the world's
energy consumption. A prominent subset of the workloads in such centers are
workflows with interdependent tasks, usually represented as directed acyclic
graphs (DAGs). To reduce the carbon emissions resulting from executing such
workflows in centers with a mixed (renewable and non-renewable) energy supply,
it is advisable to move task executions to time intervals with sufficient green
energy when possible. To this end, we formalize the above problem as a
scheduling problem with a given mapping and ordering of the tasks. We show that
this problem can be solved in polynomial time in the uniprocessor case. For at
least two processors, however, the problem becomes NP-hard. Hence, we propose a
heuristic framework called CaWoSched that combines several greedy approaches
with local search. To assess the 16 heuristics resulting from different
combinations, we also devise a simple baseline algorithm and an exact ILP-based
solution. Our experimental results show that our heuristics provide significant
savings in carbon emissions compared to the baseline.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [234] [Analysis of Propaganda in Tweets From Politically Biased Sources](https://arxiv.org/abs/2507.08169)
*Vivek Sharma,Mohammad Mahdi Shokri,Sarah Ita Levitan,Elena Filatova,Shweta Jain*

Main category: cs.SI

TL;DR: 记者，特别是那些为政治倾向极端的报纸工作的记者，更有可能使用宣传性语言。大型语言模型在检测宣传方面效果更好，但成本更高。


<details>
  <summary>Details</summary>
Motivation: 由于新闻媒体的政治关联和记者对其报道内容的影响，本研究旨在分析记者在传播政治偏见时使用类似宣传的语言的作用，并评估大型语言模型在检测此类语言方面的有效性及其成本。

Method: 本研究提出了一种分析记者通过类似宣传的语言传播其偏见作用的方法，并引入了一个名为JMBX（记者在X上的媒体偏见）的数据集，其中包含来自10家新闻机构的1874条推文。研究人员还评估了八种不同的LLM和一种基于BERT的模型在检测宣传方面的表现，并分析了LLM的财务和环境成本。

Result: 研究结果表明，与政治倾向极端的报纸相关联的记者比与政治倾向温和的报纸相关联的记者更有可能使用类似宣传的语言。在检测宣传方面，LLM的表现优于BERT模型，但其成本较高。

Conclusion: 这项研究表明，与政治倾向温和的报纸相关联的记者，相比那些与政治倾向极端的报纸相关联的记者，更有可能在他们的文章中使用类似宣传的语言。此外，研究还发现，大型语言模型在检测社交媒体和新闻文章中的宣传方面通常比基于BERT的模型表现更好，但这种改进伴随着显著的成本。

Abstract: News outlets are well known to have political associations, and many national
outlets cultivate political biases to cater to different audiences. Journalists
working for these news outlets have a big impact on the stories they cover. In
this work, we present a methodology to analyze the role of journalists,
affiliated with popular news outlets, in propagating their bias using some form
of propaganda-like language. We introduce JMBX(Journalist Media Bias on X), a
systematically collected and annotated dataset of 1874 tweets from Twitter (now
known as X). These tweets are authored by popular journalists from 10 news
outlets whose political biases range from extreme left to extreme right. We
extract several insights from the data and conclude that journalists who are
affiliated with outlets with extreme biases are more likely to use
propaganda-like language in their writings compared to those who are affiliated
with outlets with mild political leans. We compare eight different Large
Language Models (LLM) by OpenAI and Google. We find that LLMs generally
performs better when detecting propaganda in social media and news article
compared to BERT-based model which is fine-tuned for propaganda detection.
While the performance improvements of using large language models (LLMs) are
significant, they come at a notable monetary and environmental cost. This study
provides an analysis of both the financial costs, based on token usage, and the
environmental impact, utilizing tools that estimate carbon emissions associated
with LLM operations.

</details>


### [235] [Addressing overlapping communities in multiple-source detection: An edge clustering approach for complex networks](https://arxiv.org/abs/2507.08265)
*Haomin Li,Daniel K. Sewell*

Main category: cs.SI

TL;DR: 本研究提出一种新的多源检测方法，结合边聚类和标签传播，提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 网络分析中的源头检测问题，特别是涉及多个源头的复杂情况，是该研究关注的重点。

Method: 该方法将自动化的潜在空间边聚类模型应用于网络，将感染网络划分为基于边的簇，以识别多个源头。

Result: 模拟研究表明，该方法在F1分数上优于最先进的聚类算法，尤其在具有复杂和重叠源区域的网络中表现出鲁棒性。

Conclusion: 该研究通过将边聚类算法与基于社区的标签传播框架相结合，解决了网络分析中的多源检测问题，提高了检测的准确性和适应性。

Abstract: The source detection problem in network analysis involves identifying the
origins of diffusion processes, such as disease outbreaks or misinformation
propagation. Traditional methods often focus on single sources, whereas
real-world scenarios frequently involve multiple sources, complicating
detection efforts. This study addresses the multiple-source detection (MSD)
problem by integrating edge clustering algorithms into the community-based
label propagation framework, effectively handling mixed-membership issues where
nodes belong to multiple communities.
  The proposed approach applies the automated latent space edge clustering
model to a network, partitioning infected networks into edge-based clusters to
identify multiple sources. Simulation studies on ADD HEALTH social network
datasets demonstrate that this method achieves superior accuracy, as measured
by the F1-Measure, compared to state-of-the-art clustering algorithms. The
results highlight the robustness of edge clustering in accurately detecting
sources, particularly in networks with complex and overlapping source regions.
This work advances the applicability of clustering-based methods to MSD
problems, offering improved accuracy and adaptability for real-world network
analyses.

</details>


### [236] [Uncovering High-Order Cohesive Structures: Efficient (k,g)-Core Computation and Decomposition for Large Hypergraphs](https://arxiv.org/abs/2507.08328)
*Dahee Kim,Hyewon Kim,Song Kim,Minseok Kim,Junghoon Kim,Yeon-Chang Lee,Sungsu Lim*

Main category: cs.SI

TL;DR: A new indexing structure allows for efficient online retrieval of cohesive subgraphs in hypergraphs, outperforming existing methods in real-world network analysis.


<details>
  <summary>Details</summary>
Motivation: The need to address the open question of selecting appropriate parameters for cohesive subgraph discovery in hypergraphs, which are increasingly used to model complex relationships and higher-order interactions.

Method: Design an efficient indexing structure to retrieve cohesive subgraphs in an online manner, avoiding exhaustive graph traversals.

Result: The method enables faster and more effective retrieval of cohesive structures, supporting decision-making in applications requiring online analysis of large-scale hypergraphs.

Conclusion: The proposed indexing technique demonstrates superiority through extensive experiments on real-world networks, enabling faster and more effective retrieval of cohesive structures for online analysis of large-scale hypergraphs.

Abstract: Hypergraphs, increasingly utilised to model complex and diverse relationships
in modern networks, have gained significant attention for representing
intricate higher-order interactions. Among various challenges, cohesive
subgraph discovery is one of the fundamental problems and offers deep insights
into these structures, yet the task of selecting appropriate parameters is an
open question. To address this question, we aim to design an efficient indexing
structure to retrieve cohesive subgraphs in an online manner. The main idea is
to enable the discovery of corresponding structures within a reasonable time
without the need for exhaustive graph traversals. Our method enables faster and
more effective retrieval of cohesive structures, which supports decision-making
in applications that require online analysis of large-scale hypergraphs.
Through extensive experiments on real-world networks, we demonstrate the
superiority of our proposed indexing technique.

</details>


### [237] [Machine Learning for Evolutionary Graph Theory](https://arxiv.org/abs/2507.08363)
*Guoli Yang,Matteo Cavaliere,Mingtao Zhang,Giovanni Masala,Adam Miles,Mengzhu Wang*

Main category: cs.SI

TL;DR: 通过结合进化图论和机器学习，可以提前预测网络中合作的崩溃。


<details>
  <summary>Details</summary>
Motivation: 旨在解决如何提前检测到合作和社区崩溃风险的关键挑战。

Method: 结合进化图论和机器学习，利用时间和结构数据来预测网络中欺骗者的传播和合作的崩溃。

Result: 预测准确性随选择强度和观察窗口的增加而提高，其中CNN-Seq-LSTM和Seq-LSTM表现最佳。准确性还取决于博弈类型和社区结构。

Conclusion: 该研究将机器学习方法应用于进化图论，以检测和预测复杂网络中合作的突然转变，并为预测和预防合作崩溃提供潜在策略。

Abstract: The stability of communities - whether biological, social, economic,
technological or ecological depends on the balance between cooperation and
cheating. While cooperation strengthens communities, selfish individuals, or
"cheaters," exploit collective benefits without contributing. If cheaters
become too prevalent, they can trigger the collapse of cooperation and of the
community, often in an abrupt manner. A key challenge is determining whether
the risk of such a collapse can be detected in advance. To address this, we use
a combination of evolutionary graph theory and machine learning to examine how
one can predict the unravel of cooperation on complex networks. By introducing
few cheaters into a structured population, we employ machine learning to detect
and anticipate the spreading of cheaters and cooperation collapse. Using
temporal and structural data, the presented results show that prediction
accuracy improves with stronger selection strength and larger observation
windows, with CNN-Seq-LSTM and Seq-LSTM best performing models. Moreover, the
accuracy for the predictions depends crucially on the type of game played
between cooperators and cheaters (i.e., accuracy improves when it is more
advantageous to defect) and on the community structure. Overall, this work
introduces a machine learning approach into detecting abrupt shifts in
evolutionary graph theory and offer potential strategies for anticipating and
preventing cooperation collapse in complex social networks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [238] [Deep Reinforcement Learning in Applied Control: Challenges, Analysis, and Insights](https://arxiv.org/abs/2507.08196)
*Klinsmann Agyei,Pouria Sarhadi,Daniel Polani*

Main category: eess.SY

TL;DR: 深度强化学习在游戏和模拟环境中取得了巨大成功，但将其应用于现实世界仍面临挑战。本研究通过在四个基准问题上的实证分析，评估了这些方法在实际控制应用中的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络在强化学习中的广泛应用和显著进展，特别是深度Q网络在雅达利游戏中的成功，深度强化学习展现了在控制领域的前景。然而，这些方法在实际应用中的部署仍面临挑战，因此需要对其在实际控制问题上的性能进行量化理解和评估。

Method: 本研究通过对深度强化学习的现有方法进行比较分析，并在四个不同的基准问题上进行实施。

Result: 论文通过在四个不同的基准问题上进行实施和比较分析，系统地评估了深度强化学习方法在实际控制应用中的性能、能力和局限性。

Conclusion: 本篇论文旨在通过在四个不同的基准问题上进行实施和比较分析，来阐明深度强化学习方法在实际控制应用中的能力和局限性。

Abstract: Over the past decade, remarkable progress has been made in adopting deep
neural networks to enhance the performance of conventional reinforcement
learning. A notable milestone was the development of Deep Q-Networks (DQN),
which achieved human-level performance across a range of Atari games,
demonstrating the potential of deep learning to stabilise and scale
reinforcement learning. Subsequently, extensions to continuous control
algorithms paved the way for a new paradigm in control, one that has attracted
broader attention than any classical control approach in recent literature.
These developments also demonstrated strong potential for advancing
data-driven, model-free algorithms for control and for achieving higher levels
of autonomy. However, the application of these methods has remained largely
confined to simulated and gaming environments, with ongoing efforts to extend
them to real-world applications. Before such deployment can be realised, a
solid and quantitative understanding of their performance on applied control
problems is necessary. This paper conducts a comparative analysis of these
approaches on four diverse benchmark problems with implementation results. This
analysis offers a scrutinising and systematic evaluation to shed light on the
real-world capabilities and limitations of deep reinforcement learning methods
in applied control settings.

</details>


### [239] [Maneuver Detection via a Confidence Dominance Maneuver Indicator](https://arxiv.org/abs/2507.08234)
*Xingyu Zhou,Roberto Armellin,Laura Pirovano,Dong Qiao,Xiangyu Li*

Main category: eess.SY

TL;DR: 该研究提出了一种新的基于置信度量级和观测似然度比较的航天器机动检测方法（CDMI），并开发了一种集成的CDMI方法，无需手动选择状态置信度量级。仿真结果表明，该方法检测精度高，计算成本低。


<details>
  <summary>Details</summary>
Motivation: 精确高效的机动检测对于确保航天器轨迹的安全性和可预测性至关重要。

Method: 提出了一种基于置信度量级和观测似然度比较的新型机动检测方法。首先，通过为状态估计设置置信度量级并计算观测值的最大似然度及其置信度量级，提出了置信度量级支配机动指标（CDMI）。当观测值的置信度量级超过状态估计的置信度量级时，CDMI将标记机动，这表明在无机动假设下观测值不太可能，同时与先验状态估计置信度保持一致。为了有效地计算观测值的最大似然度并获得CDMI，开发了一种递归多项式优化方法，利用了凸优化和多项式逼近的优点。此外，还开发了一种集成的CDMI方法，无需手动选择状态置信度量级。

Result: 所提出的集成CDMI方法可实现高达99.33%的检测精度，比现有方法至少高出10%，同时显著降低了计算成本。

Conclusion: 仿真结果表明，所提出的集成CDMI方法可实现高达99.33%的检测精度，比现有方法至少高出10%，同时显著降低了计算成本。

Abstract: Accurate and efficient maneuver detection is critical for ensuring the safety
and predictability of spacecraft trajectories. This paper presents a novel
maneuver detection approach based on comparing the confidence levels associated
with the orbital state estimation and the observation likelihood. First, a
confidence-dominance maneuver indicator (CDMI) is proposed by setting a
confidence level for the state estimation and computing the maximum likelihood
of the observation and its confidence level. The CDMI then flag a maneuver when
the observation's confidence level exceeds that of the state estimation,
indicating that the observation is unlikely under the no-maneuver hypothesis
while maintaining consistency with the prior state estimation confidence. To
efficiently compute the maximum likelihood of the observation and obtain the
CDMI, a recursive polynomial optimization method is developed, taking advantage
of convex optimization and polynomial approximation. In addition, an integrated
CDMI approach is developed to eliminate the need to manually select the state
confidence level. The integrated CDMI approach maintains high detection
accuracy while simultaneously providing an indication of maneuver likelihood,
thereby enhancing robustness and practical applicability. The performance of
the proposed CDMI-based maneuver detection approaches is evaluated against an
optimal control distance metric and two mixture-based approaches. The
simulation results demonstrate that the proposed integrated CDMI approach can
achieve up to 99.33\% detection accuracy, at least 10% higher than the
competing methods, while substantially reducing computational costs.

</details>


### [240] [Neural Parameter-varying Data-enabled Predictive Control of Cold Atmospheric Pressure Plasma Jets](https://arxiv.org/abs/2507.08259)
*Pegah GhafGhanbari,Mircea Lazar,Javad Mohammadpour Velni*

Main category: eess.SY

TL;DR: 提出了一种名为NPV-DeePC的新型控制框架，可以更好地控制等离子体射流等复杂的非线性系统。


<details>
  <summary>Details</summary>
Motivation: 为了解决冷大气压等离子体射流（APPJs）因其非线性动力学和对操作条件（如尖端到表面距离）的强烈敏感性而难以实现鲁棒可靠的实时控制的挑战。

Method: 提出了一种神经参数时变数据驱动预测控制（NPV-DeePC）框架，通过将超神经元网络集成到神经数据驱动预测控制（DeePC）范例中，该方法能够自适应地捕捉系统非线性和参数变化，并相应地更新神经特征空间，从而实现高效准确的轨迹预测和控制。

Result: 仿真结果表明，该方法在表面温度跟踪和热剂量输送方面能够实现比现有控制器更高的精度和适应性。

Conclusion: NPV-DeePC框架在表面温度跟踪和热剂量输送的仿真中表现出色，在精度和适应性方面优于现有控制器，并且计算效率高，可用于实时应用，有望推动APPJs的安全精确控制并为其他参数变化非线性系统提供可扩展的解决方案。

Abstract: Cold Atmospheric Pressure Plasma Jets (APPJs) show significant potential for
biomedical applications, but their inherent complexity, characterized by
nonlinear dynamics and strong sensitivity to operating conditions like
tip-to-surface distance, presents considerable challenges for achieving robust
and reliable real-time control. To address these issues, this paper presents
the Neural Parameter-Varying Data-enabled Predictive Control (NPV-DeePC)
framework. By integrating hyper neural networks (hypernets) into the neural
Data-enabled Predictive Control (DeePC) paradigm, the proposed method
adaptively captures system nonlinearities and parameter variations, updates the
neural feature space accordingly, and enables efficient and accurate trajectory
prediction and control. The NPV-DeePC framework is validated through extensive
simulations involving surface temperature tracking and thermal dose delivery.
The results highlight its ability to outperform existing controllers in terms
of accuracy and adaptability. The computational efficiency of the NPV-DeePC
approach makes it a viable candidate for real-time applications. These findings
underscore its potential to advance the safe and precise control of APPJs and
provide a scalable solution for other parameter-varying nonlinear systems.

</details>


### [241] [Two-Level Distributed Interference Management for Large-Scale HAPS-Empowered vHetNets](https://arxiv.org/abs/2507.08299)
*Afsoon Alidadi Shamsabadi,Animesh Yadav,Halim Yanikomeroglu*

Main category: eess.SY

TL;DR: 为了解决HAPS赋能vHetNets中的同频干扰和大尺度网络问题，提出了一种基于ALM和三块ADMM的PFBWD算法，该算法能够有效处理非凸性，降低复杂度，并实现可扩展的分布式优化，同时保证收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了实现下一代无线网络（xG）的泛在连接和提升用户体验，需要一种颠覆性的网络架构。高空平台站（HAPS）与地面网络集成形成的HAPS赋能垂直异构网络（vHetNets）能够显著提升覆盖和容量，并支持新兴用例。然而，在频谱共享的vHetNets中存在同频干扰和大尺度网络的问题。

Method: 提出了一种两级分布式比例公平波束成形权重设计（PFBWD）算法，该算法结合了增广拉格朗日方法（ALM）和三块交替方向乘子法（ADMM）。

Result: 所提出的PFBWD算法能够有效处理非凸性，降低复杂度，并实现可扩展的分布式优化，同时保证收敛性。

Conclusion: 提出的PFBWD算法结合了增广拉格朗日方法（ALM）和三块交替方向乘子法（ADMM），有效解决了非凸性问题，降低了计算复杂度，实现了可扩展的分布式优化，并保证了收敛性。

Abstract: Next-generation wireless networks (xG) must provide ubiquitous connectivity
while enhancing user experience in both densely populated urban areas and rural
regions. To achieve this, a disruptive network architecture is essential, and
high altitude platform stations (HAPS) offer a promising solution. By
integrating HAPS with terrestrial networks, we can create HAPS-empowered
vertical heterogeneous networks (vHetNets), which significantly improve
coverage and capacity, as well as support emerging use cases. In HAPS-empowered
vHetNets, different tiers can share the same spectrum, forming harmonized
spectrum vHetNets that enhance spectral efficiency (SE). However, we face two
major challenges: i) co-channel interference in harmonized spectrum vHetNets,
and ii) the large-scale nature of the network. To address the first challenge,
we adopt a cell-free approach as the underlying network architecture for the
HAPS-empowered vHetNet. In this approach, base stations use beamforming to
direct high-gain, narrow beams toward users, which helps mitigate interference.
However, this creates a nonconvex and high-dimensional optimization problem,
which highlights the second challenge of dealing with a large-scale network.
Consequently, centralized solutions become impractical due to the computational
and communication overhead involved. The standard two-block alternating
direction method of multipliers (ADMM) is one option, but nonconvex constraints
can hinder its convergence. As an alternative, we have developed a two-level
distributed proportional fairness beamforming weight design (PFBWD) algorithm.
This algorithm uses a combination of the augmented Lagrangian method (ALM) and
a three-block ADMM framework. The proposed method effectively tackles
nonconvexity, reduces complexity, and enables scalable, distributed
optimization with guaranteed convergence.

</details>


### [242] [A Generalized Stability Analysis Method with Dynamic Phasors for LV AC Microgrids](https://arxiv.org/abs/2507.08383)
*Bülent Dağ*

Main category: eess.SY

TL;DR: 本研究提出了一种新的低压交流微电网稳定性分析方法，该方法通过动态相量改进了对感性耦合线路的表示，从而可以更准确地预测不稳定性边界。


<details>
  <summary>Details</summary>
Motivation: 现有基于相量的简化稳定性分析方法因未能充分表示感性耦合线路而存在不足。

Method: 将动态相量纳入现有的基于相量的稳定性分析方法，以对感性耦合线路进行动态建模。

Result: 所提出的包含动态相量的稳定性分析方法能够成功预测低压交流微电网的不稳定边界。

Conclusion: 提出的动态相量方法可以成功预测包含感性耦合线路的低压交流微电网的不稳定边界。

Abstract: Representation of inductive coupling lines with conventional static phasors
is the main reason of inadequacy of the existing phasors based simplified
stability analysis methods for microgrids with inductive coupling lines. In the
literature, dynamic phasors have been proposed for the dynamic modelling of
inductive lines to conserve the simplified structure of the analysis method. In
this study a generalized stability analysis method for LV AC microgrids,
composed of droop controlled inverters, is presented. The proposed analysis
method is based on the inclusion of dynamic phasors for inductive coupling
lines into the existing phasors based stability analysis method. The results
show that the stability analysis method with dynamic phasors successfully
predicts the instability boundaries of LV AC microgrids.

</details>


### [243] [PGD-based optimization of 3D bobsleigh track centerlines from 2D centerlines for simulation applications](https://arxiv.org/abs/2507.08393)
*Zhe Chen,Huichao Zhao,Yongfeng Jiang,Minghui Bai,Lun Li,Jicheng Chen*

Main category: eess.SY

TL;DR: 本研究提出了一种新的方法，利用2D雪车赛道中心线数据生成3D赛道中心线，以提高训练模拟的准确性和效率。该方法考虑了赛道长度、高差和坡度限制，并使用投影梯度下降算法进行优化。实验结果表明，该方法能够生成逼真的3D赛道中心线，且误差在可接受范围内，为赛道设计提供了有力支持。


<details>
  <summary>Details</summary>
Motivation: 为了降低雪车训练成本，利用雪车赛道中心线构建虚拟环境以复制真实比赛场景是一个有前景的解决方案。然而，公开的中心线数据有限，并且仅基于二维（2D）中心线构建训练系统是不精确的。

Method: 提出了一种基于2D中心线数据生成3D赛道中心线的方法，该方法将国际赛道设计法规纳入考虑，并构建了一个优化问题，利用投影梯度下降（PGD）算法进行求解。

Result: 生成的三维中心线与真实赛道数据进行了比较，结果表明该方法能够从原始或缩放的二维数据中重现逼真的中心线趋势。所选赛道片段的相对误差在总长度、高差和平均坡度方面分别在1.7%、3.2%和4.1%以内（真实二维数据），以及1.1%、3.5%和4.3%以内（缩放数据）。所有坡度值均在允许范围内。此外，通过调整分段或修改成本函数中高差的权重，可以生成适用于不同比赛的各种中心线样式。在不同的分段和权重因子下，最大误差分别达到4.4%、4.8%和9.8%，以及4.4%、4.8%和10.0%。

Conclusion: 该方法为支持雪车赛道中心线设计提供了一个灵活高效的工具。

Abstract: The centerline of a bobsleigh track defines its geometry and is essential for
simulation modeling. To reduce bBobsleigh training costs, leveraging the
centerline of the bobsleigh track to construct a virtual environment that
closely replicates real competitive settings presents a promising solution.
However, publicly available centerline data are typically limited and it is
imprecise to construct a training system solely based on 2-dimensional (2D)
centerline. To address this practical issue, this paper proposes a method for
generating a 3-dimensional (3D) track centerline based on 2D centerline data.
Incorporating international track design regulations, the method formulates an
optimization problem that considers total track length, height difference,
slope constraints, and geometric continuity. A Projected Gradient Descent (PGD)
algorithm is used to solve the optimization problem. The generated 3D
centerlines are compared with real track data, and the results show that the
method can reproduce realistic centerline trends from original or scaled 2D
data. For the selected track segment, the relative errors in total length,
height difference, and average slope are within 1.7%, 3.2% and 4.1%,
respectively, for real 2D data and within 1.1%, 3.5% and 4.3% respectively for
scaled data. All slope values remain within the allowable limits. Moreover, by
adjusting the segmentation or modifying the weight of height difference in the
cost function, various centerline styles applicable to different competitions
can be generated. Under different segmentation and weight factors, the maximum
errors reach up to 4.4%, 4.8%, and 9.8%, and 4.4%, 4.8%, and 10.0%,
respectively. The proposed method provides a flexible and efficient tool for
supporting bobsleigh track centerline design.

</details>


### [244] [Large-Scale Processing and Validation of Grid Data for Assessing the Fair Spatial Distribution of PV Hosting Capacity](https://arxiv.org/abs/2507.08684)
*Ali Mohamed Ali,Yaser Raeisi,Plouton Grammatikos,Davide Pavanello,Pierre Roduit,Fabrizio Sossan*

Main category: eess.SY

TL;DR: 该研究提出了一种从DSO数据库提取、验证和调整电网数据的方法，以支持大规模电网研究，并提出了一种公平分配光伏发电容量的方法。


<details>
  <summary>Details</summary>
Motivation: 为了应对光伏系统集成和电气化水平提高给传统配电网设计和运行带来的挑战。

Method: 提出了一种从配电系统运营商（DSO）数据库中提取、验证和调整电网数据的方法，以支持负荷流和最优潮流分析等大规模电网研究。验证过程结合了基于规则的健全性检查和离线自动潮流分析，以确保数据一致性并检测电网数据库中潜在的错误，从而可以进行纠正。

Result: 提出了一种评估配电网光伏输电能力的方法，重点是确保其空间分布的公平性。

Conclusion: 通过将公平性标准纳入分析，量化了与空间公平相关的成本（以销售光伏发电的收入损失来衡量）。

Abstract: The integration of PV systems and increased electrification levels present
significant challenges to the traditional design and operation of distribution
grids. This paper presents a methodology for extracting, validating, and
adapting grid data from a distribution system operator's (DSO) database to
facilitate large-scale grid studies, including load flow and optimal power flow
analyses. The validation process combines rule-based sanity checks and offline
automated power flow analyses to ensure data consistency and detect potential
errors in the grid database, allowing for their correction. As a practical
application, the paper proposes a method to assess the PV hosting capacity of
distribution grids, with a focus on ensuring fairness in their spatial
distribution. By incorporating fairness criteria into the analyses, we quantify
the costs (in terms of missed revenues from selling PV generation) associated
with spatial fairness.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [245] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探讨了AI是否具有创造力的问题，并分析了相关领域的哲学、心理学和神经科学观点。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的发展，研究AI是否具有创造力具有重要的理论和现实意义。

Method: 本文通过文献综述，探讨了人工智能在创造力方面的潜力和局限性。

Result: 研究表明，虽然AI在某些方面可以模拟创造性行为，但其是否真正拥有创造力仍然是一个悬而未决的问题。

Conclusion: AI 的创造力是一个复杂的问题，涉及哲学、心理学和神经科学等多个领域。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [246] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Nash框架将博弈论应用于LLM，通过分析提示和推理过程来模拟有限理性，并研究认知约束和学习。


<details>
  <summary>Details</summary>
Motivation: 与假设具有完全理性的效用最大化代理的经典博弈不同，该框架旨在通过明确模拟推理过程来捕捉有限理性。

Method: 提出LLM-Nash框架，一个博弈论模型，其中代理选择推理提示来通过LLM指导决策，明确地模拟了推理过程以捕捉有限理性。

Result: 通过示例表明，推理均衡可能与经典纳什均衡不同，从而为LLM驱动的系统中的战略互动奠定了新基础。

Conclusion: LLM-Nash框架为研究LLM驱动的系统中的战略互动提供了一个新基础，其均衡存在于提示空间中，行为输出由LLM推理产生。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [247] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 本论文提出了一种名为TableReasoner的LLM驱动的表格问答系统，通过改进的表格表示和迭代推理流程解决了现实世界表格数据的挑战，并在SemEval-2025 Task 8竞赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界表格数据（如数据量大、列语义不完整、实体模糊）在表格问答（TQA）任务中面临的挑战。

Method: 提出了一种名为TableReasoner的大语言模型（LLM）驱动的、基于编程的表格推理框架。该框架通过结合结构和语义表示的模式来模拟表格，以实现对大型表格的整体理解和高效处理。设计了多步模式链接计划，以生成仅包含查询相关信息的聚焦表格模式，从而消除歧义并减少幻觉。将推理工作流集成到迭代思考架构中，实现思考、推理和反思的增量循环。

Result: 提出的TableReasoner框架能够精确、充分地提供查询细化和编程所需的表格细节，并成功应用于TQA任务，在SemEval-2025 Task 8的两个子任务中均取得优异成绩。

Conclusion: 该系统在SemEval-2025 Task 8的两个子任务中均获得第一名。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [248] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 该研究为对比解释定义了新的规范问题，并在命题逻辑中进行了理论和计算分析，同时提供了实际应用实例。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在定义和分析关于对比解释的问题，以解决“为什么 P 而不是 Q？”这一类问题，并为计算 P 和 Q 的原因提供一种明确比较其差异的方法。

Method: 该研究定义了几个关于对比解释的规范问题，这些问题旨在回答“为什么是 P 而不是 Q？”这类问题，通过明确比较 P 和 Q 的差异来计算各自的原因。研究人员在命题逻辑环境下对这些定义的基本性质进行了探究，并使用答案集编程实现了这些问题，以处理 CNF 公式。

Result: 研究结果表明，该框架捕获了现有对比解释的基数最小化版本，并对问题的计算复杂度进行了广泛分析。此外，研究人员还实现了针对 CNF 公式的解决方案，并通过实例展示了其在实践中的应用。

Conclusion: 该研究为对比解释定义了几个规范问题，并研究了它们在命题逻辑下的基本性质，证明了该框架能够捕获现有对比解释的基数最小化版本，并分析了计算复杂度。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [249] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一个动态Stackelberg博弈框架来模拟LLM越狱背景下的攻击者与防御者之间的交互。该框架将提示-响应动态视为一个顺序博弈，防御者作为领导者，而紫 एजेंट（Purple Agent）则利用RRT来探索和防御潜在的攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越多地部署在关键应用程序中，越狱的挑战（即对手操纵模型以绕过安全机制）已成为一个重大问题。

Method: 提出了一种新颖的代理AI解决方案，“紫 एजेंट”，它使用快速探索随机树（RRT）整合了对抗性探索和防御策略。紫 एजेंट主动模拟潜在的攻击轨迹，并主动进行干预以防止有害输出。

Result: 该框架将提示-响应动态视为一个顺序扩展形式博弈，其中防御者作为领导者，在预期攻击者的最优响应的情况下承诺其策略。

Conclusion: 该框架为分析对抗性动态提供了一种原则性方法，并为减轻越狱风险奠定了基础。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [250] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: "Intelligent agents must balance curiosity and competence for effective exploration. This paper studies this trade-off using two agents, showing that prioritizing both curiosity and competence improves exploration and that representation learning interacts with exploration."


<details>
  <summary>Details</summary>
Motivation: "To understand how agents balance the drive for knowledge (curiosity) with the drive for mastery and control (competence) during exploration."

Method: "Comparing two model-based agents: one with handcrafted state abstractions (Tabular) and another learning an internal world model (Dreamer)."

Result: "The Tabular agent demonstrated distinct exploration patterns guided by curiosity and competence, with prioritization of both enhancing exploration. The Dreamer agent exhibited a reciprocal relationship between exploration and representation learning, analogous to the developmental interplay of curiosity and competence."

Conclusion: "Balancing curiosity and competence leads to adaptive exploration, providing insights for both cognitive theories and reinforcement learning."

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [251] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 提出了一种新的神经-符号（NeSy）接地方法，通过参数化和泛化反向链接来平衡表达能力和可扩展性，实验证明其接地标准的选取至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的神经-符号（NeSy）方法在处理复杂关系时面临组合爆炸或缺乏理论依据的问题。本研究旨在解决这个问题，提高可扩展性并提供理论保证。

Method: 提出一个参数化的接地方法家族，泛化了经典的反向链接，可以通过选择不同的接地标准来控制表达能力和可扩展性之间的权衡。

Result: 该方法家族允许获得常用的接地方法作为特例，并能控制表达能力和可扩展性之间的权衡。

Conclusion: 实验结果表明，接地标准的选取与神经-符号（NeSy）方法本身同等重要。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [252] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 本研究首次提出了一种用于量化联邦学习（QFL）的多模态方法，利用量子纠缠进行中间融合，并引入缺失模态不可知（MMA）机制以稳定训练过程，显著提高了模型在不同数据分布下的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有QFL框架主要针对单模态系统，限制了其在涉及多模态的实际任务中的应用。本研究旨在填补这一空白。

Method: 提出了一种新颖的多模态方法，采用量子纠缠进行中间融合，并引入缺失模态不可知（MMA）机制以解决训练中缺失模态导致性能下降的问题。

Result: 模拟结果表明，所提出的多模态QFL方法结合MMA，在IID和非IID数据分布上，准确率相比现有最先进方法分别提高了6.84%和7.25%。

Conclusion: 所提出的多模态量化联邦学习（QFL）方法通过中间融合和缺失模态不可知（MMA）机制，在IID和非IID数据分布上均显著优于现有技术，准确率分别提高了6.84%和7.25%。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [253] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: AI代理使用加密货币和智能合约存在新的风险，需要技术研究来防范。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理被授予访问加密货币和智能合约的权限，人们对潜在的AI危害日益关注。

Method: 通过检查加密货币和智能合约的独特属性，以及详细描述这些新的危害途径来论证。

Result: AI代理访问加密货币和智能合约可能导致新的、严峻的AI危害。

Conclusion: AI代理访问加密货币和智能合约可能会带来新的AI危害。需要更多的技术研究来预防和减轻这些危害，以确保AI代理更安全地使用加密货币和智能合约。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [254] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 本文梳理了溯因推理在不同领域的应用和计算模型的实现方式，发现现有方法在生成创造性假说方面存在不足，并为未来的研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 阐述了溯因推理在科学、设计和艺术领域的重要性，以及对其理解在不同领域存在差异的现象，旨在探讨并改进现有模型在生成创造性假说方面的不足。

Method: 本文首先回顾了溯因推理在认识论、科学和设计领域的讨论情况，然后分析了各种计算系统如何运用溯因推理。将溯因计算系统分解为各个组成部分。

Result: 理论框架未能提供一个直接生成创造性溯因假说的模型；计算系统在很大程度上实现了溯因推理的逻辑三段论形式。

Conclusion: 该分析表明，无论是理论上的论述还是计算模型，在解释或生成创造性假说方面都有所欠缺。计算系统大多采用逻辑三段论形式的溯因推理，未能充分满足创造性假说的生成需求。文章最后指出了未来在推进计算系统创造性溯因推理能力方面可供研究的具体方向。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [255] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 为能够使用工具的语言模型代理开发了一个安全框架，通过结构化推理和沙盒强化学习来防御用户和工具发起的安全威胁，并在多个基准测试中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 鉴于能够使用工具的自主语言模型代理带来了新的安全风险，包括来自用户（如对抗性提示）和工具（如受损工具的恶意输出）的威胁，因此需要一个统一的安全框架。

Method: 提出了一种三模态分类（良性、恶意、敏感）和策略驱动的决策模型，并设计了一个定制的沙盒环境来进行安全对齐和奖励塑造。

Result: 在Agent SafetyBench、InjecAgent和BFCL等基准测试中，该框架显著提高了代理对安全威胁的抵抗能力，同时保持了在良性任务上的高效率，证明了安全性和有效性可以协同优化。

Conclusion: 该研究提出了一个统一的安全对齐框架，用于工具驱动的语言模型代理，以应对用户和工具发起的威胁，并通过结构化推理和沙盒强化学习来增强安全性。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [256] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: 提出M2-Reasoning-7B模型，通过高质量数据集和动态多任务训练策略，提升了MLLMs在空间推理方面的能力，并在多个基准测试中取得了SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLMs在动态空间交互方面的不足，而这是现实世界应用的关键能力。

Method: 通过新颖的数据管道生成294.2K高质量数据样本（用于冷启动微调和RLVR），并采用动态多任务训练策略进行分步优化，以缓解数据冲突并提供特定任务的奖励。

Result: M2-Reasoning-7B在8个基准测试中均达到SOTA水平，在通用和空间推理方面均表现出色。

Conclusion: M2-Reasoning-7B模型在通用和空间推理方面均设定了新的SOTA，在8个基准测试中表现出卓越性能。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [257] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 本研究提出了一种基于LLM的伦理倡导者代理方法来自动生成伦理需求，该方法在实践中显示出潜力，但仍需人类监督。


<details>
  <summary>Details</summary>
Motivation: 手动收集伦理需求耗时耗力，且优先级低。本研究旨在提出一个更有效的框架来生成伦理需求。

Method: 通过在多智能体LLM设置中引入一个伦理倡导者代理来生成伦理需求草案，该代理根据系统描述对伦理问题进行批评和提供输入。

Result: 该框架在两个不同背景的案例研究中得到评估，结果表明该框架能捕获大部分由研究人员在访谈中识别出的伦理需求，并提出了一些额外的相关需求，但也存在可靠性问题。

Conclusion: 该研究认为，通过在多智能体LLM设置中引入伦理倡导者代理，可以为需求工程过程引入伦理考量，但需要人类反馈来解决可靠性问题，最终目标是实现更符合伦理的产品。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [258] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


### [259] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 提出了一种新颖的双层框架，通过高层任务抽象和低层逻辑生成将语言映射到逻辑，并使用双层优化方法进行优化。该框架在准确性和可解释性方面优于现有方法，并在多个推理基准上实现了高达40%的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 结构化推理是人工智能中的一个核心挑战，因为它需要弥合非结构化语言表达与形式逻辑表示之间的差距。

Method: 提出了一种新颖的双层框架，通过一个两阶段过程将语言映射到逻辑：高层任务抽象和低层逻辑生成。在高层，语言模型将自然语言查询解析为中间结构化表示，指定问题类型、目标、决策变量和符号约束。在低层，语言模型利用这些表示生成符号工作流或可执行的推理程序，以实现准确且可解释的决策制定。

Result: 在多个现实推理基准上的实验表明，该方法在准确性方面显著优于现有基线，准确性提高了40%。此外，双层设计增强了透明度和错误可追溯性。

Conclusion: 该框架通过联合优化高层抽象和低层逻辑生成阶段，支持模块化推理，强制执行显式约束，并能推广到数学问题解决、问答和逻辑推理等领域。实验结果表明，该方法在准确性方面显著优于现有基线，准确性提高了40%。此外，双层设计增强了透明度和错误可追溯性，为实现可信赖和系统的语言模型推理迈出了重要一步。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [260] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 该研究提出了一种创新的罕见病诊断框架，通过结合多粒度医学概念激活和分层知识图谱，显著提高了诊断的准确性和效率，有望改善罕见病患者的就医体验。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗大型语言模型在医疗保健领域取得了进展，但罕见病诊断仍然受到知识表示深度不足、概念理解有限和临床推理受限的阻碍。

Method: 提出了一种结合多粒度稀疏激活医学概念与分层知识图谱的框架。该框架利用四种互补的匹配算法、多样性控制和五级回退策略来实现精确的概念激活，并构建了一个包含分类、临床特征和实例的三层知识图谱来提供结构化、最新的上下文。

Result: 在BioASQ罕见病问答数据集上的实验显示，该框架在BLEU上提高了0.09，在ROUGE上提高了0.05，在准确率上提高了0.12，峰值准确率达到0.89，接近0.90的临床阈值。专家评估也证实了在信息质量、推理和专业表达方面的改进。

Conclusion: 该研究提出的框架通过多粒度稀疏激活医学概念与分层知识图谱相结合，在罕见病诊断方面取得了显著进展，提高了概念激活的精确性，并提供了结构化的上下文信息。实验结果表明，该方法在BLEU、ROUGE和准确率方面均有提升，并且通过了专家评估，证明了其在提高信息质量、推理能力和专业表达方面的有效性，有望缩短罕见病患者的“诊断历程”。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [261] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 介绍了一种利用大型多模态模型（LMM）处理和配准生物样本集合中复杂地理位置描述的新方法，该方法通过视觉化理解地图取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏地理坐标，数百万份在自然历史收藏中保存的生物样本记录难以利用。现有的自动化方法未能充分利用地图这一关键工具来处理复杂的地理位置描述，而手动配准则耗时耗力。

Method: 本文提出了一种利用大型多模态模型（LMM）的多模态能力的新方法，通过网格化方法在零样本设置下自动配准复杂的地理位置描述，使其能够视觉化地理解地图中的空间关系。

Result: 实验结果表明，该方法能够以约1公里的平均距离误差实现比单一模态地理配准方法（如大型语言模型和现有工具）更精确的地理配准。

Conclusion: 本文提出的新方法在地理配准任务中表现出比现有方法更优越的性能，并提出了一种将其集成到地理配准工作流程中的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [262] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 由于缺乏数据集和训练任务存在偏见，大型语音语言模型在遵循语音指令方面能力不足。为解决此问题，提出了一种查询重写框架，该框架利用多 LLM 知识融合和多个代理来注释和验证合成语音，从而无需人工注释即可构建高质量语音指令数据集。该方法将文本指令转换为更适合 TTS 模型的分布，提高了数据可用性，并在复杂重写任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语音语言模型在响应延迟和语音理解能力方面显示出巨大潜力，但在遵循语音指令方面能力不足，这主要是由于缺乏数据集和训练任务存在偏见。现有的方法利用大型语言模型继续语音的语言信息来构建语音指令数据集，但大型语言模型生成的文本与真实人类响应之间存在差距，这会进一步加剧这些缺点。虽然语音合成（TTS）是构建大规模语音指令数据集的一种可行的替代方案，但由于训练数据分布的限制，在将非分布内文本指令转换为语音方面存在挑战。

Method: 提出了一种查询重写框架，该框架具有多大型语言模型知识融合功能，并利用多个代理来注释和验证合成语音，以构建高质量的语音指令数据集。

Result: 该方法将文本指令转化为更适合语音合成模型语音合成的分布，将数据可用性从 72% 提高到 93%。该方法还在需要复杂知识和上下文相关能力的重写任务中显示出独特的优势。

Conclusion: 所提出的查询重写框架通过多大型语言模型知识融合，使用多个代理来注释和验证合成语音，无需人工注释即可构建高质量语音指令数据集。实验表明，该方法通过零次重写将文本指令转化为更适合语音合成模型语音合成的分布，将数据可用性从 72% 提高到 93%，并在需要复杂知识和上下文相关能力的重写任务中展现出独特的优势。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [263] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 本文比较了多智能体系统（MAS）和双智能体系统（2AS）在工程设计中的表现。MAS在设计细节上更好，但代码兼容性较差；2AS代码兼容性更好，但设计细节不足。LLM的推理能力对结果有影响，但低需求覆盖率和编码保真度仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 早期工程设计涉及复杂且迭代的推理，现有的LLM工作流在任务连续性和生成可执行模型方面存在挑战。本文旨在评估结构化多智能体系统（MAS）是否比简单的双智能体系统（2AS）能更有效地管理需求提取、功能分解和模拟器代码生成。

Method: 评估了结构化多智能体系统（MAS）与双智能体系统（2AS）在处理需求提取、功能分解和模拟器代码生成方面的效果。提出了一种名为设计状态图（DSG）的表示方法，用于整合需求、物理实体和基于Python的物理模型。MAS包含九个角色，迭代地构建和完善DSG，而2AS则将过程简化为生成器-反射器循环。实验在两种大型语言模型（LLM）上进行了60次运行，以评估JSON有效性、需求覆盖率、实体存在、代码兼容性、工作流完成情况、运行时间和图大小等指标。

Result: MAS生成了更精细的DSG（平均5-6个节点），而2AS模式坍塌。结构化多智能体协调增强了设计细节。推理蒸馏LLM提高了完成率，但需求覆盖率低和编码保真度差距仍然存在。两种系统在所有运行中都保持了完美的JSON完整性和实体标记。需求覆盖率保持在20%以下。2AS在特定设置下代码兼容性达到100%，但MAS平均低于50%。只有推理蒸馏模型能可靠地标记工作流完成情况。

Conclusion: 结构化多智能体系统（MAS）和双智能体系统（2AS）在处理早期工程设计方面各有优劣。MAS在生成更精细的设计状态图（DSG）方面表现更好，而2AS在代码兼容性方面表现更优，但存在模式坍塌问题。推理蒸馏模型提高了完成率，但需求覆盖率低和编码保真度差距仍然存在。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [264] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2 是一个 7B LLM，通过利用 Lean 4 验证器的反馈进行强化学习，能够生成形式化定理证明，并在 MiniF2F 数据集上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了进一步提高大型语言模型 (LLM) 在生成形式化定理证明方面的性能，我们推出了 Leanabell-Prover-V2，这是一个在 Lean 4 中生成形式化定理证明的 7B LLM 模型，并集成了验证器-集成长链思维 (CoT)。

Method: 通过使用 Lean 4 验证器的反馈来改进强化学习 (RL) 方法，具体包括成功指示或错误细节，使 LLM 能够“自我意识到”其推理过程的正确性并学会进行纠正。通过多轮验证器交互、反馈令牌掩码和简单的奖励策略来优化 LLM 推理轨迹。

Result: Leanabell-Prover-V2 在 MiniF2F 测试集上，使用 Kimina-Prover-Preview-Distill-7B 时提高了 3.2% (pass@128)，使用 DeepSeek-Prover-V2-7B 时提高了 2.0% (pass@128)。

Conclusion: Leanabell-Prover-V2 相比 V1 在 MiniF2F 测试集上分别提高了 Kimina-Prover-Preview-Distill-7B 3.2% (pass@128) 和 DeepSeek-Prover-V2-7B 2.0% (pass@128)。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [265] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [266] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL is an open-source Python library for applying language solutions to reinforcement learning. It uses LLMs and a GUI to generate self-completing instructions that can improve agent performance, aiming to speed up research in reward-based environments.


<details>
  <summary>Details</summary>
Motivation: To accelerate the evaluation of language solutions on reward-based environments and enable new opportunities for scientific discovery.

Method: Using LLMs to extend the Language Adapter with Self-Completing Instruction framework, with a novel GUI for text input to LLM for instruction generation and self-completion.

Result: Empirical results indicate that the generated instructions can improve a reinforcement learning agent's performance.

Conclusion: The elsciRL library facilitates the application of language solutions on reinforcement learning problems, demonstrating potential through extending an existing framework with LLMs. The approach is versatile, with minimal setup for new applications and a novel GUI for text-based instruction generation and self-completion.

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [267] [NeurOptimisation: The Spiking Way to Evolve](https://arxiv.org/abs/2507.08320)
*Jorge Mario Cruz-Duarte,El-Ghazali Talbi*

Main category: cs.NE

TL;DR: 通过使用神经形态计算和脉冲神经网络，可以开发出高效、低功耗的优化框架。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统日益增长的能源消耗，促使人们需要开发高效且可扩展的替代计算模型。神经形态计算 (NC) 通过赋能由生物启发的尖峰动力学驱动的事件驱动算法，以最小的功耗运行，从而应对这一挑战。

Method: 提出了一种名为 NeurOptimiser 的全基于脉冲的优化框架，该框架通过去中心化的神经形态计算系统实现了基于神经形态的元启发式范式。该方法包括一个由神经启发单元 (NHU) 组成的种群，每个单元结合了脉冲神经元动力学和脉冲触发扰动启发式方法，以异步演化候选解决方案。NeurOptimiser 的协调通过支持活动传播、本地信息共享和全局状态更新的原生脉冲机制来实现，而无需外部编排。在英特尔的 Lava 平台和 Loihi 2 芯片上实现了该框架，并在 BBOB 套件（高达 40 个维度）上进行了评估。

Result: 所提出的方法表现出结构化的种群动态、持续的收敛和毫瓦级的功率可行性。

Conclusion: 该方法为低功耗、实时和去中心化的优化提供了可持续的途径。

Abstract: The increasing energy footprint of artificial intelligence systems urges
alternative computational models that are both efficient and scalable.
Neuromorphic Computing (NC) addresses this challenge by empowering event-driven
algorithms that operate with minimal power requirements through biologically
inspired spiking dynamics. We present the NeurOptimiser, a fully spike-based
optimisation framework that materialises the neuromorphic-based metaheuristic
paradigm through a decentralised NC system. The proposed approach comprises a
population of Neuromorphic Heuristic Units (NHUs), each combining spiking
neuron dynamics with spike-triggered perturbation heuristics to evolve
candidate solutions asynchronously. The NeurOptimiser's coordination arises
through native spiking mechanisms that support activity propagation, local
information sharing, and global state updates without external orchestration.
We implement this framework on Intel's Lava platform, targeting the Loihi 2
chip, and evaluate it on the noiseless BBOB suite up to 40 dimensions. We
deploy several NeurOptimisers using different configurations, mainly
considering dynamic systems such as linear and Izhikevich models for spiking
neural dynamics, and fixed and Differential Evolution mutation rules for
spike-triggered heuristics. Although these configurations are implemented as a
proof of concept, we document and outline further extensions and improvements
to the framework implementation. Results show that the proposed approach
exhibits structured population dynamics, consistent convergence, and
milliwatt-level power feasibility. They also position spike-native MHs as a
viable path toward real-time, low-energy, and decentralised optimisation.

</details>


### [268] [Enhancing Parameter Control Policies with State Information](https://arxiv.org/abs/2507.08368)
*Gianluca Covini,Denis Antipov,Carola Doerr*

Main category: cs.NE

TL;DR: 本文提出新基准测试函数，使算法能利用当前状态信息（如OneMax值）动态调整参数（如变异强度k），从而在LeadingOnes优化任务中获得显著的性能提升和运行时间加速。


<details>
  <summary>Details</summary>
Motivation: 现有参数控制研究中，最优控制策略的已知情况非常有限，这阻碍了自动化方法的发展。本研究旨在通过提出新的基准测试函数并推导其最优控制策略，来推动参数控制和动态算法配置领域的发展，特别是探索利用更丰富状态信息以改进参数选择和提升算法性能。

Method: 本文提出了四种新的基准测试函数，用于优化带参数的算法（特别是RLS$_{k}$算法在LeadingOnes函数上的优化）。研究允许算法利用当前状态信息（如OneMax值）来动态调整变异强度k，并分析了这些信息和参数选择对算法性能的影响。

Result: 研究发现，利用当前OneMax值信息可以做出更好的参数选择，尤其是在算法的边缘状态下，这能带来显著的预期运行时间加速。

Conclusion: 该研究提出了四种新的基准测试函数，并为它们推导出了最优或接近最优的控制策略。研究发现，允许算法利用当前状态信息（如OneMax值）进行参数选择，可以显著提高性能，尤其是在边缘状态下，从而在预期运行时间上实现加速。

Abstract: Parameter control and dynamic algorithm configuration study how to
dynamically choose suitable configurations of a parametrized algorithm during
the optimization process. Despite being an intensively researched topic in
evolutionary computation, optimal control policies are known only for very few
cases, limiting the development of automated approaches to achieve them.
  With this work we propose four new benchmarks for which we derive optimal or
close-to-optimal control policies. More precisely, we consider the optimization
of the \LeadingOnes function via RLS$_{k}$, a local search algorithm allowing
for a dynamic choice of the mutation strength $k$. The benchmarks differ in
which information the algorithm can exploit to set its parameters and to select
offspring. In existing running time results, the exploitable information is
typically limited to the quality of the current-best solution. In this work, we
consider how additional information about the current state of the algorithm
can help to make better choices of parameters, and how these choices affect the
performance. Namely, we allow the algorithm to use information about the
current \OneMax value, and we find that it allows much better parameter
choices, especially in marginal states. Although those states are rarely
visited by the algorithm, such policies yield a notable speed-up in terms of
expected runtime. This makes the proposed benchmarks a challenging, but
promising testing ground for analysis of parameter control methods in rich
state spaces and of their ability to find optimal policies by catching the
performance improvements yielded by correct parameter choices.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [269] [AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08616)
*Florian Grötschla,Luis Müller,Jan Tönshoff,Mikhail Galkin,Bryan Perozzi*

Main category: cs.MA

TL;DR: AgentsNet是一个新的基准，用于衡量多智能体系统在不同网络拓扑下的协作能力。测试表明，虽然大型语言模型在小型网络中表现良好，但在大型网络中性能会下降。


<details>
  <summary>Details</summary>
Motivation: 现有的关于多智能体系统（尤其是在推理基准上的表现）的研究未能充分说明这些系统利用其拓扑结构的能力，需要一种新的方法来评估其在不同网络结构下的协作和自我组织能力。

Method: 提出了一种名为AgentsNet的新基准，用于评估多智能体系统在网络拓扑下的协作、自我组织和通信能力，并引入了分布式系统和图论的经典问题。

Result: 在AgentsNet基准测试中，一些前沿的大型语言模型在小型网络中表现出色，但在网络规模扩大后性能有所下降。该基准能够支持多达100个智能体，并且可以随着新一代大型语言模型的出现而扩展。

Conclusion: 需要进一步研究以提高大型语言模型在网络拓扑结构下的大规模协作能力。

Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective communication given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and communication. We find that some frontier LLMs
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of LLMs. As such, we also probe frontier
models in a setup with up to 100 agents.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [270] [Fractional Thouless pumping of solitons: a unique manifestation of bulk-edge correspondence of nonlinear eigenvalue problems](https://arxiv.org/abs/2507.08016)
*Chenxi Bai,Zhaoxin Liang*

Main category: cond-mat.mes-hall

TL;DR: 非线性与近邻耦合相互作用可产生分数 theless泵浦，这在传统方法中是无法实现的。


<details>
  <summary>Details</summary>
Motivation: 探索特征值非线性是否能产生传统方法所没有的物理现象。

Method: 通过系统研究扩展的Rice-Mele模型中非线性Thouless泵浦，并利用辅助特征值框架解释分数相位。

Result: 发现了分数 theless泵浦，其中近邻参数可以引起分数拓扑相位，这可以通过辅助特征值框架解释，将非线性谱特征与体边界对应联系起来。

Conclusion: 这项工作揭示了非线性与近邻耦合之间相互作用产生的新现象，为了设计拓扑绝缘体和控制非线性状态下的量子边缘态提供了关键见解。

Abstract: Recent foundational studies have established the bulk-edge correspondence for
nonlinear eigenvalue problems using auxiliary eigenvalues $\hat{H}\Psi=\omega
S(\omega)\Psi$, spanning both linear [T. Isobe et al., Phys. Rev. Lett. 132,
126601 (2024)] and nonlinear [Chenxi Bai and Zhaoxin Liang, Phys. Rev. A. 111,
042201 (2025)] Hamiltionians. This raises a fundamental question: Can
eigenvalue nonlinearity generate observable physical phenomena absent in
conventional approaches ($\hat{H}\Psi=E\Psi$)? In this work, we demonstrate the
first uniquely nonlinear manifestation of this correspondence: fractional
Thouless pumping of solitons. Through systematic investigation of nonlinear
Thouless pumping in an extended Rice-Mele model with next-nearest-neighbor
(NNN) couplings, we discover that the NNN parameters can induce fractional
topological phases despite quantized topological invariants from conventional
approaches. Crucially, these fractional phases are explained within the
auxiliary eigenvalue framework, linking nonlinear spectral features directly to
bulk-boundary correspondence. This work reveals novel phenomena emerging from
the interplay between nonlinearity and NNN couplings, providing key insights
for designing topological insulators and controlling quantum edge states in
nonlinear regimes.

</details>


### [271] [Ultrasensitive Magnetometer based on Cusp Points of the Photon-Magnon Synchronization Mode](https://arxiv.org/abs/2507.08032)
*Xinlin Mi,Jinwei Rao,Lijun Yan,Xudong Wang,Bingbing Lyu,Bimu Yao,Shishen Yan,Lihui Bai*

Main category: cond-mat.mes-hall

TL;DR: 开发了一种新型超灵敏磁力仪，利用光子-磁振子同步模式的尖点效应，将磁灵敏度提高了236倍，并大幅降低了线宽，有望超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 突破传统基于自旋共振的磁力仪受限于粒子g因子（如电子）的固有磁回转比限制，以实现对弱磁场更优的响应灵敏度。

Method: 通过利用光子-磁振子同步模式（PMSM）的尖点（CP），并将磁回转比放大到基准值的236倍，同时将发射线宽缩窄至0.06 Hz来实现超灵敏测量。

Result: 实现了磁回转比为236γe（其中γe为电子的磁回转比）和发射线宽为0.06 Hz的超灵敏磁力仪。

Conclusion: 该研究展示了一种基于光子-磁振子同步模式（PMSM）的尖点（CP）的新型超灵敏磁力仪，其磁回转比可达电子的236倍，并具有窄至0.06 Hz的发射线宽，有望在灵敏度上超越传统磁力仪，为下一代磁测量技术提供了一种低成本原型。

Abstract: Ultrasensitive magnetometers based on spin resonances have led to remarkable
achievements. However, the gyromagnetic ratios of these spin resonances that
determine the responsivity of magnetometers to weak magnetic fields are
inherently constrained by the Land$\acute{e}$ g-factor of particles, such as
the electron, with a constant gyromagnetic ratio of $\gamma_e=2\pi\times28$
GHz/T. Here, we demonstrate an ultrasensitive magnetometer based on the cusp
point (CP) of photon-magnon synchronization modes (PMSMs). The PMSM's
gyromagnetic ratio at the CP is enhanced to $37\gamma_e$ and further amplified
to $236\gamma_e$ by utilizing the sixth-order oscillating mode of the PMSM.
Moreover, the emission linewidth of the PMSM can be reduced to 0.06 Hz,
resulting in excellent sensitivity to weak magnetic fields. These outstanding
properties position our magnetometer to potentially achieve superior
sensitivity to conventional magnetometers. Our work introduces a cost-effective
prototype for the next generation of magnetometry, and may advance scientific
research and technologies that rely on ultrasensitive magnetic field detection.

</details>


### [272] [Quantum theory of nonlinear electromagnetic response](https://arxiv.org/abs/2507.08035)
*Anwei Zhang,C. M. Wang*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, the investigation of nonlinear electromagnetic responses has
received significant attention due to its potential for elucidating the quantum
properties of matter. Although remarkable progress has been achieved in
developing quantum theories of nonlinear responses to electric field, a
comprehensive quantum theory framework that systematically addresses nonlinear
responses to both electric and magnetic fields has yet to be thoroughly
discussed. Here, we present a systematic quantum theory of nonlinear
electromagnetic response using the Matsubara Green's function approach, which
explicitly incorporates the wave vector dependence of external electromagnetic
fields. We provide diagrammatic representation and reveal the general
properties of transport coefficients. We apply our theory to second-order
responses, deriving the nonlinear Hall effects and magneto-nonlinear Hall
effects in both time-reversal symmetric and time-reversal breaking systems.
These effects stem from diverse quantum geometric sources. Additionally, we
analyze the contributions arising from the Zeeman interaction. Our work
presents a unified quantum theory of nonlinear electromagnetic response, paving
the way for further exploration of novel phenomena in this field.

</details>


### [273] [Electric and spin current vortices in altermagnets](https://arxiv.org/abs/2507.08072)
*A. A. Herasymchuk,Karl Bergson Hallberg,Erik Wegner Hodt,Jacob Linder,E. V. Gorbar,Pavlo Sukhachov*

Main category: cond-mat.mes-hall

TL;DR: Altermagnets are magnets with no net magnetization but with momentum-dependent spin splitting. Observing this splitting directly is hard, but this paper proposes using non-uniform electric fields to probe it through transport measurements. The paper shows that altermagnets create unique swirling electric and spin currents, even in normal conditions (Ohmic regime), which can be detected using magnetometry. These swirling currents are a distinct signature of altermagnets and are not seen in ferromagnets.


<details>
  <summary>Details</summary>
Motivation: Direct observation of the characteristic altermagnetic spin splitting, however, remains challenging. Indirect signatures can be obtained via transport studies, which so far have only considered homogeneous fields driving currents. We propose to leverage nonuniform electric fields and spin density gradients to probe the shape and the spin polarization of altermagnetic Fermi surfaces via transport measurements.

Method: By using both a semiclassical Boltzmann approach and a lattice Keldysh formalism

Result: altermagnets excite swirling electric and spin currents whose profiles depend on the relative orientation of altermagnetic lobes with respect to the sample boundaries.

Conclusion: altermagnets excite swirling electric and spin currents whose profiles depend on the relative orientation of altermagnetic lobes with respect to the sample boundaries.These currents can be measured via magnetometry techniques. Unlike previous proposals considering the hydrodynamic regime of transport, swirling currents are observed even in the Ohmic regime and rely exclusively on the altermagnetic spin splitting, with no swirls observed in ferromagnets. The electric and spin current vortices predicted here provide a unique altermagnetic signature in an experimentally accessible setup.

Abstract: Altermagnets constitute a class of collinear magnets with momentum-dependent
spin splitting and vanishing net magnetization. Direct observation of the
characteristic altermagnetic spin splitting, however, remains challenging.
Indirect signatures can be obtained via transport studies, which so far have
only considered homogeneous fields driving currents. We propose to leverage
nonuniform electric fields and spin density gradients to probe the shape and
the spin polarization of altermagnetic Fermi surfaces via transport
measurements. By using both a semiclassical Boltzmann approach and a lattice
Keldysh formalism, we show that altermagnets excite swirling electric and spin
currents whose profiles depend on the relative orientation of altermagnetic
lobes with respect to the sample boundaries. These currents can be measured via
magnetometry techniques. Unlike previous proposals considering the hydrodynamic
regime of transport, swirling currents are observed even in the Ohmic regime
and rely exclusively on the altermagnetic spin splitting, with no swirls
observed in ferromagnets. The electric and spin current vortices predicted here
provide a unique altermagnetic signature in an experimentally accessible setup.

</details>


### [274] [Phonon mode splitting and phonon anomaly in multiband electron systems](https://arxiv.org/abs/2507.08127)
*Klaus Ziegler*

Main category: cond-mat.mes-hall

TL;DR: Coupling chiral fermions to phonons creates topological features in phonon spectra and currents, allowing phonons to probe electronic chirality.


<details>
  <summary>Details</summary>
Motivation: To investigate the topological consequences of coupling chiral fermions to local, dispersionless phonons.

Method: We investigate the topological consequences of coupling chiral fermions to local, dispersionless phonons. This interaction induces a splitting of the phonon spectrum into three bands: a flat band and two bands with linear dispersion, all of which are degenerate at a nodal point located at zero wavevector.

Result: The interaction induces a splitting of the phonon spectrum into three bands (flat and two linear dispersion bands), degenerate at zero wavevector. The flat band has vanishing Berry curvature, while linear bands have nontrivial topological features with hedgehog-like Berry curvature fields in momentum space, reflecting fermion chirality. An effective phonon response reveals a phonon parity anomaly, observable as a discontinuity in the phonon current, originating from fermion Green's function singularities and signaling topological information transfer from fermions to phonons.

Conclusion: The study shows that phonon currents can directly probe electronic chirality and topological structures. The framework can be used to extend topological concepts to interacting bosonic systems and engineer phononic materials with tunable properties.

Abstract: We investigate the topological consequences of coupling chiral fermions to
local, dispersionless phonons. This interaction induces a splitting of the
phonon spectrum into three bands: a flat band and two bands with linear
dispersion, all of which are degenerate at a nodal point located at zero
wavevector. The flat band exhibits vanishing Berry curvature, while the
linearly dispersing bands carry nontrivial topological features. Their Berry
curvature fields assume a hedgehog-like structure in momentum space, analogous
to monopole configurations, and reflect the chirality of the underlying
fermionic system. Moreover, the effective phonon response reveals a phonon
parity anomaly, observable as a discontinuity in the phonon current. This
anomaly originates from the singularities of the fermion Green's function and
signals the transfer of topological information from fermions to phonons. Our
results demonstrate that phonon currents provide a direct probe of electronic
chirality and topological structures. The framework offers a foundation for
extending topological concepts to interacting bosonic systems and may guide the
engineering of phononic materials with tunable geometric and topological
properties.

</details>


### [275] [Exciting terahertz magnons with amplitude modulated light: spin pumping, squeezed states, symmetry breaking and pattern formation](https://arxiv.org/abs/2507.08147)
*Egor I. Kiselev,Jonas F. Karcher,Mark S. Rudner,Rembert Duine,Netanel H. Lindner*

Main category: cond-mat.mes-hall

TL;DR: MFPD uses optical frequencies to excite THz antiferromagnetic resonances and create new dynamical spin patterns.


<details>
  <summary>Details</summary>
Motivation: The motivation is to show how amplitude modulated, coherent high-frequency drives can be used to access difficult to reach collective resonances and off-resonantly induce parametric instabilities, specifically in antiferromagnetic resonances in the THz range.

Method: The paper uses amplitude modulated, coherent high-frequency drives to access collective resonances and induce parametric instabilities. Specifically, it studies spin pumping and the formation of entangled, two-mode squeezed magnon pairs in anisotropic antiferromagnets under MFPD.

Result: The paper shows that MFPD can parametrically excite THz antiferromagnetic resonances using optical frequencies and induces transitions to symmetry-breaking steady-states with dynamical spin patterns formed by resonant magnon pairs.

Conclusion: The paper demonstrates that Modulated Floquet Parametric Driving (MFPD) can be used to parametrically excite antiferromagnetic resonances in the THz range using optical frequencies. It also shows that MFPD can induce transitions to symmetry-breaking steady states with dynamical spin patterns formed by resonant magnon pairs.

Abstract: We show how amplitude modulated, coherent high-frequency drives can be used
to access otherwise difficult to reach collective resonances and off-resonantly
induce parametric instabilities. In particular, we demonstrate that difficult
to access antiferromagnetic resonances in the THz range can be parametrically
excited with signals at optical frequencies via a mechanism that we call
Modulated Floquet Parametric Driving (MFPD). We study spin pumping and the
formation of entangled, two-mode squeezed magnon pairs in anisotropic
antiferromagnets under MFPD. Furthermore, we show that MFPD induces transitions
to symmetry breaking steady-states in which dynamical spin patterns are formed
by resonant magnon pairs.

</details>


### [276] [Off-resonant light-induced topological phase transition and thermoelectric transport in semi-Dirac materials](https://arxiv.org/abs/2507.08168)
*Vassilios Vargiamidis,P. Vasilopoulos,Neophytos Neophytou*

Main category: cond-mat.mes-hall

TL;DR: 半狄拉克系统在光照下可实现拓扑相变，并通过调控光照可反转电、热流方向。


<details>
  <summary>Details</summary>
Motivation: 研究半狄拉克系统在非共振光照射下的拓扑相变及其输运性质。

Method: 利用Floquet理论推导了能带结构、陈数和相图，并分析了光照强度、质量和费米能级对系统性质的影响。

Result: 发现半狄拉克系统在拓扑相变过程中出现单半狄拉克锥（SSDC）半金属态，并揭示了Berry曲率分布、轨道磁矩、异常能斯特电导率和热霍尔电导率与相变的关系。

Conclusion: 该研究表明，通过调整光的强度和偏振，可以实现半狄拉克系统中的拓扑相变，并控制其磁化和电、热输运性质。

Abstract: We show that a semi-Dirac (SD) system with an inversion symmetry breaking
mass exhibits a topological phase transition when irradiated with off-resonant
light. Using Floquet theory, we derive the band structure, Chern numbers, phase
diagram, and we show that as the light intensity is swept at fixed mass, the SD
system undergoes normal-Chern-normal insulator transition. Along the phase
boundaries we observe single semi-Dirac-cone (SSDC) semimetal states in which
one SD cone is gapless and the other gapped. The nontrivial Berry curvature
distribution $\Omega(\mathbf{k}) \neq -\Omega ( - \mathbf{k} )$ generates an
orbital magnetization $M$ and anomalous Nernst ($\alpha_{xy}$) and thermal Hall
($\kappa_{xy}$) conductivities. We show that $M$ remains constant as the Fermi
level $E_F$ scans the insulating gap, but it changes linearly with it in the
Chern insulator (CI) phase, as expected. In the normal insulator phase, we find
that $\alpha_{xy}$ exhibits a dip-peak profile which is reversed in the CI
phase. We also find that switching the light's circular polarization from left
to right induces a sign change in $M$, $\alpha_{xy}$, and $\kappa_{xy}$,
regardless of the topological phase, thereby allowing us to reverse the
direction of flow of the transverse charge and heat currents. Further, we
evaluate the components of the charge ($\sigma_{aa}$), thermoelectric
($\alpha_{aa}$), and thermal ($\kappa_{aa}$) conductivity tensors ($a=x, y$)
and examine the effect of light on them. With a linear dispersion along the
$y$-direction, we find that $\alpha_{yy}$ and $\kappa_{yy}$ are significantly
larger than $\alpha_{xx}$ and $\kappa_{xx}$, respectively, due to the much
larger squared Dirac velocity $v_y^2$ compared to $v_x^2$.

</details>


### [277] [Thermoelectric optimization and quantum-to-classical crossover in gate-controlled two-dimensional semiconducting nanojunctions](https://arxiv.org/abs/2507.08231)
*Yu-Chang Chen,Yu-Chen Chang*

Main category: cond-mat.mes-hall

TL;DR: 本研究利用第一性原理模拟研究了2D纳米结的热电性能，发现电子传输存在量子到经典的转变。通过调控栅极和沟道长度，实现了超过2.3的最佳ZT，为设计高效热电器件提供了原则。


<details>
  <summary>Details</summary>
Motivation: 探究2D纳米结的热电性能及其影响因素，为设计高效率纳米尺度热电设备提供指导。

Method: 本研究结合了第一性原理模拟，包括密度泛函理论（DFT）、非平衡格林函数形式主义（nanoDCAL）以及非平衡分子动力学模拟，研究了具有栅极可调结构和不同沟道长度（3至12 nm）的2D纳米结的热电性能。

Result: 研究揭示了电子传输中从量子到经典的栅极、温度和长度依赖性转变，从短结中的量子隧穿过渡到长结中的热释电发射。研究观察到由于这种跨越和栅极控制，热电品质因数对塞贝克系数、电导率和热导率产生了非平凡的依赖性。研究确定，最大化ZT需要将化学势调整到带隙外，此时系统处于绝缘和导电状态之间的过渡区。虽然在绝缘状态下观察到极高的塞贝克系数，但由于电导率的抑制和主导的声子热传输，ZT并不高。在最短的3纳米结（500 K）处实现了大于2.3的最佳ZT，其中量子隧穿和热释电发射并存。

Conclusion: 本研究为2D半导体纳米结中的传输机制提供了基本见解，并为高效率纳米尺度热电设备提出了设计原则。

Abstract: We investigate the thermoelectric performance of 2D nanojunctions with gate
tunable architectures and varying channel lengths from 3 to 12 nm using a
combination of first principles simulations, including density functional
theory, DFT with nonequilibrium Greens function formalism (nanoDCAL), and
nonequilibrium molecular dynamics simulations. Our study reveals a gate,
temperature, and length dependent transition from quantum to classical in
electron transport, transitioning from quantum tunneling in short junctions to
thermionic emission in longer ones. We observe nontrivial dependencies of the
thermoelectric figure of merit on the Seebeck coefficient, electrical
conductivities, and thermal conductivities as a result of this crossover and
gate controlling. We identify that maximizing ZT requires tuning the chemical
potential just outside the band gap, where the system lies at the transition
between insulating and conducting regimes. While extremely large Seebeck
coefficients are observed in the insulating state, they do not yield high ZT
due to suppressed electrical conductivity and dominant phononic thermal
transport. The optimal ZT larger than 2.3 is achieved in the shortest 3 nm
junction at 500 K, where quantum tunneling and thermionic emission coexist.
These findings offer fundamental insights into transport mechanisms in 2D
semiconducting nanojunctions and present design principles for high efficiency
nanoscale thermoelectric devices.

</details>


### [278] [Validity of perturbation theory in calculations of magnetocrystalline anisotropy in Co-based layered systems](https://arxiv.org/abs/2507.08233)
*M. Cinal*

Main category: cond-mat.mes-hall

TL;DR: 对于Co薄膜和Co/Pt双层膜，第二顺序微扰理论（PT）在计算磁晶各向异性能（MCA）时，尤其是在自旋-轨道耦合（SOC）较强的情况下，会高估能量振荡幅度，并可能错误预测其周期性，这与更精确的力定理（FT）计算结果存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 检验第二顺序微扰理论（PT）在计算具有增强的自旋-轨道耦合（SOC）的Co薄膜和Co/Pt双层膜的磁晶各向异性能（MCA）方面的有效性。

Method: 通过比较第二顺序微扰理论（PT）和力定理（FT）在计算Co薄膜和Co/Pt双层膜磁晶各向异性能（MCA）上的结果，并结合简化的能带模型，分析了PT失效的原因。

Result: PT在描述MCA随Co厚度变化的振荡行为时存在显著偏差，特别是在强SOC系统中，其过高估计了振荡幅度，并且在Co/Pt双层膜中未能正确再现振荡周期性。这归因于PT在布里渊区中心附近失效，导致与FT计算结果产生较大差异。同时，PT在弱和中等SOC强度下，精度随SOC常数与温度比值的增加而降低，尤其在低温下会过高估计MCA能量振荡幅度。

Conclusion: 第二顺序微扰理论（PT）在计算Co薄膜和Co/Pt双层膜的磁晶各向异性能（MCA）时，特别是在SOC较强的情况下，存在显著的局限性。PT在描述MCA随Co厚度变化的振荡行为时，会过高估计振荡幅度，甚至无法正确再现Co/Pt双层膜的周期性。这主要是因为PT在布里渊区中心附近失效，该区域能级间距与SOC强度相比很小。与此相反，力定理（FT）的计算结果显示，MCA振荡幅度受限于连续QW能级对之间的有限能级间距。此外，在弱和中等SOC强度下，PT的精度随SOC常数与温度比值的增加而降低，导致其在低温下过高估计MCA能量振荡幅度。

Abstract: Validity of second-order perturbation theory (PT) is examined for
magnetocrystalline anisotropy (MCA) energy in Co films with enhanced spin-orbit
coupling (SOC) and Co/Pt bilayers. Comparison with accurate results obtained
with the force theorem (FT) reveals significant discrepancies in the dependence
of the MCA energy on the Co thickness. For systems with strong SOC, the PT
fails to correctly describe the oscillations of the MCA energy, largely
overestimating their amplitude and even failing (for Co/Pt bilayers) to
reproduce their specific periodicity. These failures specifically concern the
dominating oscillations with the 2-monolayer period which arise from pairs of
quantum well (QW) minority-spin d states in the Co layer, degenerate at the
centre of the Brillouin zone (BZ). A simplified model of such states
demonstrates that the large discrepancies between PT and FT predictions arise
from the breakdown of the PT in a region around the BZ centre where the energy
spacing between states within each pair is small compared to the SOC strength.
The oscillation amplitude of the MCA energy calculated with the FT is limited
by the finite energy spacing between consecutive QW pairs, whereas this
amplitude grows quadratically with the SOC strength in the PT calculations.
Furthermore, for weak and moderate SOC strengths, the accuracy of the PT
diminishes with increasing the ratio of the SOC constant to temperature. This
explains why the PT overestimates the amplitude of MCA energy oscillations at
low temperatures, even for the Co film with a relatively weak nominal SOC. For
the Co/Pt bilayer, the strong temperature dependence of the oscillation
amplitude in the PT approach leads to the MCA energies markedly different at
low and zero temperatures, of opposite sign and several times larger in
magnitude, compared to the FT results.

</details>


### [279] [Symmetry-based theory of Dirac fermions on two-dimensional hyperbolic crystals: Coupling to the spin connection](https://arxiv.org/abs/2507.08276)
*Ana Djordjević,Marija Dimitrijević Ćirić,Vladimir Juričić*

Main category: cond-mat.mes-hall

TL;DR: 本文提出了一个在双曲格上模拟狄拉克费米子的新框架，首次考虑了自旋-曲率耦合。研究发现，零能量下的态密度与曲率有关，这可能影响费米子的相互作用行为。该框架为超材料实验和相关数值研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 由于在超材料平台中实现了双曲格，离散费米子和玻色子模型在多个领域引起了广泛关注。然而，一个基本且重要的方面尚未解决：在曲面空间中传播的费米子通过自旋联络与底层几何耦合，这在之前的双曲晶体研究中并未包含。

Method: 提出了一种基于对称性的狄拉克费米子在二维双曲格上的框架，明确地通过离散自旋联络纳入了自旋-曲率耦合。从庞加莱圆盘的连续对称性出发，对不可约表示进行了分类并构建了对称性适应基，建立了与连续狄拉克理论的直接对应。推导了具有施拉夫利符号{p,q}的格的离散平移和旋转对称性的显式形式，并通过平行传输显式地构建了离散自旋联络，将其表示为跳跃相位。

Result: 在具有$2 \leq D \leq 4$的D维双曲空间中，对于任何有限的曲率，该连续理论都预测了零能量下的有限态密度，这表明狄拉克费米子在弱耦合下对驱动相互作用的不稳定性更敏感。

Conclusion: 该研究为在双曲几何中研究自旋-曲率耦合和探索多体相互作用效应提供了基础，为超材料平台的实验实现以及相关的数值研究铺平了道路。

Abstract: Discrete fermionic and bosonic models for hyperbolic lattices have attracted
significant attention across a range of fields since the experimental
realization of hyperbolic lattices in metamaterial platforms, sparking the
development of hyperbolic crystallography. However, a fundamental and
experimentally consequential aspect remains unaddressed: fermions propagating
in curved space inherently couple to the underlying geometry via the spin
connection, as required by general covariance - a feature not yet incorporated
in studies of hyperbolic crystals. Here, we introduce a symmetry-based
framework for Dirac fermions on two-dimensional hyperbolic lattices, explicitly
incorporating spin-curvature coupling via a discrete spin connection. Starting
from the continuous symmetries of the Poincar\'e disk, we classify the
irreducible representations and construct a symmetry-adapted basis,
establishing a direct correspondence to the continuum Dirac theory. We show
that this continuum theory predicts a finite density of states at zero energy
for any finite curvature in $D-$dimensional hyperbolic space with $2\leq D \leq
4$, suggesting enhanced susceptibility of Dirac fermions to interaction-driven
instabilities at weak coupling. We then derive explicit forms of discrete
translational and rotational symmetries for lattices characterized by
Schl\"afli symbols $\{p,q\}$, and explicitly construct the discrete spin
connection, represented as hopping phases, via parallel transport. Our results
pave the way for experimental realization of spin-curvature effects in
metamaterial platforms and systematic numerical studies of correlated Dirac
phases in hyperbolic geometries.

</details>


### [280] [Modulation of energy and angular momentum radiation of two-dimensional altermagnets](https://arxiv.org/abs/2507.08450)
*Yong-Mei Zhang,Zhi- Ping Niu*

Main category: cond-mat.mes-hall

TL;DR: altermagnets 在 Rashba 相互作用和外部磁场下会辐射能量和角动量，其辐射特性可以通过调整参数来定制。


<details>
  <summary>Details</summary>
Motivation: 研究 altermagnets 在 Rashba 相互作用和外部磁场下的能量和角动量辐射特性

Method: 使用有效的低能哈密顿量推导能带，并通过 Kubo 公式计算光学电导率

Result: 能量辐射对 Rashba 相互作用敏感，但有一个饱和效应；角动量辐射对 Rashba 相互作用的反应很小，但在临界点表现出剧烈振荡

Conclusion: altermagnets在 Rashba 相互作用和外部磁场下表现出能量和角动量辐射，这可以通过调整参数来定制

Abstract: This paper investigates the energy and angular momentum radiation of
altermagnets under Rashba spin-orbit coupling (RSOC) and external magnetic
fields. Using an effective low-energy Hamiltonian, we derive electronic energy
bands and calculate optical conductivity via the Kubo formula. Results show
that RSOC strength, altermagnet interactions strength, and Neel vector
direction notably affect the optical conductivity of altermagnet metals. Energy
radiation is highly sensitive to Rashba spin-orbit coupling, with a saturation
effect beyond a particular value, and its peak emission rate is lower than that
of graphene due to reduced conductivity. Different from usual semi-conductor,
semi-metal or Dirac materials, eg. graphene or silicene, altermagnets generate
angular momentum radiation with specific Rashba spin-orbit coupling and
altermagnet interaction strength. Angular momentum radiation is minimally
reactive to Rashba spin-orbit coupling at low altermagnet interactions strength
values but exhibits drastic oscillations between extreme values as altermagnet
interactions strength reaches a critical point, showing high sensitivity. These
findings suggest that adjusting these parameters can tailor altermagnet
applications in spintronics and quantum technologies, potentially leading to
innovative devices with customized radiation attributes.

</details>


### [281] [Quantum register based on double quantum dots in semiconductor nanowires](https://arxiv.org/abs/2507.08485)
*Vladimir Vyurkov,Leonid Fedichkin,Igor Semenikhin,Denis Drozhzhin,Konstantin Rudenko,Vladimir Lukichev*

Main category: cond-mat.mes-hall

TL;DR: 可扩展的量子计算机可通过操纵栅极电位实现量子算法，并具有抗噪声的潜力。


<details>
  <summary>Details</summary>
Motivation: 实现可扩展的量子计算机，并可能发展为抗噪声的量子寄存器。

Method: 基于超薄半导体纳米线中场定义双量子点（DQD）的电子空间状态，实现通用固态量子寄存器。

Result: 提出了量子信息编码和处理的方法，使得库仑相互作用恒定，但其作用强度取决于相互作用的双量子点的相互状态。

Conclusion: 该固态量子寄存器通过操纵栅极电位实现量子算法，并通过解码为双量子点电荷状态并传输电流进行读出。

Abstract: An implementation of a universal solid-state quantum register based on
electron space states in field-defined double quantum dots (DQD possesses one
electron in two adjacent tunnel bound dots) in an ultrathin semiconductor
nanowire is discussed. To some extent, the structure resembles that of a
field-effect transistor with multiple controlling electrodes (gates).
Scalability is audible and it opens up a possibility of large-scale quantum
computer fabricated by advanced silicon technology. Moreover, the structure
could be developed into an ensemble quantum register where instead of single
nanowire an array of them with common controlling electrodes and contacts is
fabricated. This ensemble register is much more resistant against environment
noise caused by phonons and stray charges due to averaging and compensation. It
is crucial that an individual qubit consists of two DQDs. The basic states of
that qubit correspond to symmetric state of one DQD and antisymmetric state of
another, and vice versa. Then the quantum information can be encoded and
processed without charge transfer between dots. The probability to find an
electron in a dot constantly equals 1/2 thus the Coulomb interaction between
DQDs is also constant. Although the Coulomb interaction is incessant, the
strength of its action depends on mutual states of interacting DQDs
(in-resonance or off-resonance). Therefore, a quantum algorithm could be
effectuated via manipulation solely with steady and pulse gate potentials that
reminds an operation of a digital integrated circuit. The final read-out of the
register is performed after decoding into charge states of DQDs and a
transmission of current through the wire.

</details>


### [282] [Hydrogen toggling between Yoshimori spin spirals and elliptical Dzyaloshinskii-Moriya skyrmions in Fe on Ir(110)](https://arxiv.org/abs/2507.08531)
*Timo Knispel,Vasily Tseplyaev,Gustav Bihlmayer,Stefan Blügel,Thomas Michely,Jeison Fischer*

Main category: cond-mat.mes-hall

TL;DR: 通过在Fe/Ir(110)双层结构中吸附氢，可以拓宽磁性薄膜支持斯格明子的范围，在适中磁场下实现斯格明子的形成，为调控自旋结构提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来扩展能够承载稳定或亚稳态斯格明子的磁性薄膜的范围，因为通常需要超出实际实验室极限的磁场才能将交换稳定的自旋螺旋转化为斯格明子。

Method: 利用扫描隧道显微镜和从头计算相结合的方法，研究了在Ir(110)上赝单形Fe层的双层结构，并分析了氢吸附对自旋结构的影响。

Result: 氢吸附能够将原子尺度和介观尺度的自旋结构从交换相互作用主导的螺旋态转变为具有更长周期的Dzyaloshinskii-Moriya相互作用主导的螺旋态，从而在适中的磁场下形成椭圆斯格明子。

Conclusion: 氢吸附可以作为一种非挥发性机制，在不同的磁性状态之间进行切换，为控制自旋结构提供了多功能的平台。

Abstract: Skyrmions are particle-like spin textures that arise from spin spiral states
in the presence of an external magnetic field. These spirals can originate from
either frustrated Heisenberg exchange interactions or the interplay between
exchange interactions and the relativistic Dzyaloshinskii-Moriya interaction,
leading to atomic- and mesoscale textures, respectively. However, the
conversion of exchange-stabilized spin spirals into skyrmions typically
requires magnetic fields that exceed practical laboratory limits. Here, we
demonstrate a strategy leveraging hydrogen adsorption to expand the range of
magnetic films capable of hosting stable or metastable skyrmions. In a
structurally open and anisotropic system of two pseudomorphic Fe layers on
Ir(110), spin-polarized scanning tunneling microscopy combined with ab initio
calculations reveals that a right-handed, exchange-stabilized N\'eel-type spin
spiral propagating along the [$\overline{1}10$] direction with a $1.3$~nm
period transitions upon hydrogen adsorption to a Dzyaloshinskii-Moriya type
spiral with a sevenfold longer period of $8.5$~nm. This transition enables
elliptical skyrmions to form at moderate magnetic fields. Hydrogenation thus
provides a non-volatile mechanism to toggle between distinct magnetic states,
offering a versatile platform for controlling spin textures.

</details>


### [283] [Chiral-split magnons in the S = 1 Shastry-Sutherland model](https://arxiv.org/abs/2507.08668)
*Absur Khan Siam,Se Kwon Kim*

Main category: cond-mat.mes-hall

TL;DR: 交替磁性材料的动量带在布里渊区中具有相反的手性分裂，并表现出鲁棒的自旋塞贝克效应和自旋内恩斯特效应。


<details>
  <summary>Details</summary>
Motivation: 研究交替磁性材料中非简并的相反手性能带及其潜在的磁动和输运性质。

Method: 该研究使用S=1的Shastry-Sutherland模型，通过计算动量带结构和使用Kubo公式计算自旋和热导率来研究交替磁性材料。

Result: 在动量带中，发现了一个显著的手性分裂动量带的特征，并且这种分裂在布里渊区的不同方向上是相反的。计算结果显示，由于布里渊区中交替的手性分裂，即使在没有外部磁场和自旋-轨道耦合的情况下，也表现出鲁棒的自旋塞贝克效应和自旋内恩斯特效应。

Conclusion: 该研究发现，在S=1的Shastry-Sutherland模型中，交替磁性材料的动量分裂动量带在布里渊区中具有相反的分裂方向，并且在没有外部磁场和自旋-轨道耦合的情况下，表现出鲁棒的自旋塞贝克效应和自旋内恩斯特效应。

Abstract: In ferromagnets, magnons have only one chirality; while in common
antiferromagnets, bands with opposite chiralities are degenerate across the
Brillouin zone. Recent studies have shown that it is possible to observe
non-degenerate bands of opposite chiralities in altermagnetic materials. Here
we take the S = 1 Shastry-Sutherland model, which shows the collinear N\'eel
(I) phase, and investigate the magnon band structure showing alternate
chirality-splitting and the resulting transport properties. In magnon bands, we
find a notable feature of the chirality-split magnon bands, and the split is
opposite along two different directions in the Brillouin zone. We also
calculate the spin and thermal conductivities using Kubo formalism. Our
calculations show robust spin Seebeck and spin Nernst effects due to the
alternating chirality split across the Brillouin zone, without any external
magnetic field and spin-orbit coupling.

</details>


### [284] [Probing electron spin dynamics in single telecom InAs(P)/InP quantum dots using the Hanle effect](https://arxiv.org/abs/2507.08785)
*Maja Wasiluk,Helena Janowska,Anna Musiał,Johann P. Reithmaier,Mohamed Benyoucef,Wojciech Rudno-Rudziński*

Main category: cond-mat.mes-hall

TL;DR: Hanle effect in telecom C-band InAs(P)/InP QDs shows electron spin dephasing time is comparable to GaAs QDs, suggesting potential for spin-photon interfaces.


<details>
  <summary>Details</summary>
Motivation: Electron spin dephasing, driven by hyperfine interactions with nuclear spins, limits qubit coherence in quantum dots. This work aims to experimentally determine the electron spin dephasing time in single InAs(P)/InP quantum dots emitting in the telecom C-band.

Method: Hanle effect demonstration using polarization-resolved photoluminescence spectroscopy to determine electron spin dephasing time.

Result: Demonstrated Hanle effect in single InAs(P)/InP QDs, identified excitonic complexes and trions, and extracted an electron spin dephasing time of $T_2^{\ast} = 1.59 \pm 0.49~\mathrm{ns}$ with a degree of circular polarization of -36% under quasi-resonant excitation.

Conclusion: InP-based telecom QDs have potential for spin-photon interfaces due to comparable electron spin dephasing times to GaAs-based QDs, attributed to larger QD volumes.

Abstract: Spins of carriers confined in quantum dots (QDs) are promising candidates for
qubits due to their relatively long spin relaxation times. However, the
electron spin dephasing, primarily driven by hyperfine interactions with
nuclear spins, can limit their coherence. Here, we report the first Hanle
effect demonstration in single InAs(P)/InP QDs emitting in the telecom C-band
leading to experimental determination of electron spin dephasing time. Using
polarization-resolved photoluminescence spectroscopy, we identified excitonic
complexes and confirmed the presence of a negatively charged trion, exhibiting
a degree of circular polarization (DOCP) of $-36\%$ under quasi--resonant
excitation. From the analysis of Hanle linewidth and employing a previously
reported value of the electron $g$-factor, we extracted an electron spin
dephasing time of $T_2^{\ast} = 1.59 \pm 0.49~\mathrm{ns}$. Despite the large
indium nuclear spin, the obtained $T_2^{\ast}$ is comparable to values reported
for GaAs-based QDs, which we attribute to the larger volume of the InAs(P)/InP
QDs. These findings confirm the potential of InP-based telecom QDs for use in
spin-photon interfaces.

</details>


### [285] [Distinct Lifetimes for $X$ and $Z$ Loop Measurements in a Majorana Tetron Device](https://arxiv.org/abs/2507.08795)
*Morteza Aghaee,Zulfi Alam,Rikke Andersen,Mariusz Andrzejczuk,Andrey Antipov,Mikhail Astafev,Lukas Avilovas,Ahmad Azizimanesh,Bela Bauer,Jonathan Becker,Umesh Kumar Bhaskar,Andrea G. Boa,Srini Boddapati,Nichlaus Bohac,Jouri D. S. Bommer,Jan Borovsky,Léo Bourdet,Samuel Boutin,Lucas Casparis,Srivatsa Chakravarthi,Hamidreza Chalabi,Benjamin J. Chapman,Nikolaos Chatzaras,Tzu-Chiao Chien,Jason Cho,Patrick Codd,William Cole,Paul W. Cooper,Fabiano Corsetti,Ajuan Cui,Tareq El Dandachi,Celine Dinesen,Andreas Ekefjärd,Saeed Fallahi,Luca Galletti,Geoffrey C. Gardner,Gonzalo Leon Gonzalez,Deshan Govender,Flavio Griggio,Ruben Grigoryan,Sebastian Grijalva,Sergei Gronin,Jan Gukelberger,Marzie Hamdast,A. Ben Hamida,Esben Bork Hansen,Caroline Tynell Hansen,Sebastian Heedt,Samantha Ho,Laurens Holgaard,Kevin van Hoogdalem,John Hornibrook,Lovro Ivancevic,Max Jantos,Thomas Jensen,Jaspreet Singh Jhoja,Jeffrey C Jones,Vidul Joshi,Konstantin V. Kalashnikov,Ray Kallaher,Rachpon Kalra,Farhad Karimi,Torsten Karzig,Seth Kimes,Evelyn King,Maren Elisabeth Kloster,Christina Knapp,Jonne V. Koski,Pasi Kostamo,Tom Laeven,Jeffrey Lai,Gijs de Lange,Thorvald W. Larsen,Kyunghoon Lee,Kongyi Li,Guangze Li,Shuang Liang,Tyler Lindemann,Matthew Looij,Marijn Lucas,Roman Lutchyn,Morten Hannibal Madsen,Nasiari Madulid,Michael J. Manfra,Laveena Manjunath,Signe Markussen,Esteban Martinez,Marco Mattila,J. R. Mattinson,R. P. G. McNeil,Alba Pérez Millan,Ryan V. Mishmash,Sarang Mittal,Christian Møllgaard,M. W. A. de Moor,Eduardo Puchol Morejon,Trevor Morgan,George Moussa,B. P. Nabar,Anirudh Narla,Chetan Nayak,Jens Hedegaard Nielsen,William Hvidtfelt Padkær Nielsen,Frédéric Nolet,Michael J. Nystrom,Eoin O'Farrell,Thomas A. Ohki,Camille Papon,Karl D Petersson,Luca Petit,Dima Pikulin,Mohana Rajpalke,Alejandro Alcaraz Ramirez,David Razmadze,Ivan Sadovskyy,Lauri Sainiemi,Juan Carlos Estrada Saldaña,Irene Sanlorenzo,Tatiane Pereira dos Santos,Simon Schaal,John Schack,Emma R. Schmidgall,Christina Sfetsou,Cristina Sfiligoj,Sarat Sinha,Patrick Sohr,Thomas L. Sørensen,Kasper Spiegelhauer,Tomaš Stankević,Lieuwe J. Stek,Patrick Strøm-Hansen,Henri J. Suominen,Judith Suter,Samuel M. L. Teicher,Raj Tholapi,Mason Thomas,D. W. Tom,Emily Toomey,Joshua Tracy,Michelle Turley,Matthew D. Turner,Shivendra Upadhyay,Ivan Urban,Dmitrii V. Viazmitinov,Anna Wulff Viazmitinova,Beatriz Viegas,Dominik J. Vogel,John Watson,Alex Webster,Joseph Weston,Timothy Williamson,Georg W. Winkler,David J. van Woerkom,Brian Paquelet Wuetz,Chung-Kai Yang,Shang-Jyun,Yu,Emrah Yucelen,Jesús Herranz Zamorano,Roland Zeisel,Guoji Zheng,A. M. Zimmerman*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a hardware realization and measurements of a tetron qubit device
in a superconductor-semiconductor heterostructure. The device architecture
contains two parallel superconducting nanowires, which support four Majorana
zero modes (MZMs) when tuned into the topological phase, and a trivial
superconducting backbone. Two distinct readout interferometers are formed by
connecting the superconducting structure to a series of quantum dots. We
perform single-shot interferometric measurements of the fermion parity for the
two loops, designed to implement Pauli-$X$ and $Z$ measurements of the tetron.
Performing repeated single-shot measurements yields two widely separated time
scales $\tau_X = 14.5\pm 0.3 \, \mathrm{\mu s}$ and $\tau_Z = 12.4\pm 0.4\,
\mathrm{ms}$ for parity switches observed in the $X$ and $Z$ measurement loops,
which we attribute to intra-wire parity switches and external quasiparticle
poisoning, respectively. We estimate assignment errors of
$\mathrm{err}^X_a=16\%$ and $\mathrm{err}^Z_a=0.5\%$ for $X$ and $Z$
measurement-based operations, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [286] [CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization](https://arxiv.org/abs/2507.08406)
*Weigang Feng,Yijia Zhang,Zekun Wang,Zhengyang Wang,Yi Wang,Peijun Ma,Ningyi Xu*

Main category: cs.AR

TL;DR: CCSS is a scalable multi-core RTL simulation platform that achieves fast compilation and high simulation throughput, addressing the bottleneck of CPU-based simulation speed in functional debug.


<details>
  <summary>Details</summary>
Motivation: The complexity of RTL-level simulation and verification has grown exponentially due to increasing transistor counts, leading to simulation campaigns extending to several months, with CPU-based simulation speed being a major bottleneck for functional debug.

Method: CCSS employs a balanced DAG partitioning method and efficient boolean computation cores for combinational logic, and adopts a low-latency network-on-chip (NoC) design to synchronize sequential states across cores efficiently.

Result: Experimental results show that CCSS delivers up to 12.9x speedup over state-of-the-art multi-core simulators.

Conclusion: CCSS delivers up to 12.9x speedup over state-of-the-art multi-core simulators by accelerating combinational logic computation and sequential logic synchronization through specialized architecture and compilation strategies.

Abstract: As transistor counts in a single chip exceed tens of billions, the complexity
of RTL-level simulation and verification has grown exponentially, often
extending simulation campaigns to several months. In industry practice, RTL
simulation is divided into two phases: functional debug and system validation.
While system validation demands high simulation speed and is typically
accelerated using FPGAs, functional debug relies on rapid compilation-rendering
multi-core CPUs the primary choice. However, the limited simulation speed of
CPUs has become a major bottleneck. To address this challenge, we propose CCSS,
a scalable multi-core RTL simulation platform that achieves both fast
compilation and high simulation throughput. CCSS accelerates combinational
logic computation and sequential logic synchronization through specialized
architecture and compilation strategies. It employs a balanced DAG partitioning
method and efficient boolean computation cores for combinational logic, and
adopts a low-latency network-on-chip (NoC) design to synchronize sequential
states across cores efficiently. Experimental results show that CCSS delivers
up to 12.9x speedup over state-of-the-art multi-core simulators.

</details>


### [287] [Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters](https://arxiv.org/abs/2507.08658)
*Robert B. Kent,Marios S. Pattichis*

Main category: cs.AR

TL;DR: LOMS 是一种新的硬件合并排序器，比现有的设备更快、更高效，并且可以处理不同大小的输入列表。


<details>
  <summary>Details</summary>
Motivation: 为了在硬件中实现更快、更高效的合并排序，并解决现有合并排序设备（如 Batcher 的 Bitonic 和 Odd-Even 合并排序器）在处理不同大小的输入列表时存在的局限性。

Method: 通过引入一种新的硬件合并排序设备，称为列表偏移合并排序器（LOMS），它利用列排序和行排序阶段的组合来处理输入数据。LOMS 设备利用单级双向合并排序器（S2MS）作为其第一阶段，并且与 Batcher 的合并排序器相比，在处理不同大小的输入列表方面具有更大的灵活性。

Result: LOMS 双向排序器在合并两个包含 32 个值的列表时，速度比 Batcher 设备快 2.63 倍，耗时 2.24 纳秒。LOMS 三向排序器在合并三个包含 7 个值的列表时，速度比现有的三向合并设备快 1.36 倍，耗时 3.4 纳秒。此外，LOMS 设备使用的资源比 S2MS 设备少，使其能够在给定的 FPGA 中实现更大的设备。

Conclusion: LOMS 设备在速度和资源利用率方面都优于 Batcher 的设备，并且可以处理不同大小的输入列表，这使得它们成为一种有前途的硬件合并排序解决方案。

Abstract: A new set of hardware merge sort devices are introduced here, which merge
multiple sorted input lists into a single sorted output list in a fast and
efficient manner. In each merge sorter, the values from the sorted input lists
are arranged in an input 2-D setup array, but with the order of each sorted
input list offset from the order of each of the other sorted input lists. In
these new devices, called List Offset Merge Sorters (LOMS), a minimal set of
column sort stages alternating with row sort stages process the input setup
array into a final output array, now in the defined sorted order. LOMS 2-way
sorters, which merge 2 sorted input lists, require only 2 merge stages and are
significantly faster than Kenneth Batcher's previous state-of-the-art 2-way
merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way
sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)
in their first stage. Both LOMS and S2MS devices can merge any mixture of input
list sizes, while Batcher's merge sorters are difficult to design unless the 2
input lists are equal, and a power-of-2. By themselves, S2MS devices are the
fastest 2-way merge sorters when implemented in this study's target FPGA
devices, but they tend to use a large number of LUT resources. LOMS 2-way
devices use fewer resources than comparable S2MS devices, enabling some large
LOMS devices to be implemented in a given FPGA when comparable S2MS devices
cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with
32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup
of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging
3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a
speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

</details>
